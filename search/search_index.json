{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"symdesign","text":"<p>symdesign is python package for hypothesis-driven symmetric protein design. The package provides end to end support for the protein design process by seamlessly connecting multiple bioinformatics tools to create a rich data output describing the conformational space surrounding a design project. The code was born out of protein-protein interface design and offers extensive features for protein docking, sequence design, and finally, analysis of the resulting designs, especially for systems of symmetric proteins. Beyond protein docking, symdesign serves as a platform to perform interface and design analysis on existing structures as well as sampling new sequences or scoring existing ones with ProteinMPNN, and folding sequences into structures with AlphaFold. The integration of each of these tools in a single environment allow a user to move from hypothesis to sampling to analysis and finally sequence formatting for gene synthesis. Current limitations include the creation of new backbones, such as with generative backbone models. We are always looking for new ideas to extend the existing project features so don't be shy in generating a pull request or starting a discussion.</p>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#installation","title":"Installation","text":"<ol> <li>First, you will need to set up your python environment. After cloning this repository, run <code>conda env create --file path/to/symdesign/conda_env.yml</code>. Might I also suggest using mamba inplace of conda for an even faster experience. This command will create a suitable environment to access all the contained tools. Apple users of M1 and later chips will need different dependencies (WIP).</li> <li>Execute <code>sudo apt-get install build-essential autoconf libc++-dev libc++abi-dev</code> to install system dependencies for FreeSASA. There may be different dependencies if you are utilizing an operating system other than ubuntu.</li> <li>Follow the instructions for final set up and initialization of dependencies using the script <code>python path/to/symdesign/setup.py</code></li> </ol>"},{"location":"#symmetry","title":"Symmetry","text":"<p>Symmetry is used to simplify the investigation of different protein systems and scales. Whether there is local symmetry in an oligomer, or global symmetry, between multiple oligomers, each type of possible interface contact can be parsed apart leading to simpler analysis of a single gene from a highly repetitive symmetric material. Previously, symmetric modelling has been a roadblock to sampling conformational space because of numerous complexities in enumerating possibilities when multiple components are involved in a symmetric assembly. We have adapted the core symmetric principles from the Nanohedra project, placed a programmatic emphasis on single protein entities that combine to make up an asymmetric unit, and finally a thorough integration with sequence design methodologies. See <code>python path/to/symdesign symmetry --help</code> or <code>python path/to/symdesign symmetry --guide</code> to see the options for specifying symmetry as well as a guide to its usage. A great place to start exploring symmetry is <code>python path/to/symdesign symmetry --query result</code>.</p> <p>To use symmetry, specify <code>--symmetry</code>/<code>--sym-entry</code> during project initialization. Also, the keyword 'cryst' as in <code>--symmetry cryst</code> indicates the program should use the CRYST1 record provided with along with a file such as in the case of using 2D and 3D lattice symmetries.</p>"},{"location":"#design-methodologies","title":"Design methodologies","text":"<p>The default design methodology is ProteinMPNN. There is also the possibility to integrate Rosetta based design if a license for Rosetta is acquired. To initialize usage with Rosetta, run <code>python symdesign/setup.py --rosetta</code></p>"},{"location":"#ethos-for-evolution-guided-design","title":"Ethos for evolution guided design","text":"<p>A core feature of all design and analysis is the integration of profile information, where a profile is defined as the per-residue amino acid distribution describing the sequence-structure relationship space of a target design. A profile can be created from evolutionary information present from multiple sequence alignment based queries, termed an 'evolutionary_profile', which provide pertinent single and paired residue constraints to establish the protein tertiary topology. A profile can also be created from smaller protein fragments, a 'fragment_profile' that collectively form tertiary motifs. These are primarily used to parameterize protein interfaces, however, they are generated from sampling pairs of observed secondary structures that are fundamental to protein folding, and as such can describe nearly any tertiary structure. Whereas evolutionary_profile information provides short and long range selective pressures to the underlying stability of the protein fold, the fragment_profile in its typical usage allows augmentation of interfaces with sequence-structure relationship patterns statistically favored in natural protein systems. By mixing these two background sets of data, in design or data analysis we can leverage the data available from both genomic and structural research to shape and understand design outcomes.  </p> <p>Additionally, the profiles calculated by ProteinMPNN can be accessed. In basic <code>design</code>, the ProteinMPNN 'inference_profile' is used to score sequences it creates. However, the 'structure_profile' can also be assessed by asking ProteinMPNN for a profile conditioned on the backbone coordinates.  </p>"},{"location":"#design-protocols","title":"Design protocols","text":"<p>Design proceeds with a few options, first and foremost, which residues. As an example of tailored selection, any particular choice of entity/residue can be included in design such as the usage of <code>--design</code> or exclusion of residues such as <code>--mask</code> residues. See the 'python path/to/symdesign design-residues --guide' for more information on setting these up. Additionally, preconfigured selections based on interfaces can be used. At a minimum residues in contact across an interface are designed with <code>--interface</code>. Additionally, neighbors of interface residues can be added to the design using <code>--neighbors</code>. Each of these naive distance based metrics can be configured with the inputs <code>--interface-distance</code>/<code>--neighbor-distance</code>. Additionally, you can specify whether you'd like evolutionary information <code>--evolutionary-constraint</code> or fragment information <code>--fragment-constraint</code> applied during design sampling.  </p> <p>From this simple framework, multiple protocols for amino acid sampling can be carried out to gather a distribution of the structural possibilities given different constraints. For example, <code>--interface</code> residue contacts can be designed and subject to <code>--fragment-constraint</code> which layers the interface with commonly observed tertiary structure motifs from the PDB in the fragment_profile. This could indicate a measure of structural complementarity between the observed interface and natural tertiary motifs. Additionally, every residue in the protein <code>design</code> (module default) could use <code>--evolutionary-constraint</code> to sample only amino acids that are available given the protein's evolutionary_profile. Using both <code>--fragment-constraint</code> and <code>--evolutionary-constraint</code>, a combination protocol can be specified which constrains design to only amino acids capable of satisfying both constraints. Insight can be gained by comparing different protocols and understanding how well various information is capable of being represented in the design space. Such analysis has allowed us to understand how well suited an interface is for a low energy design given its natural context Meador et al. 2023. If the output from these design protocols can reach a consensus, it is increasingly likely that generated designs more easily mimic natural proteins which can result in improved biochemical success.  </p>"},{"location":"#your-first-run","title":"Your first run","text":"<p>In its simplest usage, a protein structure with a single protein entity can be analyzed, modeled for missing density, or redesigned sequences. In the most complicated use case, two or more entities in an infinite symmetric assembly can be docked in permissible symmetries, have the de novo interface and backbones subjected to sequence design with various constraints, and finally, fold those sequences into different homomeric or heteromeric systems. After performing any docking, design, or folding calculations, a thorough set of measurements, most focused on interfaces, however easily parameterized with other calculations can generate a rich overview of the design wise, or per residue positions characteristics present in the chosen poses/designs. The goal is to enable hypotheses to lead to design discovery through objective analysis of the design space.  </p> <p>To run symdesign, prepare a design target either with seamless integration of Nanohedra docking <code>python path/to/symdesign nanohedra</code> HelixDisplay <code>python path/to/symdesign align-helices</code> or use of another available backbone generation program. Next, sample sequences for your coordinate space through <code>python path/to/symdesign design</code>. When any design processing steps occur, analysis is automatically performed. To retrieve specific structures or sequences and their corresponding analytic data, it is easy to implement data quality filters using the select tools such as <code>python path/to/symdesign select-*</code> where * can be one of <code>poses</code>, <code>designs</code>, or <code>sequences</code>, i.e. <code>python path/to/symdesign select-designs --filter</code>. Additionally, the corresponding project directories can be accessed to view any design files (SymDesignOutput/Projects/my-new-octahedron/). All data from a select-* protocol is formatted in .csv files which can easily be viewed through Excel/Google Sheets, however, we also have integrated tools to analyze and plot data using tools as IPython Notebooks to perform plotting and analysis with pandas, seaborn, and matplotlib. Finally, if designs are deemed desirable for design in the lab, the module <code>python path/to/symdesign select-sequences</code> simplifies formatting of nucleotide sequences for subsequent ordering.  </p> <p>All of these modules come with a number of parameters that can modify the outcome. You can access the available options through <code>python path/to/symdesign --help</code> or <code>python path/to/symdesign MODULE --help</code> for module specific options.  </p> <p>All flags can be provided to any module on the command line or by using the file specification notation <code>@</code> such as <code>@my_favorite_flags.file</code> in the specified command. Alternatively, these values will take their defaults if none are provided.  </p>"},{"location":"#some-examples-of-viable-commands","title":"Some examples of viable commands:","text":"<pre><code>python path/to/symdesign --directory path/to/other/OUTPUT design --design-residues A:243-287\npython path/to/symdesign --directory path/to/other/OUTPUT design --design-residues A:243-287 --symmetry C3\n</code></pre>"},{"location":"#metrics-can-be-measured-between-interfaces-in-biologically-relevant-oligomeric-units-including-monomers","title":"metrics can be measured between interfaces in biologically relevant oligomeric units (including monomers)","text":"<pre><code>python path/to/symdesign --project SymDesignOutput/Projects/D4_interface_analysis refine --measure-pose\npython path/to/symdesign --project SymDesignOutput/Projects/D4_interface_analysis refine --measure-pose --symmetry D4:{D4}{C1}\n</code></pre>"},{"location":"#to-turn-an-asu-into-a-full-assembly-simply-run","title":"To turn an ASU into a full assembly, simply run","text":"<pre><code>python path/to/symdesign --directory designs/design_asus expand-asu --symmetry I:{C2}:{C5}\n</code></pre> <p>Where designs/design_asus is a directory containing files with an icosahedral asymmetric unit containing two chains, the C2 symmetry group first in every file, and the C5 symmetry group second.</p>"},{"location":"#scale","title":"Scale","text":"<p>symdesign was designed to access different scales for any design workflow. Whether this is on a personal laptop, a Google Colab, or a computational cluster, the goal is for you to focus on the design task and seamlessly utilize the computational power given the available  resources. Of course, there are always limits to how much can be utilized at one time, especially given symmetry and GPU space. That being said, as many minimization's as possible have been made to perform design calculations even on infinite materials. </p>"},{"location":"#relational-database","title":"Relational database","text":"<p>The key requirement to scale as your workflow does is the database backend. Out of the box, symdesign ships with sqlite which maintains a relational database between all design inputs and outputs. If large scale parallel processing is desired, a custom database should probably be created and access specified with <code>--database-url</code>. Such access only needs to be configured once for a root symdesign directory (ex. SymDesignOutput). This cautionary note is given as sqlite can run into latency at extremely high loads. Importantly, concurrent database access is unavoidably impossible if your on an NFS file system. If you are operating on one, I suggest you find an alternative if concurrency is desired. symdesign sets up its own database resources, but you will need to enable correct permissions to create tables, read, write, update and likely other database specific actions. If you are curious in setting up a custom database, please start a pull request or a discussion about the type of access you are creating, and we can ensure that the sqlalchemy does its magic.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>symdesign would not be possible without the likes of many fantastic projects. The most integral projects include: * numpy * scikit-learn * pandas * sqlalchemy * Nanohedra * ProteinMPNN * AlphaFold * FreeSASA * hhsuite * Stride * DNAChisel * conda</p>"},{"location":"user_guide/","title":"User Guide","text":"<p>This part of the project documentation focuses on a learning-oriented approach. You'll learn how to get started with the code in this project.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Help newcomers with getting started</li> <li>Teach readers about your library by making them     write code</li> <li>Inspire confidence through examples that work for     everyone, repeatably</li> <li>Give readers an immediate sense of achievement</li> <li>Show concrete examples, no abstractions</li> <li>Provide the minimum necessary explanation</li> <li>Avoid any distractions</li> </ul>"},{"location":"reference/","title":"symdesign","text":""},{"location":"reference/#symdesign--model-design-and-analyze-proteins-without-explicit-considerations-for-applying-local-and-global-symmetry","title":"Model, design, and analyze proteins without explicit considerations for applying local and global symmetry.","text":"<p>Modules exported by this package:</p> <ul> <li>flags: Setup program inputs to perform a job</li> <li>metrics: Perform calculations on a protein pose</li> <li>protocols: Implement a defined set of instructions for a protein pose</li> <li>resources: Common methods and variable to connect job instructions to protocol implementation during job runtime</li> <li>sequence: Handle biological sequences as python objects</li> <li>structure: Handle biological structures as python objects</li> <li>tools: Helper scripts, not quite yet a protocols worthy module, perhaps only accomplishes a sinlge task like file manipulation</li> <li>utils: Miscellaneous functions, methods, and tools for all modules</li> <li>visualization: Miscellaneous PyMol visualization helper functions and plotting config.</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>symdesign</li> <li>flags</li> <li>metrics<ul> <li>pose</li> </ul> </li> <li>protocols<ul> <li>align</li> <li>cluster</li> <li>config</li> <li>fragdock</li> <li>pose</li> <li>select</li> <li>utils</li> </ul> </li> <li>resources<ul> <li>config</li> <li>database</li> <li>distribute</li> <li>job</li> <li>ml</li> <li>monomer</li> <li>multimer</li> <li>query<ul> <li>pdb</li> <li>uniprot</li> </ul> </li> <li>sql</li> <li>structure_db</li> <li>wrapapi</li> </ul> </li> <li>sequence<ul> <li>constants</li> <li>expression</li> </ul> </li> <li>structure<ul> <li>base</li> <li>coordinates</li> <li>fragment<ul> <li>db</li> <li>info</li> <li>metrics</li> <li>visuals</li> </ul> </li> <li>model</li> <li>sequence</li> <li>utils</li> </ul> </li> <li>tools<ul> <li>BenchmarkCollapseHydrophobicityScale</li> <li>ConcatenatePDBFiles</li> <li>ProfileScripts</li> <li>SlurmControl</li> <li>distribute</li> <li>fetch_commands_from_slurm_output</li> <li>format_commands</li> <li>format_pose_ids</li> <li>generate_reference_docs</li> <li>get_array_numbers_from_identifiers</li> <li>list_files_in_directory</li> <li>list_overlap</li> <li>models_to_multimodel</li> <li>profile_gpu_nodes</li> <li>retrieve_oligomers</li> <li>retrieve_pdbs_by_advanced_query</li> <li>write_indices_for_C1_docking</li> </ul> </li> <li>utils<ul> <li>SymEntry</li> <li>cluster</li> <li>guide</li> <li>nanohedra_parsing</li> <li>path</li> <li>query</li> <li>rosetta</li> <li>symmetry</li> <li>types</li> </ul> </li> <li>visualization<ul> <li>SymmetryMates</li> <li>VisualizeUtils</li> <li>shapes</li> </ul> </li> </ul>"},{"location":"reference/flags/","title":"flags","text":""},{"location":"reference/flags/#flags.boolean_positional_prevent_msg","title":"boolean_positional_prevent_msg  <code>module-attribute</code>","text":"<pre><code>boolean_positional_prevent_msg = format\n</code></pre> <p>Use this message in all help keyword arguments using argparse.BooleanOptionalAction with default=True to specify the --no- prefix when the argument should be False</p>"},{"location":"reference/flags/#flags.flags_kwargs","title":"flags_kwargs  <code>module-attribute</code>","text":"<pre><code>flags_kwargs = get(parser_name, {})\n</code></pre> <p>flags_kwargs has args (flag names) as key and keyword args (flag params) as values</p>"},{"location":"reference/flags/#flags.cluster_defaults","title":"cluster_defaults  <code>module-attribute</code>","text":"<pre><code>cluster_defaults = dict(namespace='cluster')\n</code></pre> <p>Contains all the arguments and their default parameters used in clustering Poses</p>"},{"location":"reference/flags/#flags.design_defaults","title":"design_defaults  <code>module-attribute</code>","text":"<pre><code>design_defaults = dict(namespace='design')\n</code></pre> <p>Contains all the arguments and their default parameters used in design</p>"},{"location":"reference/flags/#flags.dock_defaults","title":"dock_defaults  <code>module-attribute</code>","text":"<pre><code>dock_defaults = dict(namespace='dock')\n</code></pre> <p>Contains all the arguments and their default parameters used in docking</p>"},{"location":"reference/flags/#flags.init_defaults","title":"init_defaults  <code>module-attribute</code>","text":"<pre><code>init_defaults = dict(namespace='init')\n</code></pre> <p>Contains all the arguments and their default parameters used in structure/sequence initialization</p>"},{"location":"reference/flags/#flags.predict_defaults","title":"predict_defaults  <code>module-attribute</code>","text":"<pre><code>predict_defaults = dict(namespace='predict')\n</code></pre> <p>Contains all the arguments and their default parameters used in structure prediction</p>"},{"location":"reference/flags/#flags.available_modules","title":"available_modules  <code>module-attribute</code>","text":"<pre><code>available_modules = sorted(difference(available_tools))\n</code></pre> <p>Contains all the registered modules</p>"},{"location":"reference/flags/#flags.FlagStr","title":"FlagStr","text":"<p>             Bases: <code>str</code></p> <p>Flag instances are strings which represent possible input parameters to the program with additional formatting for program runtime to properly reflect underscore and dashed versions</p>"},{"location":"reference/flags/#flags.FlagStr._","title":"_  <code>property</code>","text":"<pre><code>_: str\n</code></pre> <p>Format a string from the command line format to a program acceptable string</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The flag formatted by replacing any dash '-' with an underscore '_'</p> </li> </ul>"},{"location":"reference/flags/#flags.FlagStr.long","title":"long  <code>property</code>","text":"<pre><code>long: str\n</code></pre> <p>Format a flag for the command line</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The flag formatted by replacing any underscore '_' with a dash '-'</p> </li> </ul>"},{"location":"reference/flags/#flags.format_for_cmdline","title":"format_for_cmdline","text":"<pre><code>format_for_cmdline(string) -&gt; str\n</code></pre> <p>Format a flag for the command line</p> <p>Parameters:</p> <ul> <li> <code>string</code>         \u2013          <p>The string to format as a commandline flag</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The string formatted by replacing any underscores '_' with a dash '-'</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def format_for_cmdline(string) -&gt; str:\n    \"\"\"Format a flag for the command line\n\n    Args:\n        string: The string to format as a commandline flag\n\n    Returns:\n        The string formatted by replacing any underscores '_' with a dash '-'\n    \"\"\"\n    return string.replace('_', argparse_flag_delimiter)\n</code></pre>"},{"location":"reference/flags/#flags.format_from_cmdline","title":"format_from_cmdline","text":"<pre><code>format_from_cmdline(string) -&gt; str\n</code></pre> <p>Format a string from the command line format to a program acceptable string</p> <p>Parameters:</p> <ul> <li> <code>string</code>         \u2013          <p>The string to format as a python string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The flag formatted by replacing any dash '-' with an underscore '_'</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def format_from_cmdline(string) -&gt; str:\n    \"\"\"Format a string from the command line format to a program acceptable string\n\n    Args:\n        string: The string to format as a python string\n\n    Returns:\n        The flag formatted by replacing any dash '-' with an underscore '_'\n    \"\"\"\n    return string.replace(argparse_flag_delimiter, '_')\n</code></pre>"},{"location":"reference/flags/#flags.format_args","title":"format_args","text":"<pre><code>format_args(flag_args: Sequence[str]) -&gt; str\n</code></pre> <p>Create a string to format different flags for their various acceptance options on the command line</p> <p>Parameters:</p> <ul> <li> <code>flag_args</code>             (<code>Sequence[str]</code>)         \u2013          <p>Typically a tuple of allowed flag \"keywords\" specified using \"-\" or \"--\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The flag arguments formatted with a \"/\" between each allowed version</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def format_args(flag_args: Sequence[str]) -&gt; str:\n    \"\"\"Create a string to format different flags for their various acceptance options on the command line\n\n    Args:\n        flag_args: Typically a tuple of allowed flag \"keywords\" specified using \"-\" or \"--\"\n\n    Returns:\n        The flag arguments formatted with a \"/\" between each allowed version\n    \"\"\"\n    return '/'.join(flag_args)\n</code></pre>"},{"location":"reference/flags/#flags.parse_weights","title":"parse_weights","text":"<pre><code>parse_weights(weights: list[str] = None, file: AnyStr = None) -&gt; dict[str, list[tuple[Callable, Callable, dict, Any]]]\n</code></pre> <p>Given a command line specified set of metrics and values, parse into weights to select DataFrames accordingly</p> <p>Parameters:</p> <ul> <li> <code>weights</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>The command line collected weight arguments as specified in the weights --help</p> </li> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The path to a file specifying weights in JSON as specified in the weights --help</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[tuple[Callable, Callable, dict, Any]]]</code>         \u2013          <p>The parsed metric mapping linking each metric to a specified operation</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def parse_weights(weights: list[str] = None, file: AnyStr = None) \\\n        -&gt; dict[str, list[tuple[Callable, Callable, dict, Any]]]:\n    \"\"\"Given a command line specified set of metrics and values, parse into weights to select DataFrames accordingly\n\n    Args:\n        weights: The command line collected weight arguments as specified in the weights --help\n        file: The path to a file specifying weights in JSON as specified in the weights --help\n\n    Returns:\n        The parsed metric mapping linking each metric to a specified operation\n    \"\"\"\n    parsed_weights = parse_filters(weights, file=file)\n    # Ensure proper formatting of weight specific parameters\n    for idx, (metric_name, weight) in enumerate(parsed_weights.items()):\n        if len(weight) != 1:\n            raise InputError(\n                f\"Can't assign more than one weight for every provided metric. '{weights[idx]}' is invalid\")\n        operation, pre_operation, pre_kwargs, value = weight[0]\n        if operation != operator.eq:\n            raise InputError(\n                f\"Can't assign a selection weight with the operator '{operator_strings[operation]}'. \"\n                f\"'{weights[idx]}' is invalid\")\n        if isinstance(value, str):\n            raise InputError(\n                f\"Can't assign a numerical weight to the provided weight '{weights[idx]}'\")\n\n    return parsed_weights\n</code></pre>"},{"location":"reference/flags/#flags.parse_filters","title":"parse_filters","text":"<pre><code>parse_filters(filters: list[str] = None, file: AnyStr = None) -&gt; dict[str, list[tuple[Callable, Callable, dict, Any]]]\n</code></pre> <p>Given a command line specified set of metrics and values, parse into filters to select DataFrames accordingly</p> <p>Parameters:</p> <ul> <li> <code>filters</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>The command line collected filter arguments as specified in the filters --help</p> </li> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The path to a file specifying filters in JSON as specified in the filters --help</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[tuple[Callable, Callable, dict, Any]]]</code>         \u2013          <p>The parsed metric mapping linking each metric to a specified operation</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def parse_filters(filters: list[str] = None, file: AnyStr = None) \\\n        -&gt; dict[str, list[tuple[Callable, Callable, dict, Any]]]:\n    \"\"\"Given a command line specified set of metrics and values, parse into filters to select DataFrames accordingly\n\n    Args:\n        filters: The command line collected filter arguments as specified in the filters --help\n        file: The path to a file specifying filters in JSON as specified in the filters --help\n\n    Returns:\n        The parsed metric mapping linking each metric to a specified operation\n    \"\"\"\n    def null(df, **_kwargs):\n        \"\"\"Do nothing and return a passed DataFrame\"\"\"\n        return df\n\n    if file is not None:\n        # parsed_filters = read_json(file)\n        filters = to_iterable(file)\n\n    # Parse input filters as individual filtering directives\n    parsed_filters = {}\n    for filter_str in filters:\n        # Make an additional variable to substitute operations as they are found\n        _filter_str = filter_str\n        # Find the indices of each operation and save to use as slices\n        indices = []\n        operations_syntax = []\n        for syntax in viable_operations:\n            # if syntax in filter_str:\n            while syntax in _filter_str:\n                f_index = _filter_str.find(syntax)\n                # It is important that shorter operations do not pull out longer ones i.e. '&gt;' and '&gt;='\n                # # Check if equals is the next character and set operation_length\n                # if filter_str[f_index + 1] == '=':\n                #     operation_length = 2\n                # else:\n                #     operation_length = 1\n                operation_length = len(syntax)\n                r_index = f_index + operation_length\n                # r_index = filter_str.rfind(syntax)\n                indices.append([f_index, r_index])\n                operations_syntax.append(syntax)\n                # Substitute out the found operation for '`'\n                _filter_str = _filter_str[:f_index] + '`' * operation_length + _filter_str[r_index:]\n\n        if not indices:\n            raise InputError(\n                f\"Couldn't create a filter from '{filter_str}'. Ensure that your input contains an operation specifying\"\n                \" the relationship between the filter and the expected value\")\n\n        # Sort the indices according to the first index number\n        full_indices = []\n        for idx_pair in sorted(indices, key=lambda pair: pair[0]):\n            full_indices.extend(idx_pair)\n\n        # Separate the filter components along the operations\n        unique_components = [filter_str[:full_indices[0]]] \\\n            + [filter_str[f_index:r_index] for f_index, r_index in zip(full_indices[:-1], full_indices[1:])] \\\n            + [filter_str[full_indices[-1]:]]\n\n        # Set up function to parse values properly\n        def extract_format_value(_value: str) -&gt; Any:\n            \"\"\"Values can be of type int, float, or string. Further, strings could be a list of int/float\n            comma separated\n\n            Args:\n                _value: The string to format\n\n            Returns:\n                The value formatted from a string input to the correct python type for filter evaluation\n            \"\"\"\n            try:\n                formatted_value = int(_value)\n            except ValueError:  # Not simply an integer\n                try:\n                    formatted_value = float(_value)\n                except ValueError:  # Not numeric\n                    # This is either a list of numerics or likely a string value\n                    _values = [_component.strip() for _component in _value.split(',')]\n                    if len(_values) &gt; 1:\n                        # We have a list of values\n                        formatted_value = [extract_format_value(_value_) for _value_ in _values]\n                    else:\n                        # This is some type of string value\n                        formatted_value = _values[0]\n                        # if _value[-1] == '%':\n                        #     # This should be treated as a percentage\n                        #     formatted_value = extract_format_value(_value)\n\n            return formatted_value\n\n        # Find the values that are metrics and those that are values, then ensure they are parsed correct\n        metric = metric_specs = metric_idx = None\n        invert_ops = True\n        operations_syntax_iter = iter(operations_syntax)\n        parsed_values = []\n        parsed_operations = []\n        parsed_percents = []\n        for idx, component in enumerate(unique_components):\n            # Value, operation, value, operation, value\n            if idx % 2 == 0:  # Zero or even index which must contain metric/values\n                # Substitute any numerical characters to test if the provided component is a metric\n                substituted_component = component.translate(remove_digit_table)\n                logger.debug(f'substituted_component |{substituted_component}|')\n                _metric_specs = config.metrics.get(substituted_component.strip())\n                if _metric_specs:  # We found in the list of available program metrics\n                    logger.debug(f'metric specifications {\",\".join(f\"{k}={v}\" for k, v in _metric_specs.items())}')\n                    if metric_idx is None:\n                        metric_specs = _metric_specs\n                        metric = component.strip()\n                        metric_idx = idx\n                        # if idx != 0:\n                        # We must negate operations until the metric is found, i.e. metric &gt; 1 is expected, but\n                        # 1 &lt; metric &lt; 10 is allowed. This becomes metric &gt; 1 and metric &lt; 10\n                        invert_ops = False\n                        continue\n                    else:  # We found two metrics...\n                        raise ValueError(\n                            f\"Can't accept more than one metric name per filter\")\n                else:  # Either a value or bad metric name\n                    if component[-1] == '%':\n                        # This should be treated as a percentage. Remove and parse\n                        component = component[:-1]\n                        parsed_percents.append(True)\n                    else:\n                        parsed_percents.append(False)\n                    component = extract_format_value(component)\n\n                    parsed_values.append(component)\n                # # This may be required if section is reworked to allow more than one value per filter\n                # parsed_values.append(component)\n            else:  # This is an operation\n                syntax = next(operations_syntax_iter)\n                if invert_ops:\n                    operation = inverse_operations.get(syntax, viable_operations[syntax])\n                else:\n                    operation = viable_operations[syntax]\n\n                logger.debug(f'operation is {operation}')\n                parsed_operations.append(operation)\n\n        if metric_idx is None:\n            possible_metrics = []\n            for value in parsed_values:\n                if isinstance(value, str):\n                    possible_metrics.append(value)\n            # Todo find the closest and report to the user!\n            raise InputError(\n                f\"Couldn't coerce{' any of' if len(possible_metrics) &gt; 1 else ''} '{', '.join(possible_metrics)}' to a\"\n                f\" viable metric. Ensure you used the correct spelling\")\n\n        # Format the metric for use by select-* protocols\n        filter_specification = []\n        for value, percent, operation in zip(parsed_values, parsed_percents, parsed_operations):\n            if percent:\n                # pre_operation = pd.DataFrame.sort_values\n                pre_operation = pd.DataFrame.rank\n                pre_kwargs = dict(\n                    # Whether to sort dataframe with ascending, bigger values (higher rank) on top. Default True\n                    # ascending=metric_specs['direction'],\n                    # Will determine how ties are sorted which depends on the directional preference of the filter\n                    # 'min' makes equal values assume lower percent, 'max' higher percent\n                    method=metric_specs['direction'],\n                    # Treat the rank as a percent so that a percentage can be used to filter\n                    pct=True\n                )\n            else:\n                pre_operation = null\n                pre_kwargs = {}\n\n            filter_specification.append((operation, pre_operation, pre_kwargs, value))\n\n        parsed_filters[metric] = filter_specification\n\n    return parsed_filters\n</code></pre>"},{"location":"reference/flags/#flags.temp_gt0","title":"temp_gt0","text":"<pre><code>temp_gt0(temp: str) -&gt; float\n</code></pre> <p>Convert temperatures flags to float ensuring no 0 value</p> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def temp_gt0(temp: str) -&gt; float:\n    \"\"\"Convert temperatures flags to float ensuring no 0 value\"\"\"\n    temp = float(temp)\n    return temp if temp &gt; 0 else 0.0001\n</code></pre>"},{"location":"reference/flags/#flags.set_up_parser_with_groups","title":"set_up_parser_with_groups","text":"<pre><code>set_up_parser_with_groups(parser: ArgumentParser, parser_groups: dict[str, dict], required: bool = False)\n</code></pre> <p>Add the input arguments to the passed ArgumentParser</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>The ArgumentParser to add_argument_group() to</p> </li> <li> <code>parser_groups</code>             (<code>dict[str, dict]</code>)         \u2013          <p>The groups to add to the ArgumentParser</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the mutually exclusive group input is required</p> </li> </ul> Source code in <code>symdesign/flags/__init__.py</code> <pre><code>def set_up_parser_with_groups(parser: argparse.ArgumentParser, parser_groups: dict[str, dict], required: bool = False):\n    \"\"\"Add the input arguments to the passed ArgumentParser\n\n    Args:\n        parser: The ArgumentParser to add_argument_group() to\n        parser_groups: The groups to add to the ArgumentParser\n        required: Whether the mutually exclusive group input is required\n    \"\"\"\n    group = None  # Must get added before mutual groups can be added\n    for parser_name, parser_kwargs in parser_groups.items():\n        flags_kwargs = parser_arguments.get(parser_name, {})\n        \"\"\"flags_kwargs has args (flag names) as key and keyword args (flag params) as values\"\"\"\n        if mutual_keyword in parser_name:  # Only has a dictionary as parser_arguments\n            if group is None:\n                # Remove indication to \"mutual\" of the argparse group by removing any characters after mutual_keyword\n                group_parser_kwargs = parser_groups[parser_name[:parser_name.find(mutual_keyword)]]\n                group = parser.add_argument_group(**group_parser_kwargs)\n            exclusive_parser = group.add_mutually_exclusive_group(required=required, **parser_kwargs)\n            for flags_, kwargs in flags_kwargs.items():\n                exclusive_parser.add_argument(*flags_, **kwargs)\n        else:\n            if group is None:\n                group = parser.add_argument_group(**parser_kwargs)\n            for flags_, kwargs in flags_kwargs.items():\n                group.add_argument(*flags_, **kwargs)\n</code></pre>"},{"location":"reference/metrics/","title":"metrics","text":""},{"location":"reference/metrics/#metrics.parse_rosetta_scorefile","title":"parse_rosetta_scorefile","text":"<pre><code>parse_rosetta_scorefile(file: AnyStr, key: str = 'decoy') -&gt; dict[str, dict[str, str]]\n</code></pre> <p>Take a json formatted metrics file and incorporate entries into nested dictionaries with \"key\" as outer key</p> <p>Automatically formats scores according to conventional metric naming scheme, ex: \"R_\", \"S_\", or \"M_\" prefix removal</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>)         \u2013          <p>Location on disk of scorefile</p> </li> <li> <code>key</code>             (<code>str</code>, default:                 <code>'decoy'</code> )         \u2013          <p>Name of the json key to use as outer dictionary identifier</p> </li> </ul> <p>Returns:     The parsed scorefile         Ex {'design_identifier1': {'metric_key': metric_value, ...}, 'design_identifier2': {}, ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def parse_rosetta_scorefile(file: AnyStr, key: str = 'decoy') -&gt; dict[str, dict[str, str]]:\n    \"\"\"Take a json formatted metrics file and incorporate entries into nested dictionaries with \"key\" as outer key\n\n    Automatically formats scores according to conventional metric naming scheme, ex: \"R_\", \"S_\", or \"M_\" prefix removal\n\n    Args:\n        file: Location on disk of scorefile\n        key: Name of the json key to use as outer dictionary identifier\n    Returns:\n        The parsed scorefile\n            Ex {'design_identifier1': {'metric_key': metric_value, ...}, 'design_identifier2': {}, ...}\n    \"\"\"\n    with open(file, 'r') as f:\n        scores = {}\n        for json_entry in f.readlines():\n            formatted_scores = {}\n            for score, value in loads(json_entry).items():\n                if 'res_' in score:  # 'per_res_'):  # There are a lot of these scores in particular\n                    formatted_scores[score] = value\n                elif score.startswith('R_'):\n                    formatted_scores[score.replace('R_', '').replace('S_', '')] = value\n                else:\n                    # # res_summary replace is used to take sasa_res_summary and other res_summary metrics \"string\" off\n                    # score = score.replace('res_summary_', '')\n                    # score = score.replace('res_summary_', '').replace('solvation_total', 'solvation')\n                    formatted_scores[columns_to_rename.get(score, score)] = value\n\n            design = formatted_scores.pop(key)\n            if design not in scores:\n                scores[design] = formatted_scores\n            else:  # To ensure old trajectories don't have lingering protocol info\n                scores[design].update(formatted_scores)\n\n    return scores\n</code></pre>"},{"location":"reference/metrics/#metrics.keys_from_trajectory_number","title":"keys_from_trajectory_number","text":"<pre><code>keys_from_trajectory_number(pdb_dict)\n</code></pre> <p>Remove all string from dictionary keys except for string after last '_'. Ex 'design_0001' -&gt; '0001'</p> <p>Returns:</p> <ul> <li> <code>dict</code>         \u2013          <p>{cleaned_key: value, ...}</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def keys_from_trajectory_number(pdb_dict):\n    \"\"\"Remove all string from dictionary keys except for string after last '_'. Ex 'design_0001' -&gt; '0001'\n\n    Returns:\n        (dict): {cleaned_key: value, ...}\n    \"\"\"\n    return {key.split('_')[-1]: value for key, value in pdb_dict.items()}\n</code></pre>"},{"location":"reference/metrics/#metrics.join_columns","title":"join_columns","text":"<pre><code>join_columns(row)\n</code></pre> <p>Combine columns in a dataframe with the same column name. Keep only the last column record</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The column name</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def join_columns(row):  # UNUSED\n    \"\"\"Combine columns in a dataframe with the same column name. Keep only the last column record\n\n    Returns:\n        (str): The column name\n    \"\"\"\n    new_data = ','.join(row[row.notnull()].astype(str))\n    return new_data.split(',')[-1]\n</code></pre>"},{"location":"reference/metrics/#metrics.columns_to_new_column","title":"columns_to_new_column","text":"<pre><code>columns_to_new_column(df: DataFrame, columns: dict[str, tuple[str, ...]], mode: str = 'add')\n</code></pre> <p>Set new column value by taking an operation of one column on another</p> <p>Can perform summation and subtraction if a set of columns is provided Args:     df: Dataframe where the columns are located     columns: Keys are new column names, values are tuple of existing columns where         df[key] = value[0] mode(operation) value[1] mode(operation) ...     mode: What operator to use?         Viable options are included in the operator module {'sub', 'mul', 'truediv', ...} Returns:     Dataframe with new column values</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def columns_to_new_column(df: pd.DataFrame, columns: dict[str, tuple[str, ...]], mode: str = 'add'):\n    \"\"\"Set new column value by taking an operation of one column on another\n\n    Can perform summation and subtraction if a set of columns is provided\n    Args:\n        df: Dataframe where the columns are located\n        columns: Keys are new column names, values are tuple of existing columns where\n            df[key] = value[0] mode(operation) value[1] mode(operation) ...\n        mode: What operator to use?\n            Viable options are included in the operator module {'sub', 'mul', 'truediv', ...}\n    Returns:\n        Dataframe with new column values\n    \"\"\"\n    for new_column, column_set in columns.items():\n        try:  # Todo check why using attrgetter(mode)(operator) ?\n            df[new_column] = operator.attrgetter(mode)(operator)(df[column_set[0]], df[column_set[1]])\n        except KeyError:\n            pass\n        except IndexError:\n            raise IndexError(f'The number of columns in the set {column_set} is not &gt;= 2. {new_column} not possible!')\n        if len(column_set) &gt; 2 and mode in ['add', 'sub']:  # &gt;2 values in set, perform repeated operations Ex: SUM, SUB\n            for extra_column in column_set[2:]:  # perform an iteration for every N-2 items in the column_set\n                try:\n                    df[new_column] = operator.attrgetter(mode)(operator)(df[new_column], df[extra_column])\n                except KeyError:\n                    pass\n\n    return df\n</code></pre>"},{"location":"reference/metrics/#metrics.hbond_processing","title":"hbond_processing","text":"<pre><code>hbond_processing(design_scores: dict, columns: list[str]) -&gt; dict[str, set]\n</code></pre> <p>Process Hydrogen bond Metrics from Rosetta score dictionary</p> <p>if rosetta_numbering=\"true\" in .xml then use offset, otherwise, hbonds are PDB numbering Args:     design_scores: {'001': {'buns': 2.0, 'per_res_energy_complex_15A': -2.71, ...,                             'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...',                             'hbonds_res_selection_1_bound': '26A'}, ...}     columns : ['hbonds_res_selection_complex', 'hbonds_res_selection_1_unbound',                'hbonds_res_selection_2_unbound'] Returns:     {'0001': {34, 54, 67, 68, 106, 178}, ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def hbond_processing(design_scores: dict, columns: list[str]) -&gt; dict[str, set]:\n    \"\"\"Process Hydrogen bond Metrics from Rosetta score dictionary\n\n    if rosetta_numbering=\"true\" in .xml then use offset, otherwise, hbonds are PDB numbering\n    Args:\n        design_scores: {'001': {'buns': 2.0, 'per_res_energy_complex_15A': -2.71, ...,\n                                'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...',\n                                'hbonds_res_selection_1_bound': '26A'}, ...}\n        columns : ['hbonds_res_selection_complex', 'hbonds_res_selection_1_unbound',\n                   'hbonds_res_selection_2_unbound']\n    Returns:\n        {'0001': {34, 54, 67, 68, 106, 178}, ...}\n    \"\"\"\n    hbonds = {}\n    for design, scores in design_scores.items():\n        unbound_bonds, complex_bonds = set(), set()\n        for column in columns:\n            if column not in scores:\n                continue\n            meta_data = column.split('_')  # ['hbonds', 'res', 'selection', 'complex/interface_number', '[unbound]']\n            parsed_hbonds = set(int(hbond.translate(utils.keep_digit_table))\n                                for hbond in scores.get(column, '').split(',') if hbond != '')  # check if '' in case no hbonds\n            if meta_data[3] == 'complex':\n                complex_bonds = parsed_hbonds\n            else:  # from another state\n                unbound_bonds = unbound_bonds.union(parsed_hbonds)\n        if complex_bonds:  # 'complex', '1', '2'\n            hbonds[design] = complex_bonds.difference(unbound_bonds)\n            # hbonds[entry] = [hbonds_entry['complex'].difference(hbonds_entry['1']).difference(hbonds_entry['2']))]\n            #                                                         hbonds_entry['A']).difference(hbonds_entry['B'])\n        else:  # no hbonds were found in the complex\n            hbonds[design] = complex_bonds\n            # logger.error('%s: Missing hbonds_res_selection_ data for %s. Hbonds inaccurate!' % (pose, entry))\n\n    return hbonds\n</code></pre>"},{"location":"reference/metrics/#metrics.hot_spot","title":"hot_spot","text":"<pre><code>hot_spot(residue_dict: dict[Any, dict], energy: float = -1.5)\n</code></pre> <p>Calculate if each residue in a dictionary is a hot-spot</p> <p>Parameters:</p> <ul> <li> <code>residue_dict</code>             (<code>dict[Any, dict]</code>)         \u2013          </li> <li> <code>energy</code>             (<code>float</code>, default:                 <code>-1.5</code> )         \u2013          <p>The threshold for hot spot consideration</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The modified residue_dict.</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def hot_spot(residue_dict: dict[Any, dict], energy: float = -1.5):  # UNUSED\n    \"\"\"Calculate if each residue in a dictionary is a hot-spot\n\n    Args:\n        residue_dict:\n        energy: The threshold for hot spot consideration\n\n    Returns:\n        The modified residue_dict.\n    \"\"\"\n    for value in residue_dict.values():\n        if value['energy'] &lt;= energy:\n            value['hot_spot'] = 1\n        else:\n            value['hot_spot'] = 0\n\n    return residue_dict\n</code></pre>"},{"location":"reference/metrics/#metrics.interface_composition_similarity","title":"interface_composition_similarity","text":"<pre><code>interface_composition_similarity(series: Mapping) -&gt; float\n</code></pre> <p>Calculate the composition difference for pose residue classification</p> <p>Parameters:</p> <ul> <li> <code>series</code>             (<code>Mapping</code>)         \u2013          <p>Mapping from 'interface_area_total', 'core', 'rim', and 'support' to values</p> </li> </ul> <p>Returns:     Average similarity for expected residue classification given the observed classification</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def interface_composition_similarity(series: Mapping) -&gt; float:\n    \"\"\"Calculate the composition difference for pose residue classification\n\n    Args:\n        series: Mapping from 'interface_area_total', 'core', 'rim', and 'support' to values\n    Returns:\n        Average similarity for expected residue classification given the observed classification\n    \"\"\"\n    # Calculate modeled number of residues according to buried surface area (Levy, E 2010)\n    def core_res_fn(bsa):\n        return 0.01*bsa + 0.6\n\n    def rim_res_fn(bsa):\n        return 0.01*bsa - 2.5\n\n    def support_res_fn(bsa):\n        return 0.006*bsa + 5\n\n    # classification_fxn_d = {'core': core_res_fn, 'rim': rim_res_fn, 'support': support_res_fn}\n\n    int_area = series['interface_area_total']  # buried surface area\n    if int_area &lt;= 250:\n        return np.nan\n\n    class_ratio_differences = []\n    for residue_class, function in zip(residue_classification, (core_res_fn, rim_res_fn, support_res_fn)):\n        expected = function(int_area)\n        class_ratio_difference = (1 - (abs(series[residue_class]-expected) / expected))\n        if class_ratio_difference &lt; 0:\n            # Above calculation fails to bound between 0 and 1 with large obs values due to proportion &gt; 1\n            class_ratio_difference = 0\n        class_ratio_differences.append(class_ratio_difference)\n\n    return sum(class_ratio_differences) / len(class_ratio_differences)\n</code></pre>"},{"location":"reference/metrics/#metrics.incorporate_sequence_info","title":"incorporate_sequence_info","text":"<pre><code>incorporate_sequence_info(design_residue_scores: dict[str, dict], sequences: dict[str, Sequence[str]]) -&gt; dict[str, dict]\n</code></pre> <p>Incorporate mutation measurements into residue info. design_residue_scores and mutations must be the same index</p> <p>Parameters:</p> <ul> <li> <code>design_residue_scores</code>             (<code>dict[str, dict]</code>)         \u2013          <p>{'001': {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],                                  'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],                                  'fsp': 0., 'cst': 0.}, ...}, ...}</p> </li> <li> <code>sequences</code>             (<code>dict[str, Sequence[str]]</code>)         \u2013          <p>{'001': 'MKDLSAVLIRLAD...', '002': '', ...}</p> </li> </ul> <p>Returns:     {'001': {15: {'type': 'T', 'energy_delta': -2.71, 'coordinate_constraint': 0. 'residue_favored': 0., 'hbond': 0}              ...}, ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def incorporate_sequence_info(design_residue_scores: dict[str, dict], sequences: dict[str, Sequence[str]]) \\\n        -&gt; dict[str, dict]:\n    \"\"\"Incorporate mutation measurements into residue info. design_residue_scores and mutations must be the same index\n\n    Args:\n        design_residue_scores: {'001': {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],\n                                             'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],\n                                             'fsp': 0., 'cst': 0.}, ...}, ...}\n        sequences: {'001': 'MKDLSAVLIRLAD...', '002': '', ...}\n    Returns:\n        {'001': {15: {'type': 'T', 'energy_delta': -2.71, 'coordinate_constraint': 0. 'residue_favored': 0., 'hbond': 0}\n                 ...}, ...}\n    \"\"\"\n    # warn = False\n    # reference_data = mutations.get(putils.reference_name)\n    # pose_length = len(reference_data)\n    for design, residue_info in design_residue_scores.items():\n        sequence = sequences.get(design)\n        # mutation_data = mutations.get(design)\n        # if not mutation_data:\n        #     continue\n\n        # remove_residues = []\n        for residue_index, data in residue_info.items():\n            data['type'] = sequence[residue_index]\n            # try:  # Set residue AA type based on provided mutations\n            #     data['type'] = mutation_data[residue_index]\n            # except KeyError:  # Residue is not in mutations, probably missing as it is not a mutation\n            #     try:  # Fill in with AA from putils.reference_name seq\n            #         data['type'] = reference_data[residue_index]\n            #     except KeyError:  # Residue is out of bounds on pose length\n            #         # Possibly a virtual residue or string that was processed incorrectly from the keep_digit_table\n            #         if not warn:\n            #             logger.error(f'Encountered residue index \"{residue_index}\" which is not within the pose size '\n            #                          f'\"{pose_length}\" and will be removed from processing. This is likely an error '\n            #                          f'with residue processing or residue selection in the specified rosetta protocol.'\n            #                          f' If there were warnings produced indicating a larger residue number than pose '\n            #                          f'size, this problem was not addressable heuristically and something else has '\n            #                          f'occurred. It is likely that this residue number is not useful if you indeed have'\n            #                          f' output_as_pdb_nums=\"true\"')\n            #             warn = True\n            #         remove_residues.append(residue_index)\n            #         continue\n\n        # # Clean up any incorrect residues\n        # for residue in remove_residues:\n        #     residue_info.pop(residue)\n\n    return design_residue_scores\n</code></pre>"},{"location":"reference/metrics/#metrics.process_residue_info","title":"process_residue_info","text":"<pre><code>process_residue_info(design_residue_scores: dict, hbonds: dict = None) -&gt; dict\n</code></pre> <p>Process energy metrics to Pose formatted dictionary from multiple measurements per residue and incorporate hydrogen bond information. design_residue_scores and hbonds must be the same index</p> <p>Parameters:</p> <ul> <li> <code>design_residue_scores</code>             (<code>dict</code>)         \u2013          <p>{'001': {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],                                  'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],                                  'fsp': 0., 'cst': 0.}, ...}, ...}</p> </li> <li> <code>hbonds</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>{'001': [34, 54, 67, 68, 106, 178], ...}</p> </li> </ul> <p>Returns:     {'001': {15: {'type': 'T', 'energy_delta': -2.71, 'coordinate_constraint': 0. 'residue_favored': 0., 'hbond': 0}              ...}, ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def process_residue_info(design_residue_scores: dict, hbonds: dict = None) -&gt; dict:\n    \"\"\"Process energy metrics to Pose formatted dictionary from multiple measurements per residue\n    and incorporate hydrogen bond information. design_residue_scores and hbonds must be the same index\n\n    Args:\n        design_residue_scores: {'001': {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],\n                                             'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],\n                                             'fsp': 0., 'cst': 0.}, ...}, ...}\n        hbonds: {'001': [34, 54, 67, 68, 106, 178], ...}\n    Returns:\n        {'001': {15: {'type': 'T', 'energy_delta': -2.71, 'coordinate_constraint': 0. 'residue_favored': 0., 'hbond': 0}\n                 ...}, ...}\n    \"\"\"\n    if hbonds is None:\n        hbonds = {}\n\n    for design, residue_info in design_residue_scores.items():\n        design_hbonds = hbonds.get(design, [])\n        for residue_number, data in residue_info.items():\n            # Set hbond bool if available\n            data['hbond'] = 1 if residue_number in design_hbonds else 0\n            # Compute the energy delta which requires summing the unbound energies\n            data['unbound'] = sum(data['unbound'])\n            data['energy_delta'] = data['complex'] - data['unbound']\n            # Compute the \"preconfiguration\" energy delta which requires summing the bound energies\n            data['bound'] = sum(data['bound'])\n            # data['energy_bound_activation'] = data['bound'] - data['unbound']\n            data['solv_bound'] = sum(data['solv_bound'])\n            data['solv_unbound'] = sum(data['solv_unbound'])\n            data['coordinate_constraint'] = data.get('cst', 0.)\n            data['residue_favored'] = data.get('fsp', 0.)\n            # if residue_data[residue_number]['energy'] &lt;= hot_spot_energy:\n            #     residue_data[residue_number]['hot_spot'] = 1\n\n    return design_residue_scores\n</code></pre>"},{"location":"reference/metrics/#metrics.collapse_per_residue","title":"collapse_per_residue","text":"<pre><code>collapse_per_residue(sequence_groups: Iterable[Iterable[Sequence[str]]], residue_contact_order_z: ndarray, reference_collapse: ndarray, **kwargs) -&gt; list[dict[str, float]]\n</code></pre> <p>Measure per-residue sequence folding metrics based on reference values including contact order z score and hydrophobic collapse</p> <p>Parameters:</p> <ul> <li> <code>sequence_groups</code>             (<code>Iterable[Iterable[Sequence[str]]]</code>)         \u2013          <p>Groups of sequences, where the outer nest is each sample and the inner nest are unique polymers</p> </li> <li> <code>residue_contact_order_z</code>             (<code>ndarray</code>)         \u2013          <p>The per-residue contact order z score from a reference structure</p> </li> <li> <code>reference_collapse</code>             (<code>ndarray</code>)         \u2013          <p>The per-residue hydrophobic collapse values measured from a reference sequence</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>hydrophobicity</code>         \u2013          <p>str = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILYVW), or provide one with 'custom' keyword argument</p> </li> <li> <code>custom</code>         \u2013          <p>mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs</p> </li> <li> <code>alphabet_type</code>         \u2013          <p>alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters</p> </li> <li> <code>lower_window</code>         \u2013          <p>int = 3 \u2013 The smallest window used to measure</p> </li> <li> <code>upper_window</code>         \u2013          <p>int = 9 \u2013 The largest window used to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[dict[str, float]]</code>         \u2013          <p>The mapping of collapse metric to per-residue values for the concatenated sequence in each sequence_groups. These include: {'collapse_deviation_magnitude',  'collapse_increase_significance_by_contact_order_z',  'collapse_increased_z',  'collapse_new_positions',  'collapse_new_position_significance',  'collapse_sequential_peaks_z',  'collapse_sequential_z',  'collapse_significance_by_contact_order_z',  'hydrophobic_collapse'  }</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def collapse_per_residue(sequence_groups: Iterable[Iterable[Sequence[str]]],\n                         residue_contact_order_z: np.ndarray, reference_collapse: np.ndarray, **kwargs) \\\n        -&gt; list[dict[str, float]]:\n    # collapse_profile: np.ndarray = None,\n    # reference_mean: float | np.ndarray = None,\n    # reference_std: float | np.ndarray = None,\n    \"\"\"Measure per-residue sequence folding metrics based on reference values including contact order z score and\n    hydrophobic collapse\n\n    Args:\n        sequence_groups: Groups of sequences, where the outer nest is each sample and the inner nest are unique polymers\n        residue_contact_order_z: The per-residue contact order z score from a reference structure\n        reference_collapse: The per-residue hydrophobic collapse values measured from a reference sequence\n\n    Keyword Args:\n        hydrophobicity: str = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV),\n            'expanded' (FMILYVW), or provide one with 'custom' keyword argument\n        custom: mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs\n        alphabet_type: alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters\n        lower_window: int = 3 \u2013 The smallest window used to measure\n        upper_window: int = 9 \u2013 The largest window used to measure\n\n    Returns:\n        The mapping of collapse metric to per-residue values for the concatenated sequence in each sequence_groups.\n            These include:\n            {'collapse_deviation_magnitude',\n             'collapse_increase_significance_by_contact_order_z',\n             'collapse_increased_z',\n             'collapse_new_positions',\n             'collapse_new_position_significance',\n             'collapse_sequential_peaks_z',\n             'collapse_sequential_z',\n             'collapse_significance_by_contact_order_z',\n             'hydrophobic_collapse'\n             }\n    \"\"\"\n    #    collapse_profile: The per-residue hydrophobic collapse values measured from a reference GeneEntity\n    #    reference_mean: The hydrophobic collapse mean value(s) to use as a reference for z-score calculation\n    #    reference_std: The hydrophobic collapse deviation value(s) to use as a reference for z-score calculation\n    hydrophobicity = kwargs.get('hydrophobicity')\n    if not hydrophobicity:  # Set to the standard\n        hydrophobicity = kwargs['hydrophobicity'] = 'standard'\n    # else:\n    #     if hydrophobicity != 'standard':\n    #         logger.warning(f'Found hydrophobicity=\"{hydrophobicity}\". This is incompatible without passing '\n    #                        'reference_mean/_std. Setting hydrophobicity=\"standard\"')\n    #         kwargs['hydrophobicity'] = 'standard'\n\n    significance_threshold = collapse_thresholds[hydrophobicity]\n    # # if collapse_profile is not None and collapse_profile.size:  # Not equal to zero\n    # if reference_mean is None or reference_std is None:\n    #     reference_mean = significance_threshold\n    #     reference_std = collapse_reported_std\n    # # else:\n    # #     reference_mean = np.nanmean(collapse_profile, axis=-2)\n    # #     reference_std = np.nanstd(collapse_profile, axis=-2)\n    # #     # Use only the reference (index=0) hydrophobic_collapse_index to calculate a reference collapse z-score\n    # #     reference_collapse_z_score = utils.z_score(collapse_profile[0], reference_mean, reference_std)\n    # #     reference_collapse_bool = reference_mean &gt; collapse_significance_threshold\n\n    # reference_collapse_bool = np.where(reference_collapse &gt; collapse_significance_threshold, 1, 0)\n    # [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, ...]\n    reference_collapse_bool = (reference_collapse &gt; significance_threshold).astype(int)\n    # [False, False, False, False, True, True, False, False, True, True, ...]\n    # reference_collapse_z_score = utils.z_score(reference_collapse, reference_mean, reference_std)\n    reference_collapse_z_score = z_score(reference_collapse, significance_threshold, collapse_reported_std)\n\n    # Linearly weight residue by sequence position (early &gt; late) with the halfway position (midpoint) at .5\n    # midpoint = .5\n    scale = 1  # / midpoint\n    folding_and_collapse = []\n    # for pose_idx, pose in enumerate(poses_of_interest):\n    #     collapse = []\n    #     for entity_idx, entity in enumerate(pose.entities):\n    #         sequence_length = entity.number_of_residues\n    #         collapse.append(entity.hydrophobic_collapse())\n    for pose_idx, sequences in enumerate(sequence_groups):\n        # Gather all the collapse info for the particular sequence group\n        collapse = np.concatenate([hydrophobic_collapse_index(sequence, **kwargs)\n                                   for entity_idx, sequence in enumerate(sequences)])\n        # Scale the collapse by the standard collapse threshold and make z score\n        # collapse_z = utils.z_score(collapse, reference_mean, reference_std)\n        collapse_z = z_score(collapse, significance_threshold, collapse_reported_std)\n        # Find the difference between the sequence and the reference\n        difference_collapse_z = collapse_z - reference_collapse_z_score\n        # The sum of all sequence regions z-scores experiencing increased collapse. Measures the normalized\n        # magnitude of additional hydrophobic collapse\n        # collapse_deviation_magnitude_sum = np.abs(difference_collapse_z).sum()\n        collapse_deviation_magnitude = np.abs(difference_collapse_z)\n\n        # Find the indices where the sequence collapse has increased compared to reference collapse_profile\n        increased_collapse_z = np.maximum(difference_collapse_z, 0)\n        # collapse_increased_z_sum = increased_collapse_z.sum()\n\n        # Sum the contact order, scaled proportionally by the collapse increase. More negative is more isolated\n        # collapse. Positive indicates poor maintaning of the starting collapse\n        # collapse_increase_significance_by_contact_order_z_sum = \\\n        #     np.sum(residue_contact_order_z * increased_collapse_z)\n        # collapse_increase_significance_by_contact_order_z = residue_contact_order_z * increased_collapse_z\n\n        # Where collapse is occurring\n        collapsing_positions_z = np.maximum(collapse_z, 0)\n        # ^ [0, 0, 0, 0, 0.04, 0.06, 0, 0, 0.1, 0.07, ...]\n        collapse_bool = collapsing_positions_z != 0  # [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, ...]\n        # Check if increased collapse positions resulted in a location of \"new collapse\"\n        # i.e. sites where collapse occurs compared to reference\n        new_collapsing = (collapse_bool - reference_collapse_bool) == 1\n        # Ex, [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, ...]\n\n        # Calculate \"islands\". Ensure position is collapsing, while the reference has no collapsing neighbors\n        # list is faster to index than np.ndarray i.e. new_collapse = np.zeros_like(collapse_bool)\n        new_collapse = [True if collapse and (not ref_minus1 and not ref_plus1) else False\n                        for ref_minus1, collapse, ref_plus1 in\n                        # Trim the sequence to a 3 residue window (-1, 0, 1)\n                        zip(reference_collapse[:-2].tolist(),\n                            new_collapsing[1:-1].tolist(),\n                            reference_collapse[2:].tolist())]\n        # Finish by calculating first and last indices as well and combining\n        new_collapse = [True if new_collapsing[0] and not reference_collapse[1] else False] \\\n            + new_collapse \\\n            + [True if new_collapsing[-1] and not reference_collapse[-2] else False]\n\n        # Find new collapse positions\n        new_collapse_peak_start = [0 for _ in range(len(collapse_bool))]  # [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...]\n        # Keep track of how many discrete collapsing segments exist and where their boundaries are\n        collapse_peak_start = new_collapse_peak_start.copy()  # [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...]\n        sequential_collapse_points = np.zeros_like(collapse_bool)  # [-1, -1, -1, -1, 0, 0, 0, 0, 1, 1, ...]\n        collapse_iterator = -1  # Start at -1 so that the first point eventually is equal to a 0 subtraction. Was 0\n        for prior_idx, idx in enumerate(range(1, len(collapse_z))):\n            # Compare neighboring residues in the new_collapse and collapse_peak_start\n            # Both conditions are only True when 0 -&gt; 1 transition occurs\n            if new_collapse[prior_idx] &lt; new_collapse[idx]:\n                new_collapse_peak_start[idx] = 1\n            if collapse_bool[prior_idx] &lt; collapse_bool[idx]:\n                collapse_peak_start[idx] = 1\n                collapse_iterator += 1\n            sequential_collapse_points[idx] = collapse_iterator\n\n        # if collapse_profile is not None and collapse_profile.size:  # Not equal to zero\n        # Compare the measured collapse to the metrics gathered from the collapse_profile\n        # # _collapse_z = utils.z_score(standardized_collapse, collapse_profile_mean, collapse_profile_std)\n        # _collapse_z = utils.z_score(collapse, reference_mean, reference_std)\n        # Find the indices where the _collapse_z is increased versus the reference_collapse_z_score\n\n        try:\n            step = 1 / sum(collapse_peak_start)  # This is 1 over the \"total_collapse_points\"\n        except ZeroDivisionError:  # No collapse peaks\n            step = 1\n        # # Make array for small adjustment to account for first value equal to scale\n        # # add_step_array = collapse_bool * step\n        # v [1.1, 1.1, 1.1, 1.1, 1, 1, 1, 1, .9, .9, ...]\n        sequential_collapse_weights = scale * (1 - step*sequential_collapse_points)\n        # Make sequential_collapse_weights only useful at points where collapse increased (i.e. collapse_bool is 1)\n        # v [0, 0, 0, 0, 1, 1, 0, 0, .9, .9, ...]\n        sequential_collapse_weights *= collapse_bool\n        # collapse_sequential_peaks_z_sum = np.sum(sequential_collapse_weights * increased_collapse_z)\n        collapse_sequential_peaks_z = sequential_collapse_weights * increased_collapse_z\n        # v [1, .99, .98, .97, .96, ...]\n        sequence_length = len(collapse)\n        sequential_weights = scale * (1 - np.arange(sequence_length)/sequence_length)\n        # collapse_sequential_z_sum = np.sum(sequential_weights * increased_collapse_z)\n        collapse_sequential_z = sequential_weights * increased_collapse_z\n        # else:\n        #     # For per-residue\n        #     collapse_increase_significance_by_contact_order_z = increased_collapse_z = \\\n        #         collapse_deviation_magnitude = collapse_sequential_peaks_z = collapse_sequential_z = \\\n        #         np.zeros_like(collapse)\n        #     # # For summing\n        #     # collapse_deviation_magnitude_sum = collapse_increase_significance_by_contact_order_z_sum = \\\n        #     #     collapse_sequential_peaks_z_sum = collapse_sequential_z_sum = collapse_increased_z_sum = 0.\n\n        # Negating inverts contact order z-score to weight high contact order negatively\n        residue_contact_order_inverted_z = residue_contact_order_z * -1\n\n        # With 'collapse_new_position_significance'\n        #  Use contact order z score and hci to understand designability of an area and its folding modification\n        #  For positions experiencing collapse, multiply by inverted contact order\n        collapse_significance = residue_contact_order_inverted_z * collapsing_positions_z\n        #  Positive values indicate collapse in areas with low contact order\n        #  Negative, collapse in high contact order\n        #  Indicates the degree to which low contact order segments (+) may be reliant on collapse for folding,\n        #  while high contact order (-) may use collapse\n        # residue_contact_order_inverted_z = [-1.0, -0.4, 0.8, 0.2, -1.3, -0.2, 0.9, -1.7, ...]\n        # collapsing_positions_z = [0, 0, 0, 0, 0.04, 0.06, 0, 0, 0.1, 0.07, ...]\n\n        # Add the concatenated collapse metrics to total\n        folding_and_collapse.append({\n            'hydrophobic_collapse': collapse,\n            'collapse_deviation_magnitude': collapse_deviation_magnitude,\n            'collapse_increase_significance_by_contact_order_z':\n                residue_contact_order_inverted_z * increased_collapse_z,\n            'collapse_increased_z': increased_collapse_z,\n            'collapse_new_positions': new_collapse_peak_start,\n            'collapse_new_position_significance': new_collapse_peak_start * collapse_significance,\n            'collapse_sequential_peaks_z': collapse_sequential_peaks_z,\n            'collapse_sequential_z': collapse_sequential_z,\n            'collapse_significance_by_contact_order_z': collapse_significance\n        })\n    return folding_and_collapse\n</code></pre>"},{"location":"reference/metrics/#metrics.mutation_conserved","title":"mutation_conserved","text":"<pre><code>mutation_conserved(residue_info: dict, bkgnd: dict) -&gt; dict\n</code></pre> <p>Process residue mutations compared to evolutionary background. Returns 1 if residue is observed in background</p> <p>Both residue_dict and background must be same index Args:     residue_info: {15: {'type': 'T', ...}, ...}     bkgnd: {0: {'A': 0, 'R': 0, ...}, ...} Returns:     conservation_dict: {15: 1, 21: 0, 25: 1, ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def mutation_conserved(residue_info: dict, bkgnd: dict) -&gt; dict:\n    \"\"\"Process residue mutations compared to evolutionary background. Returns 1 if residue is observed in background\n\n    Both residue_dict and background must be same index\n    Args:\n        residue_info: {15: {'type': 'T', ...}, ...}\n        bkgnd: {0: {'A': 0, 'R': 0, ...}, ...}\n    Returns:\n        conservation_dict: {15: 1, 21: 0, 25: 1, ...}\n    \"\"\"\n    return {res: 1 if bkgnd[res][info['type']] &gt; 0 else 0 for res, info in residue_info.items() if res in bkgnd}\n</code></pre>"},{"location":"reference/metrics/#metrics.per_res_metric","title":"per_res_metric","text":"<pre><code>per_res_metric(sequence_metrics: dict[Any, float] | dict[Any, dict[str, float]], key: str = None) -&gt; float\n</code></pre> <p>Find metric value average over all residues in a per residue dictionary with metric specified by key</p> <p>Parameters:</p> <ul> <li> <code>sequence_metrics</code>             (<code>dict[Any, float] | dict[Any, dict[str, float]]</code>)         \u2013          <p>{16: {'S': 0.134, 'A': 0.050, ..., 'jsd': 0.732, 'int_jsd': 0.412}, ...}</p> </li> <li> <code>key</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Name of the metric to average</p> </li> </ul> <p>Returns:     The average metric 0.367</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def per_res_metric(sequence_metrics: dict[Any, float] | dict[Any, dict[str, float]], key: str = None) -&gt; float:\n    \"\"\"Find metric value average over all residues in a per residue dictionary with metric specified by key\n\n    Args:\n        sequence_metrics: {16: {'S': 0.134, 'A': 0.050, ..., 'jsd': 0.732, 'int_jsd': 0.412}, ...}\n        key: Name of the metric to average\n    Returns:\n        The average metric 0.367\n    \"\"\"\n    s, total = 0.0, 0\n    if key:\n        for residue_metrics in sequence_metrics.values():\n            value = residue_metrics.get(key)\n            if value:\n                s += value\n                total += 1\n    else:\n        for total, residue_metric in enumerate(sequence_metrics.values(), 1):\n            s += residue_metric\n\n    if total == 0:\n        return 0.\n    else:\n        return s / total\n</code></pre>"},{"location":"reference/metrics/#metrics.calculate_residue_buried_surface_area","title":"calculate_residue_buried_surface_area","text":"<pre><code>calculate_residue_buried_surface_area(per_residue_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>From a DataFrame with per-residue values, calculate values relating to interface surface area</p> <p>Parameters:</p> <ul> <li> <code>per_residue_df</code>             (<code>DataFrame</code>)         \u2013          <p>The DataFrame with MultiIndex columns where level1=residue_numbers, level0=residue_metric and containing the metrics [     'sasa_hydrophobic_bound',     'sasa_polar_bound',     'sasa_hydrophobic_complex',     'sasa_polar_complex' ]</p> </li> </ul> <p>Returns:     The same dataframe with added columns [         'bsa_hydrophobic',         'bsa_polar',         'bsa_total',         'sasa_total_bound',         'sasa_total_complex'     ]</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def calculate_residue_buried_surface_area(per_residue_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"From a DataFrame with per-residue values, calculate values relating to interface surface area\n\n    Args:\n        per_residue_df: The DataFrame with MultiIndex columns where level1=residue_numbers, level0=residue_metric and\n            containing the metrics [\n                'sasa_hydrophobic_bound',\n                'sasa_polar_bound',\n                'sasa_hydrophobic_complex',\n                'sasa_polar_complex'\n            ]\n    Returns:\n        The same dataframe with added columns [\n            'bsa_hydrophobic',\n            'bsa_polar',\n            'bsa_total',\n            'sasa_total_bound',\n            'sasa_total_complex'\n        ]\n    \"\"\"\n    # Make buried surface area (bsa) columns\n    bound_hydro = per_residue_df.loc[:, idx_slice[:, 'sasa_hydrophobic_bound']]\n    bound_polar = per_residue_df.loc[:, idx_slice[:, 'sasa_polar_bound']]\n    complex_hydro = per_residue_df.loc[:, idx_slice[:, 'sasa_hydrophobic_complex']]\n    complex_polar = per_residue_df.loc[:, idx_slice[:, 'sasa_polar_complex']]\n\n    bsa_hydrophobic = (bound_hydro.rename(columns={'sasa_hydrophobic_bound': 'bsa_hydrophobic'})\n                       - complex_hydro.rename(columns={'sasa_hydrophobic_complex': 'bsa_hydrophobic'}))\n    bsa_polar = (bound_polar.rename(columns={'sasa_polar_bound': 'bsa_polar'})\n                 - complex_polar.rename(columns={'sasa_polar_complex': 'bsa_polar'}))\n    bsa_total = (bsa_hydrophobic.rename(columns={'bsa_hydrophobic': 'bsa_total'})\n                 + bsa_polar.rename(columns={'bsa_polar': 'bsa_total'}))\n\n    # Make sasa_complex_total columns\n    bound_total = (bound_hydro.rename(columns={'sasa_hydrophobic_bound': 'sasa_total_bound'})\n                   + bound_polar.rename(columns={'sasa_polar_bound': 'sasa_total_bound'}))\n    complex_total = (complex_hydro.rename(columns={'sasa_hydrophobic_complex': 'sasa_total_complex'})\n                     + complex_polar.rename(columns={'sasa_polar_complex': 'sasa_total_complex'}))\n\n    return per_residue_df.join([bsa_hydrophobic, bsa_polar, bsa_total, bound_total, complex_total])\n</code></pre>"},{"location":"reference/metrics/#metrics.classify_interface_residues","title":"classify_interface_residues","text":"<pre><code>classify_interface_residues(per_residue_df: DataFrame, relative_sasa_thresh: float = default_sasa_burial_threshold) -&gt; DataFrame\n</code></pre> <p>From a DataFrame with per-residue values, calculate the classification of residues by interface surface area</p> <p>Parameters:</p> <ul> <li> <code>per_residue_df</code>             (<code>DataFrame</code>)         \u2013          <p>The DataFrame with MultiIndex columns where level1=residue_numbers, level0=residue_metric containing the metrics ['bsa_total', 'sasa_relative_complex', 'sasa_relative_bound']</p> </li> <li> <code>relative_sasa_thresh</code>             (<code>float</code>, default:                 <code>default_sasa_burial_threshold</code> )         \u2013          <p>The area threshold that the Residue should fall below before it is considered 'core' Default cutoff percent is based on Levy, E. 2010</p> </li> </ul> <p>Returns:     The same dataframe with added columns ['core', 'interior', 'rim', 'support', 'surface']</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def classify_interface_residues(per_residue_df: pd.DataFrame,\n                                relative_sasa_thresh: float = default_sasa_burial_threshold) -&gt; pd.DataFrame:\n    \"\"\"From a DataFrame with per-residue values, calculate the classification of residues by interface surface area\n\n    Args:\n        per_residue_df: The DataFrame with MultiIndex columns where level1=residue_numbers, level0=residue_metric\n            containing the metrics ['bsa_total', 'sasa_relative_complex', 'sasa_relative_bound']\n        relative_sasa_thresh: The area threshold that the Residue should fall below before it is considered 'core'\n            Default cutoff percent is based on Levy, E. 2010\n    Returns:\n        The same dataframe with added columns ['core', 'interior', 'rim', 'support', 'surface']\n    \"\"\"\n    # Find the relative sasa of the complex and the unbound fraction\n    rim_core_support = (per_residue_df.loc[:, idx_slice[:, 'bsa_total']] &gt; bsa_tolerance).to_numpy()\n    interior_surface = ~rim_core_support\n    # surface_or_rim = \\\n    #     per_residue_df.loc[:, idx_slice[index_residues, 'sasa_relative_complex']] &gt; relative_sasa_thresh\n    # v These could also be support\n    core_or_support_or_interior = \\\n        per_residue_df.loc[:, idx_slice[:, 'sasa_relative_complex']] &lt; relative_sasa_thresh\n    surface_or_rim = ~core_or_support_or_interior\n    support_or_interior_not_core_or_rim = \\\n        per_residue_df.loc[:, idx_slice[:, 'sasa_relative_bound']] &lt; relative_sasa_thresh\n    # ^ These could be interior too\n    # core_sufficient = np.logical_and(core_or_support_or_interior, rim_core_support).to_numpy()\n    interior_residues = np.logical_and(core_or_support_or_interior, interior_surface).rename(\n        columns={'sasa_relative_complex': 'interior'})\n    surface_residues = np.logical_and(surface_or_rim, interior_surface).rename(\n        columns={'sasa_relative_complex': 'surface'})\n\n    support_residues = np.logical_and(support_or_interior_not_core_or_rim, rim_core_support).rename(\n        columns={'sasa_relative_bound': 'support'})\n    rim_residues = np.logical_and(surface_or_rim, rim_core_support).rename(\n        columns={'sasa_relative_complex': 'rim'})\n    core_residues = np.logical_and(~support_residues,\n                                   np.logical_and(core_or_support_or_interior, rim_core_support).to_numpy()).rename(\n        columns={'support': 'core'})\n\n    per_residue_df = per_residue_df.join([core_residues, interior_residues, support_residues, rim_residues,\n                                          surface_residues\n                                          ])\n    # Drop intermediate columns\n    per_residue_df.drop(relative_sasa_states, axis=1, level=-1, errors='ignore', inplace=True)\n\n    return per_residue_df\n</code></pre>"},{"location":"reference/metrics/#metrics.sum_per_residue_metrics","title":"sum_per_residue_metrics","text":"<pre><code>sum_per_residue_metrics(df: DataFrame, rename_columns: Mapping[str, str] = None, mean_metrics: Sequence[str] = None) -&gt; DataFrame\n</code></pre> <p>From a DataFrame with per-residue values (i.e. a metric in level -1), tabulate all values across each residue</p> <p>Renames specific values relating to interfacial energy and solvation energy</p> <p>Parameters:</p> <ul> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>The DataFrame with MultiIndex columns where level -1 = metric</p> </li> <li> <code>rename_columns</code>             (<code>Mapping[str, str]</code>, default:                 <code>None</code> )         \u2013          <p>Columns to rename as a result of the summation</p> </li> <li> <code>mean_metrics</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>Columns to take the mean instead of the sum</p> </li> </ul> <p>Returns:     A new DataFrame with the summation of each metric from all residue_numbers in the per_residue columns</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def sum_per_residue_metrics(df: pd.DataFrame, rename_columns: Mapping[str, str] = None,\n                            mean_metrics: Sequence[str] = None) -&gt; pd.DataFrame:\n    \"\"\"From a DataFrame with per-residue values (i.e. a metric in level -1), tabulate all values across each residue\n\n    Renames specific values relating to interfacial energy and solvation energy\n\n    Args:\n        df: The DataFrame with MultiIndex columns where level -1 = metric\n        rename_columns: Columns to rename as a result of the summation\n        mean_metrics: Columns to take the mean instead of the sum\n    Returns:\n        A new DataFrame with the summation of each metric from all residue_numbers in the per_residue columns\n    \"\"\"\n    # # Drop unused particular residues_df columns that have been summed\n    # per_residue_drop_columns = per_residue_energy_states + energy_metric_names + per_residue_sasa_states \\\n    #                            + collapse_metrics + residue_classification \\\n    #                            + ['errat_deviation', 'hydrophobic_collapse', 'contact_order'] \\\n    #                            + ['hbond', 'evolution', 'fragment', 'type'] + ['surface', 'interior']\n    # residues_df = residues_df.drop(\n    #     list(residues_df.loc[:, idx_slice[:, per_residue_drop_columns]].columns),\n    #     errors='ignore', axis=1)\n\n    # Group by the columns according to the metrics (level=-1). Upper level(s) are residue identifiers\n    groupby_df = df.T.groupby(level=-1)\n    rename_columns = {\n        'hydrophobic_collapse': 'hydrophobicity',\n        **energy_metrics_rename_mapping,\n        **sasa_metrics_rename_mapping,\n        **renamed_design_metrics,\n        **(rename_columns or {})}\n    count_df = groupby_df.count().T.rename(columns=rename_columns)\n    # Using min_count=1, ensure that those columns with np.nan remain np.nan\n    summed_df = groupby_df.sum(min_count=1).T.rename(columns=rename_columns)\n    if mean_metrics is not None:\n        summed_df[mean_metrics] = summed_df[mean_metrics].div(count_df[mean_metrics], axis=0)\n\n    return summed_df\n</code></pre>"},{"location":"reference/metrics/#metrics.calculate_sequence_observations_and_divergence","title":"calculate_sequence_observations_and_divergence","text":"<pre><code>calculate_sequence_observations_and_divergence(alignment: MultipleSequenceAlignment, backgrounds: dict[str, ndarray]) -&gt; tuple[dict[str, ndarray], dict[str, ndarray]]\n</code></pre> <p>Gather the observed frequencies from each sequence in a MultipleSequenceAlignment</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def calculate_sequence_observations_and_divergence(alignment: sequence.MultipleSequenceAlignment,\n                                                   backgrounds: dict[str, np.ndarray]) \\\n        -&gt; tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:\n    #                                                select_indices: list[int] = None) \\\n    \"\"\"Gather the observed frequencies from each sequence in a MultipleSequenceAlignment\"\"\"\n    # mutation_frequencies = pose_alignment.frequencies[[residue-1 for residue in pose.interface_design_residue_numbers]]\n    # mutation_frequencies = filter_dictionary_keys(pose_alignment.frequencies, pose.interface_design_residue_numbers)\n    # mutation_frequencies = filter_dictionary_keys(pose_alignment['frequencies'], interface_residue_numbers)\n\n    # Calculate amino acid observation percent from residue_info and background SSM's\n    # observation_d = {profile: {design: mutation_conserved(info, background)\n    #                            for design, numerical_sequence in residue_info.items()}\n    # observation_d = {profile: {design: np.where(background[:, numerical_sequence] &gt; 0, 1, 0)\n    #                            for design, numerical_sequence in zip(pose_sequences,\n    #                                                                  list(pose_alignment.numerical_alignment))}\n    #                  for profile, background in profile_background.items()}\n    # Find the observed background for each profile, for each designed pose\n    # pose_observed_bkd = {profile: {design: freq.mean() for design, freq in design_obs_freqs.items()}\n    #                      for profile, design_obs_freqs in observation_d.items()}\n    # for profile, observed_frequencies in pose_observed_bkd.items():\n    #     scores_df[f'observed_{profile}'] = pd.Series(observed_frequencies)\n    # for profile, design_obs_freqs in observation_d.items():\n    #     scores_df[f'observed_{profile}'] = \\\n    #         pd.Series({design: freq.mean() for design, freq in design_obs_freqs.items()})\n    # observed_dfs = []\n    transposed_alignment = alignment.numerical_alignment.T\n    # observed = {profile: np.take_along_axis(background, transposed_alignment, axis=1).T\n    observed = {profile: np.where(np.take_along_axis(background, transposed_alignment, axis=1) &gt; 0, 1, 0).T\n                for profile, background in backgrounds.items()}\n    # for profile, background in profile_background.items():\n    #     observed[profile] = np.where(np.take_along_axis(background, transposed_alignment, axis=1) &gt; 0, 1, 0).T\n    #     # obs_df = pd.DataFrame(data=np.where(np.take_along_axis(background, transposed_alignment, axis=1) &gt; 0,\n    #     #                                     1, 0).T,\n    #     #                       index=pose_sequences,\n    #     #                       columns=pd.MultiIndex.from_product([residue_indices, [f'observed_{profile}']]))\n    #     # observed_dfs.append(obs_df)\n\n    # Calculate Jensen Shannon Divergence using different SSM occurrence data and design mutations\n    #                                              both mut_freq and profile_background[profile] are one-indexed\n    divergence = {f'divergence_{profile}':\n                  # position_specific_jsd(pose_alignment.frequencies, background)\n                  position_specific_divergence(alignment.frequencies, background)  # [select_indices]\n                  for profile, background in backgrounds.items()}\n\n    return observed, divergence\n</code></pre>"},{"location":"reference/metrics/#metrics.jensen_shannon_divergence","title":"jensen_shannon_divergence","text":"<pre><code>jensen_shannon_divergence(sequence_frequencies: ndarray, background_aa_freq: ndarray, **kwargs) -&gt; ndarray\n</code></pre> <p>Calculate Jensen-Shannon Divergence value for all residues against a background frequency dict</p> <p>Parameters:</p> <ul> <li> <code>sequence_frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[[0.05, 0.001, 0.1, ...], ...]</p> </li> <li> <code>background_aa_freq</code>             (<code>ndarray</code>)         \u2013          <p>[0.11, 0.03, 0.53, ...]</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>lambda_</code>         \u2013          <p>float = 0.5 - Bounded between 0 and 1 indicates weight of the observation versus the background</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The divergence per residue bounded between 0 and 1. 1 is more divergent from background, i.e. [0.732, ...]</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def jensen_shannon_divergence(sequence_frequencies: np.ndarray, background_aa_freq: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Calculate Jensen-Shannon Divergence value for all residues against a background frequency dict\n\n    Args:\n        sequence_frequencies: [[0.05, 0.001, 0.1, ...], ...]\n        background_aa_freq: [0.11, 0.03, 0.53, ...]\n\n    Keyword Args:\n        lambda_: float = 0.5 - Bounded between 0 and 1 indicates weight of the observation versus the background\n\n    Returns:\n        The divergence per residue bounded between 0 and 1. 1 is more divergent from background, i.e. [0.732, ...]\n    \"\"\"\n    return np.array([js_divergence(sequence_frequencies[idx], background_aa_freq, **kwargs)\n                     for idx in range(len(sequence_frequencies))])\n</code></pre>"},{"location":"reference/metrics/#metrics.position_specific_jsd","title":"position_specific_jsd","text":"<pre><code>position_specific_jsd(msa: ndarray, background: ndarray, **kwargs) -&gt; ndarray\n</code></pre> <p>Generate the Jensen-Shannon Divergence for a dictionary of residues versus a specific background frequency</p> <p>Both msa and background must be the same index</p> <p>Parameters:</p> <ul> <li> <code>msa</code>             (<code>ndarray</code>)         \u2013          <p>{15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}</p> </li> <li> <code>background</code>             (<code>ndarray</code>)         \u2013          <p>{0: {'A': 0, 'R': 0, ...}, 1: {}, ...} Containing residue index with inner dictionary of single amino acid types</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>lambda_</code>         \u2013          <p>float = 0.5 - Bounded between 0 and 1 indicates weight of the observation versus the background</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The divergence values per position, i.e [0.732, 0.552, ...]</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def position_specific_jsd(msa: np.ndarray, background: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Generate the Jensen-Shannon Divergence for a dictionary of residues versus a specific background frequency\n\n    Both msa and background must be the same index\n\n    Args:\n        msa: {15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}\n        background: {0: {'A': 0, 'R': 0, ...}, 1: {}, ...}\n            Containing residue index with inner dictionary of single amino acid types\n\n    Keyword Args:\n        lambda_: float = 0.5 - Bounded between 0 and 1 indicates weight of the observation versus the background\n\n    Returns:\n        The divergence values per position, i.e [0.732, 0.552, ...]\n    \"\"\"\n    return np.array([js_divergence(msa[idx], background[idx], **kwargs) for idx in range(len(msa))])\n</code></pre>"},{"location":"reference/metrics/#metrics.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(frequencies: ndarray, bgd_frequencies: ndarray, per_entry: bool = False, mask: array = None, axis: int | tuple[int, ...] = None) -&gt; ndarray | float\n</code></pre> <p>Calculate Kullback\u2013Leibler Divergence entropy between observed and background (true) frequency distribution(s)</p> <p>The divergence will be summed across the last axis/dimension of the input array</p> <p>Parameters:</p> <ul> <li> <code>frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0.05, 0.001, 0.1, ...] The model distribution</p> </li> <li> <code>bgd_frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0, 0, ...] The true distribution</p> </li> <li> <code>per_entry</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the result should be returned after summation over the last axis</p> </li> <li> <code>mask</code>             (<code>array</code>, default:                 <code>None</code> )         \u2013          <p>A mask to restrict calculations to certain entries</p> </li> <li> <code>axis</code>             (<code>int | tuple[int, ...]</code>, default:                 <code>None</code> )         \u2013          <p>If the input should be summed over additional axis, which one(s)?</p> </li> </ul> <p>Returns:     The additional entropy needed to represent the frequencies as the background frequencies.         The minimum divergence is 0 when both distributions are identical</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def kl_divergence(frequencies: np.ndarray, bgd_frequencies: np.ndarray, per_entry: bool = False,\n                  mask: np.array = None, axis: int | tuple[int, ...] = None) \\\n        -&gt; np.ndarray | float:\n    \"\"\"Calculate Kullback\u2013Leibler Divergence entropy between observed and background (true) frequency distribution(s)\n\n    The divergence will be summed across the last axis/dimension of the input array\n\n    Args:\n        frequencies: [0.05, 0.001, 0.1, ...] The model distribution\n        bgd_frequencies: [0, 0, ...] The true distribution\n        per_entry: Whether the result should be returned after summation over the last axis\n        mask: A mask to restrict calculations to certain entries\n        axis: If the input should be summed over additional axis, which one(s)?\n    Returns:\n        The additional entropy needed to represent the frequencies as the background frequencies.\n            The minimum divergence is 0 when both distributions are identical\n    \"\"\"\n    probs1 = bgd_frequencies * np.log(bgd_frequencies/frequencies)\n    kl_per_entry = np.sum(np.where(np.isnan(probs1), 0, probs1), axis=-1)\n\n    if per_entry:\n        return -kl_per_entry\n    elif mask is None:\n        return -np.sum(kl_per_entry, axis=axis)\n    else:\n        return -np.sum(kl_per_entry * mask, axis=axis) / np.sum(mask, axis=axis)\n</code></pre>"},{"location":"reference/metrics/#metrics.cross_entropy","title":"cross_entropy","text":"<pre><code>cross_entropy(frequencies: ndarray, bgd_frequencies: ndarray, per_entry: bool = False, mask: array = None, axis: int | tuple[int, ...] = None) -&gt; ndarray | float\n</code></pre> <p>Calculate the cross entropy between observed and background (truth) frequency distribution(s)</p> <p>The entropy will be summed across the last axis/dimension of the input array</p> <p>Parameters:</p> <ul> <li> <code>frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0.05, 0.001, 0.1, ...] The model distribution</p> </li> <li> <code>bgd_frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0, 0, ...] The true distribution</p> </li> <li> <code>per_entry</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the result should be returned after summation over the last axis</p> </li> <li> <code>mask</code>             (<code>array</code>, default:                 <code>None</code> )         \u2013          <p>A mask to restrict calculations to certain entries</p> </li> <li> <code>axis</code>             (<code>int | tuple[int, ...]</code>, default:                 <code>None</code> )         \u2013          <p>If the input should be summed over additional axis, which one(s)?</p> </li> </ul> <p>Returns:     The total entropy to represent the frequencies as the background frequencies.         The minimum entropy is 0 where both distributions are identical</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def cross_entropy(frequencies: np.ndarray, bgd_frequencies: np.ndarray, per_entry: bool = False,\n                  mask: np.array = None, axis: int | tuple[int, ...] = None) \\\n        -&gt; np.ndarray | float:\n    \"\"\"Calculate the cross entropy between observed and background (truth) frequency distribution(s)\n\n    The entropy will be summed across the last axis/dimension of the input array\n\n    Args:\n        frequencies: [0.05, 0.001, 0.1, ...] The model distribution\n        bgd_frequencies: [0, 0, ...] The true distribution\n        per_entry: Whether the result should be returned after summation over the last axis\n        mask: A mask to restrict calculations to certain entries\n        axis: If the input should be summed over additional axis, which one(s)?\n    Returns:\n        The total entropy to represent the frequencies as the background frequencies.\n            The minimum entropy is 0 where both distributions are identical\n    \"\"\"\n    probs1 = bgd_frequencies * np.log(frequencies)\n    ce_per_entry = np.sum(np.where(np.isnan(probs1), 0, probs1), axis=-1)\n\n    if per_entry:\n        return -ce_per_entry\n    elif mask is None:\n        return -np.sum(ce_per_entry, axis=axis)\n    else:\n        return -np.sum(ce_per_entry * mask, axis=axis) / np.sum(mask, axis=axis)\n</code></pre>"},{"location":"reference/metrics/#metrics.js_divergence","title":"js_divergence","text":"<pre><code>js_divergence(frequencies: ndarray, bgd_frequencies: ndarray, lambda_: float = 0.5) -&gt; float\n</code></pre> <p>Calculate Jensen-Shannon Divergence value from observed and background (true) frequencies</p> <p>Parameters:</p> <ul> <li> <code>frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0.05, 0.001, 0.1, ...] The model distribution</p> </li> <li> <code>bgd_frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0, 0, ...] The true distribution</p> </li> <li> <code>lambda_</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>Bounded between 0 and 1 indicates weight of the observation versus the background</p> </li> </ul> <p>Returns:     Bounded between 0 and 1. 1 is more divergent from background frequencies</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def js_divergence(frequencies: np.ndarray, bgd_frequencies: np.ndarray, lambda_: float = 0.5) -&gt; float:\n    \"\"\"Calculate Jensen-Shannon Divergence value from observed and background (true) frequencies\n\n    Args:\n        frequencies: [0.05, 0.001, 0.1, ...] The model distribution\n        bgd_frequencies: [0, 0, ...] The true distribution\n        lambda_: Bounded between 0 and 1 indicates weight of the observation versus the background\n    Returns:\n        Bounded between 0 and 1. 1 is more divergent from background frequencies\n    \"\"\"\n    r = (lambda_ * frequencies) + ((1 - lambda_) * bgd_frequencies)\n    probs1 = frequencies * np.log2(frequencies / r)\n    probs2 = bgd_frequencies * np.log2(bgd_frequencies / r)\n    return (lambda_ * np.where(np.isnan(probs1), 0, probs1).sum()) \\\n        + ((1 - lambda_) * np.where(np.isnan(probs2), 0, probs2).sum())\n</code></pre>"},{"location":"reference/metrics/#metrics.position_specific_divergence","title":"position_specific_divergence","text":"<pre><code>position_specific_divergence(frequencies: ndarray, bgd_frequencies: ndarray, lambda_: float = 0.5) -&gt; ndarray\n</code></pre> <p>Calculate Jensen-Shannon Divergence value from observed and background frequencies</p> <p>Parameters:</p> <ul> <li> <code>frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0.05, 0.001, 0.1, ...]</p> </li> <li> <code>bgd_frequencies</code>             (<code>ndarray</code>)         \u2013          <p>[0, 0, ...]</p> </li> <li> <code>lambda_</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>Bounded between 0 and 1 indicates weight of the observation versus the background</p> </li> </ul> <p>Returns:     An array of divergences bounded between 0 and 1. 1 indicates frequencies are more divergent from background</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def position_specific_divergence(frequencies: np.ndarray, bgd_frequencies: np.ndarray, lambda_: float = 0.5) -&gt; \\\n        np.ndarray:\n    \"\"\"Calculate Jensen-Shannon Divergence value from observed and background frequencies\n\n    Args:\n        frequencies: [0.05, 0.001, 0.1, ...]\n        bgd_frequencies: [0, 0, ...]\n        lambda_: Bounded between 0 and 1 indicates weight of the observation versus the background\n    Returns:\n        An array of divergences bounded between 0 and 1. 1 indicates frequencies are more divergent from background\n    \"\"\"\n    r = (lambda_ * frequencies) + ((1 - lambda_) * bgd_frequencies)\n    with warnings.catch_warnings():\n        # Ignore all warnings related to np.nan\n        warnings.simplefilter('ignore')\n        probs1 = frequencies * np.log2(frequencies / r)\n        probs2 = bgd_frequencies * np.log2(bgd_frequencies / r)\n    return (lambda_ * np.where(np.isnan(probs1), 0, probs1).sum(axis=1)) \\\n        + ((1 - lambda_) * np.where(np.isnan(probs2), 0, probs2).sum(axis=1))\n</code></pre>"},{"location":"reference/metrics/#metrics.df_permutation_test","title":"df_permutation_test","text":"<pre><code>df_permutation_test(grouped_df: DataFrame, diff_s: Series, group1_size: int = 0, compare: str = 'mean', permutations: int = 1000) -&gt; Series\n</code></pre> <p>Run a permutation test on a dataframe with two categorical groups. Default uses mean to compare significance</p> <p>Parameters:</p> <ul> <li> <code>grouped_df</code>             (<code>DataFrame</code>)         \u2013          <p>The features of interest in samples from two groups of interest. Doesn't need to be sorted</p> </li> <li> <code>diff_s</code>             (<code>Series</code>)         \u2013          <p>The differences in each feature in the two groups after evaluating the 'compare' stat</p> </li> <li> <code>group1_size</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Size of the observations in group1</p> </li> <li> <code>compare</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Choose from any pandas.DataFrame attribute that collapses along a column. Other options might be median</p> </li> <li> <code>permutations</code>             (<code>int</code>, default:                 <code>1000</code> )         \u2013          <p>The number of permutations to perform</p> </li> </ul> <p>Returns:     Contains the p-value(s) of the permutation test using the 'compare' statistic against diff_s</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def df_permutation_test(grouped_df: pd.DataFrame, diff_s: pd.Series, group1_size: int = 0, compare: str = 'mean',\n                        permutations: int = 1000) -&gt; pd.Series:\n    \"\"\"Run a permutation test on a dataframe with two categorical groups. Default uses mean to compare significance\n\n    Args:\n        grouped_df: The features of interest in samples from two groups of interest. Doesn't need to be sorted\n        diff_s: The differences in each feature in the two groups after evaluating the 'compare' stat\n        group1_size: Size of the observations in group1\n        compare: Choose from any pandas.DataFrame attribute that collapses along a column. Other options might be median\n        permutations: The number of permutations to perform\n    Returns:\n        Contains the p-value(s) of the permutation test using the 'compare' statistic against diff_s\n    \"\"\"\n    permut_s_array = []\n    df_length = len(grouped_df)\n    for i in range(permutations):\n        shuffled_df = grouped_df.sample(n=df_length)\n        permut_s_array.append(getattr(shuffled_df.iloc[:group1_size, :], compare)().sub(\n            getattr(shuffled_df.iloc[group1_size:, :], compare)()))\n    # How many times the magnitude of the permuted comparison set is less than the magnitude of the difference set\n    # If permuted is less than, returns True, which when taking the mean (or other 'compare'), reflects 1 while False\n    # (more than/equal to the difference set) is 0.\n    # Essentially, the returned mean is the p-value, which indicates how significant the permutation test results are\n    abs_s = diff_s.abs()\n    bool_df = pd.DataFrame([permut_s.abs().gt(abs_s) for permut_s in permut_s_array])\n\n    return bool_df.mean()\n</code></pre>"},{"location":"reference/metrics/#metrics.filter_df_for_index_by_value","title":"filter_df_for_index_by_value","text":"<pre><code>filter_df_for_index_by_value(df: DataFrame, metrics: dict[str, list | dict | str | int | float]) -&gt; dict[str, list[Any]]\n</code></pre> <p>Retrieve the indices from a DataFrame which have column values passing an indicated operation threshold</p> <p>Parameters:</p> <ul> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>DataFrame to filter indices on</p> </li> <li> <code>metrics</code>             (<code>dict[str, list | dict | str | int | float]</code>)         \u2013          <p>{metric_name: [(operation (Callable), pre_operation (Callable), pre_kwargs (dict), value (Any)),], ...} {metric_name: 0.3, ...} OR {metric_name: {'direction': 'min', 'value': 0.3}, ...} to specify a sorting direction</p> </li> </ul> <p>Returns:     {metric_name: ['0001', '0002', ...], ...}</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def filter_df_for_index_by_value(df: pd.DataFrame, metrics: dict[str, list | dict | str | int | float]) \\\n        -&gt; dict[str, list[Any]]:\n    \"\"\"Retrieve the indices from a DataFrame which have column values passing an indicated operation threshold\n\n    Args:\n        df: DataFrame to filter indices on\n        metrics: {metric_name: [(operation (Callable), pre_operation (Callable), pre_kwargs (dict), value (Any)),], ...}\n            {metric_name: 0.3, ...} OR\n            {metric_name: {'direction': 'min', 'value': 0.3}, ...} to specify a sorting direction\n    Returns:\n        {metric_name: ['0001', '0002', ...], ...}\n    \"\"\"\n    filtered_indices = {}\n    print_filters = []\n    for metric_name, filter_ops in metrics.items():\n        if isinstance(filter_ops, list):\n            multiple_ops = True if len(filter_ops) &gt; 1 else False\n            # Where the metrics = {metric: [(operation, pre_operation, pre_kwargs, value),], ...}\n            for idx, filter_op in enumerate(filter_ops, 1):\n                operation, pre_operation, pre_kwargs, value = filter_op\n                print_filters.append((metric_name, f'{flags.operator_strings[operation]} {value}'))\n\n                try:\n                    prepared_df = pre_operation(df[metric_name], **pre_kwargs)\n                except KeyError:  # metric_name is missing from df\n                    logger.error(f\"The metric {metric_name} wasn't available in the DataFrame\")\n                    filtered_df = df\n                else:\n                    # if isinstance(value, Iterable):\n                    #     # In the case that contains or not_contains operators are used\n                    #     filtered_indices = []\n                    #     print(value)\n                    #     for value_ in value:\n                    #         print(value_)\n                    #         op_return = operation(prepared_df, value_)\n                    #         print(op_return)\n                    #         filtered_df = df[op_return]\n                    #         # filtered_df = df[operation(prepared_df, value_)]\n                    #         filtered_indices.append(filtered_df.index.tolist())\n                    #     filtered_indices = index_intersection(filtered_indices)\n                    # else:\n                    filtered_df = df[operation(prepared_df, value)]\n                    # filtered_indices_ = filtered_df.index.tolist()\n\n                # Save found indices\n                if multiple_ops:\n                    # Add and index as the metric_name could be used a couple of times\n                    filter_name = f'{metric_name}({idx})'\n                else:\n                    filter_name = metric_name\n                filtered_indices[filter_name] = filtered_df.index.tolist()\n            # Currently below operations aren't necessary because of how index_intersection works\n            #  indices = operation1(pre_operation(**kwargs)[metric], value)\n            #  AND if more than one argument, only 2 args are possible...\n            #  indices = np.logical_and(operation1(pre_operation(**kwargs)[metric], value), operation2(*args))\n        else:\n            if isinstance(filter_ops, dict):\n                specification = filter_ops.get('direction')  # Todo make an ability to use boolean?\n                # Todo convert specification options 'greater' '&gt;' 'greater than' to 'max'/'min'\n                filter_ops = filter_ops.get('value', 0.)\n            else:\n                substituted_metric_name = metric_name.translate(utils.remove_digit_table)\n                specification = filter_df.loc['direction', substituted_metric_name]\n\n            if specification == 'max':\n                filtered_indices[metric_name] = df[df[metric_name] &gt;= filter_ops].index.tolist()\n                operator_string = '&gt;='\n            elif specification == 'min':\n                filtered_indices[metric_name] = df[df[metric_name] &lt;= filter_ops].index.tolist()\n                operator_string = '&lt;='\n            # Add to the filters\n            print_filters.append((metric_name, f'{operator_string} {filter_ops}'))\n\n    # Report the filtering options\n    logger.info('Applied filters:\\n\\t%s' % '\\n\\t'.join(utils.pretty_format_table(print_filters)))\n\n    return filtered_indices\n</code></pre>"},{"location":"reference/metrics/#metrics.prioritize_design_indices","title":"prioritize_design_indices","text":"<pre><code>prioritize_design_indices(df: DataFrame | AnyStr, filters: dict | bool = None, weights: dict | bool = None, protocols: str | list[str] = None, default_weight: str = 'interface_energy', **kwargs) -&gt; DataFrame\n</code></pre> <p>Return a filtered/sorted DataFrame (both optional) with indices that pass a set of filters and/or are ranked according to a feature importance. Both filter and weight instructions are provided or queried from the user</p> <p>Caution: Expects that if DataFrame is provided by filename there is particular formatting, i.e. 3 column MultiIndices, 1 index indices. If the DF file varies from this, this function will likely cause errors</p> <p>Parameters:</p> <ul> <li> <code>df</code>             (<code>DataFrame | AnyStr</code>)         \u2013          <p>DataFrame to filter/weight indices</p> </li> <li> <code>filters</code>             (<code>dict | bool</code>, default:                 <code>None</code> )         \u2013          <p>Whether to remove viable candidates by certain metric values or a mapping of value and filter threshold pairs</p> </li> <li> <code>weights</code>             (<code>dict | bool</code>, default:                 <code>None</code> )         \u2013          <p>Whether to rank the designs by metric values or a mapping of value and weight pairs where the total weight will be the sum of all individual weights</p> </li> <li> <code>protocols</code>             (<code>str | list[str]</code>, default:                 <code>None</code> )         \u2013          <p>Whether specific design protocol(s) should be chosen</p> </li> <li> <code>default_weight</code>             (<code>str</code>, default:                 <code>'interface_energy'</code> )         \u2013          <p>If there is no weight provided, what is the default metric to sort results</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>weight_function</code>         \u2013          <p>str = 'rank' - The function to use when weighting design indices</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>The sorted DataFrame based on the provided filters and weights. DataFrame contains simple Index columns</p> </li> </ul> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def prioritize_design_indices(df: pd.DataFrame | AnyStr, filters: dict | bool = None, weights: dict | bool = None,\n                              protocols: str | list[str] = None, default_weight: str = 'interface_energy', **kwargs) \\\n        -&gt; pd.DataFrame:\n    \"\"\"Return a filtered/sorted DataFrame (both optional) with indices that pass a set of filters and/or are ranked\n    according to a feature importance. Both filter and weight instructions are provided or queried from the user\n\n    Caution: Expects that if DataFrame is provided by filename there is particular formatting, i.e. 3 column\n    MultiIndices, 1 index indices. If the DF file varies from this, this function will likely cause errors\n\n    Args:\n        df: DataFrame to filter/weight indices\n        filters: Whether to remove viable candidates by certain metric values or a mapping of value and filter threshold\n            pairs\n        weights: Whether to rank the designs by metric values or a mapping of value and weight pairs where the total\n            weight will be the sum of all individual weights\n        protocols: Whether specific design protocol(s) should be chosen\n        default_weight: If there is no weight provided, what is the default metric to sort results\n\n    Keyword Args:\n        weight_function: str = 'rank' - The function to use when weighting design indices\n\n    Returns:\n        The sorted DataFrame based on the provided filters and weights. DataFrame contains simple Index columns\n    \"\"\"\n    # Grab pose info from the DateFrame and drop all classifiers in top two rows.\n    if isinstance(df, pd.DataFrame):\n        if 3 - df.columns.nlevels &gt; 0:\n            df = pd.concat([df], axis=1, keys=[tuple('pose' for _ in range(3 - df.columns.nlevels))])\n    else:\n        df = pd.read_csv(df, index_col=0, header=[0, 1, 2])\n        df.replace({False: 0, True: 1, 'False': 0, 'True': 1}, inplace=True)\n\n    if protocols:  # is not None:\n        raise NotImplementedError(\n            \"Can't filter by protocol yet. Fix upstream protocol inclusion in df\")\n        if isinstance(protocols, str):\n            # Add protocol to a list\n            protocols = [protocols]\n\n        try:\n            protocol_df = df.loc[:, idx_slice[protocols, protocol_column_types, :]]\n        except KeyError:\n            logger.warning(f\"Protocol(s) '{protocols}' weren't found in the set of designs. \"\n                           \"Skipping prioritize by protocol... \")\n            # available_protocols = df.columns.get_level_values(0).unique()\n            # while True:\n            #     protocols = input(f'What protocol would you like to choose?{describe_string}\\n'\n            #                       f'Available options are: {\", \".join(available_protocols)}{input_string}')\n            #     if protocols in available_protocols:\n            #         protocols = [protocols]  # todo make multiple protocols available for input ^\n            #         break\n            #     elif protocols in describe:\n            #         describe_data(df=df)\n            #     else:\n            #         print(f'Invalid protocol {protocols}. Please choose one of {\", \".join(available_protocols)}')\n            # protocol_df = df.loc[:, idx_slice[protocols, protocol_column_types, :]]\n        else:\n            protocol_df.dropna(how='all', inplace=True, axis=0)  # drop completely empty rows in case of groupby ops\n            # Ensure 'dock'ing data is present in all protocols\n            simple_df = pd.merge(df.loc[:, idx_slice[['pose'], ['dock'], :]], protocol_df, left_index=True, right_index=True)\n            logger.info(f'Number of designs after protocol selection: {len(simple_df)}')\n    else:\n        protocols = ['pose']  # Todo change to :?\n        simple_df = df.loc[:, idx_slice[protocols, df.columns.get_level_values(1) != 'std', :]]\n\n    # This is required for a multi-index column where the different protocols are in the top row of the df columns\n    simple_df = pd.concat([simple_df.loc[:, idx_slice[prot, :, :]].droplevel(0, axis=1).droplevel(0, axis=1)\n                           for prot in protocols])\n    simple_df.dropna(how='all', inplace=True, axis=0)\n    # simple_df = simple_df.droplevel(0, axis=1).droplevel(0, axis=1)  # simplify headers\n\n    if filters is not None:\n        if filters and isinstance(filters, dict):\n            # These were passed as parsed values\n            pass\n        else:  # --filter was provided, but as a boolean-esq dict. Query the user for them\n            available_filters = simple_df.columns.tolist()\n            filters = query_user_for_metrics(available_filters, df=simple_df, mode='filter', level='design')\n        logger.info(f'Number of starting designs: {len(df)}')\n        # When df is not ranked by percentage\n        # _filters = {metric: {'direction': filter_df.loc['direction', metric], 'value': value}\n        #             for metric, value in filters.items()}\n\n        # Filter the DataFrame to include only those values which are le/ge the specified filter\n        filtered_indices = filter_df_for_index_by_value(simple_df, filters)  # **_filters)\n        # filtered_indices = {metric: filters_with_idx[metric]['idx'] for metric in filters_with_idx}\n        logger.info('Number of designs passing filters:\\n\\t%s' %\n                    '\\n\\t'.join(utils.pretty_format_table([(metric, '=', len(indices))\n                                                           for metric, indices in filtered_indices.items()])))\n        # logger.info('Number of designs passing filters:\\n\\t%s'\n        #             % '\\n\\t'.join(f'{len(indices):6d} - {metric}' for metric, indices in filtered_indices.items()))\n        final_indices = index_intersection(filtered_indices.values())\n        number_final_indices = len(final_indices)\n        if number_final_indices == 0:\n            raise utils.MetricsError('There are no poses left after filtering. Try choosing less stringent values')\n        logger.info(f'Number of designs passing all filters: {number_final_indices}')\n        simple_df = simple_df.loc[final_indices, :]\n\n    # {column: {'direction': min_, 'value': 0.3, 'idx_slice': ['0001', '0002', ...]}, ...}\n    # if weight is not None or default_weight in simple_df.columns:\n    if weights:\n        if isinstance(weights, dict):\n            # These were passed as parsed values\n            pass\n        else:  # --weight was provided, but as a boolean-esq dict. Query the user for them\n            available_metrics = simple_df.columns.tolist()\n            weights = query_user_for_metrics(available_metrics, df=simple_df, mode='weight', level='design')\n    elif default_weight in simple_df.columns:\n        weights = None\n    else:\n        # raise KeyError(\n        logger.warning(\n            f\"No 'weight' provided and couldn't find the metric key {default_weight} in the DataFrame\\n\")\n        # f\"Available metric keys: {simple_df.columns.tolist()}\")\n        return simple_df\n\n    ranking_s = pareto_optimize_trajectories(simple_df, weights=weights, default_weight=default_weight, **kwargs)\n    # Using the sorted indices of the ranking_s, rename, then join the existing df indices to it\n    # This maintains ranking order\n    final_df = ranking_s.rename(selection_weight_column).to_frame().join(simple_df)\n\n    return final_df\n</code></pre>"},{"location":"reference/metrics/#metrics.describe_data","title":"describe_data","text":"<pre><code>describe_data(df: DataFrame = None) -&gt; None\n</code></pre> <p>Describe the DataFrame to STDOUT</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def describe_data(df: pd.DataFrame = None) -&gt; None:\n    \"\"\"Describe the DataFrame to STDOUT\"\"\"\n    print('The available metrics are located in the top row(s) of your DataFrame. Enter your selected metrics as a '\n          'comma separated input. To see descriptions for only certain metrics, enter them here. '\n          'Otherwise, hit \"Enter\"')\n    metrics_input = input(input_string)\n    chosen_metrics = set(map(str.lower, map(str.replace, map(str.strip, metrics_input.strip(',').split(',')),\n                                            repeat(' '), repeat('_'))))\\\n        .difference({''})  # Remove \"Enter\" input if that was provided\n\n    if not chosen_metrics:\n        columns_of_interest = slice(None)\n    else:\n        columns_of_interest = [idx for idx, column in enumerate(df.columns.get_level_values(-1).tolist())\n                               if column in chosen_metrics]\n    # Format rows/columns for data display, then revert\n    max_columns, min_columns = pd.get_option('display.max_columns'), pd.get_option('display.max_rows')\n    pd.set_option('display.max_columns', None), pd.set_option('display.max_rows', None)\n    print(df.iloc[:, columns_of_interest].describe())\n    pd.set_option('display.max_columns', max_columns), pd.set_option('display.max_rows', min_columns)\n</code></pre>"},{"location":"reference/metrics/#metrics.query_user_for_metrics","title":"query_user_for_metrics","text":"<pre><code>query_user_for_metrics(available_metrics: Iterable[str], df: DataFrame = None, mode: str = None, level: str = None) -&gt; dict[str, float]\n</code></pre> <p>Ask the user for the desired metrics to select indices from a dataframe</p> <p>Parameters:</p> <ul> <li> <code>available_metrics</code>             (<code>Iterable[str]</code>)         \u2013          <p>The columns available in the DataFrame to select indices by</p> </li> <li> <code>df</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>A DataFrame from which to use metrics (provided as columns)</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The mode in which to query and format metrics information. Either 'filter' or weight'</p> </li> <li> <code>level</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The hierarchy of selection to use. Could be one of 'poses', 'designs', or 'sequences'</p> </li> </ul> <p>Returns:     The mapping of metric name to value</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>@utils.handle_errors(errors=(KeyboardInterrupt,))\ndef query_user_for_metrics(available_metrics: Iterable[str], df: pd.DataFrame = None, mode: str = None,\n                           level: str = None) -&gt; dict[str, float]:\n    \"\"\"Ask the user for the desired metrics to select indices from a dataframe\n\n    Args:\n        available_metrics: The columns available in the DataFrame to select indices by\n        df: A DataFrame from which to use metrics (provided as columns)\n        mode: The mode in which to query and format metrics information. Either 'filter' or weight'\n        level: The hierarchy of selection to use. Could be one of 'poses', 'designs', or 'sequences'\n    Returns:\n        The mapping of metric name to value\n    \"\"\"\n    try:\n        direction = dict(max='higher', min='lower')\n        instructions = \\\n            {'filter': '\\nFor each metric, choose values based on supported literature or design goals to eliminate '\n                       \"designs that are certain to fail or have sub-optimal features. Ensure your cutoffs aren't too \"\n                       'exclusive. If you end up with no designs, try relaxing your filter values.',\n             'weight':\n                 '\\nFor each metric, choose a percentage signifying the metrics contribution to the total selection '\n                 'weight. The weight will be used as a linear combination of all weights according to each designs rank'\n                 ' within the specified metric category. For instance, typically the total weight should equal 1. When '\n                 'choosing 5 metrics, you can assign an equal weight to each (specify 0.2 for each) or you can weight '\n                 'several more strongly (0.3, 0.3, 0.2, 0.1, 0.1). When ranking occurs, for each selected metric the '\n                 'metric will be sorted and designs in the top percentile will be given their percentage of the full '\n                 'weight. Top percentile is defined as the most advantageous score, so the top percentile of energy is '\n                 'lowest, while for hydrogen bonds it would be the most.'}\n\n        print('\\n%s' % header_string % f'Select {level} {mode} Metrics')\n        print(f'The provided dataframe will be used to select {level}s based on the measured metrics from each pose. '\n              f'To \"{mode}\" designs, which metrics would you like to utilize?'\n              f'{\"\" if df is None else describe_string}')\n\n        print('The available metrics are located in the top row(s) of your DataFrame. Enter your selected metrics as a '\n              'comma separated input or alternatively, you can check out the available metrics by entering \"metrics\".'\n              '\\nEx: \"shape_complementarity, contact_count, etc.\"')\n        metrics_input = input(input_string)\n        chosen_metrics = set(map(str.lower, map(str.replace, map(str.strip, metrics_input.strip(',').split(',')),\n                                                repeat(' '), repeat('_'))))\n        available_metrics = sorted(available_metrics)\n        while True:  # unsupported_metrics or 'metrics' in chosen_metrics:\n            unsupported_metrics = chosen_metrics.difference(available_metrics)\n            if 'metrics' in chosen_metrics:\n                print(f'You indicated \"metrics\". Here are available metrics:\\n{\", \".join(available_metrics)}\\n')\n                metrics_input = input(input_string)\n            elif chosen_metrics.intersection(describe):\n                describe_data(df=df) if df is not None else print(\"Can't describe data without providing a DataFrame\")\n                # df.describe() if df is not None else print('Can\\'t describe data without providing a DataFrame...')\n                metrics_input = input(input_string)\n            elif unsupported_metrics:\n                # TODO catch value error in dict comprehension upon string input\n                metrics_input = input(f'Metric{\"s\" if len(unsupported_metrics) &gt; 1 else \"\"} '\n                                      f'\"{\", \".join(unsupported_metrics)}\" not found in the DataFrame!'\n                                      '\\nIs your spelling correct? Have you used the correct underscores? '\n                                      f'Please input these metrics again. Specify \"metrics\" to view available metrics'\n                                      f'{input_string}')\n            elif len(chosen_metrics) &gt; 0:\n                # We have no errors and there are metrics\n                break\n            else:\n                input_flag = flags.format_args(flags.filter_args) if mode == \"filter\" \\\n                    else flags.format_args(flags.weight_args)\n                print(\"Metrics weren't provided... If this is what you want, run this module without the \"\n                      f'{input_flag} flag')\n                if verify_choice():\n                    break\n            fixed_metrics = list(map(str.lower, map(str.replace, map(str.strip, metrics_input.strip(',').split(',')),\n                                                    repeat(' '), repeat('_'))))\n            chosen_metrics = chosen_metrics.difference(unsupported_metrics).union(fixed_metrics)\n            # unsupported_metrics = set(chosen_metrics).difference(available_metrics)\n\n        print(instructions[mode])\n        while True:  # not correct:  # correct = False\n            print(\"\" if df is None else describe_string)\n            metric_values = {}\n            for metric in chosen_metrics:\n                # Modify the provided metric of digits to get its configuration info\n                substituted_metric = metric.translate(utils.remove_digit_table)\n                while True:\n                    # Todo make ability to use boolean descriptions\n                    # Todo make ability to specify direction\n                    value = input(f'For \"{metric}\" what value should be used for {level} {mode}ing? %s{input_string}'\n                                  % ('Designs with metrics %s than this value will be included' %\n                                     direction[filter_df.loc['direction', substituted_metric]].upper()\n                                     if mode == \"filter\" else \"\"))\n                    if value in describe:\n                        describe_data(df=df) if df is not None \\\n                            else print(\"Can't describe data without providing a DataFrame...\")\n                    elif validate_type(value, dtype=float):\n                        metric_values[metric] = float(value)\n                        break\n\n            # metric_values = {metric: float(input('For \"%s\" what value should be used for %s %sing?%s%s'\n            #                                      % (metric, level, mode,\n            #                                         ' Designs with metrics %s than this value will be included'\n            #                                         % direction[filter_df.loc['direction', metric]].upper()\n            #                                         if mode == 'filter' else '', input_string)))\n            #                  for metric in chosen_metrics}\n            if metric_values:\n                print('You selected:\\n\\t%s' % '\\n\\t'.join(utils.pretty_format_table(metric_values.items())))\n            else:\n                # print('No metrics were provided, skipping value input')\n                # metric_values = None\n                break\n\n            if verify_choice():\n                break\n    except KeyboardInterrupt:\n        print('\\nSelection was ended by Ctrl-C!')\n        sys.exit(1)\n\n    return metric_values\n</code></pre>"},{"location":"reference/metrics/#metrics.pareto_optimize_trajectories","title":"pareto_optimize_trajectories","text":"<pre><code>pareto_optimize_trajectories(df: DataFrame, weights: dict[str, float] = None, function: weight_functions_literal = 'rank', default_weight: str = 'interface_energy', **kwargs) -&gt; Series\n</code></pre> <p>From a provided DataFrame with individual design trajectories, select trajectories based on provided metric and weighting parameters</p> <p>Parameters:</p> <ul> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>The designs x metrics DataFrame (single index metrics column) to select trajectories from</p> </li> <li> <code>weights</code>             (<code>dict[str, float]</code>, default:                 <code>None</code> )         \u2013          <p>{'metric': value, ...}. If not provided, sorts by default_sort</p> </li> <li> <code>function</code>             (<code>weight_functions_literal</code>, default:                 <code>'rank'</code> )         \u2013          <p>The function to use for weighting. Either 'rank' or 'normalize' is possible</p> </li> <li> <code>default_weight</code>             (<code>str</code>, default:                 <code>'interface_energy'</code> )         \u2013          <p>The metric to weight the dataframe by default if no weights are provided</p> </li> </ul> <p>Returns:     A sorted pandas.Series with the best indices first in the Series.index, and the resulting optimization values     in the corresponding value.</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def pareto_optimize_trajectories(df: pd.DataFrame, weights: dict[str, float] = None,\n                                 function: config.weight_functions_literal = 'rank',\n                                 default_weight: str = 'interface_energy', **kwargs) -&gt; pd.Series:\n    \"\"\"From a provided DataFrame with individual design trajectories, select trajectories based on provided metric and\n    weighting parameters\n\n    Args:\n        df: The designs x metrics DataFrame (single index metrics column) to select trajectories from\n        weights: {'metric': value, ...}. If not provided, sorts by default_sort\n        function: The function to use for weighting. Either 'rank' or 'normalize' is possible\n        default_weight: The metric to weight the dataframe by default if no weights are provided\n    Returns:\n        A sorted pandas.Series with the best indices first in the Series.index, and the resulting optimization values\n        in the corresponding value.\n    \"\"\"\n    if weights:  # Could be None or empty dict\n        # weights = {metric: dict(direction=filter_df.loc['direction', metric], value=value)\n        #            for metric, value in weights.items()}\n        coefficients = {}\n        print_weights = []\n        for metric_name, weight_ops in weights.items():\n            # Modify the provided metric of digits to get its configuration info\n            substituted_metric = metric_name.translate(utils.remove_digit_table)\n            direction = filter_df.loc['direction', substituted_metric]\n            if isinstance(weight_ops, list):\n                # Where the metrics = {metric: [(operation, pre_operation, pre_kwargs, value),], ...}\n                # Currently, can only have one weight per metric...\n                for idx, weight_op in enumerate(weight_ops):\n                    operation, pre_operation, pre_kwargs, value = weight_op\n                    coefficients[metric_name] = dict(direction=direction, value=value)\n                    print_weights.append((metric_name, f'= {value}'))\n            else:  # weight_ops is just the value\n                coefficients[metric_name] = dict(direction=direction, value=weight_ops)\n                print_weights.append((metric_name, f'= {weight_ops}'))\n\n        metric_df = {}\n        if function == 'rank':\n            # This puts small and negative value (when min is chosen) with higher rank\n            sort_direction = dict(max=True, min=False)  # max - ascending=True, min - ascending=False\n\n            for metric_name, parameters in coefficients.items():\n                direction = parameters['direction']\n                try:\n                    metric_series = \\\n                        df[metric_name].rank(ascending=sort_direction[direction], method=direction, pct=True) \\\n                        * parameters['value']\n                except KeyError:  # metric_name is missing from df\n                    logger.error(f\"{pareto_optimize_trajectories.__name__}: The metric {metric_name} wasn't available \"\n                                 \"for weighting in the given DataFrame\")\n                    continue\n                metric_df[metric_name] = metric_series\n            # df = pd.concat({metric: df[metric].rank(ascending=sort_direction[parameters['direction']],\n            #                                         method=parameters['direction'], pct=True) * parameters['value']\n            #                 for metric, parameters in weights.items()}, axis=1)\n        elif function == 'normalize':  # Get the MinMax normalization (df - df.min()) / (df.max() - df.min())\n            for metric_name, parameters in coefficients.items():\n                metric_s = df[metric_name]\n                if parameters['direction'] == 'max':\n                    metric_min = metric_s.min()\n                    metric_max = metric_s.max()\n                else:  # parameters['direction'] == 'min':\n                    metric_min = metric_s.max()\n                    metric_max = metric_s.min()\n                metric_df[metric_name] = \\\n                    ((metric_s - metric_min) / (metric_max - metric_min)) * parameters['value']\n        else:\n            raise ValueError(f\"The value {function} isn't a viable choice for metric weighting 'function'\")\n\n        if metric_df:\n            logger.info('Applied weights:\\n\\t%s' % '\\n\\t'.join(utils.pretty_format_table(print_weights)))\n            weighted_df = pd.concat(metric_df, axis=1)\n            return weighted_df.sum(axis=1).sort_values(ascending=False)\n\n    if default_weight in df.columns:\n        # For sort_values(), this sorts the right direction, while for rank() it sorts incorrectly\n        sort_direction = dict(max=False, min=True)  # max - ascending=False, min - ascending=True\n\n        # Just sort by the default\n        direction = filter_df.loc['direction', default_weight]\n        # return df.sort_values(default_sort, ascending=sort_direction[direction])\n        return df[default_weight].sort_values(ascending=sort_direction[direction])\n    else:\n        raise KeyError(f\"There wasn't a metric named '{default_weight}' which was specified as the default\")\n</code></pre>"},{"location":"reference/metrics/#metrics.window_function","title":"window_function","text":"<pre><code>window_function(data: Sequence[int | float], windows: Iterable[int] = None, lower: int = None, upper: int = None) -&gt; ndarray\n</code></pre> <p>Perform windowing operations on a sequence of data and return the result of the calculation. Window lengths can be specified by passing the windows to perform calculation on as an Iterable or by a range of window lengths</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Sequence[int | float]</code>)         \u2013          <p>The sequence of numeric data to perform calculations</p> </li> <li> <code>windows</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>An iterable of window lengths to use. If a single, pass as the Iterable</p> </li> <li> <code>lower</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The lower range of the window to operate on. \"window\" is inclusive of this value</p> </li> <li> <code>upper</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The upper range of the window to operate on. \"window\" is inclusive of this value</p> </li> </ul> <p>Returns:     The (number of windows, length of data) array of values with each requested window along axis=0         and the particular value of the windowed data along axis=1</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def window_function(data: Sequence[int | float], windows: Iterable[int] = None, lower: int = None,\n                    upper: int = None) -&gt; np.ndarray:\n    \"\"\"Perform windowing operations on a sequence of data and return the result of the calculation. Window lengths can\n    be specified by passing the windows to perform calculation on as an Iterable or by a range of window lengths\n\n    Args:\n        data: The sequence of numeric data to perform calculations\n        windows: An iterable of window lengths to use. If a single, pass as the Iterable\n        lower: The lower range of the window to operate on. \"window\" is inclusive of this value\n        upper: The upper range of the window to operate on. \"window\" is inclusive of this value\n    Returns:\n        The (number of windows, length of data) array of values with each requested window along axis=0\n            and the particular value of the windowed data along axis=1\n    \"\"\"\n    array_length = len(data)\n    if windows is None:\n        if lower is not None and upper is not None:\n            windows = list(range(lower, upper + 1))  # +1 makes inclusive in range\n        else:\n            raise ValueError(f'{window_function.__name__}:'\n                             f' Must provide either window, or lower and upper')\n\n    # Make an array with axis=0 equal to number of windows used, axis=1 equal to length of values\n    # range_size = len(windows)\n    # data_template = [0 for _ in range(array_length)]\n    window_array = np.zeros((len(windows), array_length))\n    for array_idx, window_size in enumerate(windows):  # Make the divisor a float\n        half_window = math.floor(window_size / 2)  # how far on each side should the window extend\n        # # Calculate score accordingly, with cases for N- and C-terminal windows\n        # for data_idx in range(half_window):  # N-terminus windows\n        #     # add 1 as high slice not inclusive\n        #     window_array[array_idx, data_idx] = sequence_array[:data_idx + half_window + 1].sum() / window_size\n        # for data_idx in range(half_window, array_length-half_window):  # continuous length windows\n        #     # add 1 as high slice not inclusive\n        #     window_array[array_idx, data_idx] = \\\n        #         sequence_array[data_idx - half_window: data_idx + half_window+1].sum() / window_size\n        # for data_idx in range(array_length-half_window, array_length):  # C-terminus windows\n        #     # No add 1 as low slice inclusive\n        #     window_array[array_idx, data_idx] = sequence_array[data_idx - half_window:].sum() / window_size\n        #\n        # # check if the range is even, then subtract 1/2 of the value of trailing and leading window values\n        # if window_size % 2 == 0.:\n        #     # subtract_half_leading_residue = sequence_array[half_window:] * 0.5 / window_size\n        #     window_array[array_idx, :array_length - half_window] -= \\\n        #         sequence_array[half_window:] * 0.5 / window_size\n        #     # subtract_half_trailing_residue = sequence_array[:array_length - half_window] * 0.5 / window_size\n        #     window_array[array_idx, half_window:] -= \\\n        #         sequence_array[:array_length - half_window] * 0.5 / window_size\n\n        # Calculate score accordingly, with cases for N- and C-terminal windows\n        # array_length_range = range(array_length)\n        # # Make a \"zeros\" list\n        # data_window = [0 for _ in range(array_length)]\n        # window_data = copy(data_template)\n        # This would be the method if the slices need to be taken with respect to the c-term\n        # for end_idx, start_idx in enumerate(range(array_length - window_size), window_size):\n        # There is no off by one error if we slice lower or higher than list so include both termini\n        # for end_idx, start_idx in enumerate(range(array_length), window_size):\n        #     idx_sum = sum(data[start_idx:end_idx])\n        #     # for window_position in range(start_idx, end_idx + 1):\n        #     # # for window_position in range(data_idx - half_window, data_idx + half_window + 1):\n        #     #     idx_sum += sum(data[start_idx:end_idx])\n        #     window_data[data_idx] = idx_sum\n\n        # Calculate each score given the window. Accounts for window cases with N- and C-termini\n        # There is no off by one error if we slice lower or higher than list so include both termini\n        window_data = [sum(data[start_idx:end_idx])\n                       for end_idx, start_idx in enumerate(range(-array_length - half_window, -half_window),\n                                                           half_window + 1)]\n\n        # # Old python list method\n        # for data_idx in array_length_range:\n        #     idx_sum = 0\n        #     if data_idx &lt; half_window:  # N-terminus\n        #         for window_position in range(data_idx + half_window + 1):\n        #             idx_sum += data[window_position]\n        #     elif data_idx + half_window &gt;= array_length:  # C-terminus\n        #         for window_position in range(data_idx - half_window, array_length):\n        #             idx_sum += data[window_position]\n        #     else:\n        #         for window_position in range(data_idx - half_window, data_idx + half_window + 1):\n        #             idx_sum += data[window_position]\n        #\n        #     # Set each idx_sum to the idx in data_window\n        #     data_window[data_idx] = idx_sum\n        # Handle data_window incorporation into numpy array\n        window_array[array_idx] = window_data\n        window_array[array_idx] /= float(window_size)\n\n        # Account for windows that have even ranges\n        if window_size % 2 == 0.:  # The range is even\n            # Calculate a modifier to subtract from each of the data values given the original value and the window size\n            even_modifier = .5 / window_size\n            even_modified_data = [value * even_modifier for value in data]\n            # subtract_half_leading_residue = sequence_array[half_window:] * 0.5 / window_size\n            window_array[array_idx, :-half_window] -= even_modified_data[half_window:]\n            # subtract_half_trailing_residue = sequence_array[:array_length - half_window] * 0.5 / window_size\n            window_array[array_idx, half_window:] -= even_modified_data[:-half_window]\n\n    return window_array\n</code></pre>"},{"location":"reference/metrics/#metrics.hydrophobic_collapse_index","title":"hydrophobic_collapse_index","text":"<pre><code>hydrophobic_collapse_index(seq: Sequence[str | int] | ndarray, hydrophobicity: hydrophobicity_scale_literal = 'standard', custom: dict[protein_letters_literal, int | float] = None, alphabet_type: alphabet_types_literal = None, lower_window: int = 3, upper_window: int = 9, **kwargs) -&gt; ndarray\n</code></pre> <p>Calculate hydrophobic collapse index for sequence(s) of interest and return an HCI array</p> <p>Parameters:</p> <ul> <li> <code>seq</code>             (<code>Sequence[str | int] | ndarray</code>)         \u2013          <p>The sequence to measure. Can be a character based sequence (or array of sequences with shape (sequences, residues)), an integer based sequence, or a sequence profile like array (residues, alphabet) where each character in the alphabet contains a typical distribution of amino acid observations</p> </li> <li> <code>hydrophobicity</code>             (<code>hydrophobicity_scale_literal</code>, default:                 <code>'standard'</code> )         \u2013          <p>The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILVW), or provide one with the keyword argument, \"custom\"</p> </li> <li> <code>custom</code>             (<code>dict[protein_letters_literal, int | float]</code>, default:                 <code>None</code> )         \u2013          <p>A user defined mapping of amino acid type, hydrophobicity value pairs</p> </li> <li> <code>alphabet_type</code>             (<code>alphabet_types_literal</code>, default:                 <code>None</code> )         \u2013          <p>The amino acid alphabet if seq consists of integer characters</p> </li> <li> <code>lower_window</code>             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>The smallest window used to measure</p> </li> <li> <code>upper_window</code>             (<code>int</code>, default:                 <code>9</code> )         \u2013          <p>The largest window used to measure</p> </li> </ul> <p>Returns:     1D array with the hydrophobic collapse index at every position on the input sequence(s)</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def hydrophobic_collapse_index(seq: Sequence[str | int] | np.ndarray,\n                               hydrophobicity: hydrophobicity_scale_literal = 'standard',\n                               custom: dict[sequence.protein_letters_literal, int | float] = None,\n                               alphabet_type: sequence.alphabet_types_literal = None,\n                               lower_window: int = 3, upper_window: int = 9, **kwargs) -&gt; np.ndarray:\n    \"\"\"Calculate hydrophobic collapse index for sequence(s) of interest and return an HCI array\n\n    Args:\n        seq: The sequence to measure. Can be a character based sequence (or array of sequences with shape\n            (sequences, residues)), an integer based sequence, or a sequence profile like array (residues, alphabet)\n            where each character in the alphabet contains a typical distribution of amino acid observations\n        hydrophobicity: The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILVW),\n            or provide one with the keyword argument, \"custom\"\n        custom: A user defined mapping of amino acid type, hydrophobicity value pairs\n        alphabet_type: The amino acid alphabet if seq consists of integer characters\n        lower_window: The smallest window used to measure\n        upper_window: The largest window used to measure\n    Returns:\n        1D array with the hydrophobic collapse index at every position on the input sequence(s)\n    \"\"\"\n    if custom is None:\n        hydrophobicity_values = hydrophobicity_scale.get(hydrophobicity)\n        # hydrophobicity == 'background':  # Todo\n        if not hydrophobicity_values:\n            raise ValueError(f'The hydrophobicity \"{hydrophobicity}\" table is not available. Add it if you think it '\n                             f'should be')\n    else:\n        hydrophobicity_values = custom\n\n    def solve_alphabet() -&gt; sequence.alphabets_literal:\n        if alphabet_type is None:\n            raise ValueError(\n                f'{hydrophobic_collapse_index.__name__}: Must pass keyword \"alphabet_type\" when calculating '\n                f'using integer sequence values')\n        else:\n            alphabet_ = sequence.alphabet_type_to_alphabet.get(alphabet_type)\n            if alphabet_ is None:\n                if sequence.alphabet_to_alphabet_type.get(alphabet_type):\n                    alphabet_ = alphabet_type\n                else:\n                    raise ValueError(\n                        f\"{hydrophobic_collapse_index.__name__}: alphabet_type '{alphabet_type}' isn't a viable \"\n                        f'alphabet_type. Choose from {\", \".join(sequence.alphabet_types)} or pass an alphabet')\n\n            return alphabet_\n\n    if isinstance(seq[0], int):  # This is an integer sequence. An alphabet is required\n        alphabet = solve_alphabet()\n        values = [hydrophobicity_values[aa] for aa in alphabet]\n        sequence_array = [values[aa_int] for aa_int in seq]\n        # raise ValueError(f\"sequence argument with type {type(sequence).__name__} isn't supported\")\n    elif isinstance(seq[0], str):  # This is a string array # if isinstance(sequence[0], str):\n        sequence_array = [hydrophobicity_values.get(aa, 0) for aa in seq]\n        # raise ValueError(f\"sequence argument with type {type(sequence).__name__} isn't supported\")\n    elif isinstance(seq, (torch.Tensor, np.ndarray)):  # This is an integer sequence. An alphabet is required\n        if seq.dtype in utils.np_torch_int_float_types:\n            alphabet = solve_alphabet()\n            # torch.Tensor and np.ndarray can multiply by np.ndarray\n            values = np.array([hydrophobicity_values[aa] for aa in alphabet])\n            if seq.ndim == 2:\n                # print('HCI debug')\n                # print('array.shape', seq.shape, 'values.shape', values.shape)\n                # The array must have shape (number_of_residues, alphabet_length)\n                sequence_array = seq * values\n                # Ensure each position is a combination of the values for each amino acid\n                sequence_array = sequence_array.sum(axis=-1)\n                # print('sequence_array', sequence_array)\n            else:\n                raise ValueError(f\"Can't process a {seq.ndim}-dimensional array yet\")\n        else:  # We assume it is a sequence array with bytes?\n            # The array must have shape (number_of_residues, alphabet_length)\n            sequence_array = seq * np.vectorize(hydrophobicity_values.__getitem__)(seq)\n            # Ensure each position is a combination of the values for each amino acid in the array\n            sequence_array = sequence_array.mean(axis=-2)\n        # elif isinstance(sequence, Sequence):\n        #     sequence_array = [hydrophobicity_values.get(aa, 0) for aa in sequence]\n    else:\n        raise ValueError(f'The provided sequence must comprise the canonical amino acid string characters or '\n                         f'integer values corresponding to numerical amino acid conversions. '\n                         f'Got type={type(seq[0]).__name__} instead')\n\n    window_array = window_function(sequence_array, lower=lower_window, upper=upper_window)\n\n    return window_array.mean(axis=0)\n</code></pre>"},{"location":"reference/metrics/#metrics.index_intersection","title":"index_intersection","text":"<pre><code>index_intersection(index_groups: Iterable[Iterable[Any]]) -&gt; list[Any]\n</code></pre> <p>Perform AND logic on objects in multiple containers of objects, where all objects must be present to be included</p> <p>Parameters:</p> <ul> <li> <code>index_groups</code>             (<code>Iterable[Iterable[Any]]</code>)         \u2013          <p>Groups of indices</p> </li> </ul> <p>Returns:     The union of all provided indices</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>def index_intersection(index_groups: Iterable[Iterable[Any]]) -&gt; list[Any]:\n    \"\"\"Perform AND logic on objects in multiple containers of objects, where all objects must be present to be included\n\n    Args:\n        index_groups: Groups of indices\n    Returns:\n        The union of all provided indices\n    \"\"\"\n    final_indices = set()\n    # Find all set union. This grabs every possible index\n    for indices in index_groups:\n        final_indices = final_indices.union(indices)\n    # Find all set intersection. This narrows down to those present only in all\n    for indices in index_groups:\n        final_indices = final_indices.intersection(indices)\n\n    return list(final_indices)\n</code></pre>"},{"location":"reference/metrics/#metrics.z_score","title":"z_score","text":"<pre><code>z_score(sample: float | ndarray, mean: float | ndarray, stdev: float | ndarray) -&gt; float | ndarray\n</code></pre> <p>From sample(s), calculate the positional z-score, i.e. z-score = (sample - mean) / stdev</p> <p>Parameters:</p> <ul> <li> <code>sample</code>             (<code>float | ndarray</code>)         \u2013          <p>An array with the sample at every position</p> </li> <li> <code>mean</code>             (<code>float | ndarray</code>)         \u2013          <p>An array with the mean at every position</p> </li> <li> <code>stdev</code>             (<code>float | ndarray</code>)         \u2013          <p>An array with the standard deviation at every position</p> </li> </ul> <p>Returns:     The z-score of every sample</p> Source code in <code>symdesign/metrics/__init__.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef z_score(sample: float | np.ndarray, mean: float | np.ndarray, stdev: float | np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"From sample(s), calculate the positional z-score, i.e. z-score = (sample - mean) / stdev\n\n    Args:\n        sample: An array with the sample at every position\n        mean: An array with the mean at every position\n        stdev: An array with the standard deviation at every position\n    Returns:\n        The z-score of every sample\n    \"\"\"\n    # try:\n    return (sample-mean) / stdev\n</code></pre>"},{"location":"reference/metrics/pose/","title":"pose","text":""},{"location":"reference/metrics/pose/#metrics.pose.pose_metric_map","title":"pose_metric_map  <code>module-attribute</code>","text":"<pre><code>pose_metric_map = {'nanohedra_score': nanohedra_score, 'nanohedra_score_center': nanohedra_score_center, 'nanohedra_score_normalized': nanohedra_score_normalized, 'nanohedra_score_center_normalized': nanohedra_score_center_normalized}\n</code></pre> <p>This is a mapping of the Nanohedra or Pose based measure to the method to retrieve it from the Pose</p>"},{"location":"reference/protocols/","title":"protocols","text":""},{"location":"reference/protocols/#protocols.predict_structure","title":"predict_structure","text":"<pre><code>predict_structure(job: PoseJob)\n</code></pre> <p>From a sequence input, predict the structure using one of various structure prediction pipelines</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef predict_structure(job: pose.PoseJob):\n    \"\"\"From a sequence input, predict the structure using one of various structure prediction pipelines\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    # No need to load the yet as the prediction only uses sequence. Load the design in individual method when needed\n    # job.identify_interface()\n    # Acquire the pose_metrics if None have been made yet\n    job.calculate_pose_metrics()\n\n    job.predict_structure()\n</code></pre>"},{"location":"reference/protocols/#protocols.interface_metrics","title":"interface_metrics","text":"<pre><code>interface_metrics(job: PoseJob)\n</code></pre> <p>Generate a script capable of running Rosetta interface metrics analysis on the bound and unbound states</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef interface_metrics(job: pose.PoseJob):\n    \"\"\"Generate a script capable of running Rosetta interface metrics analysis on the bound and unbound states\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    job.identify_interface()\n    # metrics_flags = 'repack=yes'\n    job.protocol = flags.interface_metrics\n    main_cmd = rosetta.script_cmd.copy()\n\n    job.prepare_rosetta_flags(out_dir=job.scripts_path)\n\n    if job.current_designs:\n        file_paths = [design_.structure_path for design_ in job.current_designs if design_.structure_path]\n    else:\n        file_paths = get_directory_file_paths(\n            job.designs_path, suffix=job.job.specific_protocol if job.job.specific_protocol else '', extension='.pdb')\n    # Include the pose source in the designs to perform metrics on\n    if job.job.measure_pose and os.path.exists(job.pose_path):\n        file_paths.append(job.pose_path)\n    # If no designs specified or found and the pose_path exists, add it\n    # The user probably wants pose metrics without specifying so\n    elif not file_paths:\n        with job.job.db.session(expire_on_commit=False) as session:\n            session.add(job)\n            if not job.designs and os.path.exists(job.pose_path):\n                file_paths.append(job.pose_path)\n\n    if not file_paths:\n        raise InputError(\n            f'No files found for {job.job.module}')\n\n    design_files = \\\n        os.path.join(job.scripts_path, f'{starttime}_{job.protocol}_files'\n                     f'{f\"_{job.job.specific_protocol}\" if job.job.specific_protocol else \"\"}.txt')\n    with open(design_files, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(file_paths))\n\n    # generate_files_cmd = ['python', putils.list_pdb_files, '-d', job.designs_path, '-o', design_files, '-e', '.pdb'] \\\n    #     + (['-s', job.job.specific_protocol] if job.job.specific_protocol else [])\n    main_cmd += [f'@{job.flags}', '-in:file:l', design_files,\n                 '-out:file:score_only', job.scores_file, '-no_nstruct_label', 'true', '-parser:protocol']\n    #              '-in:file:native', job.refined_pdb,\n    if job.job.mpi &gt; 0:\n        main_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(job.job.mpi)] + main_cmd\n\n    metric_cmd_bound = main_cmd.copy()\n    if job.symmetry_dimension is not None and job.symmetry_dimension &gt; 0:\n        metric_cmd_bound += ['-symmetry_definition', 'CRYST1']\n    metric_cmd_bound += \\\n        [os.path.join(putils.rosetta_scripts_dir, f'interface_metrics{\"_DEV\" if job.job.development else \"\"}.xml')]\n    job.log.info(f'Metrics command for Pose: {list2cmdline(metric_cmd_bound)}')\n    entity_cmd = main_cmd + [os.path.join(putils.rosetta_scripts_dir,\n                                          f'metrics_entity{\"_DEV\" if job.job.development else \"\"}.xml')]\n    entity_metric_cmds = job.generate_entity_metrics_commands(entity_cmd)\n\n    # Create executable to gather interface Metrics on all Designs\n    if job.job.distribute_work:\n        analysis_cmd = job.get_cmd_process_rosetta_metrics()\n        job.current_script = distribute.write_script(\n            list2cmdline(metric_cmd_bound), name=f'{starttime}_{job.protocol}.sh', out_path=job.scripts_path,\n            additional=[list2cmdline(command) for command in entity_metric_cmds]\n            + [list2cmdline(analysis_cmd)])\n    else:\n        # list_all_files_process = Popen(generate_files_cmd)\n        # list_all_files_process.communicate()\n        for metric_cmd in [metric_cmd_bound] + entity_metric_cmds:\n            metrics_process = Popen(metric_cmd)\n            metrics_process.communicate()  # wait for command to complete\n\n        # Gather metrics for each design produced from this procedure\n        if os.path.exists(job.scores_file):\n            job.process_rosetta_metrics()\n</code></pre>"},{"location":"reference/protocols/#protocols.check_unmodeled_clashes","title":"check_unmodeled_clashes","text":"<pre><code>check_unmodeled_clashes(job: PoseJob, clashing_threshold: float = 0.75)\n</code></pre> <p>Given a multimodel file, measure the number of clashes is less than a percentage threshold</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> <li> <code>clashing_threshold</code>             (<code>float</code>, default:                 <code>0.75</code> )         \u2013          <p>The number of Model instances which have observed clashes</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef check_unmodeled_clashes(job: pose.PoseJob, clashing_threshold: float = 0.75):\n    \"\"\"Given a multimodel file, measure the number of clashes is less than a percentage threshold\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n        clashing_threshold: The number of Model instances which have observed clashes\n    \"\"\"\n    raise NotImplementedError(\"This module currently isn't working\")\n    from symdesign.structure.model import MultiModel, Model\n    models = [Model.from_file(job.job.structure_db.full_models.retrieve_data(name=entity), log=job.log)\n              for entity in job.entity_names]\n    # models = [Models.from_file(job.job.structure_db.full_models.retrieve_data(name=entity))\n    #           for entity in job.entity_names]\n\n    # For each model, transform to the correct space\n    models = job.transform_structures_to_pose(models)\n    multimodel = MultiModel.from_models(models, independent=True, log=job.log)\n\n    clashes = prior_clashes = 0\n    for idx, state in enumerate(multimodel, 1):\n        clashes += (1 if state.is_clash(measure=job.job.design.clash_criteria,\n                                        distance=job.job.design.clash_distance) else 0)\n        state.write(out_path=os.path.join(job.path, f'state_{idx}.pdb'))\n        logger.info(f'State {idx} - Clashes: {\"YES\" if clashes &gt; prior_clashes else \"NO\"}')\n        prior_clashes = clashes\n\n    if clashes / float(len(multimodel)) &gt; clashing_threshold:\n        raise ClashError(\n            f'The frequency of clashes ({clashes / float(len(multimodel))}) exceeds the clashing '\n            f'threshold ({clashing_threshold})')\n</code></pre>"},{"location":"reference/protocols/#protocols.check_clashes","title":"check_clashes","text":"<pre><code>check_clashes(job: PoseJob)\n</code></pre> <p>Check for clashes in the input and in the symmetric assembly if symmetric</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef check_clashes(job: pose.PoseJob):\n    \"\"\"Check for clashes in the input and in the symmetric assembly if symmetric\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    job.load_pose()\n</code></pre>"},{"location":"reference/protocols/#protocols.rename_chains","title":"rename_chains","text":"<pre><code>rename_chains(job: PoseJob)\n</code></pre> <p>Standardize the chain names in incremental order found in the design source file</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef rename_chains(job: pose.PoseJob):\n    \"\"\"Standardize the chain names in incremental order found in the design source file\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    job.load_pose()\n    job.pose.rename_chains()\n    job.output_pose(force=True)\n</code></pre>"},{"location":"reference/protocols/#protocols.expand_asu","title":"expand_asu","text":"<pre><code>expand_asu(job: PoseJob)\n</code></pre> <p>For the design info given by a PoseJob source, initialize the Pose with job.source file, job.symmetry, and job.log objects then expand the design given the provided symmetry operators and write to a file</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef expand_asu(job: pose.PoseJob):\n    \"\"\"For the design info given by a PoseJob source, initialize the Pose with job.source file,\n    job.symmetry, and job.log objects then expand the design given the provided symmetry operators and write to a\n    file\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    if job.is_symmetric():\n        job.load_pose()\n    else:\n        raise SymmetryError(\n            warn_missing_symmetry % expand_asu.__name__)\n</code></pre>"},{"location":"reference/protocols/#protocols.generate_fragments","title":"generate_fragments","text":"<pre><code>generate_fragments(job: PoseJob)\n</code></pre> <p>For the design info given by a PoseJob source, initialize the Pose then generate interfacial fragment information between Entities. Aware of symmetry and design_selectors in fragment generation file</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef generate_fragments(job: pose.PoseJob):\n    \"\"\"For the design info given by a PoseJob source, initialize the Pose then generate interfacial fragment\n    information between Entities. Aware of symmetry and design_selectors in fragment generation file\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    job.load_pose()\n    if job.job.interface_only:\n        entities = False\n        interface = True\n    else:\n        entities = True\n        interface = job.job.interface\n    job.generate_fragments(interface=interface, oligomeric_interfaces=job.job.oligomeric_interfaces, entities=entities)\n</code></pre>"},{"location":"reference/protocols/#protocols.refine","title":"refine","text":"<pre><code>refine(job: PoseJob)\n</code></pre> <p>Refine the source Pose</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef refine(job: pose.PoseJob):\n    \"\"\"Refine the source Pose\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    job.identify_interface()\n    job.protocol = job.job.module\n    if job.current_designs:\n        file_paths = [design_.structure_path for design_ in job.current_designs if design_.structure_path]\n    else:\n        file_paths = get_directory_file_paths(\n            job.designs_path, suffix=job.job.specific_protocol if job.job.specific_protocol else '', extension='.pdb')\n    # Include the pose source in the designs to perform metrics on\n    if job.job.measure_pose and os.path.exists(job.pose_path):\n        file_paths.append(job.pose_path)\n    # If no designs specified or found and the pose_path exists, add it\n    # The user probably wants pose metrics without specifying so\n    elif not file_paths:\n        with job.job.db.session(expire_on_commit=False) as session:\n            session.add(job)\n            if not job.designs and os.path.exists(job.pose_path):\n                file_paths.append(job.pose_path)\n\n    if not file_paths:\n        raise InputError(\n            f'No files found for {job.job.module}')\n\n    job.refine(design_files=file_paths)  # Inherently utilized... gather_metrics=job.job.metrics)\n</code></pre>"},{"location":"reference/protocols/#protocols.design","title":"design","text":"<pre><code>design(job: PoseJob)\n</code></pre> <p>For the design info given by a PoseJob source, initialize the Pose then prepare all parameters for sequence design. Aware of symmetry, design_selectors, fragments, and evolutionary information</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef design(job: pose.PoseJob):\n    \"\"\"For the design info given by a PoseJob source, initialize the Pose then prepare all parameters for\n    sequence design. Aware of symmetry, design_selectors, fragments, and evolutionary information\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    # job.load_pose()\n    job.identify_interface()\n\n    putils.make_path(job.data_path)\n    # Create all files which store the evolutionary_profile and/or fragment_profile -&gt; design_profile\n    if job.job.design.method == putils.rosetta:\n        # Update upon completion given results of designs list file...\n        # NOT # Update the Pose with the number of designs\n        # raise NotImplementedError('Need to generate design_number matching job.proteinmpnn_design()...')\n        # job.update_design_data(design_parent=job.pose_source, number=job.job.design.number)\n        if job.job.design.interface:\n            pass\n        else:\n            raise NotImplementedError(\n                f\"Can't perform module 'design' using Rosetta. Try '{flags.interface_design}' instead\")\n        favor_fragments = evo_fill = True\n        if job.job.design.term_constraint:\n            job.generate_fragments(interface=True)  # job.job.design.interface\n    elif job.job.design.method == putils.consensus:\n        raise NotImplementedError('Consensus calculation needs work')\n    else:\n        favor_fragments = evo_fill = False\n        job.generate_fragments(interface=True)  # job.job.design.interface\n\n    job.pose.calculate_fragment_profile(evo_fill=evo_fill)\n    # elif isinstance(job.fragment_observations, list):\n    #     raise NotImplementedError(f\"Can't put fragment observations taken away from the pose onto the pose due to \"\n    #                               f\"entities\")\n    #     job.pose.calculate_fragment_profile(evo_fill=evo_fill)\n    # elif os.path.exists(job.frag_file):\n    #     job.retrieve_fragment_info_from_file()\n\n    job.set_up_evolutionary_profile()\n\n    # job.pose.combine_sequence_profiles()\n    # I could also add the combined profile here instead of at each Entity\n    job.pose.calculate_profile(favor_fragments=favor_fragments)\n    putils.make_path(job.designs_path)\n    # Acquire the pose_metrics if None have been made yet\n    job.calculate_pose_metrics()\n\n    # match job.job.design.method:  # Todo python 3.10\n    #     case [putils.rosetta_str | putils.consensus]:\n    #         # Write generated files\n    #         write_pssm_file(job.pose.evolutionary_profile, file_name=job.evolutionary_profile_file)\n    #         write_pssm_file(job.pose.profile, file_name=job.design_profile_file)\n    #         job.pose.fragment_profile.write(file_name=job.fragment_profile_file)\n    #         if job.job.design.interface:\n    #             job.rosetta_interface_design()  # Sets job.protocol\n    #         else:\n    #             raise NotImplementedError(f'No function for all residue Rosetta design yet')\n    #             job.rosetta_design()  # Sets job.protocol\n    #     case putils.proteinmpnn:\n    #         job.proteinmpnn_design()  # Sets job.protocol\n    #     case _:\n    #         raise ValueError(f\"The method '{job.job.design.method}' isn't available\")\n    if job.job.design.method in [putils.rosetta, putils.consensus]:\n        # Write generated files\n        write_pssm_file(job.pose.evolutionary_profile, file_name=job.evolutionary_profile_file)\n        write_pssm_file(job.pose.profile, file_name=job.design_profile_file)\n        job.pose.fragment_profile.write(file_name=job.fragment_profile_file)\n        # Ensure the Pose is refined into the current_energy_function\n        if not job.refined and not os.path.exists(job.refined_pdb):\n            job.refine(gather_metrics=False)\n        if job.job.design.interface:\n            raise NotImplementedError('Need to generate job.number_of_designs matching job.proteinmpnn_design()...')\n            job.rosetta_interface_design()  # Sets job.protocol\n        else:\n            raise NotImplementedError(f'No function for all residue Rosetta design yet')\n            job.rosetta_design()  # Sets job.protocol\n    elif job.job.design.method == putils.proteinmpnn:\n        # Sets job.protocol\n        job.proteinmpnn_design()  # interface=job.job.design.interface, neighbors=job.job.design.neighbors\n    else:\n        raise InputError(\n            f\"The method '{job.job.design.method}' isn't available\")\n</code></pre>"},{"location":"reference/protocols/#protocols.optimize_designs","title":"optimize_designs","text":"<pre><code>optimize_designs(job: PoseJob, threshold: float = 0.0)\n</code></pre> <p>To touch up and optimize a design, provide a list of optional directives to view mutational landscape around certain residues in the design as well as perform wild-type amino acid reversion to mutated residues</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> <li> <code>#</code>             (<code>residue_directives=None (dict[Residue | int, str]</code>)         \u2013          </li> <li> <code>#</code>             (<code>    {Residue object</code>)         \u2013          <p>'mutational_directive', ...}</p> </li> <li> <code>#</code>             (<code>design_file=None (str</code>)         \u2013          <p>The name of a particular design file present in the designs output</p> </li> <li> <code>threshold</code>             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>The threshold above which background amino acid frequencies are allowed for mutation</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef optimize_designs(job: pose.PoseJob, threshold: float = 0.):\n    \"\"\"To touch up and optimize a design, provide a list of optional directives to view mutational landscape around\n    certain residues in the design as well as perform wild-type amino acid reversion to mutated residues\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n        # residue_directives=None (dict[Residue | int, str]):\n        #     {Residue object: 'mutational_directive', ...}\n        # design_file=None (str): The name of a particular design file present in the designs output\n        threshold: The threshold above which background amino acid frequencies are allowed for mutation\n    \"\"\"\n    job.protocol = protocol_xml1 = flags.optimize_designs._\n    # job.protocol = putils.pross\n\n    generate_files_cmd = pose.null_cmd\n\n    # Create file output\n    raise NotImplementedError('Must make the infile an \"in:file:s\" derivative')\n    designed_files_file = os.path.join(job.scripts_path, f'{starttime}_{job.protocol}_files_output.txt')\n    if job.current_designs:\n        design_ids = [design_.id for design_ in job.current_designs]\n        design_files = [design_.structure_file for design_ in job.current_designs]\n        design_poses = [Pose.from_file(file, **job.pose_kwargs) for file in design_files]\n        design_files_file = os.path.join(job.scripts_path, f'{starttime}_{job.protocol}_files.txt')\n        with open(design_files_file, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(design_files))\n        # Write the designed_files_file with all \"tentatively\" designed file paths\n        pdb_out_path = job.designs_path\n        out_file_string = f'%s{os.sep}{pdb_out_path}{os.sep}%s'\n        with open(design_files_file, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(out_file_string % os.path.split(file) for file in design_files))\n        # -in:file:native is here to block flag file version, not actually useful for refine\n        infile = ['-in:file:l', design_files_file, '-in:file:native', job.source_path]\n        metrics_pdb = ['-in:file:l', designed_files_file, '-in:file:native', job.source_path]\n    else:\n        infile = ['-in:file:s', job.refined_pdb]\n\n    job.identify_interface()  # job.load_pose()\n    # for design in job.current_designs:\n    #     job.load_pose(structure_source=design.structure_path)\n    #     job.identify_interface()\n\n    # Format all amino acids in design with frequencies above the threshold to a set\n    # Locate the desired background profile from the pose\n    background_profile = getattr(job.pose, job.job.background_profile)\n    raise NotImplementedError(\"Chain.*_profile all need to be zero-index to account for residue.index\")\n    background = {residue: {aaa for a, aaa in protein_letters_1to3.items()\n                            if background_profile[residue.index].get(a, -1) &gt; threshold}\n                  for residue in job.pose.residues}\n    # Include the wild-type residue from PoseJob.pose and the residue identity of the selected design\n    wt = {residue: {background_profile[residue.index].get('type'), protein_letters_3to1[residue.type]}\n          for residue in background}\n    bkgnd_directives = dict(zip(background.keys(), repeat(None)))\n\n    design_directives = []\n    # design_directives = [bkgnd_directives.copy() for _ in job.directives]\n    for design_pose, directive in zip(design_poses, job.directives):\n        # Grab those residues from background that are considered designed in the current design\n        raise NotImplementedError(\n            f\"Must only use the background amino acid types for the positions that were marked as \"\n            f\"designable for each design in job.current_designs\")\n        design_directive = {residue: background[residue] for residue in design_designed_residues}\n        design_directive.update({residue: directive[residue.index]\n                                for residue in design_pose.get_residues(directive.keys())})\n        design_directives.append(design_directive)\n        # design_directive.update({residue: directive[residue.index]\n        #                          for residue in job.pose.get_residues(directive.keys())})\n\n    res_files = [design_pose.make_resfile(directive, out_path=job.data_path, include=wt, background=background)\n                 for design_pose, directive in zip(design_poses, design_directives)]\n\n    # nstruct_instruct = ['-no_nstruct_label', 'true']\n    nstruct_instruct = ['-nstruct', str(job.job.design.number)]\n    generate_files_cmd = \\\n        ['python', putils.list_pdb_files, '-d', job.designs_path, '-o', designed_files_file,  '-e', '.pdb',\n         '-s', f'_{job.protocol}']\n\n    main_cmd = rosetta.script_cmd.copy()\n    if job.symmetry_dimension is not None and job.symmetry_dimension &gt; 0:\n        main_cmd += ['-symmetry_definition', 'CRYST1']\n\n    job.prepare_rosetta_flags(out_dir=job.scripts_path)\n\n    # DESIGN: Prepare command and flags file\n    profile_cmd = ['-in:file:pssm', job.evolutionary_profile_file] \\\n        if os.path.exists(job.evolutionary_profile_file) else []\n    design_cmds = []\n    for res_file, design_ in zip(res_files, job.current_designs):\n        design_cmds.append(\n            main_cmd + profile_cmd + infile  # &lt;- infile must be 'in:file:s'\n            + [f'@{job.flags}', '-out:suffix', f'_{job.protocol}', '-packing:resfile', res_file, '-parser:protocol',\n               os.path.join(putils.rosetta_scripts_dir, f'{protocol_xml1}.xml')]\n            + nstruct_instruct + ['-parser:script_vars', f'{putils.design_parent}={design_.name}'])\n\n    # metrics_pdb = ['-in:file:l', designed_files_file]\n    # METRICS: Can remove if SimpleMetrics adopts pose metric caching and restoration\n    # Assumes all entity chains are renamed from A to Z for entities (1 to n)\n    # metric_cmd = main_cmd + ['-in:file:s', job.specific_design if job.specific_design else job.refined_pdb] + \\\n    entity_cmd = main_cmd + ['-in:file:l', designed_files_file] + \\\n        [f'@{job.flags}', '-out:file:score_only', job.scores_file, '-no_nstruct_label', 'true',\n         '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, 'metrics_entity.xml')]\n\n    if job.job.mpi &gt; 0:\n        design_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(job.job.mpi)] + design_cmd\n        entity_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(job.job.mpi)] + entity_cmd\n\n    job.log.info(f'{optimize_designs.__name__} command: {list2cmdline(design_cmd)}')\n    metric_cmds = []\n    metric_cmds.extend(job.generate_entity_metrics_commands(entity_cmd))\n\n    # Create executable/Run FastDesign on Refined ASU with RosettaScripts. Then, gather Metrics\n    if job.job.distribute_work:\n        analysis_cmd = job.get_cmd_process_rosetta_metrics()\n        self.current_script = distribute.write_script(\n            list2cmdline(design_cmd), name=f'{starttime}_{job.protocol}.sh', out_path=job.scripts_path,\n            additional=[list2cmdline(generate_files_cmd)]\n            + [list2cmdline(command) for command in metric_cmds] + [list2cmdline(analysis_cmd)])\n    else:\n        design_process = Popen(design_cmd)\n        design_process.communicate()  # wait for command to complete\n        list_all_files_process = Popen(generate_files_cmd)\n        list_all_files_process.communicate()\n        for metric_cmd in metric_cmds:\n            metrics_process = Popen(metric_cmd)\n            metrics_process.communicate()\n\n        # Gather metrics for each design produced from this procedure\n        if os.path.exists(job.scores_file):\n            job.process_rosetta_metrics()\n</code></pre>"},{"location":"reference/protocols/#protocols.process_rosetta_metrics","title":"process_rosetta_metrics","text":"<pre><code>process_rosetta_metrics(job: PoseJob) -&gt; None\n</code></pre> <p>From Rosetta based protocols, tally the resulting metrics and integrate with the metrics database</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef process_rosetta_metrics(job: pose.PoseJob) -&gt; None:\n    \"\"\"From Rosetta based protocols, tally the resulting metrics and integrate with the metrics database\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    \"\"\"\n    # Gather metrics for each design produced from other modules\n    if os.path.exists(job.scores_file):\n        job.identify_interface()\n        job.process_rosetta_metrics()\n    else:\n        raise InputError(\n            f'No scores from Rosetta present at \"{job.scores_file}\"')\n</code></pre>"},{"location":"reference/protocols/#protocols.analysis","title":"analysis","text":"<pre><code>analysis(job: PoseJob, designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; None\n</code></pre> <p>Retrieve all metrics information from a PoseJob</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> <li> <code>designs</code>             (<code>Iterable[Pose] | Iterable[AnyStr]</code>, default:                 <code>None</code> )         \u2013          <p>The subsequent designs to perform analysis on</p> </li> </ul> <p>Returns:     Series containing summary metrics for all designs in the design directory</p> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef analysis(job: pose.PoseJob, designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; None:\n    \"\"\"Retrieve all metrics information from a PoseJob\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n        designs: The subsequent designs to perform analysis on\n    Returns:\n        Series containing summary metrics for all designs in the design directory\n    \"\"\"\n    # job.load_pose()\n    job.identify_interface()\n    # Acquire the pose_metrics if None have been made yet\n    job.calculate_pose_metrics()\n\n    job.analyze_pose_designs(designs=designs)\n</code></pre>"},{"location":"reference/protocols/#protocols.helix_bending","title":"helix_bending","text":"<pre><code>helix_bending(job: PoseJob)\n</code></pre> <p>Retrieve all metrics information from a PoseJob</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> </ul> <p>Returns:     Series containing summary metrics for all designs in the design directory</p> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef helix_bending(job: pose.PoseJob):\n    \"\"\"Retrieve all metrics information from a PoseJob\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n    Returns:\n        Series containing summary metrics for all designs in the design directory\n    \"\"\"\n    job.load_pose()\n    if job.job.joint_chain:  # A chain designation was provided\n        model_to_select = job.pose.get_chain(job.job.joint_chain)\n    else:  # Just use the residue number to select\n        model_to_select = job.pose\n\n    if job.job.output_directory:\n        putils.make_path(job.job.output_directory)\n        out_dir = job.job.output_directory\n    else:\n        out_dir = job.pose_directory\n\n    joint_residue = model_to_select.residue(job.job.joint_residue)\n    bent_coords = align.bend(job.pose, joint_residue, job.job.direction, samples=job.job.sample_number)\n\n    output_number = count(1)\n    for bent_idx, coords in enumerate(bent_coords, 1):\n        job.pose.coords = coords\n        # Check for clashes\n        if job.pose.is_clash(measure=job.job.design.clash_criteria,\n                             distance=job.job.design.clash_distance, silence_exceptions=True):\n            logger.info(f'Bend index {bent_idx} clashes')\n            continue\n        if job.pose.is_symmetric() and not job.job.design.ignore_symmetric_clashes and \\\n                job.pose.symmetric_assembly_is_clash(measure=job.job.design.clash_criteria,\n                                                     distance=job.job.design.clash_distance):\n            logger.info(f'Bend index {bent_idx} has symmetric clashes')\n            continue\n\n        trial_path = os.path.join(out_dir, f'{job.name}-bent{next(output_number)}.pdb')\n        job.pose.write(out_path=trial_path)\n</code></pre>"},{"location":"reference/protocols/#protocols.select_sequences","title":"select_sequences","text":"<pre><code>select_sequences(job: PoseJob, filters: dict = None, weights: dict = None, number: int = 1, protocols: list[str] = None, **kwargs) -&gt; list[str]\n</code></pre> <p>Select sequences for further characterization. If weights, then user can prioritize by metrics, otherwise sequence with the most neighbors as calculated by sequence distance will be selected. If there is a tie, the sequence with the lowest weight will be selected</p> <p>Parameters:</p> <ul> <li> <code>job</code>             (<code>PoseJob</code>)         \u2013          <p>The PoseJob for which the protocol should be performed on</p> </li> <li> <code>filters</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>The filters to use in sequence selection</p> </li> <li> <code>weights</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>The weights to use in sequence selection</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of sequences to consider for each design</p> </li> <li> <code>protocols</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>Whether particular design protocol(s) should be chosen</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>default_weight</code>         \u2013          <p>str = 'interface_energy': The metric to sort the dataframe by default if no weights are provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>The selected designs for the Pose trajectories</p> </li> </ul> Source code in <code>symdesign/protocols/__init__.py</code> <pre><code>@protocol_decorator()\ndef select_sequences(job: pose.PoseJob, filters: dict = None, weights: dict = None, number: int = 1,\n                     protocols: list[str] = None, **kwargs) -&gt; list[str]:\n    \"\"\"Select sequences for further characterization. If weights, then user can prioritize by metrics, otherwise\n    sequence with the most neighbors as calculated by sequence distance will be selected. If there is a tie, the\n    sequence with the lowest weight will be selected\n\n    Args:\n        job: The PoseJob for which the protocol should be performed on\n        filters: The filters to use in sequence selection\n        weights: The weights to use in sequence selection\n        number: The number of sequences to consider for each design\n        protocols: Whether particular design protocol(s) should be chosen\n\n    Keyword Args:\n        default_weight: str = 'interface_energy': The metric to sort the dataframe by default if no weights are provided\n\n    Returns:\n        The selected designs for the Pose trajectories\n    \"\"\"\n    # Load relevant data from the design directory\n    designs_df = pd.read_csv(job.designs_metrics_csv, index_col=0, header=[0])\n    designs_df.dropna(inplace=True)\n    if protocols:\n        designs = []\n        for protocol in protocols:\n            designs.extend(designs_df[designs_df['protocol'] == protocol].index.tolist())\n\n        if not designs:\n            raise InputError(\n                f'No designs found for protocols {protocols}!')\n    else:\n        designs = designs_df.index.tolist()\n\n    job.log.info(f'Number of starting trajectories = {len(designs_df)}')\n    df = designs_df.loc[designs, :]\n\n    if filters:\n        job.log.info(f'Using filter parameters: {filters}')\n        # Filter the DataFrame to include only those values which are le/ge the specified filter\n        filtered_designs = metrics.index_intersection(\n            metrics.filter_df_for_index_by_value(df, filters).values())\n        df = df.loc[filtered_designs, :]\n\n    if weights:\n        # No filtering of protocol/indices to use as poses should have similar protocol scores coming in\n        job.log.info(f'Using weighting parameters: {weights}')\n        designs = metrics.pareto_optimize_trajectories(df, weights=weights, **kwargs).index.tolist()\n    else:\n        # sequences_pickle = glob(os.path.join(job.job.all_scores, '%s_Sequences.pkl' % str(job)))\n        # assert len(sequences_pickle) == 1, 'Couldn\\'t find files for %s' % \\\n        #                                     os.path.join(job.job.all_scores, '%s_Sequences.pkl' % str(job))\n        #\n        # chain_sequences = SDUtils.unpickle(sequences_pickle[0])\n        # {chain: {name: sequence, ...}, ...}\n        # designed_sequences_by_entity: list[dict[str, str]] = unpickle(job.designed_sequences)\n        # designed_sequences_by_entity: list[dict[str, str]] = job.designed_sequences\n        # entity_sequences = list(zip(*[list(designed_sequences.values())\n        #                               for designed_sequences in designed_sequences_by_entity]))\n        # concatenated_sequences = [''.join(entity_sequence) for entity_sequence in entity_sequences]\n        pose_sequences = job.designed_sequences\n        job.log.debug(f'The final concatenated sequences are:\\n{pose_sequences}')\n\n        # pairwise_sequence_diff_np = SDUtils.all_vs_all(concatenated_sequences, sequence_difference)\n        # Using concatenated sequences makes the values very similar and inflated as most residues are the same\n        # doing min/max normalization to see variation\n        pairwise_sequence_diff_l = [sequence_difference(*seq_pair)\n                                    for seq_pair in combinations(pose_sequences, 2)]\n        pairwise_sequence_diff_np = np.array(pairwise_sequence_diff_l)\n        _min = min(pairwise_sequence_diff_l)\n        # max_ = max(pairwise_sequence_diff_l)\n        pairwise_sequence_diff_np = np.subtract(pairwise_sequence_diff_np, _min)\n        # job.log.info(pairwise_sequence_diff_l)\n\n        # PCA analysis of distances\n        pairwise_sequence_diff_mat = np.zeros((len(designs), len(designs)))\n        for k, dist in enumerate(pairwise_sequence_diff_np):\n            i, j = condensed_to_square(k, len(designs))\n            pairwise_sequence_diff_mat[i, j] = dist\n        pairwise_sequence_diff_mat = sym(pairwise_sequence_diff_mat)\n\n        pairwise_sequence_diff_mat = skl.preprocessing.StandardScaler().fit_transform(pairwise_sequence_diff_mat)\n        seq_pca = skl.decomposition.PCA(default_pca_variance)\n        seq_pc_np = seq_pca.fit_transform(pairwise_sequence_diff_mat)\n        seq_pca_distance_vector = pdist(seq_pc_np)\n        # epsilon = math.sqrt(seq_pca_distance_vector.mean()) * 0.5\n        epsilon = seq_pca_distance_vector.mean() * 0.5\n        job.log.info(f'Finding maximum neighbors within distance of {epsilon}')\n\n        # job.log.info(pairwise_sequence_diff_np)\n        # epsilon = pairwise_sequence_diff_mat.mean() * 0.5\n        # epsilon = math.sqrt(seq_pc_np.myean()) * 0.5\n        # epsilon = math.sqrt(pairwise_sequence_diff_np.mean()) * 0.5\n\n        # Find the nearest neighbors for the pairwise-distance matrix using the X*X^T (PCA) matrix, linear transform\n        seq_neighbors = skl.neighbors.BallTree(seq_pc_np)\n        seq_neighbor_counts = seq_neighbors.query_radius(seq_pc_np, epsilon, count_only=True)  # sort_results=True)\n        top_count, top_idx = 0, None\n        count_list = seq_neighbor_counts.tolist()\n        for count in count_list:  # idx, enumerate()\n            if count &gt; top_count:\n                top_count = count\n\n        sorted_seqs = sorted(count_list, reverse=True)\n        top_neighbor_counts = sorted(set(sorted_seqs[:number]), reverse=True)\n\n        # Find only the designs which match the top x (number) of neighbor counts\n        final_designs = {designs[idx]: num_neighbors for num_neighbors in top_neighbor_counts\n                         for idx, count in enumerate(count_list) if count == num_neighbors}\n        job.log.info('The final sequence(s) and file(s):\\nNeighbors\\tDesign\\n%s'\n                     # % '\\n'.join('%d %s' % (top_neighbor_counts.index(neighbors) + 1,\n                     % '\\n'.join(f'\\t{neighbors}\\t{os.path.join(job.designs_path, _design)}'\n                                 for _design, neighbors in final_designs.items()))\n\n        # job.log.info('Corresponding PDB file(s):\\n%s' % '\\n'.join('%d %s' % (i, os.path.join(job.designs_path, seq))\n        #                                                         for i, seq in enumerate(final_designs, 1)))\n\n        # Compute the highest density cluster using DBSCAN algorithm\n        # seq_cluster = DBSCAN(eps=epsilon)\n        # seq_cluster.fit(pairwise_sequence_diff_np)\n        #\n        # seq_pc_df = pd.DataFrame(seq_pc, index=designs, columns=['pc' + str(x + 1)\n        #                                                          for x in range(len(seq_pca.components_))])\n        # seq_pc_df = pd.merge(protocol_s, seq_pc_df, left_index=True, right_index=True)\n\n        # If final designs contains more sequences than specified, find the one with the lowest energy\n        if len(final_designs) &gt; number:\n            energy_s = df.loc[final_designs.keys(), 'interface_energy']\n            energy_s.sort_values(inplace=True)\n            designs = energy_s.index.tolist()\n        else:\n            designs = list(final_designs.keys())\n\n    designs = designs[:number]\n    job.log.info(f'Final ranking of trajectories:\\n{\", \".join(_design for _design in designs)}')\n    return designs\n</code></pre>"},{"location":"reference/protocols/align/","title":"align","text":""},{"location":"reference/protocols/align/#protocols.align.make_guide","title":"make_guide","text":"<pre><code>make_guide(n_ca_c_atoms: ndarray, scale: float = 1.0) -&gt; ndarray\n</code></pre> <p>Returns 3 guide position vectors. The 1st vector is the Ca position, the second is displaced from the Ca position along the direction to the C atom with a length set by the scale quantity. The 3rd position is likewise offset from the Ca position along a direction in the plane of the 3 atoms given, also with length given by scale.</p> <p>Parameters:</p> <ul> <li> <code>n_ca_c_atoms</code>             (<code>ndarray</code>)         \u2013          <p>A 'F' order (vectors as columns) array with shape (3, 3) representing N, Ca, and C coordinates</p> </li> <li> <code>scale</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The magnitude of the guide vectors</p> </li> </ul> <p>Returns:     The guide coordinates in a</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def make_guide(n_ca_c_atoms: np.ndarray, scale: float = 1.) -&gt; np.ndarray:\n    \"\"\"Returns 3 guide position vectors. The 1st vector is the Ca position, the second is\n    displaced from the Ca position along the direction to the C atom with a length set by\n    the scale quantity. The 3rd position is likewise offset from the Ca position along a\n    direction in the plane of the 3 atoms given, also with length given by scale.\n\n    Args:\n        n_ca_c_atoms: A 'F' order (vectors as columns) array with shape (3, 3) representing N, Ca, and C coordinates\n        scale: The magnitude of the guide vectors\n    Returns:\n        The guide coordinates in a\n    \"\"\"\n    ca = n_ca_c_atoms[:, 1].flatten()\n    v1 = n_ca_c_atoms[:, 2].flatten() - ca\n    v2 = n_ca_c_atoms[:, 0].flatten() - ca\n    v1n = norm(v1)\n    v2t = v2 - v1n * np.dot(v2, v1n)\n    v2tn = norm(v2t)\n\n    guide1 = ca + scale * v1n\n    guide2 = ca + scale * v2tn\n\n    guide = np.zeros((3, 3))\n    guide[:, 0], guide[:, 1], guide[:, 2] = ca, guide1, guide2\n\n    return guide\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.get_frame_from_joint","title":"get_frame_from_joint","text":"<pre><code>get_frame_from_joint(joint_points: ndarray) -&gt; ndarray\n</code></pre> <p>Create a 'frame' which consists of a matrix with</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The Fortran ordered array with shape (3, 4) that contains 3 basis vectors (x, y, z) of the point in question</p> </li> <li> <code>ndarray</code>         \u2013          <p>along the first 3 columns, then the 4th column is the translation to the provided joint_point</p> </li> </ul> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def get_frame_from_joint(joint_points: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Create a 'frame' which consists of a matrix with\n\n    Returns:\n        The Fortran ordered array with shape (3, 4) that contains 3 basis vectors (x, y, z) of the point in question\n        along the first 3 columns, then the 4th column is the translation to the provided joint_point\n    \"\"\"\n    guide_target_1 = make_guide(joint_points)\n    ca = joint_points[:, 1].flatten()\n    v1 = guide_target_1[:, 1] - ca\n    v2 = guide_target_1[:, 2] - ca\n    v3 = cross(v1, v2)\n    rot = np.array([v1, v2, v3]).T\n    # logger.debug(f'frame rot:\\n{rot}')\n    # logger.debug(f'frame trans:\\n{guide_points[:, 1]} {guide_target_1[:, 0]}')\n    frame_out = np.zeros((3, 4))\n    frame_out[:, :3] = rot\n    frame_out[:, 3] = joint_points[:, 1]\n    return frame_out\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.compose_3x4","title":"compose_3x4","text":"<pre><code>compose_3x4(a3x4: ndarray, b3x4: ndarray) -&gt; ndarray\n</code></pre> <p>Apply a rotation and translation of one array with shape (3, 4) to another array with same shape</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def compose_3x4(a3x4: np.ndarray, b3x4: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Apply a rotation and translation of one array with shape (3, 4) to another array with same shape\"\"\"\n    r1 = a3x4[:, :3]\n    t1 = a3x4[:, 3]\n    # r2=b3x4[:,0:3]\n    # t2=b3x4[:,3]\n\n    c3x4 = np.matmul(r1, b3x4)\n    c3x4[:, 3] += t1\n    # print('rot: ', np.matmul(r2,r1))\n    # print('trans: ', np.matmul(r2,t1) + t2)\n\n    return c3x4\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.generate_bend_transformations","title":"generate_bend_transformations","text":"<pre><code>generate_bend_transformations(joint_residue: Residue, direction: termini_literal = None) -&gt; Iterator[TransformationMapping]\n</code></pre> <p>Generate transformations compatible with bending a helix at a Residue in that helix in either the 'n' or 'c' terminal direction</p> <p>Parameters:</p> <ul> <li> <code>joint_residue</code>             (<code>Residue</code>)         \u2013          <p>The Residue where the bending should be applied</p> </li> <li> <code>direction</code>             (<code>termini_literal</code>, default:                 <code>None</code> )         \u2013          <p>Specify which direction, compared to the Residue, should be bent. 'c' bends the c-terminal residues, 'n' the n-terminal residues</p> </li> </ul> <p>Returns:     A generator which yields a transformation mapping for a single 'bending mode' upon each access</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def generate_bend_transformations(joint_residue: Residue, direction: termini_literal = None) \\\n        -&gt; Iterator[types.TransformationMapping]:\n    \"\"\"Generate transformations compatible with bending a helix at a Residue in that helix in either the 'n' or 'c'\n    terminal direction\n\n    Args:\n        joint_residue: The Residue where the bending should be applied\n        direction: Specify which direction, compared to the Residue, should be bent.\n            'c' bends the c-terminal residues, 'n' the n-terminal residues\n    Returns:\n        A generator which yields a transformation mapping for a single 'bending mode' upon each access\n    \"\"\"\n    if direction == 'c':\n        modes3x4 = modes3x4_F\n    elif direction == 'n':\n        modes3x4 = modes3x4_R\n    else:\n        raise ValueError(\n            f\"'direction' must be either 'n' or 'c', not {direction}\")\n\n    model_frame = np.array(\n        [joint_residue.n.coords, joint_residue.ca.coords, joint_residue.c.coords]).T\n    joint_frame = get_frame_from_joint(model_frame)\n    # logger.debug(f'joint_frame:\\n{joint_frame}')\n    jinv = invert_3x4(joint_frame)\n    # Fixed parameters\n    bend_dim = 4\n    bend_scale = 1.\n    # ntaper = 5\n\n    # Generate various transformations\n    while True:\n        bend_coeffs = np.random.normal(size=bend_dim) * bend_scale\n        blend_mode = combine_modes(modes3x4, bend_coeffs)\n\n        # Compose a trial bending mode in the frame of the fixed structure\n        tmp1 = compose_3x4(blend_mode, jinv)\n        mode_in_frame = compose_3x4(joint_frame, tmp1)\n        # print('mode_in_frame:\\n', mode_in_frame)\n\n        # Separate the operations to their components\n        rotation = mode_in_frame[:, 0:3]\n        translation = mode_in_frame[:, 3].flatten()\n        yield dict(rotation=rotation, translation=translation)\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.bend","title":"bend","text":"<pre><code>bend(pose: Pose, joint_residue: Residue, direction: termini_literal, samples: int = 1, additional_entity_ids: Iterable[str] = None) -&gt; list[ndarray]\n</code></pre> <p>Bend a Pose at a helix specified by a Residue on the helix according to typical helix bending modes</p> <p>Parameters:</p> <ul> <li> <code>pose</code>             (<code>Pose</code>)         \u2013          <p>The Pose of interest to generate bent coordinates for</p> </li> <li> <code>joint_residue</code>             (<code>Residue</code>)         \u2013          <p>The Residue where the bending should be applied</p> </li> <li> <code>direction</code>             (<code>termini_literal</code>)         \u2013          <p>Specify which termini of the joint_residue Entity, compared to the joint_residue, should be bent. 'c' bends the c-terminal coordinates, 'n' the n-terminal coordinates</p> </li> <li> <code>samples</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>How many times should the coordinates be bent</p> </li> <li> <code>additional_entity_ids</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>If there are additional Entity instances desired to be carried through bending, pass their Entity.name attributes</p> </li> </ul> <p>Returns:     A list of the transformed pose coordinates at the bent site</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def bend(pose: Pose, joint_residue: Residue, direction: termini_literal, samples: int = 1,\n         additional_entity_ids: Iterable[str] = None) -&gt; list[np.ndarray]:\n    \"\"\"Bend a Pose at a helix specified by a Residue on the helix according to typical helix bending modes\n\n    Args:\n        pose: The Pose of interest to generate bent coordinates for\n        joint_residue: The Residue where the bending should be applied\n        direction: Specify which termini of the joint_residue Entity, compared to the joint_residue, should be bent.\n            'c' bends the c-terminal coordinates, 'n' the n-terminal coordinates\n        samples: How many times should the coordinates be bent\n        additional_entity_ids: If there are additional Entity instances desired to be carried through bending,\n            pass their Entity.name attributes\n    Returns:\n        A list of the transformed pose coordinates at the bent site\n    \"\"\"\n    residue_chain = pose.get_chain(joint_residue.chain_id)\n    bending_entity = residue_chain.entity\n\n    if direction == 'n':\n        set_coords_slice = slice(bending_entity.n_terminal_residue.start_index, joint_residue.start_index)\n    elif direction == 'c':\n        set_coords_slice = slice(joint_residue.end_index + 1, bending_entity.c_terminal_residue.end_index)\n    else:\n        raise ValueError(\n            f\"'direction' must be either 'n' or 'c', not {direction}\")\n    additional_entities = []\n    if additional_entity_ids:\n        for name in additional_entity_ids:\n            entity = pose.get_entity(name)\n            if entity is None:\n                raise ValueError(\n                    f\"The entity_id '{name}' wasn't found in the {repr(pose)}. \"\n                    f\"Available entity_ids={', '.join(entity.name for entity in pose.entities)}\")\n            else:\n                additional_entities.append(entity)\n\n    # Get the model coords before\n    pose_coords = pose.coords\n    entity_coords_to_bend = pose.coords[set_coords_slice]\n    bent_coords_samples = []\n    for trial, transformation in enumerate(generate_bend_transformations(joint_residue, direction=direction), 1):\n\n        bent_coords = np.matmul(entity_coords_to_bend, transformation['rotation'].T) \\\n                      + transformation['translation']\n        copied_pose_coords = pose_coords.copy()\n        copied_pose_coords[set_coords_slice] = bent_coords\n\n        for entity in additional_entities:\n            copied_pose_coords[entity.atom_indices] = np.matmul(entity.coords, transformation['rotation'].T) \\\n                                                      + transformation['translation']\n\n        bent_coords_samples.append(copied_pose_coords)\n        if trial == samples:\n            break\n\n    return bent_coords_samples\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.prepare_alignment_motif","title":"prepare_alignment_motif","text":"<pre><code>prepare_alignment_motif(model: ContainsResidues, model_start: int, motif_length: int, termini: termini_literal, extension_length: int = 0, alignment_length: int = 5) -&gt; tuple[ContainsResidues, Chain]\n</code></pre> <p>From a ContainsResidues, select helices of interest from a termini of the model and separate the model into the original model and the selected helix</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>ContainsResidues</code>)         \u2013          <p>The model to acquire helices from</p> </li> <li> <code>model_start</code>             (<code>int</code>)         \u2013          <p>The residue index to start motif selection at</p> </li> <li> <code>motif_length</code>             (<code>int</code>)         \u2013          <p>The length of the helical motif</p> </li> <li> <code>termini</code>             (<code>termini_literal</code>)         \u2013          <p>The termini to utilize</p> </li> <li> <code>extension_length</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>How many residues should the helix be extended</p> </li> <li> <code>alignment_length</code>             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>The number of residues used to calculate overlap of the target to the ideal helix</p> </li> </ul> <p>Returns:     The original model without the selected helix and the selected helix</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def prepare_alignment_motif(\n    model: ContainsResidues, model_start: int, motif_length: int,\n    termini: termini_literal, extension_length: int = 0, alignment_length: int = 5\n) -&gt; tuple[ContainsResidues, Chain]:\n    \"\"\"From a ContainsResidues, select helices of interest from a termini of the model and separate the model into the\n    original model and the selected helix\n\n    Args:\n        model: The model to acquire helices from\n        model_start: The residue index to start motif selection at\n        motif_length: The length of the helical motif\n        termini: The termini to utilize\n        extension_length: How many residues should the helix be extended\n        alignment_length: The number of residues used to calculate overlap of the target to the ideal helix\n    Returns:\n        The original model without the selected helix and the selected helix\n    \"\"\"\n    model_residue_indices = list(range(model_start, model_start + motif_length))\n    helix_residues = model.get_residues(indices=model_residue_indices)\n    helix_model = Chain.from_residues(helix_residues)\n\n    if extension_length:\n        # if termini is None:\n        #     n_terminal_number = helix_model.n_terminal_residue.number\n        #     c_terminal_number = helix_model.c_terminal_residue.number\n        #     if n_terminal_number in model_residue_range or n_terminal_number &lt; model_start:\n        #         termini = 'n'\n        #     elif c_terminal_number in model_residue_range or c_terminal_number &gt; model_start + alignment_length:\n        #         termini = 'c'\n        #     else:  # Can't extend...\n        #         raise ValueError(f\"Couldn't automatically determine the desired termini to extend\")\n        helix_model.add_ideal_helix(termini=termini, length=extension_length, alignment_length=alignment_length)\n\n    if termini == 'n':\n        remove_indices = list(range(model.n_terminal_residue.index, helix_residues[-1].index + 1))\n    else:\n        remove_indices = list(range(helix_residues[0].index, model.c_terminal_residue.index + 1))\n\n    deleted_model = model.copy()\n    deleted_model.delete_residues(indices=remove_indices)\n\n    return deleted_model, helix_model\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.get_terminal_helix_start_index_and_length","title":"get_terminal_helix_start_index_and_length","text":"<pre><code>get_terminal_helix_start_index_and_length(secondary_structure: str, termini: termini_literal, start_index: int = None, end_index: int = None) -&gt; tuple[int, int]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>secondary_structure</code>             (<code>str</code>)         \u2013          <p>The secondary structure sequence of characters to search</p> </li> <li> <code>termini</code>             (<code>termini_literal</code>)         \u2013          <p>The termini to search</p> </li> <li> <code>start_index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The termini to search</p> </li> <li> <code>start_index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The first index in the terminal helix</p> </li> <li> <code>end_index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The last index in the terminal helix</p> </li> </ul> <p>Returns:     A tuple of the index from the secondary_structure argument where SS_HELIX_IDENTIFIERS begin at the provided         termini and the length that they persist for</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def get_terminal_helix_start_index_and_length(secondary_structure: str, termini: termini_literal,\n                                              start_index: int = None, end_index: int = None) -&gt; tuple[int, int]:\n    \"\"\"\n\n    Args:\n        secondary_structure: The secondary structure sequence of characters to search\n        termini: The termini to search\n        start_index: The termini to search\n        start_index: The first index in the terminal helix\n        end_index: The last index in the terminal helix\n    Returns:\n        A tuple of the index from the secondary_structure argument where SS_HELIX_IDENTIFIERS begin at the provided\n            termini and the length that they persist for\n    \"\"\"\n    # The start index of the specified termini\n    # Todo debug\n    if termini == 'n':\n        if not start_index:\n            start_index = secondary_structure.find(SS_HELIX_IDENTIFIERS)\n\n        if not end_index:\n            # Search for the last_index in a contiguous block of helical residues at the n-termini\n            for idx, sstruct in enumerate(secondary_structure[start_index:], start_index):\n                if sstruct != 'H':\n                    # end_index = idx\n                    break\n            # else:  # The whole structure is helical\n            end_index = idx  # - 1\n    else:  # termini == 'c':\n        if not end_index:\n            end_index = secondary_structure.rfind(SS_HELIX_IDENTIFIERS) + 1\n\n        if not start_index:\n            # Search for the last_index in a contiguous block of helical residues at the c-termini\n            # input(secondary_structure[end_index::-1])\n            # for idx, sstruct in enumerate(secondary_structure[end_index::-1]):\n            # Add 1 to the end index to include end_index secondary_structure in the reverse search\n            # for idx, sstruct in enumerate(reversed(secondary_structure[:end_index + 1])):\n            for idx, sstruct in enumerate(reversed(secondary_structure[:end_index])):\n                if sstruct != 'H':\n                    # idx -= 1\n                    break\n            # else:  # The whole structure is helical\n            start_index = end_index - idx\n\n    alignment_length = end_index - start_index\n\n    return start_index, alignment_length\n</code></pre>"},{"location":"reference/protocols/align/#protocols.align.align_helices","title":"align_helices","text":"<pre><code>align_helices(models: Sequence[ContainsEntities]) -&gt; list[PoseJob] | list\n</code></pre> <p>Parameters:</p> <ul> <li> <code>models</code>             (<code>Sequence[ContainsEntities]</code>)         \u2013          <p>The Structure instances to be used in docking</p> </li> </ul> <p>Returns:     The PoseJob instances created as a result of fusion</p> Source code in <code>symdesign/protocols/align.py</code> <pre><code>def align_helices(models: Sequence[ContainsEntities]) -&gt; list[PoseJob] | list:\n    \"\"\"\n\n    Args:\n        models: The Structure instances to be used in docking\n    Returns:\n        The PoseJob instances created as a result of fusion\n    \"\"\"\n    align_time_start = time.time()\n    # Retrieve symjob.JobResources for all flags\n    job = symjob.job_resources_factory.get()\n\n    sym_entry: SymEntry = job.sym_entry\n    \"\"\"The SymmetryEntry object describing the material\"\"\"\n\n    # Initialize incoming Structures\n    if len(models) != 2:\n        raise ValueError(\n            f\"Can't perform {align_helices.__name__} with {len(models)} models. Only 2 are allowed\")\n\n    model1: Pose\n    model2: Model\n    model1, model2 = models\n\n    if job.alignment_length:\n        alignment_length = job.alignment_length\n    else:\n        alignment_length = default_alignment_length = 5\n\n    project = f'Alignment_{model1.name}-{model2.name}'\n    project_dir = os.path.join(job.projects, project)\n    putils.make_path(project_dir)\n    protocol_name = 'helix_align'\n    # fusion_chain_id = 'A'\n    pose_jobs = []\n    opposite_termini = {'n': 'c', 'c': 'n'}\n    # Limit the calculation to a particular piece of the model\n    if job.target_chain is None:  # Use the whole model\n        selected_models1 = model1.entities\n        remaining_entities1 = [entity for entity in model1.entities]\n    else:\n        selected_chain1 = model1.get_chain(job.target_chain)\n        if not selected_chain1:\n            raise ValueError(\n                f\"The provided {flags.format_args(flags.target_chain_args)} '{job.target_chain}' wasn't found in the \"\n                f\"target model. Available chains = {', '.join(model1.chain_ids)}\")\n        # Todo make selection based off Entity\n        entity1 = model1.match_entity_by_seq(selected_chain1.sequence)\n        # Place a None token where the selected entity should be so that the SymEntry is accurate upon fusion\n        remaining_entities1 = [entity if entity != entity1 else None for entity in model1.entities]\n        selected_models1 = [entity1]\n\n    if job.output_trajectory:\n        if sym_entry.unit_cell:\n            logger.warning('No unit cell dimensions applicable to the trajectory file')\n\n    model2_entities_after_fusion = model2.number_of_entities - 1\n    # Create the corresponding SymEntry from the original SymEntry and the fusion\n    if sym_entry:\n        model1.set_symmetry(sym_entry=sym_entry)\n        # Todo\n        #  Currently only C1 can be fused. Remove hard coding when changed\n        # Use the model1.sym_entry as this could be crystalline\n        sym_entry_chimera = model1.sym_entry\n        for _ in range(model2_entities_after_fusion):\n            sym_entry_chimera.append_group('C1')\n        logger.debug(f'sym_entry_chimera: {repr(sym_entry_chimera)}, '\n                     f'specification {sym_entry_chimera.specification}')\n    else:\n        # Todo this can't be none as .groups[] is indexed later...\n        sym_entry_chimera = None\n\n    # Create the entity_transformations for model1\n    model1_entity_transformations = []\n    for transformation, set_mat_number in zip(model1.entity_transformations,\n                                              sym_entry.setting_matrices_numbers):\n        if transformation:\n            if transformation['translation'] is None:\n                internal_tx_x = internal_tx_y = internal_tx_z = None\n            else:\n                internal_tx_x, internal_tx_y, internal_tx_z = transformation['translation']\n            if transformation['translation2'] is None:\n                external_tx_x = external_tx_y = external_tx_z = None\n            else:\n                external_tx_x, external_tx_y, external_tx_z = transformation['translation2']\n\n            entity_transform = dict(\n                # rotation_x=rotation_degrees_x,\n                # rotation_y=rotation_degrees_y,\n                # rotation_z=rotation_degrees_z,\n                internal_translation_z=internal_tx_z,\n                setting_matrix=set_mat_number,\n                external_translation_x=external_tx_x,\n                external_translation_y=external_tx_y,\n                external_translation_z=external_tx_z)\n        else:\n            entity_transform = {}\n        model1_entity_transformations.append(entity_transform)\n    model2_entity_transformations = [{} for _ in range(model2_entities_after_fusion)]\n    _pose_count = count(1)\n\n    def _output_pose(name: str) -&gt; None:\n        \"\"\"Handle output of the identified pose\n\n        Args:\n            name: The name to associate with this job\n        Returns:\n            None\n        \"\"\"\n        # Output the pose as a PoseJob\n        # name = f'{termini}-term{helix_start_index + 1}'\n        logger.debug(f'Creating PoseJob')\n        pose_job = PoseJob.from_name(name, project=project, protocol=protocol_name, sym_entry=sym_entry_chimera)\n\n        # if job.output:\n        if job.output_fragments:\n            pose.find_and_split_interface()\n            # Query fragments\n            pose.generate_interface_fragments()\n        if job.output_trajectory:\n            available_chain_ids = chain_id_generator()\n\n            def _exhaust_n_chain_ids(n: int) -&gt; str:\n                for _ in range(n - 1):\n                    next(available_chain_ids)\n                return next(available_chain_ids)\n\n            # Write trajectory\n            if sym_entry.unit_cell:\n                logger.warning('No unit cell dimensions used to the trajectory file.')\n\n            with open(os.path.join(project_dir, 'trajectory_oligomeric_models.pdb'), 'a') as f_traj:\n                # pose.write(file_handle=f_traj, assembly=True)\n                f_traj.write('{:9s}{:&gt;4d}\\n'.format('MODEL', next(_pose_count)))\n                model_specific_chain_id = next(available_chain_ids)\n                for entity in pose.entities:\n                    starting_chain_id = entity.chain_id\n                    entity.chain_id = model_specific_chain_id\n                    entity.write(file_handle=f_traj, assembly=True)\n                    entity.chain_id = starting_chain_id\n                    model_specific_chain_id = _exhaust_n_chain_ids(entity.number_of_symmetry_mates)\n                f_traj.write(f'ENDMDL\\n')\n\n        # # Set the ASU, then write to a file\n        # pose.set_contacting_asu()\n        try:  # Remove existing cryst_record\n            del pose._cryst_record\n        except AttributeError:\n            pass\n        try:\n            pose.uc_dimensions = model1.uc_dimensions\n        except AttributeError:  # model1 isn't a crystalline symmetry\n            pass\n\n        # Todo the number of entities and the number of transformations could be different\n        # entity_transforms = []\n        for entity, transform in zip(pose.entities, model1_entity_transformations + model2_entity_transformations):\n            transformation = sql.EntityTransform(**transform)\n            # entity_transforms.append(transformation)\n            pose_job.entity_data.append(sql.EntityData(\n                meta=entity.metadata,\n                metrics=entity.metrics,\n                transform=transformation)\n            )\n\n        # session.add_all(entity_transforms)  # + entity_data)\n        # # Need to generate the EntityData.id\n        # session.flush()\n\n        pose_job.pose = pose\n        # pose_job.calculate_pose_design_metrics(session)\n        putils.make_path(pose_job.pose_directory)\n        pose_job.output_pose()\n        pose_job.source_path = pose_job.pose_path\n        pose_job.pose = None\n        if job.output_to_directory:\n            logger.info(f'Alignment output -&gt; {pose_job.output_pose_path}')\n        else:\n            logger.info(f'Alignment output -&gt; {pose_job.pose_path}')\n\n        pose_jobs.append(pose_job)\n\n    # Set parameters as null\n    observed_protein_data = {}\n    # Start the alignment search\n    for selected_idx1, entity1 in enumerate(selected_models1):\n        logger.info(f'Target component {entity1.name}')\n\n        if all(remaining_entities1):\n            additional_entities1 = remaining_entities1.copy()\n            additional_entities1[selected_idx1] = None\n        else:\n            additional_entities1 = remaining_entities1\n\n        # Solve for target/aligned residue features\n        half_entity1_length = entity1.number_of_residues / 2\n\n        # Check target_end first as the secondary_structure1 slice is dependent on lengths\n        if job.target_end:\n            target_end_residue = entity1.residue(job.target_end)\n            if target_end_residue is None:\n                raise DesignError(\n                    f\"Couldn't find the {flags.format_args(flags.target_end_args)} residue number {job.target_end} \"\n                    f\"in the {entity1.__class__.__name__}, {entity1.name}\")\n            target_end_index_ = target_end_residue.index\n            # See if the specified aligned_start/aligned_end lie in this termini orientation\n            if target_end_index_ &lt; half_entity1_length:\n                desired_end_target_termini = 'n'\n            else:  # Closer to c-termini\n                desired_end_target_termini = 'c'\n        else:\n            desired_end_target_termini = target_end_index_ = None\n\n        if job.target_start:\n            target_start_residue = entity1.residue(job.target_start)\n            if target_start_residue is None:\n                raise DesignError(\n                    f\"Couldn't find the {flags.format_args(flags.target_start_args)} residue number \"\n                    f\"{job.target_start} in the {entity1.__class__.__name__}, {entity1.name}\")\n            target_start_index_ = target_start_residue.index\n            # See if the specified aligned_start/aligned_end lie in this termini orientation\n            if target_start_index_ &lt; half_entity1_length:\n                desired_start_target_termini = 'n'\n            else:  # Closer to c-termini\n                desired_start_target_termini = 'c'\n        else:\n            desired_start_target_termini = target_start_index_ = None\n\n        # Set up desired_aligned_termini\n        if desired_start_target_termini:\n            if desired_end_target_termini:\n                if desired_start_target_termini != desired_end_target_termini:\n                    raise DesignError(\n                        f\"Found different termini specified for addition by your flags \"\n                        f\"{flags.format_args(flags.aligned_start_args)} ({desired_start_target_termini}-termini) \"\n                        f\"and{flags.format_args(flags.aligned_end_args)} ({desired_end_target_termini}-termini)\")\n\n            termini = desired_start_target_termini\n        elif desired_end_target_termini:\n            termini = desired_end_target_termini\n        else:\n            termini = None\n\n        if job.target_termini:\n            desired_termini = job.target_termini.copy()\n            if termini and termini not in desired_termini:\n                if desired_start_target_termini:\n                    flag = flags.target_start_args\n                    arg = job.target_start\n                else:\n                    flag = flags.target_end_args\n                    arg = job.target_end\n                raise DesignError(\n                    f\"The {flags.format_args(flags.target_termini_args)} '{job.target_termini}' isn't compatible with \"\n                    f\"your flag {flags.format_args(flag)} '{arg}' which would specify the {termini}-termini\")\n        elif termini:\n            desired_termini = [termini]\n        else:  # None specified, try all\n            desired_termini = ['n', 'c']\n\n        if job.trim_termini:\n            # Remove any unstructured termini from the Entity to enable most successful fusion\n            logger.info('Trimming unstructured termini to check for available helices')\n            entity1_copy = entity1.copy()\n            entity1_copy.delete_termini('unstructured')\n        else:\n            entity1_copy = entity1\n        # Check for helical termini on the target building block and remove those that are not available\n        for termini in reversed(desired_termini):\n            if not entity1_copy.is_termini_helical(termini, window=alignment_length):\n                logger.error(f\"The specified termini '{termini}' isn't helical\")\n                desired_termini.remove(termini)\n\n        if not desired_termini:\n            logger.info(f'Target component {entity1.name} has no termini remaining')\n            continue\n\n        if job.aligned_chain is None:  # Use the whole model\n            selected_models2 = model2.entities\n            remaining_entities2 = [entity for entity in model2.entities]\n        else:\n            selected_chain2 = model2.get_chain(job.aligned_chain)\n            if not selected_chain2:\n                raise ValueError(\n                    f\"The provided {flags.format_args(flags.aligned_chain_args)} '{job.aligned_chain}' wasn't found in \"\n                    f\"the aligned model. Available chains = {', '.join(model2.chain_ids)}\")\n            # Todo make selection based off Entity\n            entity2 = model2.match_entity_by_seq(selected_chain2.sequence)\n            remaining_entities2 = [entity for entity in model2.entities if entity != entity2]\n            selected_models2 = [entity2]\n\n        for selected_idx2, entity2 in enumerate(selected_models2):\n            logger.info(f'Aligned component {entity2.name}')\n\n            # Set variables for the additional entities\n            if len(remaining_entities2) == model2.number_of_entities:\n                additional_entities2 = remaining_entities2.copy()\n                additional_entities2.pop(selected_idx2)\n            else:\n                additional_entities2 = remaining_entities2\n            additional_entity_ids2 = [entity.name for entity in additional_entities2]\n\n            # Throw away chain ids that are in use by model1 to increment additional model2 entities to correct chain_id\n            available_chain_ids = chain_id_generator()\n            chain_id = next(available_chain_ids)\n            while chain_id in model1.chain_ids:\n                chain_id = next(available_chain_ids)\n\n            for add_ent2 in additional_entities2:\n                add_ent2.chain_id = chain_id\n                chain_id = next(available_chain_ids)\n\n            half_entity2_length = entity2.number_of_residues / 2\n            if job.aligned_start:\n                aligned_start_residue = entity2.residue(job.aligned_start)\n                if aligned_start_residue is None:\n                    raise DesignError(\n                        f\"Couldn't find the {flags.format_args(flags.aligned_start_args)} residue number \"\n                        f\"{job.aligned_start} in the {entity2.__class__.__name__}, {entity2.name}\")\n                aligned_start_index_ = aligned_start_residue.index\n\n                # See if the specified aligned_start/aligned_end lie in this termini orientation\n                if aligned_start_index_ &lt; half_entity2_length:\n                    desired_start_aligned_termini = 'n'\n                else:  # Closer to c-termini\n                    desired_start_aligned_termini = 'c'\n            else:\n                desired_start_aligned_termini = aligned_start_index_ = None\n\n            if job.aligned_end:\n                aligned_end_residue = entity2.residue(job.aligned_end)\n                if aligned_end_residue is None:\n                    raise DesignError(\n                        f\"Couldn't find the {flags.format_args(flags.aligned_end_args)} residue number {job.aligned_end} in \"\n                        f\"the {entity2.__class__.__name__}, {entity2.name}\")\n\n                aligned_end_index_ = aligned_end_residue.index\n                # See if the specified aligned_start/aligned_end lie in this termini orientation\n                if aligned_end_index_ &lt; half_entity2_length:\n                    desired_end_aligned_termini = 'n'\n                else:  # Closer to c-termini\n                    desired_end_aligned_termini = 'c'\n            else:\n                desired_end_aligned_termini = aligned_end_index_ = None\n\n            # Set the desired_aligned_termini\n            if desired_start_aligned_termini:\n                if desired_end_aligned_termini:\n                    if desired_start_aligned_termini != desired_end_aligned_termini:\n                        raise DesignError(\n                            f\"Found different termini specified for addition by your flags \"\n                            f\"{flags.format_args(flags.aligned_start_args)} ({desired_start_aligned_termini}-termini) and\"\n                            f\"{flags.format_args(flags.aligned_end_args)} ({desired_end_aligned_termini}-termini)\")\n\n                desired_aligned_termini = desired_start_aligned_termini\n            elif desired_end_aligned_termini:\n                desired_aligned_termini = desired_end_aligned_termini\n            else:\n                desired_aligned_termini = None\n\n            if job.trim_termini:\n                # Remove any unstructured termini from the Entity to enable most successful fusion\n                logger.info('Trimming unstructured termini to check for available helices')\n                entity2_copy = entity2.copy()\n                entity2_copy.delete_termini('to_helices')\n            else:\n                entity2_copy = entity2\n\n            termini_to_align = []\n            for termini in desired_termini:\n                # Check if the desired termini in the aligned structure is available\n                align_termini = opposite_termini[termini]\n                if entity2_copy.is_termini_helical(align_termini, window=alignment_length):\n                    if desired_aligned_termini and align_termini == desired_aligned_termini:\n                        # This is the correct termini\n                        termini_to_align.append(termini)\n                        break  # As only one termini can be specified and this was it\n                    else:\n                        termini_to_align.append(termini)\n                else:\n                    logger.info(f\"{align_helices.__name__} isn't possible for target {termini} to aligned \"\n                                f'{align_termini} since {entity2.name} is missing a helical {align_termini}-termini')\n\n            if not termini_to_align:\n                logger.info(f'Target component {entity2.name} has no termini remaining')\n\n            for termini in termini_to_align:\n                align_termini = opposite_termini[termini]\n                entity1_copy = entity1.copy()\n                entity2_copy = entity2.copy()\n                if job.trim_termini:\n                    # Remove any unstructured termini from the Entity to enable most successful fusion\n                    entity1_copy.delete_termini('to_helices', termini=termini)\n                    entity2_copy.delete_termini('to_helices', termini=align_termini)\n                    # entity.delete_unstructured_termini()\n                    # entity.delete_unstructured_termini()\n\n                logger.info(f'Starting {entity1.name} {termini}-termini')\n                # Get the target_start_index the length_of_target_helix\n                logger.debug(f'Checking {termini}-termini for helices:\\n\\t{entity1_copy.secondary_structure}')\n                target_start_index, length_of_target_helix = \\\n                    get_terminal_helix_start_index_and_length(entity1_copy.secondary_structure, termini,\n                                                              start_index=target_start_index_,\n                                                              end_index=target_end_index_)\n                logger.debug(f'Found {termini}-termini start index {target_start_index} and '\n                             f'length {length_of_target_helix}')\n                if job.extension_length:\n                    extension_length = job.extension_length\n                    # Add the extension length to the residue window if an ideal helix was added\n                    # length_of_helix_model = length_of_target_helix + extension_length\n                    logger.info(f'Adding {extension_length} residues to the target helix. This will result in a maximum'\n                                f' extension of {extension_length - alignment_length - 1 } residues')\n                else:\n                    extension_length = 0\n\n                truncated_entity1, helix_model = prepare_alignment_motif(\n                    entity1_copy, target_start_index, length_of_target_helix,\n                    termini=termini, extension_length=extension_length, alignment_length=alignment_length)\n                # Rename the models to enable fusion\n                chain_id = truncated_entity1.chain_id\n                helix_model.chain_id = chain_id\n                entity2.chain_id = chain_id\n\n                length_of_helix_model = helix_model.number_of_residues\n                logger.debug(f'length_of_helix_model: {length_of_helix_model}')\n                max_target_helix_length = length_of_helix_model - alignment_length\n                logger.debug(f'Number of helical positions on target: {max_target_helix_length}')\n                target_start_indices_sequence = range(max_target_helix_length)\n\n                # Get the aligned_start_index and length_of_aligned_helix\n                logger.debug(f'Checking {align_termini}-termini for helices:\\n\\t{entity2_copy.secondary_structure}')\n                aligned_start_index, length_of_aligned_helix = \\\n                    get_terminal_helix_start_index_and_length(entity2_copy.secondary_structure, align_termini,\n                                                              start_index=aligned_start_index_,\n                                                              end_index=aligned_end_index_)\n                logger.debug(f'Found {align_termini}-termini start index {aligned_start_index} and '\n                             f'length {length_of_aligned_helix}')\n\n                # Scan along the aligned helix length\n                logger.debug(f'length_of_aligned_helix: {length_of_aligned_helix}')\n                logger.debug(f'alignment_length: {alignment_length}')\n                aligned_length = length_of_aligned_helix + 1 - alignment_length\n                if aligned_length &lt; 1:\n                    logger.info(\n                        f\"Aligned component {entity2.name} {align_termini}-termini isn't long enough for alignment\")\n                    continue\n\n                sample_all_alignments = False  # Debugging True  # First draft\n                if sample_all_alignments:\n                    align_iteration_direction = iter\n                    target_iteration_direction = iter\n                    target_start_iterator = target_start_indices_sequence\n                else:\n                    # Todo\n                    #  targeted method which searches only possible\n                    # Need to work out the align and sample function to wrap each index pair inside to clean for loops\n                    if termini == 'n':\n                        align_iteration_direction = iter\n                        target_iteration_direction = reversed\n                        target_pause_index_during_aligned_loop = max_target_helix_length - 1\n                    else:  # termini == 'c'\n                        align_iteration_direction = reversed\n                        target_iteration_direction = iter\n                        target_pause_index_during_aligned_loop = 0\n                    target_start_iterator = [target_pause_index_during_aligned_loop]\n\n                aligned_count = count(1)\n                aligned_range_end = aligned_start_index + aligned_length\n                align_start_indices_sequence = range(aligned_start_index, aligned_range_end)\n                for aligned_start_index in align_iteration_direction(align_start_indices_sequence):\n                    aligned_idx = next(aligned_count)\n                    # logger.debug(f'aligned_idx: {aligned_idx}')\n                    logger.debug(f'aligned_start_index: {aligned_start_index}')\n                    # logger.debug(f'number of residues: {entity2_copy.number_of_residues}')\n\n                    # # Todo use full model_helix mode\n                    # truncated_entity2, helix_model2 = prepare_alignment_motif(\n                    #     entity2_copy, aligned_start_index, alignment_length, termini=align_termini)\n                    # # Todo? , extend_helix=extension_length)\n                    # # Rename the models to enable fusion\n                    # truncated_entity2.chain_id = chain_id\n\n                    aligned_end_index = aligned_start_index + alignment_length\n                    # Calculate the entity2 indices to delete after alignment position is found\n                    if align_termini == 'c':\n                        delete_indices2 = list(range(aligned_end_index, entity2_copy.c_terminal_residue.index + 1))\n                    else:\n                        delete_indices2 = list(range(entity2_copy.n_terminal_residue.index, aligned_start_index))\n                    # Get aligned coords\n                    coords2 = entity2_copy.get_coords_subset(\n                        indices=list(range(aligned_start_index, aligned_end_index)),\n                        dtype='backbone')\n\n                    # Todo\n                    #  For every iteration of the aligned_start_index, perform the alignment procedure\n                    #  Need to perform short target loops during aligned loop or make alignment procedure a function...\n\n                    if aligned_idx == aligned_length:\n                        # The maximum number of aligned_start_index have been reached, iterate over the target now\n                        target_start_iterator = target_iteration_direction(target_start_indices_sequence)\n                    # else:\n                    #     target_start_iterator = []\n\n                    # # Scan along the target helix length\n                    # # helix_start_index = 0\n                    # max_target_helix_length = length_of_helix_model - alignment_length\n                    # logger.debug(f'Number of helical positions on target: {max_target_helix_length}')\n                    # target_start_indices_sequence = range(max_target_helix_length)\n                    # for helix_start_index in target_iteration_direction(target_start_indices_sequence):\n                    #\n                    for helix_start_index in target_start_iterator:\n                        logger.debug(f'helix_start_index: {helix_start_index}')\n                        helix_end_index = helix_start_index + alignment_length\n\n                        # if helix_end_index &gt; maximum_helix_alignment_length:\n                        #     break  # This isn't allowed\n                        sampling_index = f'{aligned_idx}/{aligned_length}, ' \\\n                                         f'{helix_start_index + 1}/{max_target_helix_length}'\n                        # Get target coords\n                        coords1 = helix_model.get_coords_subset(\n                            indices=list(range(helix_start_index, helix_end_index)),\n                            dtype='backbone')\n                        try:\n                            # # Use helix_model2 start index = 0 as helix_model2 is always truncated\n                            # # Use transformed_entity2 mode\n                            # rmsd, rot, tx = align_model_to_helix(\n                            #     entity2_copy, aligned_start_index, helix_model, helix_start_index, alignment_length)\n                            rmsd, rot, tx = superposition3d(coords1, coords2)\n                            # # Use full model_helix mode\n                            # rmsd, rot, tx = align_model_to_helix(\n                            #     helix_model2, 0, helix_model, helix_start_index, alignment_length)\n                        except ValueError as error:  # The lengths of the coords aren't equal, report and proceed\n                            logger.warning(str(error))\n                            continue\n                        else:\n                            logger.info(f'{entity1.name} {termini}-termini to {entity2.name} {align_termini}-termini '\n                                        f'alignment {sampling_index} has RMSD of {rmsd:.4f}')\n\n                        # Transform and copy to facilitate delete\n                        transformed_entity2 = entity2_copy.get_transformed_copy(rotation=rot, translation=tx)\n                        # Delete overhanging residues\n                        transformed_entity2.delete_residues(indices=delete_indices2)\n\n                        # Order the models, slice the helix for overlapped segments\n                        if termini == 'n':\n                            ordered_entity1 = transformed_entity2\n                            ordered_entity2 = truncated_entity1\n                            helix_model_start_index = helix_end_index\n                            helix_model_range = range(helix_model_start_index, length_of_helix_model)\n\n                            # Get Residue instances that mark the boundary of the fusion\n                            start_residue1 = transformed_entity2.n_terminal_residue\n                            end_residue1 = transformed_entity2.c_terminal_residue  # residues[aligned_end_index - 1]\n                            if helix_end_index &lt; extension_length:  # Zero-indexed &lt;= one-indexed\n                                entity1_first_residue_index = target_start_index\n                                extension_str = f'-extend-{extension_length - helix_end_index}'\n                            else:  # First time this runs, it adds 0 to target_start_index\n                                entity1_first_residue_index = target_start_index + helix_end_index - extension_length\n                                extension_str = ''\n                            # Use entity1_copy due to the removed helical portion on truncated_entity1\n                            start_residue2 = entity1_copy.residues[entity1_first_residue_index]\n                            end_residue2 = entity1_copy.c_terminal_residue\n                        else:  # termini == 'c'\n                            ordered_entity1 = truncated_entity1\n                            ordered_entity2 = transformed_entity2\n                            helix_model_start_index = 0\n                            helix_model_range = range(helix_model_start_index, helix_start_index)\n\n                            # Get Residue instances that mark the boundary of the fusion\n                            # Use entity1_copy due to the removed helical portion on truncated_entity1\n                            start_residue1 = entity1_copy.n_terminal_residue\n                            if helix_start_index &lt;= length_of_target_helix:  # Zero-indexed &lt; one-indexed\n                                entity1_last_residue_index = target_start_index + helix_start_index - 1\n                                extension_str = ''\n                            else:\n                                entity1_last_residue_index = target_start_index + length_of_target_helix - 1\n                                extension_str = f'-extend-{helix_start_index - length_of_target_helix}'\n                            end_residue1 = entity1_copy.residues[entity1_last_residue_index]  # c_terminal_residue\n                            start_residue2 = transformed_entity2.n_terminal_residue  # residues[aligned_start_index]\n                            end_residue2 = transformed_entity2.c_terminal_residue\n\n                        # Get the new fusion name\n                        # alignment_numbers = f'{start_residue1.number}-{end_residue1.number}+{extension_str}/' \\\n                        #                     f'{start_residue2.number}-{end_residue2.number}'\n                        fusion_name = f'{ordered_entity1.name}_{start_residue1.number}-' \\\n                                      f'{end_residue1.number}_fused{extension_str}-to' \\\n                                      f'_{ordered_entity2.name}_{start_residue2.number}-' \\\n                                      f'{end_residue2.number}'\n                        # Reformat residues for new chain\n                        helix_n_terminal_residue_number = ordered_entity1.c_terminal_residue.number + 1\n                        helix_model.renumber_residues(index=helix_model_start_index, at=helix_n_terminal_residue_number)\n                        helix_residues = helix_model.get_residues(indices=list(helix_model_range))\n                        ordered_entity2.renumber_residues(at=helix_n_terminal_residue_number + len(helix_residues))\n\n                        if job.bend:  # Get the joint_residue for later manipulation\n                            joint_residue = transformed_entity2.residues[aligned_start_index + alignment_length//2]\n\n                        # Create fused Entity and rename select attributes\n                        logger.debug(f'Fusing {ordered_entity1} to {ordered_entity2}')\n                        fused_entity = Entity.from_residues(\n                            ordered_entity1.residues + helix_residues + ordered_entity2.residues,\n                            name=fusion_name, chain_ids=[chain_id],\n                            # Using sequence as reference_sequence is uncertain given the fusion\n                            reference_sequence=ordered_entity1.sequence\n                            + ''.join(r.type1 for r in helix_residues) + ordered_entity2.sequence,\n                            uniprot_ids=tuple(uniprot_id for entity in [ordered_entity1, ordered_entity2]\n                                              for uniprot_id in entity.uniprot_ids)\n                        )\n\n                        # ordered_entity1.write(out_path='DEBUG_1.pdb')\n                        # helix_model.write(out_path='DEBUG_H.pdb')\n                        # ordered_entity2.write(out_path='DEBUG_2.pdb')\n                        # fused_entity.write(out_path='DEBUG_FUSED.pdb')\n\n                        # Correct the .metadata attribute for each entity in the full assembly\n                        # This is crucial for sql usage\n                        protein_metadata = observed_protein_data.get(fusion_name)\n                        if not protein_metadata:\n                            protein_metadata = sql.ProteinMetadata(\n                                entity_id=fused_entity.name,  # model_source=None\n                                reference_sequence=fused_entity.sequence,\n                                thermophilicity=sum((entity1.thermophilicity, entity2.thermophilicity)),\n                                # symmetry_group=sym_entry_chimera.groups[entity_idx],\n                                n_terminal_helix=ordered_entity1.is_termini_helical(),\n                                c_terminal_helix=ordered_entity2.is_termini_helical('c'),\n                                uniprot_entities=tuple(uniprot_entity for entity in [ordered_entity1, ordered_entity2]\n                                                       for uniprot_entity in entity.metadata.uniprot_entities))\n                            observed_protein_data[fusion_name] = protein_metadata\n                        fused_entity.metadata = protein_metadata\n\n                        # Create the list of Entity instances for the new Pose\n                        # logger.debug(f'Loading pose')\n                        if additional_entities1:\n                            all_entities = []\n                            for entity_idx, entity in enumerate(additional_entities1):\n                                if entity is None:\n                                    fused_entity.metadata.symmetry_group = sym_entry_chimera.groups[entity_idx]\n                                    all_entities.append(fused_entity)\n                                else:\n                                    all_entities.append(entity)\n                        else:\n                            fused_entity.metadata.symmetry_group = sym_entry_chimera.groups[0]\n                            all_entities = [fused_entity]\n\n                        if additional_entities2:\n                            transformed_additional_entities2 = [\n                                entity.get_transformed_copy(rotation=rot, translation=tx)\n                                for entity in additional_entities2]\n                            # transformed_additional_entities2[0].write(out_path='DEBUG_Additional2.pdb')\n                            # input('DEBUG_Additional2.pdb')\n                            all_entities += transformed_additional_entities2\n\n                        logger.debug(f'Creating Pose from entities: '\n                                     f'{\", \".join(repr(entity) for entity in all_entities)}')\n                        pose = Pose.from_entities(all_entities, sym_entry=sym_entry_chimera)\n                        # pose.entities[0].write(assembly=True, out_path='DEBUG_oligomer.pdb')\n                        # pose.write(out_path='DEBUG_POSE.pdb', increment_chains=True)\n                        # pose.write(assembly=True, out_path='DEBUG_ASSEMBLY.pdb')  # , increment_chains=True)\n\n                        name = fusion_name\n                        # name = f'{termini}-term_{aligned_idx + 1}-{helix_start_index + 1}'\n                        if job.bend:\n                            central_aligned_residue = pose.get_chain(chain_id).residue(joint_residue.number)\n                            if central_aligned_residue is None:\n                                logger.warning(\"Couldn't locate the joint_residue with residue number \"\n                                               f\"{joint_residue.number} from chain {chain_id}\")\n                                continue\n                            # print(central_aligned_residue)\n                            bent_coords = bend(pose, central_aligned_residue, termini, samples=job.bend,\n                                               additional_entity_ids=[entity.name for entity in pose.entities\n                                                                      if entity.name in additional_entity_ids2])\n                            for bend_idx, coords in enumerate(bent_coords, 1):\n                                pose.coords = coords\n                                if pose.is_clash(measure=self.job.design.clash_criteria,\n                                                 distance=self.job.design.clash_distance, silence_exceptions=True):\n                                    logger.info(f'Alignment {fusion_name}, bend {bend_idx} clashes')\n                                    if job.design.ignore_pose_clashes:\n                                        pass\n                                    else:\n                                        continue\n\n                                if pose.is_symmetric():\n                                    if pose.symmetric_assembly_is_clash(measure=self.job.design.clash_criteria,\n                                                                        distance=self.job.design.clash_distance):\n                                        logger.info(f'Alignment {fusion_name}, bend {bend_idx} has '\n                                                    'symmetric clashes')\n                                        if job.design.ignore_symmetric_clashes:\n                                            pass\n                                        else:\n                                            continue\n\n                                _output_pose(name + f'-bend{bend_idx}')\n                        else:\n                            if pose.is_clash(measure=job.design.clash_criteria,\n                                             distance=job.design.clash_distance, silence_exceptions=True):\n                                logger.info(f'Alignment {fusion_name} clashes')\n                                if job.design.ignore_pose_clashes:\n                                    pass\n                                else:\n                                    continue\n\n                            if pose.is_symmetric():\n                                if pose.symmetric_assembly_is_clash(measure=job.design.clash_criteria,\n                                                                    distance=job.design.clash_distance):\n                                    logger.info(f'Alignment {fusion_name} has symmetric clashes')\n                                    if job.design.ignore_symmetric_clashes:\n                                        pass\n                                    else:\n                                        continue\n\n                            _output_pose(name)\n\n    logger.info(f'Total {project} trajectory took {time.time() - align_time_start:.2f}s')\n    if not pose_jobs:\n        logger.info(f'Found no viable outputs')\n        return []\n\n    def terminate(pose_jobs: list[PoseJob]) -&gt; list[PoseJob]:  # poses_df_: pd.DataFrame, residues_df_: pd.DataFrame)\n        \"\"\"Finalize any remaining work and return to the caller\"\"\"\n        pose_jobs = insert_pose_jobs(session, pose_jobs, project)\n\n        # # Format output data, fix missing\n        # if job.db:\n        #     pose_ids = [pose_job.id for pose_job in pose_jobs]\n        # else:\n        #     pose_ids = pose_names\n\n        # # Extract transformation parameters for output\n        # def populate_pose_metadata():\n        #     \"\"\"Add all required PoseJob information to output the created Pose instances for persistent storage\"\"\"\n        #     # nonlocal poses_df_, residues_df_\n        #     # Save all pose transformation information\n        #     # From here out, the transforms used should be only those of interest for outputting/sequence design\n        #\n        #     # # Format pose transformations for output\n        #     # # Get all rotations in terms of the degree of rotation along the z-axis\n        #     # # Using the x, y rotation to enforce the degeneracy matrix...\n        #     # rotation_degrees_x, rotation_degrees_y, rotation_degrees_z = \\\n        #     #     zip(*Rotation.from_matrix(rotation).as_rotvec(degrees=True).tolist())\n        #     #\n        #     # blank_parameter = [None, None, None]\n        #     # if sym_entry.is_internal_tx1:\n        #     #     # nonlocal full_int_tx1\n        #     #     if len(full_int_tx1) &gt; 1:\n        #     #         full_int_tx1 = full_int_tx1.squeeze()\n        #     #     z_height1 = full_int_tx1[:, -1]\n        #     # else:\n        #     #     z_height1 = blank_parameter\n        #     #\n        #     # set_mat1_number, set_mat2_number, *_extra = sym_entry.setting_matrices_numbers\n        #     # # if sym_entry.unit_cell:\n        #     # #     full_uc_dimensions = full_uc_dimensions[passing_symmetric_clash_indices_perturb]\n        #     # #     full_ext_tx1 = full_ext_tx1[:]\n        #     # #     full_ext_tx2 = full_ext_tx2[:]\n        #     # #     full_ext_tx_sum = full_ext_tx2 - full_ext_tx1\n        #     # external_translation_x, external_translation_y, external_translation_z = \\\n        #     #     blank_parameter if ext_translation is None else ext_translation\n        #\n        #     # Update the sql.EntityData with transformations\n        #     # for idx, pose_job in enumerate(pose_jobs):\n        #     #     # Update sql.EntityData, sql.EntityMetrics, sql.EntityTransform\n        #     #     # pose_id = pose_job.id\n        #\n        #\n        #     # if job.db:\n        #     #     # Update the poses_df_ and residues_df_ index to reflect the new pose_ids\n        #     #     poses_df_.index = pd.Index(pose_ids, name=sql.PoseMetrics.pose_id.name)\n        #     #     # Write dataframes to the sql database\n        #     #     metrics.sql.write_dataframe(session, poses=poses_df_)\n        #     #     output_residues = False\n        #     #     if output_residues:  # Todo job.metrics.residues\n        #     #         residues_df_.index = pd.Index(pose_ids, name=sql.PoseResidueMetrics.pose_id.name)\n        #     #         metrics.sql.write_dataframe(session, pose_residues=residues_df_)\n        #     # else:  # Write to disk\n        #     #     residues_df_.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)  # ascending=False\n        #     #     putils.make_path(job.all_scores)\n        #     #     residue_metrics_csv = os.path.join(job.all_scores, f'{building_blocks}_docked_poses_Residues.csv')\n        #     #     residues_df_.to_csv(residue_metrics_csv)\n        #     #     logger.info(f'Wrote residue metrics to {residue_metrics_csv}')\n        #     #     trajectory_metrics_csv = \\\n        #     #         os.path.join(job.all_scores, f'{building_blocks}_docked_poses_Trajectories.csv')\n        #     #     job.dataframe = trajectory_metrics_csv\n        #     #     poses_df_ = pd.concat([poses_df_], keys=[('dock', 'pose')], axis=1)\n        #     #     poses_df_.columns = poses_df_.columns.swaplevel(0, 1)\n        #     #     poses_df_.sort_index(level=2, axis=1, inplace=True, sort_remaining=False)\n        #     #     poses_df_.sort_index(level=1, axis=1, inplace=True, sort_remaining=False)\n        #     #     poses_df_.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)\n        #     #     poses_df_.to_csv(trajectory_metrics_csv)\n        #     #     logger.info(f'Wrote trajectory metrics to {trajectory_metrics_csv}')\n        #\n        # # Populate the database with pose information. Has access to nonlocal session\n        # populate_pose_metadata()\n\n        return pose_jobs\n\n    with job.db.session(expire_on_commit=False) as session:\n        pose_jobs = terminate(pose_jobs)  # , poses_df, residues_df)\n        session.commit()\n        metrics_stmt = select(PoseJob).where(PoseJob.id.in_([pose_job.id for pose_job in pose_jobs])) \\\n            .execution_options(populate_existing=True) \\\n            .options(selectinload(PoseJob.metrics))\n        pose_jobs = session.scalars(metrics_stmt).all()\n\n    return pose_jobs\n</code></pre>"},{"location":"reference/protocols/cluster/","title":"cluster","text":""},{"location":"reference/protocols/cluster/#protocols.cluster.pose_pair_rmsd","title":"pose_pair_rmsd","text":"<pre><code>pose_pair_rmsd(pose1: PoseJob, pose2: PoseJob) -&gt; float\n</code></pre> <p>Calculate the rmsd between pairs of Poses using CB coordinates. Must be the same length pose</p> <p>Parameters:</p> <ul> <li> <code>pose1</code>             (<code>PoseJob</code>)         \u2013          <p>First PoseJob object</p> </li> <li> <code>pose2</code>             (<code>PoseJob</code>)         \u2013          <p>Second PoseJob object</p> </li> </ul> <p>Returns:     RMSD value</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def pose_pair_rmsd(pose1: PoseJob, pose2: PoseJob) -&gt; float:\n    \"\"\"Calculate the rmsd between pairs of Poses using CB coordinates. Must be the same length pose\n\n    Args:\n        pose1: First PoseJob object\n        pose2: Second PoseJob object\n    Returns:\n        RMSD value\n    \"\"\"\n    # This focuses on all residues, not any particular set of residues\n    rmsd, rot, tx = superposition3d(pose1.pose.cb_coords, pose2.pose.cb_coords)\n    return rmsd\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.pose_pair_by_rmsd","title":"pose_pair_by_rmsd","text":"<pre><code>pose_pair_by_rmsd(compositions: Iterable[Sequence[PoseJob]]) -&gt; dict[str | PoseJob, list[str | PoseJob]]\n</code></pre> <p>Perform rmsd comparison for all compositions of PoseJob instances</p> <p>Parameters:</p> <ul> <li> <code>compositions</code>             (<code>Iterable[Sequence[PoseJob]]</code>)         \u2013          <p>Groups of PoseJob instances that should be measured against one another pairwise</p> </li> </ul> <p>Returns:     {PoseJob representative: [PoseJob members], ... }</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def pose_pair_by_rmsd(compositions: Iterable[Sequence[PoseJob]]) -&gt; dict[str | PoseJob, list[str | PoseJob]]:\n    \"\"\"Perform rmsd comparison for all compositions of PoseJob instances\n\n    Args:\n        compositions: Groups of PoseJob instances that should be measured against one another pairwise\n    Returns:\n        {PoseJob representative: [PoseJob members], ... }\n    \"\"\"\n    for pose_jobs in compositions:\n        # Make all PoseJob combinations for this pair\n        pose_job_pairs = list(combinations(pose_jobs, 2))\n        results = [pose_pair_rmsd(*pair) for pair in pose_job_pairs]\n        # Add all identical comparison results (all rmsd are 0 as they are with themselves\n        results.extend(list(repeat(0, len(pose_jobs))))\n        # Add all identical PoseJob combinations to pose_job_pairs\n        pose_job_pairs.extend(list(zip(pose_jobs, pose_jobs)))\n\n        return cluster_poses_by_value(pose_job_pairs, results)\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.ialign","title":"ialign","text":"<pre><code>ialign(*pdb_files: AnyStr, chain1: str = None, chain2: str = None, out_path: AnyStr = os.path.join(os.getcwd(), 'ialign')) -&gt; float\n</code></pre> <p>Run non-sequential iAlign on two .pdb files</p> <p>Parameters:</p> <ul> <li> <code>pdb_files</code>             (<code>AnyStr</code>, default:                 <code>()</code> )         \u2013          </li> <li> <code>#</code>             (<code>pdb_file1</code>)         \u2013          </li> <li> <code>#</code>             (<code>pdb_file2</code>)         \u2013          </li> <li> <code>chain1</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain2</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>join(getcwd(), 'ialign')</code> )         \u2013          <p>The path to write iAlign results to</p> </li> </ul> <p>Returns:     The IS score from Mu &amp; Skolnic 2010</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def ialign(*pdb_files: AnyStr, chain1: str = None, chain2: str = None,\n           out_path: AnyStr = os.path.join(os.getcwd(), 'ialign')) -&gt; float:\n    \"\"\"Run non-sequential iAlign on two .pdb files\n\n    Args:\n        pdb_files:\n        # pdb_file1:\n        # pdb_file2:\n        chain1:\n        chain2:\n        out_path: The path to write iAlign results to\n    Returns:\n        The IS score from Mu &amp; Skolnic 2010\n    \"\"\"\n    if chain1 is None:\n        chain1 = 'AB'\n    if chain2 is None:\n        chain2 = 'AB'\n    chains = ['-c1', chain1, '-c2', chain2]\n\n    pdb_file1, pdb_file2, *_ = pdb_files\n    temp_pdb_file1 = os.path.join(os.getcwd(), 'temp',\n                                  os.path.basename(pdb_file1.translate(utils.keep_digit_table)))\n    temp_pdb_file2 = os.path.join(os.getcwd(), 'temp',\n                                  os.path.basename(pdb_file2.translate(utils.keep_digit_table)))\n    # Move the desired files to a temporary file location\n    os.system(f'scp {pdb_file1} {temp_pdb_file1}')\n    os.system(f'scp {pdb_file2} {temp_pdb_file2}')\n    # Perform the iAlign process\n    # Example: perl ../bin/ialign.pl -w output -s -a 0 1lyl.pdb AC 12as.pdb AB | grep \"IS-score = \"\n    cmd = ['perl', putils.ialign_exe_path, '-s', '-w', out_path, '-p1', temp_pdb_file1, '-p2', temp_pdb_file2] + chains\n    logger.debug(f'iAlign command: {subprocess.list2cmdline(cmd)}')\n    ialign_p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    ialign_out, ialign_err = ialign_p.communicate()\n    # Format the output\n    # Example: IS-score = 0.38840, P-value = 0.3808E-003, Z-score =  7.873\n    grep_p = subprocess.Popen(['grep', 'IS-score = '], stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n    ialign_is_score, err = grep_p.communicate(input=ialign_out)\n    ialign_is_score = ialign_is_score.decode()\n    logger.debug(f'iAlign interface alignment: {ialign_is_score.strip()}')\n    is_score, pvalue, z_score = [score.split('=')[-1].strip() for score in ialign_is_score.split(',')]\n    try:\n        is_score = float(is_score)\n    except ValueError:  # is_score isn't a number\n        logger.debug('No significant interface found')\n        is_score = 0.\n\n    return is_score\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.cluster_poses_by_value","title":"cluster_poses_by_value","text":"<pre><code>cluster_poses_by_value(identifier_pairs: Iterable[tuple[Any, Any]], values: Iterable[float], epsilon: float = 1.0) -&gt; dict[str | PoseJob, list[str | PoseJob]]\n</code></pre> <p>Take pairs of identifiers and a precomputed distance metric (such as RMSD) and cluster using DBSCAN algorithm</p> <p>Parameters:</p> <ul> <li> <code>identifier_pairs</code>             (<code>Iterable[tuple[Any, Any]]</code>)         \u2013          <p>The identifiers for each pair measurement</p> </li> <li> <code>values</code>             (<code>Iterable[float]</code>)         \u2013          <p>The corresponding measurement values for each pair of identifiers</p> </li> <li> <code>epsilon</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The parameter for DBSCAN to influence the spread of clusters, needs to be tuned for measurement values</p> </li> </ul> <p>Returns:     {PoseJob representative: [PoseJob members], ... }</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def cluster_poses_by_value(identifier_pairs: Iterable[tuple[Any, Any]], values: Iterable[float], epsilon: float = 1.) \\\n        -&gt; dict[str | PoseJob, list[str | PoseJob]]:\n    \"\"\"Take pairs of identifiers and a precomputed distance metric (such as RMSD) and cluster using DBSCAN algorithm\n\n    Args:\n        identifier_pairs: The identifiers for each pair measurement\n        values: The corresponding measurement values for each pair of identifiers\n        epsilon: The parameter for DBSCAN to influence the spread of clusters, needs to be tuned for measurement values\n    Returns:\n        {PoseJob representative: [PoseJob members], ... }\n    \"\"\"\n    # BELOW IS THE INPUT FORMAT I WANT FOR cluster_poses_by_value()\n    # index = list(combinations(pose_jobs, 2)) + list(zip(pose_jobs, pose_jobs))\n    # values = values + tuple(repeat(0, len(pose_jobs)))\n    # pd.Series(values, index=pd.MultiIndex.from_tuples(index)).unstack()\n\n    pair_df = pd.Series(values, index=pd.MultiIndex.from_tuples(identifier_pairs)).fillna(0.).unstack()\n    # symmetric_pair_values = sym(pair_df.values)\n\n    # PCA analysis of distances\n    # building_block_rmsd_matrix = sklearn.preprocessing.StandardScaler().fit_transform(symmetric_pair_values)\n    # pca = PCA(putils.default_pca_variance)\n    # building_block_rmsd_pc_np = pca.fit_transform(building_block_rmsd_matrix)\n    # pca_distance_vector = pdist(building_block_rmsd_pc_np)\n    # epsilon = pca_distance_vector.mean() * 0.5\n    # Compute pose clusters using DBSCAN algorithm\n    # precomputed specifies that a precomputed distance matrix is being passed\n    dbscan = sklearn.cluster.DBSCAN(eps=epsilon, min_samples=2, metric='precomputed')\n    dbscan.fit(utils.sym(pair_df.to_numpy()))\n    # find the cluster representative by minimizing the cluster mean\n    cluster_ids = set(dbscan.labels_)\n    # print(dbscan.labels_)\n    # Use of dbscan.core_sample_indices_ returns all core_samples which is not a nearest neighbors mean index\n    # print(dbscan.core_sample_indices_)\n    outlier = -1\n    try:\n        cluster_ids.remove(outlier)  # Remove outlier label, will add all these later\n    except KeyError:\n        pass\n\n    # Find the cluster representative and members\n    clustered_poses = {}\n    for cluster_id in cluster_ids:\n        # loc_indices = pair_df.index[np.where(cluster_id == dbscan.labels_)]\n        # cluster_representative = pair_df.loc[loc_indices, loc_indices].mean().argmax()\n        iloc_indices = np.where(dbscan.labels_ == cluster_id)\n        # take mean (doesn't matter which axis) and find the minimum (most similar to others) as representative\n        cluster_representative_idx = pair_df.iloc[iloc_indices, iloc_indices].mean().argmin()\n        # set all the cluster members belonging to the cluster representative\n        # pose_cluster_members = pair_df.index[iloc_indices].tolist()\n        clustered_poses[pair_df.index[cluster_representative_idx]] = pair_df.index[iloc_indices].tolist()\n\n    # Add all outliers to the clustered poses as a representative\n    outlier_poses = pair_df.index[np.where(dbscan.labels_ == outlier)]\n    clustered_poses.update(dict(zip(outlier_poses, outlier_poses)))\n\n    return clustered_poses\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.apply_transform_groups_to_guide_coordinates","title":"apply_transform_groups_to_guide_coordinates","text":"<pre><code>apply_transform_groups_to_guide_coordinates(*transforms: tuple[dict[str:ndarray]]) -&gt; list[ndarray]\n</code></pre> <p>For each incoming transformation, transform guide coordinates according to the specified transformations</p> <p>Parameters:</p> <ul> <li> <code>transforms</code>             (<code>tuple[dict[str:ndarray]]</code>, default:                 <code>()</code> )         \u2013          <p>The individual transformation groups that should be applied to a guide coordinate</p> </li> </ul> <p>Returns:     Guide coordinates transformed for each passed transform in each passed transform group</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def apply_transform_groups_to_guide_coordinates(*transforms: tuple[dict[str: np.ndarray]]) -&gt; list[np.ndarray]:\n    \"\"\"For each incoming transformation, transform guide coordinates according to the specified transformations\n\n    Args:\n        transforms: The individual transformation groups that should be applied to a guide coordinate\n    Returns:\n        Guide coordinates transformed for each passed transform in each passed transform group\n    \"\"\"\n    # Make a blank set of guide coordinates for each incoming transformation\n    # number_of_coordinate_values = 9\n    guide_coords = np.array([[0., 0., 0.], [1., 0., 0.], [0., 1., 0.]])\n    try:\n        allowed_keys = ['rotation', 'translation']\n        operation_lengths = []\n        for key in allowed_keys:\n            operation = transforms[0].get(key)\n            if operation is not None:\n                operation_lengths.append(len(operation))\n        try:\n            tiled_length = max(operation_lengths)\n        except ValueError:  # operation_lengths is empty\n            raise KeyError(\n                f'{apply_transform_groups_to_guide_coordinates.__name__}: Must pass one of the values '\n                f'{\" or \".join(allowed_keys)}')\n\n        tiled_guide_coords = np.tile(guide_coords, (tiled_length, 1, 1))\n    except IndexError:  # transforms[0] failed\n        raise IndexError(\n            f'{apply_transform_groups_to_guide_coordinates.__name__}: No arguments passed for transforms')\n\n    transformed_guide_coords_sets = \\\n        [transform_coordinate_sets(tiled_guide_coords, **transform) for transform in transforms]\n\n    return transformed_guide_coords_sets\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.cluster_transformation_pairs","title":"cluster_transformation_pairs","text":"<pre><code>cluster_transformation_pairs(*transforms: tuple[dict[str, ndarray]], distance: float = 1.0, minimum_members: int = 2) -&gt; tuple[NearestNeighbors, DBSCAN]\n</code></pre> <p>Cluster a group of transformation parameters sets to find those which occupy essentially the same space</p> <p>Parameters:</p> <ul> <li> <code>transforms</code>             (<code>tuple[dict[str, ndarray]]</code>, default:                 <code>()</code> )         \u2013          <p>Group containing multiple sets of transformation operations where each transformation operation set takes the form {'rotation': rot_array, 'translation': tx_array,                 'rotation2': rot2_array, 'translation2': tx2_array}</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The distance to query neighbors in transformational space</p> </li> <li> <code>minimum_members</code>             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>The minimum number of members in each cluster</p> </li> </ul> <p>Returns:     The sklearn tree with the calculated nearest neighbors, the DBSCAN clustering object     Representative indices, DBSCAN cluster membership indices</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def cluster_transformation_pairs(*transforms: tuple[dict[str, np.ndarray]], distance: float = 1.,\n                                 minimum_members: int = 2) \\\n        -&gt; tuple[sklearn.neighbors._unsupervised.NearestNeighbors, sklearn.cluster._dbscan.DBSCAN]:\n    \"\"\"Cluster a group of transformation parameters sets to find those which occupy essentially the same space\n\n    Args:\n        transforms: Group containing multiple sets of transformation operations where each transformation operation set\n            takes the form {'rotation': rot_array, 'translation': tx_array,\n                            'rotation2': rot2_array, 'translation2': tx2_array}\n        distance: The distance to query neighbors in transformational space\n        minimum_members: The minimum number of members in each cluster\n    Returns:\n        The sklearn tree with the calculated nearest neighbors, the DBSCAN clustering object\n        Representative indices, DBSCAN cluster membership indices\n    \"\"\"\n    transformed_guide_coord_pairs = apply_transform_groups_to_guide_coordinates(*transforms)\n    transformed_guide_coords = np.concatenate(\n        [coords.reshape(-1, number_of_coordinate_values) for coords in transformed_guide_coord_pairs], axis=1)\n\n    # Create a tree structure describing the distances of all transformed points relative to one another\n    nearest_neightbors_ball_tree = sklearn.neighbors.NearestNeighbors(algorithm='ball_tree', radius=distance)\n    nearest_neightbors_ball_tree.fit(transformed_guide_coords)\n    # sort_results only returns non-zero entries with the smallest distance first, however it doesn't seem to work...?\n    distance_graph = nearest_neightbors_ball_tree.radius_neighbors_graph(mode='distance', sort_results=True)\n    #                                                                    X=transformed_guide_coords is implied\n    # Because this doesn't work to sort_results and pull out indices, I have to do another step 'radius_neighbors'\n    # Todo Why is this happening? Perhaps when the precomputed data is too small?\n    # Caution /home/kylemeador/miniconda3/envs/dev/lib/python3.10/site-packages/sklearn/neighbors/_base.py:206:\n    # EfficiencyWarning: Precomputed sparse input was not sorted by data.\n    dbscan_cluster: sklearn.cluster.DBSCAN = \\\n        sklearn.cluster.DBSCAN(eps=distance, min_samples=minimum_members, metric='precomputed').fit(distance_graph)\n    #                                         sample_weight=A WEIGHT?\n\n    # if return_representatives:\n    #     return find_cluster_representatives(nearest_neightbors_ball_tree, dbscan_cluster)\n    # else:  # return data structure\n    return nearest_neightbors_ball_tree, dbscan_cluster  # .labels_\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.find_cluster_representatives","title":"find_cluster_representatives","text":"<pre><code>find_cluster_representatives(transform_tree: NearestNeighbors, cluster: DBSCAN) -&gt; tuple[list[int], ndarray]\n</code></pre> <p>Return the cluster representative indices and the cluster membership identity for all member data</p> <p>Parameters:</p> <ul> <li> <code>transform_tree</code>             (<code>NearestNeighbors</code>)         \u2013          <p>The sklearn tree with the calculated nearest neighbors</p> </li> <li> <code>cluster</code>             (<code>DBSCAN</code>)         \u2013          <p>The DBSCAN clustering object</p> </li> </ul> <p>Returns:     The list of representative indices, array of all indices membership</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def find_cluster_representatives(transform_tree: sklearn.neighbors._unsupervised.NearestNeighbors,\n                                 cluster: sklearn.cluster._dbscan.DBSCAN) \\\n        -&gt; tuple[list[int], np.ndarray]:\n    \"\"\"Return the cluster representative indices and the cluster membership identity for all member data\n\n    Args:\n        transform_tree: The sklearn tree with the calculated nearest neighbors\n        cluster: The DBSCAN clustering object\n    Returns:\n        The list of representative indices, array of all indices membership\n    \"\"\"\n    # Get the neighbors for each point in the tree according to the fit distance\n    tree_distances, tree_indices = transform_tree.radius_neighbors(sort_results=True)\n    # Find mean distance to all neighbors for each index\n    with warnings.catch_warnings():\n        # Empty slices can't compute mean, so catch warning if cluster is an outlier\n        warnings.simplefilter('ignore', category=RuntimeWarning)\n        mean_cluster_dist = np.array([tree_distance.mean() for tree_distance in list(tree_distances)])\n\n    # For each label (cluster), add the minimal mean (representative) the representative transformation indices\n    outlier = -1  # -1 are outliers in DBSCAN\n    representative_transformation_indices = []\n    for label in set(cluster.labels_) - {outlier}:  # labels live here\n        cluster_indices = np.flatnonzero(cluster.labels_ == label)\n        # Get the minimal argument from the mean distances for each index in the cluster\n        # This index is the cluster representative\n        representative_transformation_indices.append(cluster_indices[mean_cluster_dist[cluster_indices].argmin()])\n    # Add all outliers to representatives\n    representative_transformation_indices.extend(np.flatnonzero(cluster.labels_ == outlier).tolist())\n\n    return representative_transformation_indices, cluster.labels_\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.cluster_pose_by_transformations","title":"cluster_pose_by_transformations","text":"<pre><code>cluster_pose_by_transformations(compositions: list[PoseJob], **kwargs) -&gt; dict[str | PoseJob, list[str | PoseJob]]\n</code></pre> <p>From a group of poses with matching protein composition, cluster the designs according to transformational parameters to identify the unique poses in each composition</p> <p>Parameters:</p> <ul> <li> <code>compositions</code>             (<code>list[PoseJob]</code>)         \u2013          <p>The group of PoseJob objects to pull transformation data from</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 1. - The distance to query neighbors in transformational space</p> </li> <li> <code>minimum_members</code>         \u2013          <p>int = 2 - The minimum number of members in each cluster</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str | PoseJob, list[str | PoseJob]]</code>         \u2013          <p>Cluster with representative pose as the key and matching poses as the values</p> </li> </ul> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def cluster_pose_by_transformations(compositions: list[PoseJob], **kwargs) -&gt; dict[str | PoseJob, list[str | PoseJob]]:\n    \"\"\"From a group of poses with matching protein composition, cluster the designs according to transformational\n    parameters to identify the unique poses in each composition\n\n    Args:\n        compositions: The group of PoseJob objects to pull transformation data from\n\n    Keyword Args:\n        distance: float = 1. - The distance to query neighbors in transformational space\n        minimum_members: int = 2 - The minimum number of members in each cluster\n\n    Returns:\n        Cluster with representative pose as the key and matching poses as the values\n    \"\"\"\n    # Format transforms for the selected compositions\n    stacked_transforms1, stacked_transforms2 = zip(*[pose_jobs.transformations for pose_jobs in compositions])\n    trans1_rot1, trans1_tx1, trans1_rot2, trans1_tx2 = \\\n        zip(*[transform.values() for transform in stacked_transforms1])\n    trans2_rot1, trans2_tx1, trans2_rot2, trans2_tx2 = \\\n        zip(*[transform.values() for transform in stacked_transforms2])\n\n    # Must add a new axis to translations so the operations are broadcast together in transform_coordinate_sets()\n    transformation1 = {'rotation': np.array(trans1_rot1), 'translation': np.array(trans1_tx1)[:, np.newaxis, :],\n                       'rotation2': np.array(trans1_rot2), 'translation2': np.array(trans1_tx2)[:, np.newaxis, :]}\n    transformation2 = {'rotation': np.array(trans2_rot1), 'translation': np.array(trans2_tx1)[:, np.newaxis, :],\n                       'rotation2': np.array(trans2_rot2), 'translation2': np.array(trans2_tx2)[:, np.newaxis, :]}\n\n    # Find the representatives of the cluster based on minimal distance of each point to its nearest neighbors\n    return cluster_by_transformations(transformation1, transformation2, values=compositions, **kwargs)\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.cluster_by_transformations","title":"cluster_by_transformations","text":"<pre><code>cluster_by_transformations(*transforms: tuple[dict[str, ndarray]], values: list[Any] = None, **kwargs) -&gt; dict[Any, list[Any]]\n</code></pre> <p>From a set of objects with associated transformational parameters, identify and cluster the unique objects by representatives and members</p> <p>Parameters:</p> <ul> <li> <code>transforms</code>             (<code>tuple[dict[str, ndarray]]</code>, default:                 <code>()</code> )         \u2013          <p>Group containing multiple sets of transformation operations where each transformation operation set takes the form {'rotation': rot_array, 'translation': tx_array,                 'rotation2': rot2_array, 'translation2': tx2_array}</p> </li> <li> <code>values</code>             (<code>list[Any]</code>, default:                 <code>None</code> )         \u2013          <p>The group of objects to cluster</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 1. - The distance to query neighbors in transformational space</p> </li> <li> <code>minimum_members</code>         \u2013          <p>int = 2 - The minimum number of members in each cluster</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[Any, list[Any]]</code>         \u2013          <p>Clustered objects with representative as the key and members as the values</p> </li> </ul> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def cluster_by_transformations(*transforms: tuple[dict[str, np.ndarray]], values: list[Any] = None, **kwargs) \\\n        -&gt; dict[Any, list[Any]]:\n    \"\"\"From a set of objects with associated transformational parameters, identify and cluster the unique objects by\n    representatives and members\n\n    Args:\n        transforms: Group containing multiple sets of transformation operations where each transformation operation set\n            takes the form {'rotation': rot_array, 'translation': tx_array,\n                            'rotation2': rot2_array, 'translation2': tx2_array}\n        values: The group of objects to cluster\n\n    Keyword Args:\n        distance: float = 1. - The distance to query neighbors in transformational space\n        minimum_members: int = 2 - The minimum number of members in each cluster\n\n    Returns:\n        Clustered objects with representative as the key and members as the values\n    \"\"\"\n    # Find the representatives of the cluster based on minimal distance of each point to its nearest neighbors\n    # This section could be added to the Nanohedra docking routine\n    cluster_representative_indices, cluster_labels = \\\n        find_cluster_representatives(*cluster_transformation_pairs(*transforms, **kwargs))\n\n    representative_labels = cluster_labels[cluster_representative_indices]\n\n    # Sort out clustered transform_values from the input transform_values\n    outlier = -1\n    cluster_map = \\\n        {values[rep_idx]: [values[idx] for idx in np.flatnonzero(cluster_labels == rep_label).tolist()]\n         for rep_idx, rep_label in zip(cluster_representative_indices, representative_labels)\n         if rep_label != outlier}\n    # Add all outliers\n    cluster_map.update({values[idx]: [values[idx]] for idx in np.flatnonzero(cluster_labels == outlier).tolist()})\n\n    return cluster_map\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.group_compositions","title":"group_compositions","text":"<pre><code>group_compositions(pose_jobs: list[PoseJob]) -&gt; dict[tuple[str, ...], list[PoseJob]]\n</code></pre> <p>From a set of DesignDirectories, find all the compositions and group together</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>list[PoseJob]</code>)         \u2013          <p>The PoseJob to group according to composition</p> </li> </ul> <p>Returns:     List of similarly named PoseJob mapped to their name</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def group_compositions(pose_jobs: list[PoseJob]) -&gt; dict[tuple[str, ...], list[PoseJob]]:\n    \"\"\"From a set of DesignDirectories, find all the compositions and group together\n\n    Args:\n        pose_jobs: The PoseJob to group according to composition\n    Returns:\n        List of similarly named PoseJob mapped to their name\n    \"\"\"\n    compositions = {}\n    for pose_job in pose_jobs:\n        entity_names = tuple(pose_job.entity_names)\n        found_composition = None\n        for permutation in combinations(entity_names, len(entity_names)):\n            found_composition = compositions.get(permutation, None)\n            if found_composition:\n                break\n\n        if found_composition:\n            compositions[entity_names].append(pose_job)\n        else:\n            compositions[entity_names] = [pose_job]\n\n    return compositions\n</code></pre>"},{"location":"reference/protocols/cluster/#protocols.cluster.invert_cluster_map","title":"invert_cluster_map","text":"<pre><code>invert_cluster_map(cluster_map: dict[Any, list[Any]])\n</code></pre> <p>Return an inverted cluster map where the cluster members map to the representative</p> <p>Parameters:</p> <ul> <li> <code>cluster_map</code>             (<code>dict[Any, list[Any]]</code>)         \u2013          <p>The standard pose_cluster_map format</p> </li> </ul> <p>Returns:     An inverted cluster_map where the members are keys and the representative is the value</p> Source code in <code>symdesign/protocols/cluster.py</code> <pre><code>def invert_cluster_map(cluster_map: dict[Any, list[Any]]):\n    \"\"\"Return an inverted cluster map where the cluster members map to the representative\n\n    Args:\n        cluster_map: The standard pose_cluster_map format\n    Returns:\n        An inverted cluster_map where the members are keys and the representative is the value\n    \"\"\"\n    inverted_map = {member: cluster_rep for cluster_rep, members in cluster_map.items() for member in members}\n    # Add all representatives\n    inverted_map.update({cluster_rep: cluster_rep for cluster_rep in cluster_map})\n\n    return inverted_map\n</code></pre>"},{"location":"reference/protocols/config/","title":"config","text":""},{"location":"reference/protocols/fragdock/","title":"fragdock","text":""},{"location":"reference/protocols/fragdock/#protocols.fragdock.compute_ij_type_lookup","title":"compute_ij_type_lookup","text":"<pre><code>compute_ij_type_lookup(indices1: ndarray | Iterable, indices2: ndarray | Iterable) -&gt; ndarray\n</code></pre> <p>Compute a lookup table where the array elements are indexed to boolean values if the indices match. Axis 0 is indices1, Axis 1 is indices2</p> <p>Parameters:</p> <ul> <li> <code>indices1</code>             (<code>ndarray | Iterable</code>)         \u2013          <p>The array elements from group 1</p> </li> <li> <code>indices2</code>             (<code>ndarray | Iterable</code>)         \u2013          <p>The array elements from group 2</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>A 2D boolean array where the first index maps to the input 1, second index maps to index 2</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def compute_ij_type_lookup(indices1: np.ndarray | Iterable, indices2: np.ndarray | Iterable) -&gt; np.ndarray:\n    \"\"\"Compute a lookup table where the array elements are indexed to boolean values if the indices match.\n    Axis 0 is indices1, Axis 1 is indices2\n\n    Args:\n        indices1: The array elements from group 1\n        indices2: The array elements from group 2\n\n    Returns:\n        A 2D boolean array where the first index maps to the input 1, second index maps to index 2\n    \"\"\"\n    # TODO use broadcasting to compute true/false instead of tiling (memory saving)\n    indices1_repeated = np.repeat(indices1, len(indices2))\n    len_indices1 = len(indices1)\n    indices2_tiled = np.tile(indices2, len_indices1)\n    # TODO keep as table or flatten? Can use one or the other as memory and take a view of the other as needed...\n    # return np.where(indices1_repeated == indices2_tiled, True, False).reshape(len_indices1, -1)\n    return (indices1_repeated == indices2_tiled).reshape(len_indices1, -1)\n</code></pre>"},{"location":"reference/protocols/fragdock/#protocols.fragdock.get_perturb_matrices","title":"get_perturb_matrices","text":"<pre><code>get_perturb_matrices(rotation_degrees: float, number: int = 10) -&gt; ndarray\n</code></pre> <p>Using a sampled degree of rotation, create z-axis rotation matrices in equal increments between +/- rotation_degrees/2</p> <p>Parameters:</p> <ul> <li> <code>rotation_degrees</code>             (<code>float</code>)         \u2013          <p>The range of degrees to create matrices for. Will be centered at the identity perturbation</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>The number of steps to take</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>A 3D numpy array where each subsequent rotation is along axis=0, and each 3x3 rotation matrix is along axis=1/2</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def get_perturb_matrices(rotation_degrees: float, number: int = 10) -&gt; np.ndarray:\n    \"\"\"Using a sampled degree of rotation, create z-axis rotation matrices in equal increments between +/- rotation_degrees/2\n\n    Args:\n        rotation_degrees: The range of degrees to create matrices for. Will be centered at the identity perturbation\n        number: The number of steps to take\n\n    Returns:\n        A 3D numpy array where each subsequent rotation is along axis=0,\n            and each 3x3 rotation matrix is along axis=1/2\n    \"\"\"\n    half_grid_range, remainder = divmod(number, 2)\n    # If the number is odd we should center on 0 else center with more on the negative side\n    if remainder:\n        upper_range = half_grid_range + 1\n    else:\n        upper_range = half_grid_range\n\n    step_degrees = rotation_degrees / number\n    perturb_matrices = []\n    for step in range(-half_grid_range, upper_range):  # Range from -5 to 4(5) for example. 0 is identity matrix\n        rad = math.radians(step * step_degrees)\n        rad_s = math.sin(rad)\n        rad_c = math.cos(rad)\n        # Perform rotational perturbation on z-axis\n        perturb_matrices.append([[rad_c, -rad_s, 0.], [rad_s, rad_c, 0.], [0., 0., 1.]])\n\n    return np.array(perturb_matrices)\n</code></pre>"},{"location":"reference/protocols/fragdock/#protocols.fragdock.create_perturbation_transformations","title":"create_perturbation_transformations","text":"<pre><code>create_perturbation_transformations(sym_entry: SymEntry, number_of_rotations: int = 1, number_of_translations: int = 1, rotation_steps: Iterable[float] = None, translation_steps: Iterable[float] = None) -&gt; dict[str, ndarray]\n</code></pre> <p>From a specified SymEntry and sampling schedule, create perturbations to degrees of freedom for each available</p> <p>Parameters:</p> <ul> <li> <code>sym_entry</code>             (<code>SymEntry</code>)         \u2013          <p>The SymEntry whose degrees of freedom should be expanded</p> </li> <li> <code>number_of_rotations</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of times to sample from the allowed rotation space. 1 means no perturbation</p> </li> <li> <code>number_of_translations</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of times to sample from the allowed translation space. 1 means no perturbation</p> </li> <li> <code>rotation_steps</code>             (<code>Iterable[float]</code>, default:                 <code>None</code> )         \u2013          <p>The step to sample rotations +/- the identified rotation in degrees. Expected type is an iterable of length comparable to the number of rotational degrees of freedom</p> </li> <li> <code>translation_steps</code>             (<code>Iterable[float]</code>, default:                 <code>None</code> )         \u2013          <p>The step to sample translations +/- the identified translation in Angstroms Expected type is an iterable of length comparable to the number of translational degrees of freedom</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>A mapping between the perturbation type and the corresponding transformation operation</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def create_perturbation_transformations(sym_entry: SymEntry, number_of_rotations: int = 1, number_of_translations: int = 1,\n                                        rotation_steps: Iterable[float] = None,\n                                        translation_steps: Iterable[float] = None) -&gt; dict[str, np.ndarray]:\n    \"\"\"From a specified SymEntry and sampling schedule, create perturbations to degrees of freedom for each available\n\n    Args:\n        sym_entry: The SymEntry whose degrees of freedom should be expanded\n        number_of_rotations: The number of times to sample from the allowed rotation space. 1 means no perturbation\n        number_of_translations: The number of times to sample from the allowed translation space. 1 means no perturbation\n        rotation_steps: The step to sample rotations +/- the identified rotation in degrees.\n            Expected type is an iterable of length comparable to the number of rotational degrees of freedom\n        translation_steps: The step to sample translations +/- the identified translation in Angstroms\n            Expected type is an iterable of length comparable to the number of translational degrees of freedom\n\n    Returns:\n        A mapping between the perturbation type and the corresponding transformation operation\n    \"\"\"\n    # Get the perturbation parameters\n    # Total number of perturbations desired using the total_dof possible in the symmetry and those requested\n    target_dof = total_dof = sym_entry.total_dof\n    rotational_dof = sym_entry.number_dof_rotation\n    translational_dof = sym_entry.number_dof_translation\n    n_dof_external = sym_entry.number_dof_external\n\n    if number_of_rotations &lt; 1:\n        logger.warning(f\"Can't create perturbation transformations with rotation_number of {number_of_rotations}. \"\n                       f\"Setting to 1\")\n        number_of_rotations = 1\n        # raise ValueError(f\"Can't create perturbation transformations with rotation_number of {rotation_number}\")\n    if number_of_rotations == 1:\n        target_dof -= rotational_dof\n        rotational_dof = 0\n\n    if number_of_translations &lt; 1:\n        logger.warning(f\"Can't create perturbation transformations with translation_number of {number_of_translations}. \"\n                       f\"Setting to 1\")\n        number_of_translations = 1\n\n    if number_of_translations == 1:\n        target_dof -= translational_dof\n        translational_dof = 0\n        # Set to 0 so there is no deviation from the current value and remove any provided values\n        default_translation = 0\n        translation_steps = None\n    else:\n        # Default translation range is 0.5 Angstroms\n        default_translation = .5\n\n    if translation_steps is None:\n        translation_steps = tuple(repeat(default_translation, sym_entry.number_of_groups))\n\n    if n_dof_external:\n        if translation_steps is None:\n            logger.warning(f'Using the default value of {default_translation} for external translation steps')\n            ext_translation_steps = tuple(repeat(default_translation, n_dof_external))\n        else:\n            # Todo allow ext_translation_steps to be a parameter\n            ext_translation_steps = translation_steps\n    # # Make a vector of the perturbation number [1, 2, 2, 3, 3, 1] with 1 as constants on each end\n    # dof_number_perturbations = [1] \\\n    #     + [rotation_number for dof in range(rotational_dof)] \\\n    # Make a vector of the perturbation number [2, 2, 3, 3]\n    dof_number_perturbations = \\\n        [number_of_rotations for dof in range(rotational_dof)] \\\n        + [number_of_translations for dof in range(translational_dof)] \\\n        # + [1]\n    # translation_stack_size = translation_number**translational_dof\n    # stack_size = rotation_number**rotational_dof * translation_stack_size\n    # number = rotation_number + translation_number\n\n    stack_size = prod(dof_number_perturbations)\n    # Initialize a translation grid for any translational degrees of freedom\n    translation_grid = np.zeros((stack_size, 3), dtype=float)\n    # # Begin with total dof minus 1\n    # remaining_dof = total_dof - 1\n    # # Begin with 0\n    # seen_dof = 0\n\n    if rotation_steps is None:\n        # Default rotation range is 1. degree\n        default_rotation = 1.\n        rotation_steps = tuple(repeat(default_rotation, sym_entry.number_of_groups))\n\n    dof_idx = 0\n    perturbation_mapping = {}\n    for idx, group in enumerate(sym_entry.groups):\n        group_idx = idx + 1\n        if getattr(sym_entry, f'is_internal_rot{group_idx}'):\n            rotation_step = rotation_steps[idx]  # * 2\n            perturb_matrices = get_perturb_matrices(rotation_step, number=number_of_rotations)\n            # Repeat (tile then reshape) the matrices according to the number of perturbations raised to the power of\n            # the remaining dof (remaining_dof), then tile that by how many dof have been seen (seen_dof)\n            # perturb_matrices = \\\n            #     np.tile(np.tile(perturb_matrices,\n            #                     (1, 1, rotation_number**remaining_dof * translation_stack_size)).reshape(-1, 3, 3),\n            #             (number**seen_dof, 1, 1))\n            # remaining_dof -= 1\n            # seen_dof += 1\n            # Get the product of the number of perturbations before and after the current index\n            repeat_number = prod(dof_number_perturbations[dof_idx + 1:])\n            tile_number = prod(dof_number_perturbations[:dof_idx])\n            # Repeat (tile then reshape) the matrices according to the product of the remaining dof,\n            # number of perturbations, then tile by the product of how many perturbations have been seen\n            perturb_matrices = np.tile(np.tile(perturb_matrices, (1, repeat_number, 1)).reshape(-1, 3, 3),\n                                       (tile_number, 1, 1))\n            # Increment the dof seen\n            dof_idx += 1\n        else:\n            # This is a requirement as currently, all SymEntry are assumed to have rotations\n            # np.tile the identity matrix to make equally sized.\n            perturb_matrices = np.tile(identity_matrix, (stack_size, 1, 1))\n\n        perturbation_mapping[f'rotation{group_idx}'] = perturb_matrices\n\n    for idx, group in enumerate(sym_entry.groups):\n        group_idx = idx + 1\n        if getattr(sym_entry, f'is_internal_tx{group_idx}'):\n            # Repeat the translation according to the number of perturbations raised to the power of the\n            # remaining dof (remaining_dof), then tile that by how many dof have been seen (seen_dof)\n            internal_translation_grid = translation_grid.copy()\n            translation_perturb_vector = \\\n                np.linspace(-translation_steps[idx], translation_steps[idx], number_of_translations)\n            # internal_translation_grid[:, 2] = np.tile(np.repeat(translation_perturb_vector, number**remaining_dof),\n            #                                           number**seen_dof)\n            # remaining_dof -= 1\n            # seen_dof += 1\n            # Get the product of the number of perturbations before and after the current index\n            repeat_number = prod(dof_number_perturbations[dof_idx + 1:])\n            tile_number = prod(dof_number_perturbations[:dof_idx])\n            internal_translation_grid[:, 2] = np.tile(np.repeat(translation_perturb_vector, repeat_number),\n                                                      tile_number)\n            # Increment the dof seen\n            dof_idx += 1\n            perturbation_mapping[f'translation{group_idx}'] = internal_translation_grid\n\n    # if n_dof_external:\n    #     # sym_entry.number_dof_external are included in the sym_entry.total_dof calculation\n    #     # Need to perturb this many dofs. Each additional ext DOF increments e, f, g.\n    #     # So 2 number_dof_external gives e, f. 3 gives e, f, g. This way the correct number of axis can be perturbed..\n    #     # This solution doesn't vary the translation_grid in all dofs\n    #     # ext_dof_perturbs[:, :number_dof_external] = np.tile(translation_grid, (number_dof_external, 1)).T\n    # This solution iterates over the translation_grid, adding a new grid over all remaining dofs\n    external_translation_grid = translation_grid.copy()\n    for idx, ext_idx in enumerate(range(n_dof_external)):\n        translation_perturb_vector = \\\n            np.linspace(-ext_translation_steps[idx], ext_translation_steps[idx], number_of_translations)\n        # external_translation_grid[:, ext_idx] = np.tile(np.repeat(translation_perturb_vector,\n        #                                                           number**remaining_dof),\n        #                                                 number**seen_dof)\n        # remaining_dof -= 1\n        # seen_dof += 1\n        # Get the product of the number of perturbations before and after the current index\n        repeat_number = prod(dof_number_perturbations[dof_idx + 1:])\n        tile_number = prod(dof_number_perturbations[:dof_idx])\n        external_translation_grid[:, ext_idx] = np.tile(np.repeat(translation_perturb_vector, repeat_number),\n                                                        tile_number)\n        # Increment the dof seen\n        dof_idx += 1\n\n    perturbation_mapping['external_translations'] = external_translation_grid\n\n    # if remaining_dof + 1 != 0 and seen_dof != total_dof:\n    #     logger.critical(f'The number of perturbations is unstable! {remaining_dof + 1} != 0 and '\n    #                     f'{seen_dof} != {total_dof} total_dof')\n    if dof_idx != target_dof:\n        logger.critical(f'The number of perturbations is unstable! '\n                        f'perturbed dof used {dof_idx} != {target_dof}, the targeted dof to perturb resulting from '\n                        f'{total_dof} total_dof = {rotational_dof} rotational_dof, {translational_dof} '\n                        f'translational_dof, and {n_dof_external} external_translational_dof')\n\n    return perturbation_mapping, target_dof\n</code></pre>"},{"location":"reference/protocols/fragdock/#protocols.fragdock.make_contiguous_ghosts","title":"make_contiguous_ghosts","text":"<pre><code>make_contiguous_ghosts(ghost_frags_by_residue: list[list[GhostFragment]], residues: list[Residue], distance: float = 16.0, initial_z_value: float = 1.0) -&gt; ndarray\n</code></pre> <p>Identify GhostFragment overlap originating from a group of residues related bby spatial proximity</p> <p>Parameters:</p> <ul> <li> <code>ghost_frags_by_residue</code>             (<code>list[list[GhostFragment]]</code>)         \u2013          <p>The list of GhostFragments separated by Residue instances. GhostFragments should align with residues</p> </li> <li> <code>residues</code>             (<code>list[Residue]</code>)         \u2013          <p>Residue instance for which the ghost_frags_by_residue belong to</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>16.0</code> )         \u2013          <p>The distance to measure neighbors between residues</p> </li> <li> <code>initial_z_value</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The acceptable standard deviation z score for initial fragment overlap identification.             Smaller values lead to more stringent matching criteria</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The indices of the ghost fragments (when the ghost_frags_by_residue) is unstacked such as by itertool.chain</p> </li> <li> <code>ndarray</code>         \u2013          <p>chained</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def make_contiguous_ghosts(ghost_frags_by_residue: list[list[GhostFragment]], residues: list[Residue],\n                           distance: float = 16., initial_z_value: float = 1.) -&gt; np.ndarray:\n    #     -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]\n    \"\"\"Identify GhostFragment overlap originating from a group of residues related bby spatial proximity\n\n    Args:\n        ghost_frags_by_residue: The list of GhostFragments separated by Residue instances.\n            GhostFragments should align with residues\n        residues: Residue instance for which the ghost_frags_by_residue belong to\n        # Todo residues currently requires a cb_coord to measure Residue-Residue distances. This could be some other feature\n        #  of fragments such as a coordinate that identifies its spatial proximity to other fragments\n        distance: The distance to measure neighbors between residues\n        initial_z_value: The acceptable standard deviation z score for initial fragment overlap identification. \\\n            Smaller values lead to more stringent matching criteria\n\n    Returns:\n        The indices of the ghost fragments (when the ghost_frags_by_residue) is unstacked such as by itertool.chain\n        chained\n    \"\"\"\n    # Set up the output array with the number of residues by the length of the max number of ghost fragments\n    max_ghost_frags = max([len(ghost_frags) for ghost_frags in ghost_frags_by_residue])\n    number_or_surface_frags = len(residues)\n    same_component_overlapping_ghost_frags = np.zeros((number_or_surface_frags, max_ghost_frags), dtype=int)\n    ghost_frag_rmsds_by_residue = np.zeros_like(same_component_overlapping_ghost_frags, dtype=float)\n    ghost_guide_coords_by_residue = np.zeros((number_or_surface_frags, max_ghost_frags, 3, 3))\n    for idx, residue_ghosts in enumerate(ghost_frags_by_residue):\n        number_of_ghosts = len(residue_ghosts)\n        # Set any viable index to 1 to distinguish between padding with 0\n        same_component_overlapping_ghost_frags[idx, :number_of_ghosts] = 1\n        ghost_frag_rmsds_by_residue[idx, :number_of_ghosts] = [ghost.rmsd for ghost in residue_ghosts]\n        ghost_guide_coords_by_residue[idx, :number_of_ghosts] = [ghost.guide_coords for ghost in residue_ghosts]\n    # ghost_frag_rmsds_by_residue = np.array([[ghost.rmsd for ghost in residue_ghosts]\n    #                                         for residue_ghosts in ghost_frags_by_residue], dtype=object)\n    # ghost_guide_coords_by_residue = np.array([[ghost.guide_coords for ghost in residue_ghosts]\n    #                                           for residue_ghosts in ghost_frags_by_residue], dtype=object)\n    # surface_frag_residue_numbers = [residue.number for residue in residues]\n\n    # Query for residue-residue distances for each surface fragment\n    # surface_frag_cb_coords = np.concatenate([residue.cb_coords for residue in residues], axis=0)\n    surface_frag_cb_coords = np.array([residue.cb_coords for residue in residues])\n    model1_surface_cb_ball_tree = BallTree(surface_frag_cb_coords)\n    residue_contact_query: np.ndarray = \\\n        model1_surface_cb_ball_tree.query_radius(surface_frag_cb_coords, distance)\n    surface_frag_residue_indices = list(range(number_or_surface_frags))\n    contacting_residue_idx_pairs: list[tuple[int, int]] = \\\n        [(surface_frag_residue_indices[idx1], surface_frag_residue_indices[idx2])\n         for idx2, idx1_contacts in enumerate(residue_contact_query.tolist())\n         for idx1 in idx1_contacts.tolist()]\n\n    # Separate residue-residue contacts into a unique set of residue pairs\n    asymmetric_contacting_residue_pairs, found_pairs = [], []\n    for residue_idx1, residue_idx2 in contacting_residue_idx_pairs:\n        # Add to unique set (asymmetric_contacting_residue_pairs) if we have never observed either\n        if residue_idx1 == residue_idx2:\n            continue  # We don't need to add because this check rules possibility out\n        elif (residue_idx1, residue_idx2) not in found_pairs:\n            # or (residue2, residue1) not in found_pairs\n            # Checking both directions isn't required because the overlap would be the same...\n            asymmetric_contacting_residue_pairs.append((residue_idx1, residue_idx2))\n        # Add both pair orientations (1, 2) or (2, 1) regardless\n        found_pairs.extend([(residue_idx1, residue_idx2), (residue_idx2, residue_idx1)])\n\n    # Now, use asymmetric_contacting_residue_pairs indices to find the ghost_fragments that overlap for each residue\n    # Todo 3, there are multiple indexing steps for residue_idx1/2 which only occur once if below code was used\n    #  found_pairs = []\n    #  for residue_idx2 in range(residue_contact_query.size):\n    #      residue_ghost_frag_type2 = ghost_frag_type_by_residue[residue_idx2]\n    #      residue_ghost_guide_coords2 = ghost_guide_coords_by_residue[residue_idx2]\n    #      residue_ghost_reference_rmsds2 = ghost_frag_rmsds_by_residue[residue_idx2]\n    #      for residue_idx1 in residue_contact_query[residue_idx2]]\n    #          # Check if the pair has been seen before. Work from second visited index(1) to first visited(2)\n    #          if (residue_idx1, residue_idx2) in found_pairs:\n    #              continue\n    #          # else:\n    #          found_pairs.append((residue_idx1, residue_idx2))\n    #          type_bool_matrix = compute_ij_type_lookup(ghost_frag_type_by_residue[residue_idx1],\n    #                                                    residue_ghost_frag_type2)\n    #          # Separate indices for each type-matched, ghost fragment in the residue pair\n    #          residue_idx1_ghost_indices, residue_idx2_ghost_indices = np.nonzero(type_bool_matrix)\n    # Set up the input array types with the various information needed for each pairwise check\n    ghost_frag_type_by_residue = [[ghost.frag_type for ghost in residue_ghosts]\n                                  for residue_ghosts in ghost_frags_by_residue]\n    for residue_idx1, residue_idx2 in asymmetric_contacting_residue_pairs:\n        # Check if each of the associated ghost frags have the same secondary structure type\n        #   Fragment1\n        # F T  F  F\n        # R F  F  T\n        # A F  F  F\n        # G F  F  F\n        # 2 T  T  F\n        type_bool_matrix = compute_ij_type_lookup(ghost_frag_type_by_residue[residue_idx1],\n                                                  ghost_frag_type_by_residue[residue_idx2])\n        # Separate indices for each type-matched, ghost fragment in the residue pair\n        residue_idx1_ghost_indices, residue_idx2_ghost_indices = np.nonzero(type_bool_matrix)\n        # # Iterate over each matrix rox/column to pull out necessary guide coordinate pairs\n        # # HERE v\n        # # ij_matching_ghost1_indices = (type_bool_matrix * np.arange(len(type_bool_matrix)))[type_bool_matrix]\n\n        # These should pick out each instance of the guide_coords identified as overlapping by indexing bool type\n        # Resulting instances should be present multiple times from residue_idxN_ghost_indices\n        ghost_coords_residue1 = ghost_guide_coords_by_residue[residue_idx1][residue_idx1_ghost_indices]\n        ghost_coords_residue2 = ghost_guide_coords_by_residue[residue_idx2][residue_idx2_ghost_indices]\n        if len(ghost_coords_residue1) != len(residue_idx1_ghost_indices):\n            raise IndexError('There was an issue indexing')\n        ghost_reference_rmsds_residue1 = ghost_frag_rmsds_by_residue[residue_idx1][residue_idx1_ghost_indices]\n\n        # Perform the overlap calculation and find indices with overlap\n        overlapping_z_score = rmsd_z_score(ghost_coords_residue1,\n                                           ghost_coords_residue2,\n                                           ghost_reference_rmsds_residue1)  # , max_z_value=initial_z_value)\n        same_component_overlapping_indices = np.flatnonzero(overlapping_z_score &lt;= initial_z_value)\n        # Increment indices of overlapping ghost fragments for each residue\n        # same_component_overlapping_ghost_frags[\n        #     residue_idx1, residue_idx1_ghost_indices[same_component_overlapping_indices]\n        # ] += 1\n        # This double counts as each is actually overlapping at the same location in space.\n        # Just need one for initial overlap check...\n        # Todo situation where more info needed?\n        same_component_overlapping_ghost_frags[\n            residue_idx2, residue_idx2_ghost_indices[same_component_overlapping_indices]\n        ] += 1\n        #       Ghost Fragments\n        # S F - 1  0  0\n        # U R - 0  0  0\n        # R A - 0  0  0\n        # F G - 1  1  0\n        # Todo\n        #  Could use the identified ghost fragments to further stitch together continuous ghost fragments...\n        #  This proceeds by doing guide coord overlap between fragment adjacent CA (i.e. +-2 fragment length 5) and\n        #  other identified ghost fragments\n        #  Or guide coords cross product points along direction of persistence and euler angles are matched between\n        #  all overlap ghosts to identify those which share \"moment of extension\"\n\n    # Measure where greater than 0 as a value of 1 was included to mark viable fragment indices\n    fragment_overlap_counts = \\\n        same_component_overlapping_ghost_frags[np.nonzero(same_component_overlapping_ghost_frags)]\n    # logger.debug(f'fragment_overlap_counts.shape: {fragment_overlap_counts.shape}')\n    viable_same_component_overlapping_ghost_frags = np.flatnonzero(fragment_overlap_counts &gt; 1)\n    # logger.debug(f'viable_same_component_overlapping_ghost_frags.shape: '\n    #              f'{viable_same_component_overlapping_ghost_frags.shape}')\n    # logger.debug(f'viable_same_component_overlapping_ghost_frags[:10]: '\n    #              f'{viable_same_component_overlapping_ghost_frags[:10]}')\n    return viable_same_component_overlapping_ghost_frags\n</code></pre>"},{"location":"reference/protocols/fragdock/#protocols.fragdock.check_tree_for_query_overlap","title":"check_tree_for_query_overlap","text":"<pre><code>check_tree_for_query_overlap(batch_slice: slice, binarytree: BinaryTreeType = None, query_points: ndarray = None, rotation: ndarray = None, translation: ndarray = None, rotation2: ndarray = None, translation2: ndarray = None, rotation3: ndarray = None, translation3: ndarray = None, rotation4: ndarray = None, translation4: ndarray = None, clash_distance: float = 2.1) -&gt; dict[str, list]\n</code></pre> <p>Check for overlapping coordinates between a BinaryTree and a collection of query_points. Transform the query over multiple iterations</p> <p>Parameters:</p> <ul> <li> <code>batch_slice</code>             (<code>slice</code>)         \u2013          <p>The slice of the incoming work to operate on</p> </li> <li> <code>binarytree</code>             (<code>BinaryTreeType</code>, default:                 <code>None</code> )         \u2013          <p>The tree to check all queries against</p> </li> <li> <code>query_points</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The points to transform, then query against the tree</p> </li> <li> <code>rotation</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>translation</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>rotation2</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>translation2</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>rotation3</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>translation3</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>rotation4</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>translation4</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>clash_distance</code>             (<code>float</code>, default:                 <code>2.1</code> )         \u2013          <p>The distance to measure for query_point overlap, i.e. clashing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list]</code>         \u2013          <p>The number of overlaps found at each transformed query point as a dictionary</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def check_tree_for_query_overlap(batch_slice: slice,\n                                 binarytree: BinaryTreeType = None, query_points: np.ndarray = None,\n                                 rotation: np.ndarray = None, translation: np.ndarray = None,\n                                 rotation2: np.ndarray = None, translation2: np.ndarray = None,\n                                 rotation3: np.ndarray = None, translation3: np.ndarray = None,\n                                 rotation4: np.ndarray = None, translation4: np.ndarray = None,\n                                 clash_distance: float = 2.1) -&gt; dict[str, list]:\n    \"\"\"Check for overlapping coordinates between a BinaryTree and a collection of query_points.\n    Transform the query over multiple iterations\n\n    Args:\n        batch_slice: The slice of the incoming work to operate on\n        binarytree: The tree to check all queries against\n        query_points: The points to transform, then query against the tree\n        rotation:\n        translation:\n        rotation2:\n        translation2:\n        rotation3:\n        translation3:\n        rotation4:\n        translation4:\n        clash_distance: The distance to measure for query_point overlap, i.e. clashing\n\n    Returns:\n        The number of overlaps found at each transformed query point as a dictionary\n    \"\"\"\n    _rotation = rotation[batch_slice]\n    actual_batch_length = len(_rotation)\n    # Transform the coordinates\n    # Todo 3 for performing broadcasting of this operation\n    #  s_broad = np.matmul(tiled_coords2[None, :, None, :], _full_rotation2[:, None, :, :])\n    #  produces a shape of (len(_full_rotation2), len(tiled_coords2), 1, 3)\n    #  inverse_transformed_model2_tiled_coords = transform_coordinate_sets(transform_coordinate_sets()).squeeze()\n    transformed_query_points = \\\n        transform_coordinate_sets(\n            transform_coordinate_sets(query_points[:actual_batch_length],  # Slice ensures same size\n                                      rotation=_rotation,\n                                      translation=None if translation is None\n                                      else translation[batch_slice, None, :],\n                                      rotation2=rotation2,  # setting matrix, no slice\n                                      translation2=None if translation2 is None\n                                      else translation2[batch_slice, None, :]),\n            rotation=rotation3,  # setting matrix, no slice\n            translation=None if translation3 is None else translation3[batch_slice, None, :],\n            rotation2=rotation4[batch_slice],\n            translation2=None if translation4 is None else translation4[batch_slice, None, :])\n\n    clash_vect = [clash_distance]\n    overlap_counts = \\\n        [binarytree.two_point_correlation(transformed_query_points[idx], clash_vect)[0]\n         for idx in range(actual_batch_length)]\n\n    return {'overlap_counts': overlap_counts}\n</code></pre>"},{"location":"reference/protocols/fragdock/#protocols.fragdock.fragment_dock","title":"fragment_dock","text":"<pre><code>fragment_dock(input_models: Iterable[ContainsEntities]) -&gt; list[PoseJob] | list\n</code></pre> <p>Perform the fragment docking routine described in Laniado, Meador, &amp; Yeates, PEDS. 2021</p> <p>Parameters:</p> <ul> <li> <code>input_models</code>             (<code>Iterable[ContainsEntities]</code>)         \u2013          <p>The Structures to be used in docking</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[PoseJob] | list</code>         \u2013          <p>The resulting Poses satisfying docking criteria</p> </li> </ul> Source code in <code>symdesign/protocols/fragdock.py</code> <pre><code>def fragment_dock(input_models: Iterable[ContainsEntities]) -&gt; list[PoseJob] | list:\n    \"\"\"Perform the fragment docking routine described in Laniado, Meador, &amp; Yeates, PEDS. 2021\n\n    Args:\n        input_models: The Structures to be used in docking\n\n    Returns:\n        The resulting Poses satisfying docking criteria\n    \"\"\"\n    frag_dock_time_start = time.time()\n\n    # Todo reimplement this feature to write a log to the Project directory?\n    # # Setup logger\n    # if logger is None:\n    #     log_file_path = os.path.join(project_dir, f'{building_blocks}_log.txt')\n    # else:\n    #     try:\n    #         log_file_path = getattr(logger.handlers[0], 'baseFilename', None)\n    #     except IndexError:  # No handler attached to this logger. Probably passing to a parent logger\n    #         log_file_path = None\n    #\n    # if log_file_path:  # Start logging to a file in addition\n    #     logger = start_log(name=building_blocks, handler=2, location=log_file_path, format_log=False, propagate=True)\n\n    # Retrieve symjob.JobResources for all flags\n    job = symjob.job_resources_factory.get()\n\n    sym_entry: SymEntry = job.sym_entry\n    \"\"\"The SymmetryEntry object describing the material\"\"\"\n    if sym_entry:\n        protocol_name = putils.nanohedra\n    else:\n        protocol_name = putils.fragment_docking\n    #\n    # protocol = Protocol(name=protocol_name)\n    job.fragment_db = fragment_factory(source=job.fragment_source)\n    euler_lookup = job.fragment_db.euler_lookup\n    # This is used in clustering algorithms to define an observation outside the found clusters\n    outlier = -1\n    initial_z_value = job.dock.initial_z_value\n    \"\"\"The acceptable standard deviation z score for initial fragment overlap identification. Smaller values lead to \n    more stringent matching criteria\n    \"\"\"\n    min_matched = job.dock.minimum_matched\n    \"\"\"How many high quality fragment pairs should be present before a pose is identified?\"\"\"\n    high_quality_match_value = job.dock.match_value\n    \"\"\"The value to exceed before a high quality fragment is matched. When z-value was used this was 1.0, however, 0.5\n    when match score is used\n    \"\"\"\n    rotation_step1 = job.dock.rotation_step1\n    rotation_step2 = job.dock.rotation_step2\n    # Todo 3 set below as parameters?\n    measure_interface_during_dock = True\n    low_quality_match_value = .2\n    \"\"\"The lower bounds on an acceptable match. Was upper bound of 2 using z-score\"\"\"\n    clash_dist: float = 2.1\n    \"\"\"The distance to measure for clashing atoms\"\"\"\n    cb_distance = 9.  # change to 8.?\n    \"\"\"The distance to measure for interface atoms\"\"\"\n    # Testing if this is too strict when strict overlaps are used\n    cluster_translations = not job.dock.contiguous_ghosts  # True\n    translation_cluster_epsilon = 1\n    # 1 works well at recapitulating the results without it while reducing number of checks\n    # More stringent -&gt; 0.75\n    cluster_transforms = False  # True\n    \"\"\"Whether the entire transformation space should be clustered. This was found to be redundant with a translation\n    clustering search only, and instead, decreases Pose solutions at the edge of oligomeric search slices  \n    \"\"\"\n    transformation_cluster_epsilon = 1\n    # 1 seems to work well at recapitulating the results without it\n    # less stringent -&gt; 0.75, removes about 20% found solutions\n    # stringent -&gt; 0.5, removes about %50 found solutions\n    forward_reverse = False  # True\n    # Todo 3 set above as parameters?\n    high_quality_z_value = z_value_from_match_score(high_quality_match_value)\n    low_quality_z_value = z_value_from_match_score(low_quality_match_value)\n\n    if job.dock.perturb_dof_tx:\n        if sym_entry.unit_cell:\n            logger.critical(f\"{create_perturbation_transformations.__name__} hasn't been tested for lattice symmetries\")\n\n    # Get score functions from input\n    if job.dock.weight and isinstance(job.dock.weight, dict):\n        # Todo actually use these during optimize_found_transformations_by_metrics()\n        #  score_functions = metrics.pose.format_metric_functions(job.dock.weight.keys())\n        default_weight_metric = None\n    else:\n        #  score_functions = {}\n        if job.dock.proteinmpnn_score:\n            weight_method = f'{putils.nanohedra}+{putils.proteinmpnn}'\n        else:\n            weight_method = putils.nanohedra\n\n        default_weight_metric = resources.config.default_weight_parameter[weight_method]\n\n    # Initialize incoming Structures\n    models = []\n    \"\"\"The Structure instances to be used in docking\"\"\"\n    # Assumes model is oriented with major axis of symmetry along z\n    entity_count = count(1)\n    for idx, (input_model, symmetry) in enumerate(zip(input_models, sym_entry.groups)):\n        for entity in input_model.entities:\n            if entity.is_symmetric():\n                pass\n            else:\n                # Remove any unstructured termini from the Entity to allow the best secondary structure docking\n                if job.trim_termini:\n                    entity.delete_termini(how='unstructured')\n                # Ensure models are oligomeric\n                entity.make_oligomer(symmetry=symmetry)\n\n        # Make, then save a new model based on the symmetric version of each Entity in the Model\n        if input_model.number_of_entities &gt; 2:\n            # If this was a Pose, this is essentially what is happening\n            # model = input_model.assembly\n            model = Model.from_chains([chain for entity in input_model.entities for chain in entity.chains],\n                                      name=input_model.name)\n            raise NotImplementedError(f\"Can't dock 2 Model instances with &gt; 2 total Entity instances\")\n        else:\n            model = input_model.entities[0].assembly\n            model.name = input_model.name\n\n        model.fragment_db = job.fragment_db\n        # # Ensure the .metadata attribute is passed to each entity in the full assembly\n        # # This is crucial for sql usage\n        # for _entity, entity in zip(model.entities, input_model.entities):\n        #     _entity.metadata = entity.metadata\n        models.append(model)\n\n    model1: Model\n    model2: Model\n    model1, model2 = models\n    del models\n    logger.info(f'DOCKING {model1.name} TO {model2.name}')\n\n    # Set up output mechanism\n    entry_string = f'NanohedraEntry{sym_entry.number}'\n    building_blocks = '-'.join(input_model.name for input_model in input_models)\n    if job.prefix:\n        project = f'{job.prefix}{building_blocks}'\n    else:\n        project = building_blocks\n    if job.suffix:\n        project = f'{project}{job.suffix}'\n\n    project = f'{entry_string}_{project}'\n    project_dir = os.path.join(job.projects, project)\n    putils.make_path(project_dir)\n    if job.output_trajectory:\n        if sym_entry.unit_cell:\n            logger.warning('No unit cell dimensions applicable to the trajectory file')\n\n\n    # Set up the TransformHasher to assist in scoring/pose output\n    radius1 = model1.radius  # .distance_from_reference(measure='max')\n    radius2 = model2.radius  # .distance_from_reference(measure='max')\n    # Assume the maximum distance the box could get is the radius of each plus the interface distance\n    box_width = radius1 + radius2 + cb_distance\n    model_transform_hasher = TransformHasher(box_width)\n\n    # Set up Building Block1\n    get_complete_surf_frags1_time_start = time.time()\n    surf_frags1 = model1.get_fragment_residues(residues=model1.surface_residues, fragment_db=model1.fragment_db)\n\n    # Calculate the initial match type by finding the predominant surface type\n    fragment_content1 = np.bincount([surf_frag.i_type for surf_frag in surf_frags1])\n    initial_surf_type1 = np.argmax(fragment_content1)\n    init_surf_frags1 = [surf_frag for surf_frag in surf_frags1 if surf_frag.i_type == initial_surf_type1]\n    # For reverse/forward matching these two arrays must be made\n    if forward_reverse:\n        init_surf_guide_coords1 = np.array([surf_frag.guide_coords for surf_frag in init_surf_frags1])\n        init_surf_residue_indices1 = np.array([surf_frag.index for surf_frag in init_surf_frags1])\n    # surf_frag1_indices = [surf_frag.index for surf_frag in surf_frags1]\n    idx = 1\n    # logger.debug(f'Found surface guide coordinates {idx} with shape {surf_guide_coords1.shape}')\n    # logger.debug(f'Found surface residue numbers {idx} with shape {surf_residue_numbers1.shape}')\n    # logger.debug(f'Found surface indices {idx} with shape {surf_i_indices1.shape}')\n    logger.debug(f'Found {len(init_surf_frags1)} initial surface {idx} fragments with type: {initial_surf_type1}')\n    # logger.debug('Found component 2 fragment content: %s' % fragment_content2)\n    # logger.debug('init_surf_frag_indices2: %s' % slice_variable_for_log(init_surf_frag_indices2))\n    # logger.debug('init_surf_guide_coords2: %s' % slice_variable_for_log(init_surf_guide_coords2))\n    # logger.debug('init_surf_residue_indices2: %s' % slice_variable_for_log(init_surf_residue_indices2))\n    # logger.debug('init_surf_guide_coords1: %s' % slice_variable_for_log(init_surf_guide_coords1))\n    # logger.debug('init_surf_residue_indices1: %s' % slice_variable_for_log(init_surf_residue_indices1))\n\n    logger.info(f'Retrieved component{idx}-{model1.name} surface fragments and guide coordinates took '\n                f'{time.time() - get_complete_surf_frags1_time_start:8f}s')\n\n    #################################\n    # Set up Building Block2\n    # Get Surface Fragments With Guide Coordinates Using COMPLETE Fragment Database\n    get_complete_surf_frags2_time_start = time.time()\n    surf_frags2 = \\\n        model2.get_fragment_residues(residues=model2.surface_residues, fragment_db=model2.fragment_db)\n\n    # Calculate the initial match type by finding the predominant surface type\n    surf_guide_coords2 = np.array([surf_frag.guide_coords for surf_frag in surf_frags2])\n    surf_residue_indices2 = np.array([surf_frag.index for surf_frag in surf_frags2])\n    surf_i_indices2 = np.array([surf_frag.i_type for surf_frag in surf_frags2])\n    fragment_content2 = np.bincount(surf_i_indices2)\n    initial_surf_type2 = np.argmax(fragment_content2)\n    init_surf_frag_indices2 = \\\n        [idx for idx, surf_frag in enumerate(surf_frags2) if surf_frag.i_type == initial_surf_type2]\n    init_surf_guide_coords2 = surf_guide_coords2[init_surf_frag_indices2]\n    init_surf_residue_indices2 = surf_residue_indices2[init_surf_frag_indices2]\n    idx = 2\n    logger.debug(f'Found surface guide coordinates {idx} with shape {surf_guide_coords2.shape}')\n    logger.debug(f'Found surface residue numbers {idx} with shape {surf_residue_indices2.shape}')\n    logger.debug(f'Found surface indices {idx} with shape {surf_i_indices2.shape}')\n    logger.debug(\n        f'Found {len(init_surf_residue_indices2)} initial surface {idx} fragments with type: {initial_surf_type2}')\n\n    logger.info(f'Retrieved component{idx}-{model2.name} surface fragments and guide coordinates took '\n                f'{time.time() - get_complete_surf_frags2_time_start:8f}s')\n\n    # logger.debug('init_surf_frag_indices2: %s' % slice_variable_for_log(init_surf_frag_indices2))\n    # logger.debug('init_surf_guide_coords2: %s' % slice_variable_for_log(init_surf_guide_coords2))\n    # logger.debug('init_surf_residue_indices2: %s' % slice_variable_for_log(init_surf_residue_indices2))\n\n    #################################\n    # Get component 1 ghost fragments and associated data from complete fragment database\n    component1_backbone_cb_tree = BallTree(model1.backbone_and_cb_coords)\n    get_complete_ghost_frags1_time_start = time.time()\n    ghost_frags_by_residue1 = \\\n        [frag.get_ghost_fragments(clash_tree=component1_backbone_cb_tree) for frag in surf_frags1]\n\n    complete_ghost_frags1: list[GhostFragment] = \\\n        [ghost for ghosts in ghost_frags_by_residue1 for ghost in ghosts]\n\n    ghost_guide_coords1 = np.array([ghost_frag.guide_coords for ghost_frag in complete_ghost_frags1])\n    ghost_rmsds1 = np.array([ghost_frag.rmsd for ghost_frag in complete_ghost_frags1])\n    ghost_residue_indices1 = np.array([ghost_frag.index for ghost_frag in complete_ghost_frags1])\n    ghost_j_indices1 = np.array([ghost_frag.j_type for ghost_frag in complete_ghost_frags1])\n\n    # Whether to use the overlap potential on the same component to filter ghost fragments\n    if job.dock.contiguous_ghosts:\n        # Prioritize search at those fragments which have same component, ghost fragment overlap\n        contiguous_ghost_indices1 = make_contiguous_ghosts(ghost_frags_by_residue1, surf_frags1,\n                                                           # distance=cb_distance,\n                                                           initial_z_value=initial_z_value)\n        initial_ghost_frags1 = [complete_ghost_frags1[idx] for idx in contiguous_ghost_indices1.tolist()]\n        init_ghost_guide_coords1 = np.array([ghost_frag.guide_coords for ghost_frag in initial_ghost_frags1])\n        init_ghost_rmsds1 = np.array([ghost_frag.rmsd for ghost_frag in initial_ghost_frags1])\n        init_ghost_residue_indices1 = np.array([ghost_frag.index for ghost_frag in initial_ghost_frags1])\n        # init_ghost_guide_coords1, init_ghost_rmsds1, init_ghost_residue_indices1 = \\\n        #     make_contiguous_ghosts(ghost_frags_by_residue1, surf_frags)\n    else:\n        init_ghost_frag_indices1 = \\\n            [idx for idx, ghost_frag in enumerate(complete_ghost_frags1) if ghost_frag.j_type == initial_surf_type2]\n        init_ghost_guide_coords1: np.ndarray = ghost_guide_coords1[init_ghost_frag_indices1]\n        init_ghost_rmsds1: np.ndarray = ghost_rmsds1[init_ghost_frag_indices1]\n        init_ghost_residue_indices1: np.ndarray = ghost_residue_indices1[init_ghost_frag_indices1]\n\n    idx = 1\n    logger.debug(f'Found ghost guide coordinates {idx} with shape: {ghost_guide_coords1.shape}')\n    logger.debug(f'Found ghost residue numbers {idx} with shape: {ghost_residue_indices1.shape}')\n    logger.debug(f'Found ghost indices {idx} with shape: {ghost_j_indices1.shape}')\n    logger.debug(f'Found ghost rmsds {idx} with shape: {ghost_rmsds1.shape}')\n    logger.debug(f'Found {len(init_ghost_guide_coords1)} initial ghost {idx} fragments with type:'\n                 f' {initial_surf_type2}')\n\n    logger.info(f'Retrieved component{idx}-{model1.name} ghost fragments and guide coordinates '\n                f'took {time.time() - get_complete_ghost_frags1_time_start:8f}s')\n    #################################\n    # Implemented for Todd to work on C1 instances\n    if job.only_write_frag_info:\n        # Whether to write fragment information to a directory (useful for fragment based docking w/o Nanohedra)\n        guide_file_ghost = os.path.join(project_dir, f'{model1.name}_ghost_coords.txt')\n        with open(guide_file_ghost, 'w') as f:\n            for coord_group in ghost_guide_coords1.tolist():\n                f.write('%s\\n' % ' '.join('%f,%f,%f' % tuple(coords) for coords in coord_group))\n        guide_file_ghost_idx = os.path.join(project_dir, f'{model1.name}_ghost_coords_index.txt')\n        with open(guide_file_ghost_idx, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(map(str, ghost_j_indices1.tolist())))\n        guide_file_ghost_res_num = os.path.join(project_dir, f'{model1.name}_ghost_coords_residue_number.txt')\n        with open(guide_file_ghost_res_num, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(map(str, ghost_residue_indices1.tolist())))\n\n        guide_file_surf = os.path.join(project_dir, f'{model2.name}_surf_coords.txt')\n        with open(guide_file_surf, 'w') as f:\n            for coord_group in surf_guide_coords2.tolist():\n                f.write('%s\\n' % ' '.join('%f,%f,%f' % tuple(coords) for coords in coord_group))\n        guide_file_surf_idx = os.path.join(project_dir, f'{model2.name}_surf_coords_index.txt')\n        with open(guide_file_surf_idx, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(map(str, surf_i_indices2.tolist())))\n        guide_file_surf_res_num = os.path.join(project_dir, f'{model2.name}_surf_coords_residue_number.txt')\n        with open(guide_file_surf_res_num, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(map(str, surf_residue_indices2.tolist())))\n\n        start_slice = 0\n        visualize_number = 15\n        indices_of_interest = [0, 3, 5, 10]\n        for idx, frags in enumerate(ghost_frags_by_residue1):\n            if idx in indices_of_interest:\n                number_of_fragments = len(frags)\n                step_size = number_of_fragments // visualize_number\n                # end_slice = start_slice * step_size\n                residue_number = frags[0].number\n                write_fragment_pairs_as_accumulating_states(\n                    ghost_frags_by_residue1[idx][start_slice:number_of_fragments:step_size],\n                    os.path.join(project_dir, f'{model1.name}_{residue_number}_paired_frags_'\n                                              f'{start_slice}:{number_of_fragments}:{visualize_number}.pdb'))\n\n        raise RuntimeError(\n            f'Suspending operation of {model1.name}/{model2.name} after write')\n\n    ij_type_match_lookup_table = compute_ij_type_lookup(ghost_j_indices1, surf_i_indices2)\n    # Axis 0 is ghost frag, 1 is surface frag\n    # ij_matching_ghost1_indices = \\\n    #     (ij_type_match_lookup_table * np.arange(len(ij_type_match_lookup_table)))[ij_type_match_lookup_table]\n    # ij_matching_surf2_indices = \\\n    #     (ij_type_match_lookup_table * np.arange(ij_type_match_lookup_table.shape[1])[:, None])[\n    #         ij_type_match_lookup_table]\n    # Tod0 apparently this works to grab the flattened indices where there is overlap\n    #  row_indices, column_indices = np.indices(ij_type_match_lookup_table.shape)\n    #  # row index vary with ghost, column surf\n    #  # transpose to index the first axis (axis=0) along the 1D row indices\n    #  ij_matching_ghost1_indices = row_indices[ij_type_match_lookup_table.T]\n    #  ij_matching_surf2_indices = column_indices[ij_type_match_lookup_table]\n    #  &gt;&gt;&gt; j = np.ones(22)\n    #  &gt;&gt;&gt; k = np.array([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n    #  &gt;&gt;&gt; k.shape\n    #  (2, 22)\n    #  &gt;&gt;&gt; j[k]\n    #  array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n    #          1., 1., 1., 1., 1., 1.],\n    #         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n    #          1., 1., 1., 1., 1., 1.]])\n    #  This will allow pulling out the indices where there is overlap which may be useful\n    #  for limiting scope of overlap checks\n\n    # Get component 2 ghost fragments and associated data from complete fragment database\n    bb_cb_coords2 = model2.backbone_and_cb_coords\n\n    # Whether to use the overlap potential on the same component to filter ghost fragments\n    if forward_reverse:\n        bb_cb_tree2 = BallTree(bb_cb_coords2)\n        get_complete_ghost_frags2_time_start = time.time()\n        ghost_frags_by_residue2 = \\\n            [frag.get_ghost_fragments(clash_tree=bb_cb_tree2) for frag in surf_frags2]\n\n        complete_ghost_frags2: list[GhostFragment] = \\\n            [ghost for ghosts in ghost_frags_by_residue2 for ghost in ghosts]\n        # complete_ghost_frags2 = []\n        # for frag in surf_frags2:\n        #     complete_ghost_frags2.extend(frag.get_ghost_fragments(clash_tree=bb_cb_tree2))\n\n        if job.dock.contiguous_ghosts:\n            # Prioritize search at those fragments which have same component, ghost fragment overlap\n            contiguous_ghost_indices2 = make_contiguous_ghosts(ghost_frags_by_residue2, surf_frags2,\n                                                               # distance=cb_distance,\n                                                               initial_z_value=initial_z_value)\n            initial_ghost_frags2 = [complete_ghost_frags2[idx] for idx in contiguous_ghost_indices2.tolist()]\n            init_ghost_guide_coords2 = np.array([ghost_frag.guide_coords for ghost_frag in initial_ghost_frags2])\n            # init_ghost_rmsds2 = np.array([ghost_frag.rmsd for ghost_frag in initial_ghost_frags2])\n            init_ghost_residue_indices2 = np.array([ghost_frag.index for ghost_frag in initial_ghost_frags2])\n            # init_ghost_guide_coords1, init_ghost_rmsds1, init_ghost_residue_indices1 = \\\n            #     make_contiguous_ghosts(ghost_frags_by_residue1, surf_frags)\n        else:\n            # init_ghost_frag_indices2 = \\\n            #     [idx for idx, ghost_frag in enumerate(complete_ghost_frags2) if ghost_frag.j_type == initial_surf_type1]\n            # init_ghost_guide_coords2: np.ndarray = ghost_guide_coords2[init_ghost_frag_indices2]\n            # # init_ghost_rmsds2: np.ndarray = ghost_rmsds2[init_ghost_frag_indices2]\n            # init_ghost_residue_indices2: np.ndarray = ghost_residue_indices2[init_ghost_frag_indices2]\n            initial_ghost_frags2 = [ghost_frag for ghost_frag in complete_ghost_frags2 if\n                                    ghost_frag.j_type == initial_surf_type1]\n            init_ghost_guide_coords2 = np.array([ghost_frag.guide_coords for ghost_frag in initial_ghost_frags2])\n            init_ghost_residue_indices2 = np.array([ghost_frag.index for ghost_frag in initial_ghost_frags2])\n\n        idx = 2\n        logger.debug(\n            f'Found {len(init_ghost_guide_coords2)} initial ghost {idx} fragments with type {initial_surf_type1}')\n        # logger.debug('init_ghost_guide_coords2: %s' % slice_variable_for_log(init_ghost_guide_coords2))\n        # logger.debug('init_ghost_residue_indices2: %s' % slice_variable_for_log(init_ghost_residue_indices2))\n        # ghost2_residue_array = np.repeat(init_ghost_residue_indices2, len(init_surf_residue_indices1))\n        # surface1_residue_array = np.tile(init_surf_residue_indices1, len(init_ghost_residue_indices2))\n        logger.info(f'Retrieved component{idx}-{model2.name} ghost fragments and guide coordinates '\n                    f'took {time.time() - get_complete_ghost_frags2_time_start:8f}s')\n\n    # logger.debug(f'Found ghost guide coordinates {idx} with shape {ghost_guide_coords2.shape}')\n    # logger.debug(f'Found ghost residue numbers {idx} with shape {ghost_residue_numbers2.shape}')\n    # logger.debug(f'Found ghost indices {idx} with shape {ghost_j_indices2.shape}')\n    # logger.debug(f'Found ghost rmsds {idx} with shape {ghost_rmsds2.shape}')\n    # Prepare precomputed arrays for fast pair lookup\n    # ghost1_residue_array = np.repeat(init_ghost_residue_indices1, len(init_surf_residue_indices2))\n    # surface2_residue_array = np.tile(init_surf_residue_indices2, len(init_ghost_residue_indices1))\n\n    logger.info('Obtaining rotation/degeneracy matrices')\n\n    translation_perturb_steps = tuple(.5 for _ in range(sym_entry.number_dof_translation))\n    \"\"\"The number of angstroms to increment the translation degrees of freedom search for each model\"\"\"\n    rotation_steps = [rotation_step1, rotation_step2]\n    \"\"\"The number of degrees to increment the rotational degrees of freedom search for each model\"\"\"\n    number_of_degens = []\n    number_of_rotations = []\n    rotation_matrices = []\n    for idx, rotation_step in enumerate(rotation_steps, 1):\n        if getattr(sym_entry, f'is_internal_rot{idx}'):  # if rotation step required\n            if rotation_step is None:\n                rotation_step = 3  # Set rotation_step to default\n            # Set sym_entry.rotation_step\n            setattr(sym_entry, f'rotation_step{idx}', rotation_step)\n        else:\n            if rotation_step:\n                logger.warning(f\"Specified rotation_step{idx} was ignored. Oligomer {idx} doesn't have rotational DOF\")\n            rotation_step = 1  # Set rotation step to 1\n\n        rotation_steps[idx - 1] = rotation_step\n        degeneracy_matrices = getattr(sym_entry, f'degeneracy_matrices{idx}')\n        # Todo 3 make reliant on scipy...Rotation\n        # rotation_matrix = \\\n        #     scipy.spatial.transform.Rotation.from_euler('Z', [step * rotation_step\n        #                                                       for step in range(number_of_steps)],\n        #                                                 degrees=True).as_matrix()\n        # rotations = \\\n        #     scipy.spatial.transform.Rotation.from_euler('Z', np.linspace(0, getattr(sym_entry, f'rotation_range{idx}'),\n        #                                                                  number_of_steps),\n        #                                                 degrees=True).as_matrix()\n        # rot_degen_matrices = []\n        # for idx in range(degeneracy_matrices):\n        #    rot_degen_matrices = rotations * degeneracy_matrices[idx]\n        # rot_degen_matrices = rotations * degeneracy_matrices\n        # rotation_matrix = rotations.as_matrix()\n        rotation_matrix = get_rot_matrices(rotation_step, 'z', getattr(sym_entry, f'rotation_range{idx}'))\n        rot_degen_matrices = make_rotations_degenerate(rotation_matrix, degeneracy_matrices)\n        logger.debug(f'Degeneracy shape for component {idx}: {degeneracy_matrices.shape}')\n        logger.debug(f'Combined rotation/degeneracy shape for component {idx}: {rot_degen_matrices.shape}')\n        degen_len = len(degeneracy_matrices)\n        number_of_degens.append(degen_len)\n        # logger.debug(f'Rotation shape for component {idx}: {rot_degen_matrices.shape}')\n        number_of_rotations.append(len(rot_degen_matrices) // degen_len)\n        rotation_matrices.append(rot_degen_matrices)\n\n    set_mat1, set_mat2 = sym_entry.setting_matrix1, sym_entry.setting_matrix2\n\n    # def check_forward_and_reverse(test_ghost_guide_coords, stack_rot1, stack_tx1,\n    #                               test_surf_guide_coords, stack_rot2, stack_tx2,\n    #                               reference_rmsds):\n    #     \"\"\"Debug forward versus reverse guide coordinate fragment matching\n    #\n    #     All guide_coords and reference_rmsds should be indexed to the same length and overlap\n    #     \"\"\"\n    #     mismatch = False\n    #     inv_set_mat1 = np.linalg.inv(set_mat1)\n    #     for shift_idx in range(1):\n    #         rot1 = stack_rot1[shift_idx]\n    #         tx1 = stack_tx1[shift_idx]\n    #         rot2 = stack_rot2[shift_idx]\n    #         tx2 = stack_tx2[shift_idx]\n    #\n    #         tnsfmd_ghost_coords = transform_coordinate_sets(test_ghost_guide_coords,\n    #                                                         rotation=rot1,\n    #                                                         translation=tx1,\n    #                                                         rotation2=set_mat1)\n    #         tnsfmd_surf_coords = transform_coordinate_sets(test_surf_guide_coords,\n    #                                                        rotation=rot2,\n    #                                                        translation=tx2,\n    #                                                        rotation2=set_mat2)\n    #         int_euler_matching_ghost_indices, int_euler_matching_surf_indices = \\\n    #             euler_lookup.check_lookup_table(tnsfmd_ghost_coords, tnsfmd_surf_coords)\n    #\n    #         all_fragment_match = calculate_match(tnsfmd_ghost_coords[int_euler_matching_ghost_indices],\n    #                                              tnsfmd_surf_coords[int_euler_matching_surf_indices],\n    #                                              reference_rmsds[int_euler_matching_ghost_indices])\n    #         high_qual_match_indices = np.flatnonzero(all_fragment_match &gt;= high_quality_match_value)\n    #         high_qual_match_count = len(high_qual_match_indices)\n    #         if high_qual_match_count &lt; min_matched:\n    #             logger.info(\n    #                 f'\\t{high_qual_match_count} &lt; {min_matched} Which is Set as the Minimal Required Amount of '\n    #                 f'High Quality Fragment Matches')\n    #\n    #         # Find the passing overlaps to limit the output to only those passing the low_quality_match_value\n    #         passing_overlaps_indices = np.flatnonzero(all_fragment_match &gt;= low_quality_match_value)\n    #         number_passing_overlaps = len(passing_overlaps_indices)\n    #         logger.info(\n    #             f'\\t{high_qual_match_count} High Quality Fragments Out of {number_passing_overlaps} '\n    #             f'Matches Found in Complete Fragment Library')\n    #\n    #         # now try inverse\n    #         inv_rot_mat1 = np.linalg.inv(rot1)\n    #         tnsfmd_surf_coords_inv = transform_coordinate_sets(tnsfmd_surf_coords,\n    #                                                            rotation=inv_set_mat1,\n    #                                                            translation=tx1 * -1,\n    #                                                            rotation2=inv_rot_mat1)\n    #         int_euler_matching_ghost_indices_inv, int_euler_matching_surf_indices_inv = \\\n    #             euler_lookup.check_lookup_table(test_ghost_guide_coords, tnsfmd_surf_coords_inv)\n    #\n    #         all_fragment_match = calculate_match(test_ghost_guide_coords[int_euler_matching_ghost_indices_inv],\n    #                                              tnsfmd_surf_coords_inv[int_euler_matching_surf_indices_inv],\n    #                                              reference_rmsds[int_euler_matching_ghost_indices_inv])\n    #         high_qual_match_indices = np.flatnonzero(all_fragment_match &gt;= high_quality_match_value)\n    #         high_qual_match_count = len(high_qual_match_indices)\n    #         if high_qual_match_count &lt; min_matched:\n    #             logger.info(\n    #                 f'\\tINV {high_qual_match_count} &lt; {min_matched} Which is Set as the Minimal Required Amount'\n    #                 f' of High Quality Fragment Matches')\n    #\n    #         # Find the passing overlaps to limit the output to only those passing the low_quality_match_value\n    #         passing_overlaps_indices = np.flatnonzero(all_fragment_match &gt;= low_quality_match_value)\n    #         number_passing_overlaps = len(passing_overlaps_indices)\n    #\n    #         logger.info(\n    #             f'\\t{high_qual_match_count} High Quality Fragments Out of {number_passing_overlaps} '\n    #             f'Matches Found in Complete Fragment Library')\n    #\n    #         def investigate_mismatch():\n    #             logger.info(f'Euler True ghost/surf indices forward and inverse don\\'t match. '\n    #                      f'Shapes: Forward={int_euler_matching_ghost_indices.shape}, '\n    #                      f'Inverse={int_euler_matching_ghost_indices_inv.shape}')\n    #             logger.debug(f'tnsfmd_ghost_coords.shape {tnsfmd_ghost_coords.shape}')\n    #             logger.debug(f'tnsfmd_surf_coords.shape {tnsfmd_surf_coords.shape}')\n    #             int_euler_matching_array = \\\n    #                 euler_lookup.check_lookup_table(tnsfmd_ghost_coords, tnsfmd_surf_coords, return_bool=True)\n    #             int_euler_matching_array_inv = \\\n    #                 euler_lookup.check_lookup_table(test_ghost_guide_coords, tnsfmd_surf_coords_inv, return_bool=True)\n    #             # Change the shape to allow for relation to guide_coords\n    #             different = np.where(int_euler_matching_array != int_euler_matching_array_inv,\n    #                                  True, False).reshape(len(tnsfmd_ghost_coords), -1)\n    #             ghost_indices, surface_indices = np.nonzero(different)\n    #             logger.debug(f'different.shape {different.shape}')\n    #\n    #             different_ghosts = tnsfmd_ghost_coords[ghost_indices]\n    #             different_surf = tnsfmd_surf_coords[surface_indices]\n    #             tnsfmd_ghost_ints1, tnsfmd_ghost_ints2, tnsfmd_ghost_ints3 = \\\n    #                 euler_lookup.get_eulint_from_guides(different_ghosts)\n    #             tnsfmd_surf_ints1, tnsfmd_surf_ints2, tnsfmd_surf_ints3 = \\\n    #                 euler_lookup.get_eulint_from_guides(different_surf)\n    #             stacked_ints = np.stack([tnsfmd_ghost_ints1, tnsfmd_ghost_ints2, tnsfmd_ghost_ints3,\n    #                                      tnsfmd_surf_ints1, tnsfmd_surf_ints2, tnsfmd_surf_ints3], axis=0).T\n    #             logger.info(\n    #                 f'The mismatched forward Euler ints are\\n{[ints for ints in list(stacked_ints)[:10]]}\\n')\n    #\n    #             different_ghosts_inv = test_ghost_guide_coords[ghost_indices]\n    #             different_surf_inv = tnsfmd_surf_coords_inv[surface_indices]\n    #             tnsfmd_ghost_ints_inv1, tnsfmd_ghost_ints_inv2, tnsfmd_ghost_ints_inv3 = \\\n    #                 euler_lookup.get_eulint_from_guides(different_ghosts_inv)\n    #             tnsfmd_surf_ints_inv1, tnsfmd_surf_ints_inv2, tnsfmd_surf_ints_inv3 = \\\n    #                 euler_lookup.get_eulint_from_guides(different_surf_inv)\n    #\n    #             stacked_ints_inv = \\\n    #                 np.stack([tnsfmd_ghost_ints_inv1, tnsfmd_ghost_ints_inv2, tnsfmd_ghost_ints_inv3,\n    #                           tnsfmd_surf_ints_inv1, tnsfmd_surf_ints_inv2, tnsfmd_surf_ints_inv3], axis=0).T\n    #             logger.info(\n    #                 f'The mismatched inverse Euler ints are\\n{[ints for ints in list(stacked_ints_inv)[:10]]}\\n')\n    #\n    #         if not np.array_equal(int_euler_matching_ghost_indices, int_euler_matching_ghost_indices_inv):\n    #             mismatch = True\n    #\n    #         if not np.array_equal(int_euler_matching_surf_indices, int_euler_matching_surf_indices_inv):\n    #             mismatch = True\n    #\n    #         if mismatch:\n    #             investigate_mismatch()\n\n    # if job.skip_transformation:\n    #     transformation1 = unpickle(kwargs.get('transformation_file1'))\n    #     full_rotation1, full_int_tx1, full_setting1, full_ext_tx1 = transformation1.values()\n    #     transformation2 = unpickle(kwargs.get('transformation_file2'))\n    #     full_rotation2, full_int_tx2, full_setting2, full_ext_tx2 = transformation2.values()\n    # else:\n\n    # Set up internal translation parameters\n    # zshift1/2 must be 2d array, thus the , 2:3].T instead of , 2].T\n    # [:, None, 2] would also work\n    if sym_entry.is_internal_tx1:  # Add the translation to Z (axis=1)\n        full_int_tx1 = []\n        zshift1 = set_mat1[:, None, 2].T\n    else:\n        full_int_tx1 = zshift1 = None\n\n    if sym_entry.is_internal_tx2:\n        full_int_tx2 = []\n        zshift2 = set_mat2[:, None, 2].T\n    else:\n        full_int_tx2 = zshift2 = None\n\n    # Set up external translation parameters\n    full_optimal_ext_dof_shifts = []\n    if sym_entry.unit_cell:\n        positive_indices = None\n    else:\n        # Ensure to slice by nothing, as None alone creates a new axis\n        positive_indices = slice(None)\n\n    # Initialize the OptimalTx object\n    logger.debug(f'zshift1={zshift1}, zshift2={zshift2}, max_z_value={initial_z_value:2f}')\n    optimal_tx = resources.OptimalTx.from_dof(sym_entry.external_dof, zshift1=zshift1, zshift2=zshift2,\n                                              max_z_value=initial_z_value)\n\n    number_of_init_ghost = len(init_ghost_guide_coords1)\n    number_of_init_surf = len(init_surf_guide_coords2)\n    total_ghost_surf_combinations = number_of_init_ghost * number_of_init_surf\n    full_rotation1, full_rotation2 = [], []\n    rotation_matrices1, rotation_matrices2 = rotation_matrices\n    rotation_matrices_len1, rotation_matrices_len2 = len(rotation_matrices1), len(rotation_matrices2)\n    number_of_rotations1, number_of_rotations2 = number_of_rotations\n    # number_of_degens1, number_of_degens2 = number_of_degens\n\n    # Perform Euler integer extraction for all rotations\n    init_translation_time_start = time.time()\n    # Rotate Oligomer1 surface and ghost guide coordinates using rotation_matrices1 and set_mat1\n    # Must add a new axis so that the multiplication is broadcast\n    ghost_frag1_guide_coords_rot_and_set = \\\n        transform_coordinate_sets(init_ghost_guide_coords1[None, :, :, :],\n                                  rotation=rotation_matrices1[:, None, :, :],\n                                  rotation2=set_mat1[None, None, :, :])\n    # Unstack the guide coords to be shape (N, 3, 3)\n    # eulerint_ghost_component1_1, eulerint_ghost_component1_2, eulerint_ghost_component1_3 = \\\n    #     euler_lookup.get_eulint_from_guides(ghost_frag1_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n    eulerint_ghost_component1 = \\\n        euler_lookup.get_eulint_from_guides_as_array(ghost_frag1_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n    # Next, for component 2\n    surf_frags2_guide_coords_rot_and_set = \\\n        transform_coordinate_sets(init_surf_guide_coords2[None, :, :, :],\n                                  rotation=rotation_matrices2[:, None, :, :],\n                                  rotation2=set_mat2[None, None, :, :])\n    # Reshape with the first axis (0) containing all the guide coordinate rotations stacked\n    eulerint_surf_component2 = \\\n        euler_lookup.get_eulint_from_guides_as_array(surf_frags2_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n    if forward_reverse:\n        surf_frag1_guide_coords_rot_and_set = \\\n            transform_coordinate_sets(init_surf_guide_coords1[None, :, :, :],\n                                      rotation=rotation_matrices1[:, None, :, :],\n                                      rotation2=set_mat1[None, None, :, :])\n        ghost_frags2_guide_coords_rot_and_set = \\\n            transform_coordinate_sets(init_ghost_guide_coords2[None, :, :, :],\n                                      rotation=rotation_matrices2[:, None, :, :],\n                                      rotation2=set_mat2[None, None, :, :])\n        eulerint_surf_component1 = \\\n            euler_lookup.get_eulint_from_guides_as_array(surf_frag1_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n        eulerint_ghost_component2 = \\\n            euler_lookup.get_eulint_from_guides_as_array(ghost_frags2_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n\n        stacked_surf_euler_int1 = eulerint_surf_component1.reshape((rotation_matrices_len1, -1, 3))\n        stacked_ghost_euler_int2 = eulerint_ghost_component2.reshape((rotation_matrices_len2, -1, 3))\n        # Improve indexing time by precomputing python objects\n        stacked_ghost_euler_int2 = list(stacked_ghost_euler_int2)\n        stacked_surf_euler_int1 = list(stacked_surf_euler_int1)\n    # eulerint_surf_component2_1, eulerint_surf_component2_2, eulerint_surf_component2_3 = \\\n    #     euler_lookup.get_eulint_from_guides(surf_frags2_guide_coords_rot_and_set.reshape((-1, 3, 3)))\n\n    # Reshape the reduced dimensional eulerint_components to again have the number_of_rotations length on axis 0,\n    # the number of init_guide_coords on axis 1, and the 3 euler intergers on axis 2\n    stacked_surf_euler_int2 = eulerint_surf_component2.reshape((rotation_matrices_len2, -1, 3))\n    stacked_ghost_euler_int1 = eulerint_ghost_component1.reshape((rotation_matrices_len1, -1, 3))\n    # Improve indexing time by precomputing python objects\n    stacked_surf_euler_int2 = list(stacked_surf_euler_int2)\n    stacked_ghost_euler_int1 = list(stacked_ghost_euler_int1)\n\n    # stacked_surf_euler_int2_1 = eulerint_surf_component2_1.reshape((rotation_matrices_len2, -1))\n    # stacked_surf_euler_int2_2 = eulerint_surf_component2_2.reshape((rotation_matrices_len2, -1))\n    # stacked_surf_euler_int2_3 = eulerint_surf_component2_3.reshape((rotation_matrices_len2, -1))\n    # stacked_ghost_euler_int1_1 = eulerint_ghost_component1_1.reshape((rotation_matrices_len1, -1))\n    # stacked_ghost_euler_int1_2 = eulerint_ghost_component1_2.reshape((rotation_matrices_len1, -1))\n    # stacked_ghost_euler_int1_3 = eulerint_ghost_component1_3.reshape((rotation_matrices_len1, -1))\n\n    # The fragments being added to the pose are different than the fragments generated on the pose. This function\n    # helped me elucidate that this was occurring\n    # def check_offset_index(title):\n    #     if pose.entities[-1].offset_index == 0:\n    #         raise RuntimeError('The offset_index has changed to 0')\n    #     else:\n    #         print(f'{title} offset_index: {pose.entities[-1].offset_index}')\n    if job.dock.quick:  # job.development:\n        rotations_to_perform1 = min(len(rotation_matrices1), 13)\n        rotations_to_perform2 = min(len(rotation_matrices2), 12)\n        logger.critical(f'Development: Only sampling {rotations_to_perform1} by {rotations_to_perform2} rotations')\n    else:\n        rotations_to_perform1 = len(rotation_matrices1)\n        rotations_to_perform2 = len(rotation_matrices2)\n\n    # Todo 2 multiprocessing\n    def initial_euler_search():\n        pass\n\n    if job.multi_processing:\n        raise NotImplementedError(\n            f\"Can't perform {fragment_dock.__name__} using {flags.multi_processing.long} yet\")\n        rotation_pairs = None\n        results = utils.mp_map(initial_euler_search, rotation_pairs, processes=job.cores)\n    else:\n        pass\n        # results = []\n        # for rot_pair in rotation_pairs:\n        #     results.append(initial_euler_search(rot_pair))\n\n    # Todo 3 resolve which mechanisms to use. guide coords or eulerints\n    #  Below uses eulerints which work just fine.\n    #  Timings on these from improved protocols shows about similar times to euler_lookup and calculate_overlap\n    #  even with vastly different scales of the arrays. This ignores the fact that calculate_overlap uses a\n    #  number of indexing steps including making the ij_match array formation, indexing against the ghost and\n    #  surface arrays, the rmsd_reference construction\n    #  |\n    #  Given the lookups sort of irrelevance to the scoring (given very poor alignment), I could remove that\n    #  step if it interfered with differentiability\n    #  |\n    #  Majority of time is spent indexing the 6D euler overlap arrays which should be quite easy to speed up given\n    #  understanding of different computational efficiencies at this check\n    logger.info('Querying building blocks for initial fragment overlap')\n    # Get rotated component1 ghost fragment, component2 surface fragment guide coodinate pairs in the same Euler space\n    perturb_dof = job.dock.perturb_dof\n    total_dof_combinations = rotations_to_perform1 * rotations_to_perform2\n    progress_iter = iter(tqdm(range(total_dof_combinations), bar_format=TQDM_BAR_FORMAT))\n    for idx1 in range(rotations_to_perform1):\n        rot1_count = idx1%number_of_rotations1 + 1\n        degen1_count = idx1//number_of_rotations1 + 1\n        rot_mat1 = rotation_matrices1[idx1]\n        rotation_ghost_euler_ints1 = stacked_ghost_euler_int1[idx1]\n        if forward_reverse:\n            rotation_surf_euler_ints1 = stacked_surf_euler_int1[idx1]\n        for idx2 in range(rotations_to_perform2):\n            next(progress_iter)  # Updates progress bar\n            # Rotate component2 surface and ghost fragment guide coordinates using rot_mat2 and set_mat2\n            rot2_count = idx2%number_of_rotations2 + 1\n            degen2_count = idx2//number_of_rotations2 + 1\n            rot_mat2 = rotation_matrices2[idx2]\n\n            logger.debug(f'***** OLIGOMER 1: Degeneracy {degen1_count} Rotation {rot1_count} | '\n                         f'OLIGOMER 2: Degeneracy {degen2_count} Rotation {rot2_count} *****')\n\n            euler_start = time.time()\n            # euler_matched_surf_indices2, euler_matched_ghost_indices1 = \\\n            #     euler_lookup.lookup_by_euler_integers(stacked_surf_euler_int2_1[idx2],\n            #                                           stacked_surf_euler_int2_2[idx2],\n            #                                           stacked_surf_euler_int2_3[idx2],\n            #                                           stacked_ghost_euler_int1_1[idx1],\n            #                                           stacked_ghost_euler_int1_2[idx1],\n            #                                           stacked_ghost_euler_int1_3[idx1],\n            #                                           )\n            euler_matched_surf_indices2, euler_matched_ghost_indices1 = \\\n                euler_lookup.lookup_by_euler_integers_as_array(stacked_surf_euler_int2[idx2],\n                                                               rotation_ghost_euler_ints1)\n            # # euler_lookup.lookup_by_euler_integers_as_array(eulerint_ghost_component2.reshape(number_of_rotations2,\n            # #                                                                                  1, 3),\n            # #                                                eulerint_surf_component1.reshape(number_of_rotations1,\n            # #                                                                                 1, 3))\n            if forward_reverse:\n                euler_matched_ghost_indices_rev2, euler_matched_surf_indices_rev1 = \\\n                    euler_lookup.lookup_by_euler_integers_as_array(stacked_ghost_euler_int2[idx2],\n                                                                   rotation_surf_euler_ints1)\n            # Todo 3 resolve. eulerints\n\n    # Todo 3 resolve. Below uses guide coords\n    # # for idx1 in range(rotation_matrices):\n    # # Iterating over more than 2 rotation matrix sets becomes hard to program dynamically owing to the permutations\n    # # of the rotations and the application of the rotation/setting to each set of fragment information. It would be a\n    # # bit easier if the same logic that is applied to the following routines, (similarity matrix calculation) putting\n    # # the rotation of the second set of fragment information into the setting of the first by applying the inverse\n    # # rotation and setting matrices to the second (or third...) set of fragments. Forget about this for now\n    # init_time_start = time.time()\n    # for idx1 in range(len(rotation_matrices1)):  # min(len(rotation_matrices1), 5)):  # Todo remove min\n    #     # Rotate Oligomer1 Surface and Ghost Fragment Guide Coordinates using rot_mat1 and set_mat1\n    #     rot1_count = idx1 % number_of_rotations1 + 1\n    #     degen1_count = idx1 // number_of_rotations1 + 1\n    #     rot_mat1 = rotation_matrices1[idx1]\n    #     ghost_guide_coords_rot_and_set1 = \\\n    #         transform_coordinate_sets(init_ghost_guide_coords1, rotation=rot_mat1, rotation2=set_mat1)\n    #     # surf_guide_coords_rot_and_set1 = \\\n    #     #     transform_coordinate_sets(init_surf_guide_coords1, rotation=rot_mat1, rotation2=set_mat1)\n    #\n    #     for idx2 in range(len(rotation_matrices2)):  # min(len(rotation_matrices2), 5)):  # Todo remove min\n    #         # Rotate Oligomer2 Surface and Ghost Fragment Guide Coordinates using rot_mat2 and set_mat2\n    #         rot2_count = idx2 % number_of_rotations2 + 1\n    #         degen2_count = idx2 // number_of_rotations2 + 1\n    #         rot_mat2 = rotation_matrices2[idx2]\n    #         surf_guide_coords_rot_and_set2 = \\\n    #             transform_coordinate_sets(init_surf_guide_coords2, rotation=rot_mat2, rotation2=set_mat2)\n    #         # ghost_guide_coords_rot_and_set2 = \\\n    #         #     transform_coordinate_sets(init_ghost_guide_coords2, rotation=rot_mat2, rotation2=set_mat2)\n    #\n    #         logger.info(f'***** OLIGOMER 1: Degeneracy {degen1_count} Rotation {rot1_count} | '\n    #                     f'OLIGOMER 2: Degeneracy {degen2_count} Rotation {rot2_count} *****')\n    #\n    #         euler_start = time.time()\n    #         # First returned variable has indices increasing 0,0,0,0,1,1,1,1,1,2,2,2,3,...\n    #         # Second returned variable has indices increasing 2,3,4,14,...\n    #         euler_matched_surf_indices2, euler_matched_ghost_indices1 = \\\n    #             euler_lookup.check_lookup_table(surf_guide_coords_rot_and_set2,\n    #                                             ghost_guide_coords_rot_and_set1)\n    #         # euler_matched_ghost_indices_rev2, euler_matched_surf_indices_rev1 = \\\n    #         #     euler_lookup.check_lookup_table(ghost_guide_coords_rot_and_set2,\n    #         #                                     surf_guide_coords_rot_and_set1)\n    # Todo 3 resolve. guide coords\n            logger.debug(f'\\tEuler search took {time.time() - euler_start:8f}s for '\n                         f'{total_ghost_surf_combinations} ghost/surf pairs')\n\n            if forward_reverse:\n                # Ensure pairs are similar between euler_matched_surf_indices2 and euler_matched_ghost_indices_rev2\n                # by indexing the residue_numbers\n                # forward_reverse_comparison_start = time.time()\n                # logger.debug(f'Euler indices forward, index 0: {euler_matched_surf_indices2[:10]}')\n                forward_surface_indices2 = init_surf_residue_indices2[euler_matched_surf_indices2]\n                # logger.debug(f'Euler indices forward, index 1: {euler_matched_ghost_indices1[:10]}')\n                forward_ghosts_indices1 = init_ghost_residue_indices1[euler_matched_ghost_indices1]\n                # logger.debug(f'Euler indices reverse, index 0: {euler_matched_ghost_indices_rev2[:10]}')\n                reverse_ghosts_indices2 = init_ghost_residue_indices2[euler_matched_ghost_indices_rev2]\n                # logger.debug(f'Euler indices reverse, index 1: {euler_matched_surf_indices_rev1[:10]}')\n                reverse_surface_indices1 = init_surf_residue_indices1[euler_matched_surf_indices_rev1]\n\n                # Make an index indicating where the forward and reverse euler lookups have the same residue pairs\n                # Important! This method only pulls out initial fragment matches that go both ways, i.e. component1\n                # surface (type1) matches with component2 ghost (type1) and vice versa, so the expanded checks of\n                # for instance the surface loop (i type 3,4,5) with ghost helical (i type 1) matches is completely\n                # unnecessary during euler look up as this will never be included\n                # Also, this assumes that the ghost fragment display is symmetric, i.e. 1 (i) 1 (j) 10 (K) has an\n                # inverse transform at 1 (i) 1 (j) 230 (k) for instance\n\n                prior = 0\n                number_overlapping_pairs = len(euler_matched_ghost_indices1)\n                possible_overlaps = np.ones(number_overlapping_pairs, dtype=np.bool8)\n                # Residue numbers are in order for forward_surface_indices2 and reverse_ghosts_indices2\n                for residue_index in init_surf_residue_indices2:\n                    # Where the residue number of component 2 is equal pull out the indices\n                    forward_index = np.flatnonzero(forward_surface_indices2 == residue_index)\n                    reverse_index = np.flatnonzero(reverse_ghosts_indices2 == residue_index)\n                    # Next, use residue number indices to search for the same residue numbers in the extracted pairs\n                    # The output array slice is only valid if the forward_index is the result of\n                    # forward_surface_indices2 being in ascending order, which for check_lookup_table is True\n                    current = prior + len(forward_index)\n                    possible_overlaps[prior:current] = \\\n                        np.in1d(forward_ghosts_indices1[forward_index], reverse_surface_indices1[reverse_index])\n                    prior = current\n\n            # # Use for residue number debugging\n            # possible_overlaps = np.ones(number_overlapping_pairs, dtype=np.bool8)\n\n            # forward_ghosts_indices1[possible_overlaps]\n            # forward_surface_indices2[possible_overlaps]\n\n            # indexing_possible_overlap_time = time.time() - indexing_possible_overlap_start\n\n            # number_of_successful = possible_overlaps.sum()\n            # logger.info(f'\\tIndexing {number_overlapping_pairs * len(euler_matched_surf_indices2)} '\n            #             f'possible overlap pairs found only {number_of_successful} possible out of '\n            #             f'{number_overlapping_pairs} (took {time.time() - forward_reverse_comparison_start:8f}s)')\n\n            # passing_ghost_coords = ghost_guide_coords_rot_and_set1[possible_ghost_frag_indices]\n            # passing_surf_coords = surf_guide_coords_rot_and_set2[euler_matched_surf_indices2[possible_overlaps]]\n\n            # Get optimal shift parameters for initial (Ghost Fragment, Surface Fragment) guide coordinate pairs\n            # # Todo these are from Guides\n            # passing_ghost_coords = ghost_guide_coords_rot_and_set1[euler_matched_ghost_indices1]\n            # passing_surf_coords = surf_guide_coords_rot_and_set2[euler_matched_surf_indices2]\n            # # Todo these are from Guides\n            # Todo debug With EulerInteger calculation\n            if forward_reverse:\n                # Take the boolean index of the indices\n                possible_ghost_frag_indices = euler_matched_ghost_indices1[possible_overlaps]\n                # possible_surf_frag_indices = euler_matched_surf_indices2[possible_overlaps]\n                passing_ghost_coords = \\\n                    ghost_frag1_guide_coords_rot_and_set[idx1, possible_ghost_frag_indices]\n                passing_surf_coords = \\\n                    surf_frags2_guide_coords_rot_and_set[idx2, euler_matched_surf_indices2[possible_overlaps]]\n                reference_rmsds = init_ghost_rmsds1[possible_ghost_frag_indices]\n            else:\n                passing_ghost_coords = ghost_frag1_guide_coords_rot_and_set[idx1, euler_matched_ghost_indices1]\n                # passing_ghost_coords = transform_coordinate_sets(init_ghost_guide_coords1[euler_matched_ghost_indices1],\n                #                                                  rotation=rot_mat1, rotation2=set_mat1)\n                passing_surf_coords = surf_frags2_guide_coords_rot_and_set[idx2, euler_matched_surf_indices2]\n                # passing_surf_coords = transform_coordinate_sets(init_surf_guide_coords2[euler_matched_surf_indices2],\n                #                                                 rotation=rot_mat2, rotation2=set_mat2)\n                reference_rmsds = init_ghost_rmsds1[euler_matched_ghost_indices1]\n            # Todo debug With EulerInteger calculation\n\n            optimal_shifts_start = time.time()\n            transform_passing_shifts = \\\n                optimal_tx.solve_optimal_shifts(passing_ghost_coords, passing_surf_coords, reference_rmsds)\n            optimal_shifts_time = time.time() - optimal_shifts_start\n\n            pre_cluster_passing_shifts = len(transform_passing_shifts)\n            if pre_cluster_passing_shifts == 0:\n                # logger.debug(f'optimal_shifts length: {len(optimal_shifts)}')\n                # logger.debug(f'transform_passing_shifts shape: {len(transform_passing_shifts)}')\n                logger.debug(f'\\tNo transforms were found passing optimal shift criteria '\n                             f'(took {optimal_shifts_time:8f}s)')\n                continue\n            elif cluster_translations:\n                cluster_time_start = time.time()\n                translation_cluster = \\\n                    DBSCAN(eps=translation_cluster_epsilon, min_samples=min_matched).fit(transform_passing_shifts)\n                if perturb_dof:  # Later will be sampled more finely, so\n                    # Get the core indices, i.e. the most dense translation regions only\n                    transform_passing_shift_indexer = translation_cluster.core_sample_indices_\n                else:  # Get any transform which isn't an outlier\n                    transform_passing_shift_indexer = translation_cluster.labels_ != outlier\n                transform_passing_shifts = transform_passing_shifts[transform_passing_shift_indexer]\n                cluster_time = time.time() - cluster_time_start\n                logger.debug(f'Clustering {pre_cluster_passing_shifts} possible transforms (took {cluster_time:8f}s)')\n            # else:  # Use all translations\n            #     pass\n\n            if sym_entry.unit_cell:\n                # Must take the optimal_ext_dof_shifts and multiply the column number by the corresponding row\n                # in the sym_entry.external_dof#\n                # optimal_ext_dof_shifts[0] scalar * sym_entry.group_external_dof[0] (1 row, 3 columns)\n                # Repeat for additional DOFs, then add all up within each row.\n                # For a single DOF, multiplication won't matter as only one matrix element will be available\n                #\n                # Must find positive indices before external_dof1 multiplication in case negatives there\n                positive_indices = \\\n                    np.flatnonzero(np.all(transform_passing_shifts[:, :sym_entry.number_dof_external] &gt;= 0, axis=1))\n                number_passing_shifts = len(positive_indices)\n                optimal_ext_dof_shifts = np.zeros((number_passing_shifts, 3), dtype=float)\n                optimal_ext_dof_shifts[:, :sym_entry.number_dof_external] = \\\n                    transform_passing_shifts[positive_indices, :sym_entry.number_dof_external]\n                # ^ I think for the sake of cleanliness, I need to make this matrix\n\n                full_optimal_ext_dof_shifts.append(optimal_ext_dof_shifts)\n            else:\n                number_passing_shifts = len(transform_passing_shifts)\n\n            # logger.debug(f'\\tFound {number_passing_shifts} transforms after clustering from '\n            #              f'{pre_cluster_passing_shifts} possible transforms (took '\n            #              f'{time.time() - cluster_time_start:8f}s)')\n\n            # Prepare the transformation parameters for storage in full transformation arrays\n            # Use of [:, None] transforms the array into an array with each internal dof sored as a scalar in\n            # axis 1 and each successive index along axis 0 as each passing shift\n\n            # Stack each internal parameter along with a blank vector, this isolates the tx vector along z axis\n            if full_int_tx1 is not None:\n                # Store transformation parameters, indexing only those that are positive in the case of lattice syms\n                full_int_tx1.extend(\n                    transform_passing_shifts[positive_indices, sym_entry.number_dof_external].tolist())\n\n            if full_int_tx2 is not None:\n                # Store transformation parameters, indexing only those that are positive in the case of lattice syms\n                full_int_tx2.extend(\n                    transform_passing_shifts[positive_indices, sym_entry.number_dof_external + 1].tolist())\n\n            full_rotation1.append(np.tile(rot_mat1, (number_passing_shifts, 1, 1)))\n            full_rotation2.append(np.tile(rot_mat2, (number_passing_shifts, 1, 1)))\n\n            logger.debug(f'\\tOptimal shift search took {optimal_shifts_time:8f}s for '\n                         f'{len(euler_matched_ghost_indices1)} guide coordinate pairs')\n            logger.debug(f'\\t{number_passing_shifts if number_passing_shifts else \"No\"} initial interface '\n                         f'match{\"es\" if number_passing_shifts != 1 else \"\"} found '\n                         f'(took {time.time() - euler_start:8f}s)')\n\n    try:\n        next(progress_iter)  # Updates progress bar\n    except StopIteration:\n        pass\n    # -----------------------------------------------------------------------------------------------------------------\n    # Below creates vectors for cluster transformations\n    # Then asu clash testing, scoring, and symmetric clash testing are performed\n    # -----------------------------------------------------------------------------------------------------------------\n    if sym_entry.unit_cell:\n        # optimal_ext_dof_shifts[:, :, None] &lt;- None expands the axis to make multiplication accurate\n        full_optimal_ext_dof_shifts = np.concatenate(full_optimal_ext_dof_shifts, axis=0)\n        unsqueezed_optimal_ext_dof_shifts = full_optimal_ext_dof_shifts[:, :, None]\n        external_dof1, external_dof2, *_ = sym_entry.external_dofs\n        full_ext_tx1 = np.sum(unsqueezed_optimal_ext_dof_shifts * external_dof1, axis=-2)\n        full_ext_tx2 = np.sum(unsqueezed_optimal_ext_dof_shifts * external_dof2, axis=-2)\n        full_ext_tx_sum = full_ext_tx2 - full_ext_tx1\n        del unsqueezed_optimal_ext_dof_shifts\n    else:\n        full_ext_tx1 = full_ext_tx2 = full_ext_tx_sum = full_optimal_ext_dof_shifts = None\n        # full_optimal_ext_dof_shifts = list(repeat(None, number_passing_shifts))\n\n    if not full_rotation1:  # There were no successful transforms\n        logger.warning(f'No optimal translations found. Terminating {building_blocks} docking')\n        return []\n        # ------------------ TERMINATE DOCKING ------------------------\n    # Make full, numpy vectorized transformations overwriting individual variables for memory management\n    full_rotation1 = np.concatenate(full_rotation1, axis=0)\n    full_rotation2 = np.concatenate(full_rotation2, axis=0)\n    starting_transforms = len(full_rotation1)\n    logger.info(f'Initial optimal translation search found {starting_transforms} total transforms '\n                f'in {time.time() - init_translation_time_start:8f}s')\n\n    if sym_entry.is_internal_tx1:\n        stacked_internal_tx_vectors1 = np.zeros((starting_transforms, 3), dtype=float)\n        # Add the translation to Z (axis=1)\n        stacked_internal_tx_vectors1[:, -1] = full_int_tx1\n        full_int_tx1 = stacked_internal_tx_vectors1\n        del stacked_internal_tx_vectors1\n\n    if sym_entry.is_internal_tx2:\n        stacked_internal_tx_vectors2 = np.zeros((starting_transforms, 3), dtype=float)\n        # Add the translation to Z (axis=1)\n        stacked_internal_tx_vectors2[:, -1] = full_int_tx2\n        full_int_tx2 = stacked_internal_tx_vectors2\n        del stacked_internal_tx_vectors2\n\n    # Make inverted transformations\n    inv_setting1 = np.linalg.inv(set_mat1)\n    full_inv_rotation1 = np.linalg.inv(full_rotation1)\n    _full_rotation2 = full_rotation2.copy()\n    if sym_entry.is_internal_tx1:\n        # Invert by multiplying by -1\n        full_int_tx_inv1 = full_int_tx1 * -1\n    else:\n        full_int_tx_inv1 = None\n    if sym_entry.is_internal_tx2:\n        _full_int_tx2 = full_int_tx2.copy()\n    else:\n        _full_int_tx2 = None\n\n    # Define functions for making active transformation arrays and removing indices from them\n    def create_transformation_group() -&gt; tuple[dict[str, np.ndarray | None], dict[str, np.ndarray | None]]:\n        \"\"\"Create the transformation mapping for each transformation in the current docking trajectory\n\n        Returns:\n            Every stacked transformation operation for the two separate models being docked in two separate dictionaries\n        \"\"\"\n        return (\n            dict(rotation=full_rotation1, translation=None if full_int_tx1 is None else full_int_tx1[:, None, :],\n                 rotation2=set_mat1, translation2=None if full_ext_tx1 is None else full_ext_tx1[:, None, :]),\n            dict(rotation=full_rotation2, translation=None if full_int_tx2 is None else full_int_tx2[:, None, :],\n                 rotation2=set_mat2, translation2=None if full_ext_tx2 is None else full_ext_tx2[:, None, :])\n        )\n\n    def remove_non_viable_indices_inverse(passing_indices: np.ndarray | list[int]):\n        \"\"\"Responsible for updating docking intermediate transformation parameters for inverse transform operations\n        These include: full_inv_rotation1, _full_rotation2, full_int_tx_inv1, _full_int_tx2, and full_ext_tx_sum\n        \"\"\"\n        nonlocal full_inv_rotation1, _full_rotation2, full_int_tx_inv1, _full_int_tx2, full_ext_tx_sum\n        full_inv_rotation1 = full_inv_rotation1[passing_indices]\n        _full_rotation2 = _full_rotation2[passing_indices]\n        if sym_entry.is_internal_tx1:\n            full_int_tx_inv1 = full_int_tx_inv1[passing_indices]\n        if sym_entry.is_internal_tx2:\n            _full_int_tx2 = _full_int_tx2[passing_indices]\n        if sym_entry.unit_cell:\n            full_ext_tx_sum = full_ext_tx_sum[passing_indices]\n\n    def filter_transforms_by_indices(passing_indices: np.ndarray | list[int]):\n        \"\"\"Responsible for updating docking transformation parameters for transform operations. Will set the\n        transformation in the order of the passing_indices\n\n        Includes:\n            full_rotation1, full_rotation2, full_int_tx1, full_int_tx2, full_optimal_ext_dof_shifts, full_ext_tx1, and\n            full_ext_tx2\n\n        Args:\n            passing_indices: The indices which should be kept\n        \"\"\"\n        nonlocal full_rotation1, full_rotation2, full_int_tx1, full_int_tx2\n        full_rotation1 = full_rotation1[passing_indices]\n        full_rotation2 = full_rotation2[passing_indices]\n        if sym_entry.is_internal_tx1:\n            full_int_tx1 = full_int_tx1[passing_indices]\n        if sym_entry.is_internal_tx2:\n            full_int_tx2 = full_int_tx2[passing_indices]\n\n        if sym_entry.unit_cell:\n            nonlocal full_optimal_ext_dof_shifts, full_ext_tx1, full_ext_tx2\n            full_optimal_ext_dof_shifts = full_optimal_ext_dof_shifts[passing_indices]\n            full_ext_tx1 = full_ext_tx1[passing_indices]\n            full_ext_tx2 = full_ext_tx2[passing_indices]\n\n    # Find the clustered transformations to expedite search of ASU clashing\n    if cluster_transforms:\n        clustering_start = time.time()\n        # Todo 3\n        #  Can I use cluster.cluster_transformation_pairs distance graph to provide feedback on other aspects of the\n        #  dock? Seems that I could use the distances to expedite clashing checks, especially for more time consuming\n        #  expansion checks such as the full material...\n        # Must add a new axis to translations so the operations are broadcast together in transform_coordinate_sets()\n        transform_neighbor_tree, transform_cluster = \\\n            cluster.cluster_transformation_pairs(*create_transformation_group(),\n                                                 distance=transformation_cluster_epsilon,\n                                                 minimum_members=min_matched\n                                                 )\n        # cluster_representative_indices, cluster_labels = \\\n        #     find_cluster_representatives(transform_neighbor_tree, transform_cluster)\n        # representative_labels = cluster_labels[cluster_representative_indices]\n        # Todo 3\n        #  _, cluster_labels = find_cluster_representatives(transform_neighbor_tree, transform_cluster)\n        # cluster_labels = transform_cluster.labels_\n        # logger.debug(f'Shape of cluster_labels: {cluster_labels.shape}')\n        # passing_transforms = cluster_labels != -1\n        sufficiently_dense_indices = np.flatnonzero(transform_cluster.labels_ != -1)\n        number_of_dense_transforms = len(sufficiently_dense_indices)\n\n        logger.info(f'After clustering, {starting_transforms - number_of_dense_transforms} are missing the minimum '\n                    f'number of close transforms to be viable. {number_of_dense_transforms} transforms '\n                    f'remain (took {time.time() - clustering_start:8f}s)')\n        if not number_of_dense_transforms:  # There were no successful transforms\n            logger.warning(f'No viable transformations found. Terminating {building_blocks} docking')\n            return []\n        # ------------------ TERMINATE DOCKING ------------------------\n        # Update the transformation array and counts with the sufficiently_dense_indices\n        # Remove non-viable transforms by indexing sufficiently_dense_indices\n        remove_non_viable_indices_inverse(sufficiently_dense_indices)\n        del transform_neighbor_tree, transform_cluster\n    else:\n        sufficiently_dense_indices = np.arange(starting_transforms)\n        number_of_dense_transforms = starting_transforms\n\n    # if job.design.ignore_pose_clashes:\n    #     logger.warning(f'Not checking for pose clashes per requested flag '\n    #                    f'{flags.format_args(flags.ignore_pose_clashes_args)}')\n    # else:\n    # Transform coords to query for clashes\n    # Set up chunks of coordinate transforms for clash testing\n    check_clash_coords_start = time.time()\n    calculation_size = number_of_dense_transforms\n    # Start with the assumption that all tested clashes are clashing\n    asu_clash_counts = np.ones(calculation_size)\n    batch_length = get_check_tree_for_query_overlap_batch_length(bb_cb_coords2)\n    batch_length = min((batch_length, calculation_size))\n\n    # Setup function is performed before the function is executed\n    def np_tile_wrap(length: int, coords: np.ndarray, *args, **kwargs):\n        return dict(query_points=np.tile(coords, (length, 1, 1)))\n\n    # Todo this is used in perturb_transformations with different 'size' and 'batch_length' that are going to snag\n    #  at some point given the size of the perturb could be larger than the batch_length\n    # Create the balltree clash check as a batched function\n    @resources.ml.batch_calculation(size=calculation_size, batch_length=batch_length, setup=np_tile_wrap,\n                                    compute_failure_exceptions=(np.core._exceptions._ArrayMemoryError,))\n    def init_check_tree_for_query_overlap(*args, **kwargs):\n        return check_tree_for_query_overlap(*args, **kwargs)\n\n    logger.info(f'Testing found transforms for ASU clashes')\n    # Using the inverse transform of the model2 backbone and cb (surface fragment) coordinates, check for clashes\n    # with the model1 backbone and cb coordinates BinaryTree\n    ball_tree_kwargs = dict(binarytree=component1_backbone_cb_tree, clash_distance=clash_dist,\n                            rotation=_full_rotation2, translation=_full_int_tx2,\n                            rotation2=set_mat2, translation2=full_ext_tx_sum,\n                            rotation3=inv_setting1, translation3=full_int_tx_inv1,\n                            rotation4=full_inv_rotation1)\n\n    overlap_return = init_check_tree_for_query_overlap(\n        **ball_tree_kwargs, return_containers={'overlap_counts': asu_clash_counts}, setup_args=(bb_cb_coords2,))\n    # Extract the data\n    asu_clash_counts = overlap_return['overlap_counts']\n    # Find those indices where the asu_clash_counts is not zero (inverse of nonzero by using the array == 0)\n    asu_is_viable_indices = np.flatnonzero(asu_clash_counts == 0)\n    number_non_clashing_transforms = len(asu_is_viable_indices)\n    # Update the passing_transforms\n    # passing_transforms contains all the transformations that are still passing\n    # index the previously passing indices (sufficiently_dense_indices) by new pasing indices (asu_is_viable_indices)\n    # and set each of these indices to 1 (True)\n    # passing_transforms[sufficiently_dense_indices[asu_is_viable_indices]] = 1\n    logger.info(f\"Clash testing for identified poses found {number_non_clashing_transforms} viable ASU's out of \"\n                f'{number_of_dense_transforms} (took {time.time() - check_clash_coords_start:8f}s)')\n\n    if not number_non_clashing_transforms:\n        logger.warning(f'No viable asymmetric units. Terminating {building_blocks} docking')\n        return []\n    # ------------------ TERMINATE DOCKING ------------------------\n    # Clean memory\n    del asu_clash_counts, ball_tree_kwargs, overlap_return\n    # Remove non-viable transforms by indexing asu_is_viable_indices\n    remove_non_viable_indices_inverse(asu_is_viable_indices)\n\n    # Query PDB1 CB Tree for all PDB2 CB Atoms within \"cb_distance\" in A of a PDB1 CB Atom\n    # alternative route to measure clashes of each transform. Move copies of component2 to interact with model1 ORIGINAL\n    int_cb_and_frags_start = time.time()\n    # Transform the CB coords of component 2 to each identified transformation\n    # Transforming only surface frags has large speed benefits from not having to transform all ghosts\n    inverse_transformed_model2_tiled_cb_coords = \\\n        transform_coordinate_sets(\n            transform_coordinate_sets(np.tile(model2.cb_coords, (number_non_clashing_transforms, 1, 1)),\n                                      rotation=_full_rotation2,\n                                      translation=None if full_int_tx2 is None else _full_int_tx2[:, None, :],\n                                      rotation2=set_mat2,\n                                      translation2=None if sym_entry.unit_cell is None\n                                      else full_ext_tx_sum[:, None, :]),\n            rotation=inv_setting1,\n            translation=None if full_int_tx1 is None else full_int_tx_inv1[:, None, :],\n            rotation2=full_inv_rotation1)\n\n    # Transform the surface guide coords of component 2 to each identified transformation\n    # Makes a shape (len(full_rotations), len(surf_guide_coords), 3, 3)\n    inverse_transformed_surf_frags2_guide_coords = \\\n        transform_coordinate_sets(\n            transform_coordinate_sets(surf_guide_coords2[None, :, :, :],\n                                      rotation=_full_rotation2[:, None, :, :],\n                                      translation=None if full_int_tx2 is None else _full_int_tx2[:, None, None, :],\n                                      rotation2=set_mat2[None, None, :, :],\n                                      translation2=None if sym_entry.unit_cell is None\n                                      else full_ext_tx_sum[:, None, None, :]),\n            rotation=inv_setting1[None, None, :, :],\n            translation=None if full_int_tx1 is None else full_int_tx_inv1[:, None, None, :],\n            rotation2=full_inv_rotation1[:, None, :, :])\n\n    logger.info('Transformation of viable component 2 CB atoms and surface fragments '\n                f'(took {time.time() - int_cb_and_frags_start:8f}s)')\n\n    del full_inv_rotation1, _full_rotation2, full_int_tx_inv1, _full_int_tx2, full_ext_tx_sum\n    del surf_guide_coords2\n    # Use below instead of this until can Todo 3 vectorize asu_interface_residue_processing\n    # asu_interface_residues = \\\n    #     np.array([component1_backbone_cb_tree.query_radius(inverse_transformed_model2_tiled_cb_coords[idx],\n    #                                                       cb_distance)\n    #               for idx in range(len(inverse_transformed_model2_tiled_cb_coords))])\n\n    # Full Interface Fragment Match\n    # Gather the data for efficient querying of model1 and model2 interactions\n    model1_cb_balltree = BallTree(model1.cb_coords)\n    model1_cb_indices = model1.cb_indices\n    model1_coords_indexed_residues = model1.coords_indexed_residues\n    model2_cb_indices = model2.cb_indices\n    model2_coords_indexed_residues = model2.coords_indexed_residues\n    zero_counts = []\n\n    # Save all the indices were matching fragments are identified\n    interface_is_viable = []\n    # all_passing_ghost_indices = []\n    # all_passing_surf_indices = []\n    # all_passing_z_scores = []\n    # Get residue number for all model1, model2 CB Pairs that interact within cb_distance\n    for idx in tqdm(range(number_non_clashing_transforms), bar_format=TQDM_BAR_FORMAT):\n        # query/contact pairs/isin  - 0.028367  &lt;- I predict query is about 0.015\n        # indexing guide_coords     - 0.000389\n        # total get_int_frags_time  - 0.028756 s\n\n        # indexing guide_coords     - 0.000389\n        # Euler Lookup              - 0.008161 s for 71435 fragment pairs\n        # Overlap Score Calculation - 0.000365 s for 2949 fragment pairs\n        # Total Match time          - 0.008915 s\n\n        # query                     - 0.000895 s &lt;- 100 fold shorter than predict\n        # contact pairs             - 0.019595\n        # isin indexing             - 0.008992 s\n        # indexing guide_coords     - 0.000438\n        # get_int_frags_time        - 0.029920 s\n\n        # indexing guide_coords     - 0.000438\n        # Euler Lookup              - 0.005603 s for 35400 fragment pairs\n        # Overlap Score Calculation - 0.000209 s for 887 fragment pairs\n        # Total Match time          - 0.006250 s\n\n        # int_frags_time_start = time.time()\n        model2_query = model1_cb_balltree.query_radius(inverse_transformed_model2_tiled_cb_coords[idx], cb_distance)\n        # model1_cb_balltree_time = time.time() - int_frags_time_start\n\n        contacting_residue_idx_pairs = [(model1_coords_indexed_residues[model1_cb_indices[model1_idx]].index,\n                                         model2_coords_indexed_residues[model2_cb_indices[model2_idx]].index)\n                                        for model2_idx, model1_contacts in enumerate(model2_query.tolist())\n                                        for model1_idx in model1_contacts.tolist()]\n        try:\n            interface_residue_indices1, interface_residue_indices2 = \\\n                map(list, map(set, zip(*contacting_residue_idx_pairs)))\n        except ValueError:  # Interface contains no residues, so not enough values to unpack\n            logger.warning('Interface contains no residues')\n            continue\n\n        # Find the indices where the fragment residue numbers are found the interface residue numbers\n        # is_in_index_start = time.time()\n        # Since *_residue_numbers1/2 are the same index as the complete fragment arrays, these interface indices are the\n        # same index as the complete guide coords and rmsds as well\n        # Both residue numbers are one-indexed vv\n        # Todo 3 make ghost_residue_indices1 unique -&gt; unique_ghost_residue_numbers1\n        #  index selected numbers against per_residue_ghost_indices 2d (number surface frag residues,\n        ghost_indices_in_interface1 = \\\n            np.flatnonzero(np.isin(ghost_residue_indices1, interface_residue_indices1))\n        surf_indices_in_interface2 = \\\n            np.flatnonzero(np.isin(surf_residue_indices2, interface_residue_indices2, assume_unique=True))\n\n        # is_in_index_time = time.time() - is_in_index_start\n        all_fragment_match_time_start = time.time()\n\n        # unique_interface_frag_count_model1, unique_interface_frag_count_model2 = \\\n        #     len(ghost_indices_in_interface1), len(surf_indices_in_interface2)\n        # get_int_frags_time = time.time() - int_frags_time_start\n        # logger.debug(f'\\tNewly formed interface contains {unique_interface_frag_count_model1} unique Fragments on '\n        #              f'Oligomer 1 from {len(interface_residue_numbers1)} Residues and '\n        #              f'{unique_interface_frag_count_model2} on Oligomer 2 from {len(interface_residue_numbers2)} '\n        #              f'Residues\\n\\t(took {get_int_frags_time:8f}s to get interface fragments, including '\n        #              f'{model1_cb_balltree_time:8f}s to query distances, '\n        #              f'{is_in_index_time:8f}s to index residue numbers)')\n\n        number_int_surf = len(surf_indices_in_interface2)\n        number_int_ghost = len(ghost_indices_in_interface1)\n        # maximum_number_of_pairs = number_int_ghost*number_int_surf\n        # if maximum_number_of_pairs &lt; euler_lookup_size_threshold:\n        # Tod0 at one point, there might have been a memory leak by Pose objects sharing memory with persistent objects\n        #  that prevent garbage collection and stay attached to the run\n        # Skipping EulerLookup as it has issues with precision\n        index_ij_pairs_start_time = time.time()\n        ghost_indices_repeated = np.repeat(ghost_indices_in_interface1, number_int_surf)\n        surf_indices_tiled = np.tile(surf_indices_in_interface2, number_int_ghost)\n        ij_type_match = ij_type_match_lookup_table[ghost_indices_repeated, surf_indices_tiled]\n        # DEBUG: If ij_type_match needs to be removed for testing\n        # ij_type_match = np.array([True for _ in range(len(ij_type_match))])\n        # Surface selecting\n        # [0, 1, 3, 5, ...] with fancy indexing [0, 1, 5, 10, 12, 13, 34, ...]\n        # possible_fragments_pairs = len(ghost_indices_repeated)\n        passing_ghost_indices = ghost_indices_repeated[ij_type_match]\n        passing_surf_indices = surf_indices_tiled[ij_type_match]\n        # else:  # Narrow candidates by EulerLookup\n        #     Get (Oligomer1 Interface Ghost Fragment, Oligomer2 Interface Surface Fragment) guide coordinate pairs\n        #     in the same Euler rotational space bucket\n        #     DON'T think this is crucial! ###\n        #     int_euler_matching_ghost_indices1, int_euler_matching_surf_indices2 = \\\n        #         euler_lookup.check_lookup_table(int_trans_ghost_guide_coords, int_trans_surf_guide_coords2)\n        #     logger.debug('Euler lookup')\n        #     logger.warning(f'The interface size is too large ({maximum_number_of_pairs} maximum pairs). '\n        #                    f'Trimming possible fragments by EulerLookup')\n        #     eul_lookup_start_time = time.time()\n        #     int_ghost_guide_coords1 = ghost_guide_coords1[ghost_indices_in_interface1]\n        #     int_trans_surf_guide_coords2 = inverse_transformed_surf_frags2_guide_coords[idx, surf_indices_in_interface2]\n        #     # Todo Debug skipping EulerLookup to see if issues with precision\n        #     int_euler_matching_ghost_indices1, int_euler_matching_surf_indices2 = \\\n        #         euler_lookup.check_lookup_table(int_ghost_guide_coords1, int_trans_surf_guide_coords2)\n        #     # logger.debug(f'int_euler_matching_ghost_indices1: {int_euler_matching_ghost_indices1[:5]}')\n        #     # logger.debug(f'int_euler_matching_surf_indices2: {int_euler_matching_surf_indices2[:5]}')\n        #     eul_lookup_time = time.time() - eul_lookup_start_time\n        #\n        #     # Find the ij_type_match which is the same length as the int_euler_matching indices\n        #     # this has data type bool so indexing selects al original\n        #     index_ij_pairs_start_time = time.time()\n        #     ij_type_match = \\\n        #         ij_type_match_lookup_table[\n        #             ghost_indices_in_interface1[int_euler_matching_ghost_indices1],\n        #             surf_indices_in_interface2[int_euler_matching_surf_indices2]]\n        #     possible_fragments_pairs = len(int_euler_matching_ghost_indices1)\n        #\n        #     # Get only euler matching fragment indices that pass ij filter. Then index their associated coords\n        #     passing_ghost_indices = int_euler_matching_ghost_indices1[ij_type_match]\n        #     # passing_ghost_coords = int_trans_ghost_guide_coords[passing_ghost_indices]\n        #     # passing_ghost_coords = int_ghost_guide_coords1[passing_ghost_indices]\n        #     passing_surf_indices = int_euler_matching_surf_indices2[ij_type_match]\n        #     # passing_surf_coords = int_trans_surf_guide_coords2[passing_surf_indices]\n        #     DON'T think this is crucial! ###\n\n        # Calculate z_value for the selected (Ghost Fragment, Interface Fragment) guide coordinate pairs\n        # Calculate match score for the selected (Ghost Fragment, Interface Fragment) guide coordinate pairs\n        overlap_score_time_start = time.time()\n\n        all_fragment_z_score = rmsd_z_score(ghost_guide_coords1[passing_ghost_indices],\n                                            inverse_transformed_surf_frags2_guide_coords[idx, passing_surf_indices],\n                                            ghost_rmsds1[passing_ghost_indices])\n        # all_fragment_match = calculate_match(ghost_guide_coords1[passing_ghost_indices],\n        #                                      inverse_transformed_surf_frags2_guide_coords[idx, passing_surf_indices],\n        #                                      ghost_rmsds1[passing_ghost_indices])\n        # number_of_checks = len(passing_ghost_indices)\n        logger.debug(\n            # f'\\tEuler Lookup found {len(int_euler_matching_ghost_indices1)} passing overlaps '\n            #      f'(took {eul_lookup_time:8f}s) for '\n            #      f'{unique_interface_frag_count_model1 * unique_interface_frag_count_model2} fragment pairs and '\n            f'\\tZ-score calculation took {time.time() - overlap_score_time_start:8f}s for '\n            f'{len(passing_ghost_indices)} successful ij type matches (indexing time '\n            f'{overlap_score_time_start - index_ij_pairs_start_time:8f}s) from '\n            f'{len(ghost_indices_repeated)} possible fragment pairs')\n\n        # Check if the pose has enough high quality fragment matches\n        # high_qual_match_indices = np.flatnonzero(all_fragment_match &gt;= high_quality_match_value)\n        # high_qual_match_indices = np.flatnonzero(all_fragment_z_score &lt;= high_quality_z_value)\n        # Using ar.size() - np.count_nonzero(ar) to count\n        # high_qual_match_count = number_of_checks - np.count_nonzero(all_fragment_z_score &lt;= high_quality_z_value)\n        high_qual_match_count = np.count_nonzero(all_fragment_z_score &lt;= high_quality_z_value)\n        all_fragment_match_time = time.time() - all_fragment_match_time_start\n        if high_qual_match_count &lt; min_matched:\n            logger.debug(f'\\t{high_qual_match_count} &lt; {min_matched}, the minimal high quality fragment matches '\n                         f'(took {all_fragment_match_time:8f}s)')\n            # Debug. Why are there no matches... cb_distance?\n            # I think it is the accuracy of binned euler_angle lookup\n            if high_qual_match_count == 0:\n                zero_counts.append(1)\n            continue\n        else:\n            # Find the passing overlaps to limit the output to only those passing the low_quality_match_value\n            # passing_overlaps_indices = np.flatnonzero(all_fragment_match &gt;= low_quality_match_value)\n            # passing_overlaps_indices = np.flatnonzero(all_fragment_z_score &lt;= low_quality_z_value)\n            # number_passing_overlaps = len(passing_overlaps_indices)\n            # number_passing_overlaps = number_of_checks - np.count_nonzero(all_fragment_z_score &lt;= low_quality_z_value)\n            number_passing_overlaps = np.count_nonzero(all_fragment_z_score &lt;= low_quality_z_value)\n            logger.debug(f'\\t{high_qual_match_count} high quality fragments out of {number_passing_overlaps} matches '\n                         f'found (took {all_fragment_match_time:8f}s)')\n            # Return the indices sorted by z_value in ascending order, truncated at the number of passing\n            # sorted_fragment_indices = np.argsort(all_fragment_z_score)[:number_passing_overlaps]\n            # sorted_match_scores = match_score_from_z_value(sorted_z_values)\n            # logger.debug('Overlapping Match Scores: %s' % sorted_match_scores)\n            # sorted_overlap_indices = passing_overlaps_indices[sorted_fragment_indices]\n            # interface_ghost_frags = \\\n            #     complete_ghost_frags1[interface_ghost_indices1][passing_ghost_indices[sorted_overlap_indices]]\n            # interface_surf_frags = \\\n            #     surf_frags2[surf_indices_in_interface2][passing_surf_indices[sorted_overlap_indices]]\n            # overlap_passing_ghosts = passing_ghost_indices[sorted_fragment_indices]\n            # all_passing_ghost_indices.append(passing_ghost_indices[sorted_fragment_indices])\n            # all_passing_surf_indices.append(passing_surf_indices[sorted_fragment_indices])\n            # all_passing_z_scores.append(all_fragment_z_score[sorted_fragment_indices])\n            interface_is_viable.append(idx)\n            # logger.debug(f'\\tInterface fragment search time took {time.time() - int_frags_time_start:8f}')\n            continue\n\n    logger.debug(f'Found {len(zero_counts)} zero counts')\n    number_viable_pose_interfaces = len(interface_is_viable)\n    if number_viable_pose_interfaces == 0:\n        logger.warning(f'No interfaces have enough fragment matches. Terminating {building_blocks} docking')\n        return []\n    # ------------------ TERMINATE DOCKING ------------------------\n    # Clean memory\n    del inverse_transformed_model2_tiled_cb_coords, inverse_transformed_surf_frags2_guide_coords\n    del ghost_residue_indices1, surf_residue_indices2\n    del ghost_guide_coords1, ghost_rmsds1\n    del model1, model1_cb_indices, model1_coords_indexed_residues\n    del model2, model2_cb_indices, model2_coords_indexed_residues\n    del model1_cb_balltree, model2_query, contacting_residue_idx_pairs\n    del interface_residue_indices1, interface_residue_indices2\n    del ghost_indices_in_interface1, surf_indices_in_interface2\n    del ij_type_match, ghost_indices_repeated, surf_indices_tiled\n    del all_fragment_z_score, zero_counts\n    # del all_fragment_z_score, zero_counts, passing_overlaps_indices\n    logger.info(f'Found {number_viable_pose_interfaces} poses with viable interfaces')\n    # # Tod0 remove below. It is never accessed by pose. Perhaps downstream it could be but Entity instances are\n    # #  already solved now with sql.ProteinMetadata\n    # # Generate the Pose for output handling\n    # entity_info = {entity_name: data for model in models\n    #                for entity_name, data in model.entity_info.items()}\n    # chain_gen = chain_id_generator()\n    # for entity_name, data in entity_info.items():\n    #     data['chains'] = [next(chain_gen)]\n    if sym_entry.unit_cell:\n        # Calculate the vectorized uc_dimensions\n        full_uc_dimensions = sym_entry.get_uc_dimensions(full_optimal_ext_dof_shifts)\n        uc_dimensions = full_uc_dimensions[0]\n    else:\n        uc_dimensions = None\n\n    pose = Pose.from_entities([entity for model in input_models for entity in model.entities],\n                              log=True if job.debug else None,\n                              sym_entry=sym_entry, name='asu', fragment_db=job.fragment_db,  # entity_info=entity_info,\n                              surrounding_uc=True, rename_chains=True, uc_dimensions=uc_dimensions)\n\n    # Ensure .metadata attribute is passed to each entity in the full assembly\n    # This is crucial for sql usage\n    entity_idx = count()\n    for input_model in input_models:\n        for entity in input_model.entities:\n            pose.entities[next(entity_idx)].metadata = entity.metadata\n    # Set up coordinates to transform the Pose with each Entity individually\n    entity_start_coords = [entity.coords for input_model in input_models for entity in input_model.entities]\n    entity_idx = count()\n    transform_indices = {next(entity_idx): transform_idx\n                         for transform_idx, input_model in enumerate(input_models)\n                         for _ in input_model.entities}\n    # Calculate thermophilicity\n    is_thermophilic = [entity.thermophilicity for idx, entity in enumerate(pose.entities, idx)]\n    pose_thermophilicity = sum(is_thermophilic) / pose.number_of_entities\n\n    # Define functions for updating the single Pose instance coordinates\n    # def create_specific_transformation(idx: int) -&gt; tuple[dict[str, np.ndarray], ...]:\n    #     \"\"\"Take the current transformation index and create a mapping of the transformation operations\n    #\n    #     Args:\n    #         idx: The index of the transformation to select\n    #     Returns:\n    #         A tuple containing the transformation operations for each model\n    #     \"\"\"\n    #     if sym_entry.is_internal_tx1:\n    #         internal_tx_param1 = full_int_tx1[idx]\n    #     else:\n    #         internal_tx_param1 = None\n    #\n    #     if sym_entry.is_internal_tx2:\n    #         internal_tx_param2 = full_int_tx2[idx]\n    #     else:\n    #         internal_tx_param2 = None\n    #\n    #     if sym_entry.unit_cell:\n    #         external_tx1 = full_ext_tx1[idx]\n    #         external_tx2 = full_ext_tx2[idx]\n    #     else:\n    #         external_tx1 = external_tx2 = None\n    #\n    #     specific_transformation1 = dict(rotation=full_rotation1[idx], translation=internal_tx_param1,\n    #                                     rotation2=set_mat1, translation2=external_tx1)\n    #     specific_transformation2 = dict(rotation=full_rotation2[idx], translation=internal_tx_param2,\n    #                                     rotation2=set_mat2, translation2=external_tx2)\n    #     return specific_transformation1, specific_transformation2\n\n    # Todo 3 if using individual Poses\n    #  def clone_pose(idx: int) -&gt; Pose:\n    #      # Create a copy of the base Pose\n    #      new_pose = copy.copy(pose)\n    #      if sym_entry.unit_cell:\n    #          # Set the next unit cell dimensions\n    #          new_pose.uc_dimensions = full_uc_dimensions[idx]\n    #      # Update the Pose coords\n    #      new_pose.coords = np.concatenate(new_coords)\n    #      return new_pose\n\n    def update_pose_coords(idx: int):\n        \"\"\"Take the current transformation index and update the reference coordinates with the provided transforms\n\n        Args:\n            idx: The index of the transformation to select\n        \"\"\"\n        copy_model_start = time.time()\n        if sym_entry.is_internal_tx1:\n            internal_tx_param1 = full_int_tx1[idx]\n        else:\n            internal_tx_param1 = None\n\n        if sym_entry.is_internal_tx2:\n            internal_tx_param2 = full_int_tx2[idx]\n        else:\n            internal_tx_param2 = None\n\n        if sym_entry.unit_cell:\n            external_tx1 = full_ext_tx1[idx]\n            external_tx2 = full_ext_tx2[idx]\n            # uc_dimensions = full_uc_dimensions[idx]\n            # Set the next unit cell dimensions\n            pose.uc_dimensions = full_uc_dimensions[idx]\n        else:\n            external_tx1 = external_tx2 = None\n\n        specific_transformation1 = dict(rotation=full_rotation1[idx], translation=internal_tx_param1,\n                                        rotation2=set_mat1, translation2=external_tx1)\n        specific_transformation2 = dict(rotation=full_rotation2[idx], translation=internal_tx_param2,\n                                        rotation2=set_mat2, translation2=external_tx2)\n        specific_transformations = [specific_transformation1, specific_transformation2]\n\n        # Transform each starting coords to the candidate pose coords then update the Pose coords\n        new_coords = []\n        for entity_idx, entity in enumerate(pose.entities):\n            new_coords.append(transform_coordinate_sets(entity_start_coords[entity_idx],\n                                                        **specific_transformations[transform_indices[entity_idx]]))\n        pose.coords = np.concatenate(new_coords)\n\n        logger.debug(f'\\tCopy and Transform Oligomer1 and Oligomer2 (took {time.time() - copy_model_start:8f}s)')\n\n    def find_viable_symmetric_indices(viable_pose_indices: list[int]) -&gt; np.ndarray:\n        \"\"\"Using the nonlocal Pose and transformation indices, check each transformation index for symmetric viability\n\n        Args:\n            viable_pose_indices: The indices from the transform array to test for clashes\n        Returns:\n            An array with the transformation indices that passed clash testing\n        \"\"\"\n        # Assume the pose will fail the clash test (0), otherwise, (1) for passing\n        _passing_symmetric_clashes = [0 for _ in range(len(viable_pose_indices))]\n        for result_idx, transform_idx in enumerate(viable_pose_indices):\n            # Find the pose\n            update_pose_coords(transform_idx)\n            if not pose.symmetric_assembly_is_clash(measure=job.design.clash_criteria,\n                                                    distance=job.design.clash_distance):\n                _passing_symmetric_clashes[result_idx] = 1\n\n        return np.flatnonzero(_passing_symmetric_clashes)\n\n    # Make the indices into an array\n    interface_is_viable = np.array(interface_is_viable, dtype=int)\n\n    # Update the passing_transforms\n    # passing_transforms contains all the transformations that are still passing\n    # index the previously passing indices (sufficiently_dense_indices) and (asu_is_viable_indices)\n    # by new passing indices (interface_is_viable)\n    # and set each of these indices to 1 (True)\n    # passing_transforms[sufficiently_dense_indices[asu_is_viable_indices[interface_is_viable]]] = 1\n    # # Remove non-viable transforms from the original transformation parameters by indexing interface_is_viable\n    # passing_transforms_indices = np.flatnonzero(passing_transforms)\n    # # filter_transforms_by_indices(passing_transforms_indices)\n    passing_transforms_indices = sufficiently_dense_indices[asu_is_viable_indices[interface_is_viable]]\n\n    if job.design.ignore_symmetric_clashes:\n        logger.warning(f'Not checking for symmetric clashes per requested flag '\n                       f'{flags.format_args(flags.ignore_symmetric_clashes_args)}')\n        passing_symmetric_clash_indices_perturb = slice(None)\n    else:\n        logger.info('Checking solutions for symmetric clashes')\n\n        # passing_symmetric_clash_indices = find_viable_symmetric_indices(number_viable_pose_interfaces)\n        passing_symmetric_clash_indices = find_viable_symmetric_indices(passing_transforms_indices.tolist())\n        number_passing_symmetric_clashes = len(passing_symmetric_clash_indices)\n        logger.info(f'After symmetric clash testing, found {number_passing_symmetric_clashes} viable poses')\n\n        if number_passing_symmetric_clashes == 0:  # There were no successful transforms\n            logger.warning(f'No viable poses without symmetric clashes. Terminating {building_blocks} docking')\n            return []\n        # ------------------ TERMINATE DOCKING ------------------------\n        # Update the passing_transforms\n        # passing_transforms contains all the transformations that are still passing\n        # index the previously passing indices (sufficiently_dense_indices)\n        # and (asu_is_viable_indices) and (interface_is_viable)\n        # by new passing indices (passing_symmetric_clash_indices)\n        # and set each of these indices to 1 (True)\n        # passing_transforms_indices = \\\n        #     sufficiently_dense_indices[asu_is_viable_indices[interface_is_viable[passing_symmetric_clash_indices]]]\n        passing_transforms_indices = passing_transforms_indices[passing_symmetric_clash_indices]\n        # Todo could this be used?\n        # passing_transforms[passing_transforms_indices] = 1\n\n    # Remove non-viable transforms from the original transformations due to clashing\n    filter_transforms_by_indices(passing_transforms_indices)\n    number_of_transforms = len(passing_transforms_indices)\n    # Clean memory\n    del passing_transforms_indices, sufficiently_dense_indices, passing_symmetric_clash_indices\n    del asu_is_viable_indices, interface_is_viable\n\n    # # all_passing_ghost_indices = [all_passing_ghost_indices[idx] for idx in passing_symmetric_clash_indices.tolist()]\n    # # all_passing_surf_indices = [all_passing_surf_indices[idx] for idx in passing_symmetric_clash_indices.tolist()]\n    # # all_passing_z_scores = [all_passing_z_scores[idx] for idx in passing_symmetric_clash_indices.tolist()]\n\n    if sym_entry.unit_cell:\n        # Calculate the vectorized uc_dimensions\n        full_uc_dimensions = sym_entry.get_uc_dimensions(full_optimal_ext_dof_shifts)\n\n    # Calculate metrics on input Pose before any manipulation\n    pose_length = pose.number_of_residues\n    residue_indices = list(range(pose_length))\n\n    def add_fragments_to_pose():\n        \"\"\"Add observed fragments to the Pose or generate new observations given the Pose state\n\n        If no arguments are passed, the fragment observations will be generated new\n        \"\"\"\n        # First, clear any pose information and force identification of the interface\n        pose._interface_residue_indices_by_interface = {}\n        pose.find_and_split_interface(by_distance=True, distance=cb_distance)\n\n        # # Next, set the interface fragment info for gathering of interface metrics\n        # if overlap_ghosts is None or overlap_surf is None or sorted_z_scores is None:\n        # Remove old fragments\n        pose._fragment_info_by_entity_pair = {}\n        # Query fragments\n        pose.generate_interface_fragments()\n\n    # Load evolutionary profiles of interest for optimization/analysis\n    if job.use_evolution:\n        measure_evolution, measure_alignment = load_evolutionary_profile(job.api_db, pose)\n\n        evolutionary_profile_array = pssm_as_array(pose.evolutionary_profile)\n        with catch_warnings():\n            simplefilter('ignore', category=RuntimeWarning)\n            # Divide by zero encountered in log\n            corrected_log_evolutionary_profile = np.nan_to_num(\n                np.log(evolutionary_profile_array), copy=False, nan=np.nan,\n                neginf=metrics.zero_probability_evol_value)\n        batch_evolutionary_profile = \\\n            torch.from_numpy(np.tile(evolutionary_profile_array, (batch_length, 1, 1)))\n    else:  # Make an empty collapse_profile\n        # pose.add_profile(null=True)  # &lt;- Actually, don't use, keep pose.evolutionary_profile == {}\n        measure_evolution = measure_alignment = False\n        collapse_profile = np.empty(0)\n        evolutionary_profile_array = corrected_log_evolutionary_profile = None\n\n    # Calculate hydrophobic collapse for each dock using the collapse_profile if it was calculated\n    if measure_evolution:\n        hydrophobicity = 'expanded'\n    else:\n        hydrophobicity = 'standard'\n    contact_order_per_res_z, reference_collapse, collapse_profile = \\\n        pose.get_folding_metrics(hydrophobicity=hydrophobicity)\n    if measure_evolution:  # collapse_profile.size:  # Not equal to zero, use the profile instead\n        reference_collapse = collapse_profile\n\n    # Todo\n    #  enable precise metric acquisition\n    # def collect_dock_metrics(score_functions: dict[str, Callable]) -&gt; dict[str, np.ndarray]:\n    #     \"\"\"Perform analysis on the docked Pose instances\"\"\"\n    #     pose_functions = {}\n    #     residue_functions = {}\n    #     for score, function in score_functions:\n    #         if getattr(sql.PoseMetrics, score, None):\n    #             pose_functions[score] = function\n    #         else:\n    #             residue_functions[score] = function\n    #\n    #     pose_metrics = []\n    #     per_residue_metrics = []\n    #     for idx in range(number_of_transforms):\n    #         # Add the next set of coordinates\n    #         update_pose_coords(idx)\n    #\n    #         # if number_perturbations_applied &gt; 1:\n    #         add_fragments_to_pose()\n    #\n    #         pose_metrics.append({score: function(pose) for score, function in pose_functions})\n    #         per_residue_metrics.append({score: function(pose) for score, function in residue_functions})\n    #\n    #         # Reset the fragment_map and fragment_profile for each Entity before calculate_fragment_profile\n    #         for entity in pose.entities:\n    #             entity.fragment_map = None\n    #             # entity.alpha.clear()\n    #\n    #         # Load fragment_profile (and fragment_metrics) into the analysis\n    #         pose.calculate_fragment_profile()\n    #         # This could be an empty array if no fragments were found\n    #         fragment_profile_array = pose.fragment_profile.as_array()\n    #         with catch_warnings():\n    #             simplefilter('ignore', category=RuntimeWarning)\n    #             # np.log causes -inf at 0, thus we correct these to a 'large' number\n    #             corrected_frag_array = np.nan_to_num(np.log(fragment_profile_array), copy=False,\n    #                                                  nan=np.nan, neginf=metrics.zero_probability_frag_value)\n    #         per_residue_fragment_profile_loss = \\\n    #             resources.ml.sequence_nllloss(torch_numeric_sequence, torch.from_numpy(corrected_frag_array))\n    #\n    #         # Remove saved pose attributes from the prior iteration calculations\n    #         pose.ss_sequence_indices.clear(), pose.ss_type_sequence.clear()\n    #         pose.fragment_metrics.clear()\n    #         for attribute in ['_design_residues', '_interface_residues']:  # _assembly_minimally_contacting\n    #             try:\n    #                 delattr(pose, attribute)\n    #             except AttributeError:\n    #                 pass\n    #\n    #         # Save pose metrics\n    #         # pose_metrics[pose_id] = {\n    #         pose_metrics.append({\n    #             **pose.calculate_metrics(),  # Also calculates entity.metrics\n    #             'dock_collapse_violation': collapse_violation[idx],\n    #         })\n\n    # def format_docking_metrics(metrics_: dict[str, np.ndarray]) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    #     \"\"\"From the current pool of docked poses and their collected metrics, format the metrics for selection/output\n    #\n    #     Args:\n    #         metrics_: A dictionary of metric name to metric value where the values are per-residue measurements the\n    #             length of the active transformation pool\n    #     Returns:\n    #         A tuple of DataFrames representing the per-pose and the per-residue metrics. Each has indices from 0-N\n    #     \"\"\"\n    idx_slice = pd.IndexSlice\n\n    def collect_dock_metrics(proteinmpnn_score: bool = False) -&gt; tuple[pd.DataFrame, pd.DataFrame]:  # -&gt; dict[str,\n        # np.ndarray]:\n        \"\"\"Perform analysis on the docked Pose instances\n\n        Args:\n            proteinmpnn_score: Whether proteinmpnn scores should be collected\n        Returns:\n            A tuple of DataFrames representing the per-pose and the per-residue metrics. Each has indices from 0-N\n        \"\"\"\n        logger.info(f'Collecting metrics for {number_of_transforms} active Poses')\n        # Remove old DataFrames to save memory. These are going to be calculated fresh\n        nonlocal poses_df, residues_df\n        poses_df = residues_df = pd.DataFrame()\n\n        # nan_blank_data = list(repeat(np.nan, pose_length))\n        # unbound_errat = []\n        # for idx, entity in enumerate(pose.entities):\n        #     _, oligomeric_errat = entity.assembly.errat(out_path=os.path.devnull)\n        #     unbound_errat.append(oligomeric_errat[:entity.number_of_residues])\n\n        torch_numeric_sequence = torch.from_numpy(pose.sequence_numeric)\n        if evolutionary_profile_array is None:\n            profile_loss = {}\n        else:\n            torch_log_evolutionary_profile = torch.from_numpy(corrected_log_evolutionary_profile)\n            per_residue_evolutionary_profile_loss = \\\n                resources.ml.sequence_nllloss(torch_numeric_sequence, torch_log_evolutionary_profile)\n            profile_loss = {\n                # 'sequence_loss_design': per_residue_design_profile_loss,\n                'sequence_loss_evolution': per_residue_evolutionary_profile_loss,\n            }\n\n        sequence_params = {\n            **pose.per_residue_contact_order(),\n            # 'errat_deviation': np.concatenate(unbound_errat),\n            'type': tuple(pose.sequence),\n            **profile_loss\n            # Todo 1 each pose...\n            #  'sequence_loss_fragment': per_residue_fragment_profile_loss\n        }\n\n        if proteinmpnn_score:\n            # Initialize and retrieve the ProteinMPNN model for dock analysis\n            mpnn_model = ml.proteinmpnn_factory(ca_only=job.design.ca_only, model_name=job.design.proteinmpnn_model)\n            # Set up model sampling type based on symmetry\n            if pose.is_symmetric():\n                number_of_residues = pose.number_of_symmetric_residues\n            else:\n                number_of_residues = pose_length\n\n            # Modulate memory requirements\n            # calculation_size = len(full_rotation1)  # This is the number of transformations\n            # The batch_length indicates how many models could fit in the allocated memory\n            batch_length = ml.calculate_proteinmpnn_batch_length(mpnn_model, number_of_residues)\n            logger.info(f'Found ProteinMPNN batch_length={batch_length}')\n\n            # Set up parameters to run ProteinMPNN design\n            if job.design.ca_only:\n                coords_type = 'ca_coords'\n                num_model_residues = 1\n            else:\n                coords_type = 'backbone_coords'\n                num_model_residues = 4\n\n            # Set up Pose parameters\n            parameters = pose.get_proteinmpnn_params(ca_only=job.design.ca_only,\n                                                     interface=measure_interface_during_dock)\n            # Todo 2 reinstate if conditional_log_probs\n            # # Todo\n            # #  Must calculate randn individually if using some feature to describe order\n            # parameters['randn'] = pose.generate_proteinmpnn_decode_order()  # to_device=device)\n\n            # Set up interface unbound coordinates\n            mpnn_null_idx = resources.ml.MPNN_NULL_IDX\n            if measure_interface_during_dock:\n                X_unbound = pose.get_proteinmpnn_unbound_coords(ca_only=job.design.ca_only)\n                unbound_batch = \\\n                    ml.setup_pose_batch_for_proteinmpnn(1, mpnn_model.device, X_unbound=X_unbound,\n                                                        mask=parameters['mask'], residue_idx=parameters['residue_idx'],\n                                                        chain_encoding=parameters['chain_encoding'])\n                with torch.no_grad():\n                    unconditional_log_probs_unbound = \\\n                        mpnn_model.unconditional_probs(unbound_batch['X_unbound'], unbound_batch['mask'],\n                                                       unbound_batch['residue_idx'],\n                                                       unbound_batch['chain_encoding']).cpu()\n                    asu_unconditional_softmax_seq_unbound = \\\n                        np.exp(unconditional_log_probs_unbound[:, :pose_length, :mpnn_null_idx])\n                # Remove any unnecessary reserved memory\n                del unbound_batch\n            else:\n                raise NotImplementedError(\n                    f\"{fragment_dock.__name__} isn't written to only measure the complex state\")\n                asu_unconditional_softmax_seq_unbound = None\n            # Disregard X, chain_M_pos, and bias_by_res parameters return and use the pose specific data from below\n            # parameters.pop('X')  # overwritten by X_unbound\n            parameters.pop('chain_M_pos')\n            parameters.pop('bias_by_res')\n            # tied_pos = parameters.pop('tied_pos')\n            # # Todo 2 if modifying the amount of weight given to each of the copies\n            # tied_beta = parameters.pop('tied_beta')\n            # # Set the design temperature\n            # temperature = job.design.temperatures[0]\n\n            batch_parameters = ml.setup_pose_batch_for_proteinmpnn(batch_length, mpnn_model.device, **parameters)\n            mask = batch_parameters['mask']\n            residue_idx = batch_parameters['residue_idx']\n            chain_encoding = batch_parameters['chain_encoding']\n\n            def proteinmpnn_score_batched_coords(batched_coords: list[np.ndarray]) -&gt; list[dict[str, np.ndarray]]:\n                \"\"\"\"\"\"\n                actual_batch_length = len(batched_coords)\n                if actual_batch_length != batch_length:\n                    # if not actual_batch_length:\n                    #     return []\n                    _mask = mask[:actual_batch_length]\n                    _residue_idx = residue_idx[:actual_batch_length]\n                    _chain_encoding = chain_encoding[:actual_batch_length]\n                else:\n                    _mask = mask[:actual_batch_length]\n                    _residue_idx = residue_idx[:actual_batch_length]\n                    _chain_encoding = chain_encoding[:actual_batch_length]\n\n                # Format the bb coords for ProteinMPNN\n                if pose.is_symmetric():\n                    # Make each set of coordinates symmetric\n                    # Lattice cases have .uc_dimensions set in update_pose_coords()\n                    perturbed_bb_coords = np.concatenate(\n                        [pose.return_symmetric_coords(coords_) for coords_ in batched_coords])\n\n                    # Todo 2 reinstate if conditional_log_probs\n                    # # Symmetrize other arrays\n                    # number_of_symmetry_mates = pose.number_of_symmetry_mates\n                    # # (batch, number_of_sym_residues, ...)\n                    # residue_mask_cpu = np.tile(residue_mask_cpu, (1, number_of_symmetry_mates))\n                    # # bias_by_res = np.tile(bias_by_res, (1, number_of_symmetry_mates, 1))\n                else:\n                    # When batched_coords are individually transformed, axis=0 works\n                    perturbed_bb_coords = np.concatenate(batched_coords, axis=0)\n\n                # Reshape for ProteinMPNN\n                # Let -1 fill in the pose length dimension with the number of residues\n                # 4 is shape of backbone coords (N, Ca, C, O), 3 is x,y,z\n                # logger.debug(f'perturbed_bb_coords.shape: {perturbed_bb_coords.shape}')\n                X = perturbed_bb_coords.reshape((actual_batch_length, -1, num_model_residues, 3))\n                # logger.debug(f'X.shape: {X.shape}')\n\n                # Start a unit of work\n                #  Taking the KL divergence would indicate how divergent the interfaces are from the\n                #  surface. This should be simultaneously minimized (i.e. the lowest evolutionary divergence)\n                #  while the aa frequency distribution cross_entropy compared to the fragment profile is\n                #  minimized\n                # -------------------------------------------\n                with torch.no_grad():\n                    X = torch.from_numpy(X).to(dtype=torch.float32, device=mpnn_model.device)\n                    unconditional_log_probs = \\\n                        mpnn_model.unconditional_probs(X, _mask, _residue_idx, _chain_encoding).cpu()\n                    # Use the sequence as an unknown token then guess the probabilities given the remaining\n                    # information, i.e. the sequence and the backbone\n                    # Calculations with this are done using cpu memory and numpy\n                    # Todo 2 reinstate if conditional_log_probs\n                    # S_design_null[residue_mask.type(torch.bool)] = mpnn_null_idx\n                    # chain_residue_mask = chain_mask * residue_mask * mask\n                    # decoding_order = \\\n                    #     ml.create_decoding_order(randn, chain_residue_mask, tied_pos=tied_pos, to_device=device)\n                    # conditional_log_probs_null_seq = \\\n                    #     mpnn_model(X, S_design_null, mask, chain_residue_mask, residue_idx, chain_encoding,\n                    #                None,  # This argument is provided but with below args, is not used\n                    #                use_input_decoding_order=True, decoding_order=decoding_order).cpu()\n                    # asu_conditional_softmax_null_seq = \\\n                    #     np.exp(conditional_log_probs_null_seq[:, :pose_length, :mpnn_null_idx])\n\n                # Remove the gaps index from the softmax input with :mpnn_null_idx]\n                asu_unconditional_softmax_seq = \\\n                    np.exp(unconditional_log_probs[:, :pose_length, :mpnn_null_idx])\n                # asu_unconditional_softmax_seq\n                # tensor([[[0.0273, 0.0125, 0.0200,  ..., 0.0073, 0.0102, 0.0052],\n                #          ...,\n                #          [0.0091, 0.0078, 0.0101,  ..., 0.0038, 0.0029, 0.0059]],\n                #          ...\n                #         [[0.0273, 0.0125, 0.0200,  ..., 0.0073, 0.0102, 0.0052],\n                #          ...,\n                #          [0.0091, 0.0078, 0.0101,  ..., 0.0038, 0.0029, 0.0059]]])\n                per_residue_dock_cross_entropy = \\\n                    metrics.cross_entropy(asu_unconditional_softmax_seq,\n                                          asu_unconditional_softmax_seq_unbound[:actual_batch_length],\n                                          per_entry=True)\n\n                # Set up per_residue metrics\n                # All have the shape (this batch length, pose.number_of_residues)\n                # Set up the interface indices where interface residues are 1, others are 0\n                per_residue_design_indices = np.zeros((actual_batch_length, pose_length), dtype=np.int32)\n                for chunk_idx, interface_residues in enumerate(interface_mask):\n                    per_residue_design_indices[chunk_idx, interface_residues] = 1\n\n                # Set up various profiles for cross entropy against the softmax(complexed ProteinMPNN logits)\n                if pose.fragment_profile:\n                    # Process the fragment_profiles into an array for cross entropy\n                    fragment_profile_array = np.nan_to_num(np.array(fragment_profiles), copy=False, nan=np.nan)\n                    # RuntimeWarning: divide by zero encountered in log\n                    # np.log causes -inf at 0, thus we need to correct these to a very large number\n                    batch_fragment_profile = torch.from_numpy(fragment_profile_array)\n                    per_residue_fragment_cross_entropy = \\\n                        metrics.cross_entropy(asu_unconditional_softmax_seq,\n                                              batch_fragment_profile,\n                                              per_entry=True)\n                    # per_residue_fragment_cross_entropy\n                    # [[-3.0685883 -3.575249  -2.967545  ... -3.3111317 -3.1204746 -3.1201541]\n                    #  [-3.0685873 -3.5752504 -2.9675443 ... -3.3111336 -3.1204753 -3.1201541]\n                    #  [-3.0685952 -3.575687  -2.9675474 ... -3.3111277 -3.1428783 -3.1201544]]\n                else:  # Populate with null data\n                    per_residue_fragment_cross_entropy = np.empty_like(per_residue_design_indices, dtype=np.float32)\n                    per_residue_fragment_cross_entropy[:] = np.nan\n\n                if pose.evolutionary_profile:\n                    per_residue_evolution_cross_entropy = \\\n                        metrics.cross_entropy(asu_unconditional_softmax_seq,\n                                              batch_evolutionary_profile[:actual_batch_length],\n                                              per_entry=True)\n                else:  # Populate with null data\n                    per_residue_evolution_cross_entropy = np.empty_like(per_residue_fragment_cross_entropy)\n                    per_residue_evolution_cross_entropy[:] = np.nan\n\n                if pose.profile:\n                    # Process the design_profiles into an array for cross entropy\n                    with catch_warnings():\n                        # Divide by zero encountered in log\n                        simplefilter('ignore', category=RuntimeWarning)\n                        corrected_log_design_profile = np.nan_to_num(\n                            np.log(np.array(design_profiles)), copy=False, nan=np.nan,\n                            neginf=metrics.zero_probability_evol_value)\n                    batch_design_profile = torch.from_numpy(corrected_log_design_profile)\n                    per_residue_design_cross_entropy = \\\n                        metrics.cross_entropy(asu_unconditional_softmax_seq,\n                                              batch_design_profile,\n                                              per_entry=True)\n                else:  # Populate with null data\n                    per_residue_design_cross_entropy = np.empty_like(per_residue_fragment_cross_entropy)\n                    per_residue_design_cross_entropy[:] = np.nan\n\n                # Convert to axis=0 list's for below indexing\n                per_residue_design_indices = list(per_residue_design_indices)\n                per_residue_dock_cross_entropy = list(per_residue_dock_cross_entropy)\n                per_residue_design_cross_entropy = list(per_residue_design_cross_entropy)\n                per_residue_evolution_cross_entropy = list(per_residue_evolution_cross_entropy)\n                per_residue_fragment_cross_entropy = list(per_residue_fragment_cross_entropy)\n                _per_residue_data_batched = []\n                for idx in range(actual_batch_length):\n                    _per_residue_data_batched.append({\n                        # This is required to save the interface_residues\n                        'interface_residue': per_residue_design_indices[idx],\n                        'proteinmpnn_dock_cross_entropy_loss': per_residue_dock_cross_entropy[idx],\n                        'proteinmpnn_v_design_probability_cross_entropy_loss':\n                            per_residue_design_cross_entropy[idx],\n                        'proteinmpnn_v_evolution_probability_cross_entropy_loss':\n                            per_residue_evolution_cross_entropy[idx],\n                        'proteinmpnn_v_fragment_probability_cross_entropy_loss':\n                            per_residue_fragment_cross_entropy[idx],\n                    })\n\n                if collapse_profile.size:  # Not equal to zero\n                    # Include new axis for the sequence iteration to work on an array v\n                    collapse_by_pose = \\\n                        metrics.collapse_per_residue(asu_unconditional_softmax_seq[:, None],\n                                                     contact_order_per_res_z, reference_collapse,\n                                                     alphabet_type=protein_letters_alph1,\n                                                     hydrophobicity='expanded')\n                    for _data_batched, collapse_metrics in zip(_per_residue_data_batched, collapse_by_pose):\n                        _data_batched.update(collapse_metrics)\n\n                return _per_residue_data_batched\n        else:\n            batch_parameters = {}\n            mask = residue_idx = chain_encoding = None\n\n        # Initialize pose data\n        pose_metrics = []\n        per_residue_data = []\n        pose_ids = list(range(number_of_transforms))\n        # ProteinMPNN\n        batch_idx = 0\n        # pose_metrics_batched = []\n        per_residue_data_batched = []\n        design_profiles = []\n        fragment_profiles = []\n        interface_mask = []\n        # Stack the entity coordinates to make up a contiguous block for each pose\n        new_coords = []\n        # Get metrics for each Pose\n        for idx in tqdm(pose_ids, bar_format=TQDM_BAR_FORMAT):\n            # logger.info(f'Metrics for Pose {idx + 1}/{number_of_transforms}')\n            # Add the next set of coordinates\n            update_pose_coords(idx)\n\n            # if number_perturbations_applied &gt; 1:\n            add_fragments_to_pose()\n\n            # Reset the fragment_map and fragment_profile for each Entity before calculate_fragment_profile\n            for entity in pose.entities:\n                entity.fragment_map = None\n                # entity.alpha.clear()\n\n            # Load fragment_profile (and fragment_metrics) into the analysis\n            pose.calculate_fragment_profile()\n            # This could be an empty array if no fragments were found\n            fragment_profile_array = pose.fragment_profile.as_array()\n            with catch_warnings():\n                simplefilter('ignore', category=RuntimeWarning)\n                # np.log causes -inf at 0, so correct these to a \"large\" number\n                corrected_frag_array = np.nan_to_num(np.log(fragment_profile_array), copy=False,\n                                                     nan=np.nan, neginf=metrics.zero_probability_frag_value)\n            per_residue_fragment_profile_loss = \\\n                resources.ml.sequence_nllloss(torch_numeric_sequence, torch.from_numpy(corrected_frag_array))\n            # per_residue_data[pose_id] = {\n            per_residue_data.append({\n                **sequence_params,\n                'sequence_loss_fragment': per_residue_fragment_profile_loss\n            })\n\n            # Remove saved pose attributes from the prior iteration calculations\n            pose.ss_sequence_indices.clear(), pose.ss_type_sequence.clear()\n            for attribute in ['_design_residues', '_interface_residues']:  # _assembly_minimally_contacting\n                try:\n                    delattr(pose, attribute)\n                except AttributeError:\n                    pass\n\n            # Save pose metrics\n            pose_metrics.append(pose.calculate_metrics())\n\n            if proteinmpnn_score:\n                # Save profiles\n                fragment_profiles.append(pose.fragment_profile.as_array())\n                if measure_evolution:\n                    pose.calculate_profile()\n                    design_profiles.append(pssm_as_array(pose.profile))\n\n                # Add all interface residues\n                # if measure_interface_during_dock:  # job.design.interface:\n                design_residues = []\n                for number, residues in pose.interface_residues_by_interface_unique.items():\n                    design_residues.extend([residue.index for residue in residues])\n                # else:\n                #     design_residues = list(range(pose_length))\n                interface_mask.append(design_residues)\n                # Set coords\n                new_coords.append(getattr(pose, coords_type))\n\n                batch_idx += 1\n                # If the current iteration marks a batch\"-sized\" unit of work, execute it\n                if batch_idx == batch_length:  # or idx + 1 == number_of_transforms:\n                    per_residue_data_batched.extend(proteinmpnn_score_batched_coords(new_coords))\n\n                    # Set batch containers to zero for the next iteration\n                    batch_idx = 0\n                    design_profiles = []\n                    fragment_profiles = []\n                    interface_mask = []\n                    new_coords = []\n\n        if proteinmpnn_score:\n            # Finish the routine with any remaining proteinmpnn calculations adding last batched dataset from\n            # proteinmpnn_score_batched_coords()\n            if new_coords:\n                per_residue_data_batched.extend(proteinmpnn_score_batched_coords(new_coords))\n            # Consolidate the iterative and batched data\n            for data, batched_data in zip(per_residue_data, per_residue_data_batched):\n                data.update(batched_data)\n            # for data, batched_data in zip(pose_metrics, pose_metrics_batched):\n            #     data.update(batched_data)\n\n        # Construct the main DataFrames, poses_df and residues_df\n        poses_df = pd.DataFrame.from_dict(dict(zip(pose_ids, pose_metrics)), orient='index')\n        residues_df = pd.concat({pose_id: pd.DataFrame(data, index=residue_indices)\n                                 for pose_id, data in zip(pose_ids, per_residue_data)}) \\\n            .unstack().swaplevel(0, 1, axis=1)\n\n        # Set up column renaming\n        # if proteinmpnn_score:\n        #     per_res_columns = [\n        #         'proteinmpnn_v_design_probability_cross_entropy_loss',\n        #         'proteinmpnn_v_evolution_probability_cross_entropy_loss'\n        #     ]\n        #     mean_columns = [\n        #         'proteinmpnn_v_design_probability_cross_entropy_per_residue',\n        #         'proteinmpnn_v_evolution_probability_cross_entropy_per_residue'\n        #     ]\n        #     _rename = dict(zip(per_res_columns, mean_columns))\n        if proteinmpnn_score and collapse_profile.size:\n            # collapse_profile required\n            collapse_metrics = (\n                'collapse_deviation_magnitude',\n                'collapse_increase_significance_by_contact_order_z',\n                'collapse_increased_z',\n                'collapse_new_positions',\n                'collapse_new_position_significance',\n                'collapse_significance_by_contact_order_z',\n                'collapse_sequential_peaks_z',\n                'collapse_sequential_z',\n                'hydrophobic_collapse')\n            unique_columns = residues_df.columns.unique(level=-1)\n            _columns = unique_columns.tolist()\n            remap_columns = dict(zip(_columns, _columns))\n            remap_columns.update(dict(zip(collapse_metrics, (f'dock_{metric_}' for metric_ in collapse_metrics))))\n            residues_df.columns = residues_df.columns.set_levels(unique_columns.map(remap_columns), level=-1)\n            # 'dock' metrics aren't included by default\n            per_res_columns = ['dock_hydrophobic_collapse', 'dock_collapse_deviation_magnitude']\n            mean_columns = ['dock_hydrophobicity', 'dock_collapse_variance']\n            _rename = dict(zip(per_res_columns, mean_columns))\n        else:\n            mean_columns = []\n            _rename = {}\n\n        # Calculate new metrics from combinations of other metrics\n        # Add summed residue information to poses_df\n        summed_poses_df = metrics.sum_per_residue_metrics(\n            residues_df, rename_columns=_rename, mean_metrics=mean_columns)\n        # # Need to remove sequence as it is in pose.calculate_metrics()\n        # poses_df = poses_df.join(summed_poses_df.drop('sequence', axis=1))\n        if proteinmpnn_score:\n            poses_df = poses_df.join(summed_poses_df.drop('number_residues_interface', axis=1))\n            # .droplevel(-1, axis=1) operations are REQUIRED here or the calculations are messed up\n            interface_df = residues_df.loc[:, idx_slice[:, 'interface_residue']].droplevel(-1, axis=1)\n            # number_interface_residues_s = interface_df.sum(axis=1)\n            number_interface_residues_s = poses_df['number_residues_interface']\n            # Update the total loss according to those residues that were actually specified as designable\n            poses_df['proteinmpnn_dock_cross_entropy_per_residue'] = \\\n                (residues_df.loc[:, idx_slice[:, 'proteinmpnn_dock_cross_entropy_loss']].droplevel(-1, axis=1)\n                 * interface_df).sum(axis=1)\n            poses_df['proteinmpnn_dock_cross_entropy_per_residue'] /= number_interface_residues_s\n            poses_df['proteinmpnn_v_design_probability_cross_entropy_per_residue'] = \\\n                (residues_df.loc[:, idx_slice[:, 'proteinmpnn_v_design_probability_cross_entropy_loss']]\n                 .droplevel(-1, axis=1) * interface_df).sum(axis=1)\n            poses_df['proteinmpnn_v_design_probability_cross_entropy_per_residue'] /= number_interface_residues_s\n            poses_df['proteinmpnn_v_evolution_probability_cross_entropy_per_residue'] = \\\n                (residues_df.loc[:, idx_slice[:, 'proteinmpnn_v_evolution_probability_cross_entropy_loss']]\n                 .droplevel(-1, axis=1) * interface_df).sum(axis=1)\n            poses_df['proteinmpnn_v_evolution_probability_cross_entropy_per_residue'] /= number_interface_residues_s\n            # poses_df['proteinmpnn_v_fragment_probability_cross_entropy_per_residue'] = \\\n            #     (residues_df.loc[:, idx_slice[:, 'proteinmpnn_v_fragment_probability_cross_entropy_loss']]\n            #      .droplevel(-1, axis=1) * interface_df).sum(axis=1)\n            # Update the per_residue loss according to fragment residues involved in the scoring\n            poses_df['proteinmpnn_v_fragment_probability_cross_entropy_per_residue'] = \\\n                poses_df['proteinmpnn_v_fragment_probability_cross_entropy_loss'] \\\n                / poses_df['number_residues_interface_fragment_total']\n\n            if collapse_profile.size:\n                # Check if there are new collapse islands and count\n                poses_df['dock_collapse_new_positions'] = \\\n                    (residues_df.loc[:, idx_slice[:, 'dock_collapse_new_positions']].droplevel(-1, axis=1)\n                     * interface_df).sum(axis=1)\n                # If there are any dock_collapse_new_positions there is a collapse violation\n                poses_df['dock_collapse_violation'] = poses_df['dock_collapse_new_positions'] &gt; 0\n\n                poses_df['dock_collapse_significance_by_contact_order_z_mean'] = \\\n                    poses_df['dock_collapse_significance_by_contact_order_z'] / \\\n                    (residues_df.loc[:, idx_slice[:, 'dock_collapse_significance_by_contact_order_z']] != 0) \\\n                    .sum(axis=1)\n                # if measure_alignment:\n                dock_collapse_increased_df = residues_df.loc[:, idx_slice[:, 'dock_collapse_increased_z']]\n                total_increased_collapse = (dock_collapse_increased_df != 0).sum(axis=1)\n                poses_df['dock_collapse_increased_z_mean'] = \\\n                    dock_collapse_increased_df.sum(axis=1) / total_increased_collapse\n                poses_df['dock_collapse_sequential_peaks_z_mean'] = \\\n                    poses_df['dock_collapse_sequential_peaks_z'] / total_increased_collapse\n                poses_df['dock_collapse_sequential_z_mean'] = \\\n                    poses_df['dock_collapse_sequential_z'] / total_increased_collapse\n        else:\n            poses_df = poses_df.join(summed_poses_df)\n\n        # Finally add the precalculated pose_thermophilicity for completeness\n        poses_df['pose_thermophilicity'] = pose_thermophilicity\n        # logger.debug(f'Found poses_df with columns: {poses_df.columns.tolist()}')\n        # logger.debug(f'Found poses_df with index: {poses_df.index.tolist()}')\n\n        return poses_df, residues_df\n\n    def perturb_transformations() -&gt; tuple[np.ndarray, list[int], int]:\n        \"\"\"From existing transformation parameters, sample parameters within a range of spatial perturbation\n\n        Returns:\n            A tuple consisting of the elements (\n            transformation hash - Integer mapping the possible 3D space for docking to each perturbed transformation,\n            size of each perturbation cluster - Number of perturbed transformations possible from starting transform,\n            degrees of freedom sampled - How many degrees of freedom were perturbed\n            )\n        \"\"\"\n        logger.info(f'Perturbing transformations')\n        perturb_rotation1, perturb_rotation2, perturb_int_tx1, perturb_int_tx2, perturb_optimal_ext_dof_shifts = \\\n            [], [], [], [], []\n\n        # Define a function to stack the transforms\n        def stack_viable_transforms(passing_indices: np.ndarray | list[int]):\n            \"\"\"From indices with viable transformations, stack the corresponding transformations into full\n            perturbation transformations\n\n            Args:\n                passing_indices: The indices that should be selected from the full transformation sets\n            \"\"\"\n            # nonlocal perturb_rotation1, perturb_rotation2, perturb_int_tx1, perturb_int_tx2\n            logger.debug(f'Perturb expansion found {len(passing_indices)} passing_perturbations')\n            perturb_rotation1.append(full_rotation1[passing_indices])\n            perturb_rotation2.append(full_rotation2[passing_indices])\n            if sym_entry.is_internal_tx1:\n                perturb_int_tx1.extend(full_int_tx1[passing_indices, -1])\n            if sym_entry.is_internal_tx2:\n                perturb_int_tx2.extend(full_int_tx2[passing_indices, -1])\n\n            if sym_entry.unit_cell:\n                nonlocal full_optimal_ext_dof_shifts  # , full_ext_tx1, full_ext_tx2\n                perturb_optimal_ext_dof_shifts.append(full_optimal_ext_dof_shifts[passing_indices])\n                # full_uc_dimensions = full_uc_dimensions[passing_indices]\n                # full_ext_tx1 = full_ext_tx1[passing_indices]\n                # full_ext_tx2 = full_ext_tx2[passing_indices]\n\n        # Expand successful poses from coarse search of transformational space to randomly perturbed offset\n        # By perturbing the transformation a random small amount, we generate transformational diversity from\n        # the already identified solutions.\n        perturbations, n_perturbed_dof = \\\n            create_perturbation_transformations(sym_entry, number_of_rotations=job.dock.perturb_dof_steps_rot,\n                                                number_of_translations=job.dock.perturb_dof_steps_tx,\n                                                rotation_steps=rotation_steps,\n                                                translation_steps=translation_perturb_steps)\n        # Extract perturbation parameters and set the original transformation parameters to a new variable\n        nonlocal number_perturbations_applied\n        rotation_perturbations1 = perturbations['rotation1']\n        # Compute the length of each perturbation to separate into unique perturbation spaces\n        number_perturbations_applied = calculation_size_ = len(rotation_perturbations1)\n        # logger.debug(f'rotation_perturbations1.shape: {rotation_perturbations1.shape}')\n        # logger.debug(f'rotation_perturbations1[:5]: {rotation_perturbations1[:5]}')\n        batch_length_ = get_check_tree_for_query_overlap_batch_length(bb_cb_coords2)\n        batch_length_ = min((batch_length_, number_perturbations_applied))\n\n        # Define local check_tree_for_query_overlap function to check clashes\n        @resources.ml.batch_calculation(size=calculation_size_, batch_length=batch_length_, setup=np_tile_wrap,\n                                        compute_failure_exceptions=(np.core._exceptions._ArrayMemoryError,))\n        def perturb_check_tree_for_query_overlap(*args, **kwargs):\n            return check_tree_for_query_overlap(*args, **kwargs)\n\n        nonlocal number_of_transforms, full_rotation1, full_rotation2\n        # if sym_entry.is_internal_rot1:  # Todo 2\n        original_rotation1 = full_rotation1\n        # if sym_entry.is_internal_rot2:  # Todo 2\n        original_rotation2 = full_rotation2\n        rotation_perturbations2 = perturbations['rotation2']\n        # logger.debug(f'rotation_perturbations2.shape: {rotation_perturbations2.shape}')\n        # logger.debug(f'rotation_perturbations2[:5]: {rotation_perturbations2[:5]}')\n        # blank_parameter = list(repeat([None, None, None], number_of_transforms))\n        if sym_entry.is_internal_tx1:\n            nonlocal full_int_tx1\n            original_int_tx1 = full_int_tx1\n            translation_perturbations1 = perturbations['translation1']\n            # logger.debug(f'translation_perturbations1.shape: {translation_perturbations1.shape}')\n            # logger.debug(f'translation_perturbations1[:5]: {translation_perturbations1[:5]}')\n        # else:\n        #     translation_perturbations1 = blank_parameter\n\n        if sym_entry.is_internal_tx2:\n            nonlocal full_int_tx2\n            original_int_tx2 = full_int_tx2\n            translation_perturbations2 = perturbations['translation2']\n            # logger.debug(f'translation_perturbations2.shape: {translation_perturbations2.shape}')\n            # logger.debug(f'translation_perturbations2[:5]: {translation_perturbations2[:5]}')\n        # else:\n        #     translation_perturbations2 = blank_parameter\n\n        if sym_entry.unit_cell:\n            nonlocal full_optimal_ext_dof_shifts\n            nonlocal full_ext_tx1, full_ext_tx2\n            ext_dof_perturbations = perturbations['external_translations']\n            original_optimal_ext_dof_shifts = full_optimal_ext_dof_shifts\n            # original_ext_tx1 = full_ext_tx1\n            # original_ext_tx2 = full_ext_tx2\n            external_dof1, external_dof2, *_ = sym_entry.external_dofs\n        else:\n            full_ext_tx1 = full_ext_tx2 = full_ext_tx_sum = None\n\n        # Apply the perturbation to each existing transformation\n        logger.info(f'Perturbing each transform {number_perturbations_applied} times')\n        for idx in range(number_of_transforms):\n            logger.info(f'Perturbing transform {idx + 1}')\n            # Rotate the unique rotation by the perturb_matrix_grid and set equal to the full_rotation* array\n            full_rotation1 = np.matmul(original_rotation1[idx], rotation_perturbations1.swapaxes(-1, -2))  # rotation1\n            full_inv_rotation1 = np.linalg.inv(full_rotation1)\n            full_rotation2 = np.matmul(original_rotation2[idx], rotation_perturbations2.swapaxes(-1, -2))  # rotation2\n\n            # Translate the unique translation according to the perturb_translation_grid\n            if sym_entry.is_internal_tx1:\n                full_int_tx1 = original_int_tx1[idx] + translation_perturbations1  # translation1\n            if sym_entry.is_internal_tx2:\n                full_int_tx2 = original_int_tx2[idx] + translation_perturbations2  # translation2\n            if sym_entry.unit_cell:\n                # perturbed_optimal_ext_dof_shifts = full_optimal_ext_dof_shifts[None] + ext_dof_perturbations\n                # full_ext_tx_perturb1 = (perturbed_optimal_ext_dof_shifts[:, :, None] \\\n                #     * sym_entry.external_dof1).sum(axis=-2)\n                # full_ext_tx_perturb2 = (perturbed_optimal_ext_dof_shifts[:, :, None] \\\n                #     * sym_entry.external_dof2).sum(axis=-2)\n                # Below is for the individual perturbation\n                # optimal_ext_dof_shift = full_optimal_ext_dof_shifts[idx]\n                # perturbed_ext_dof_shift = optimal_ext_dof_shift + ext_dof_perturbations\n                unsqueezed_perturbed_ext_dof_shifts = \\\n                    (original_optimal_ext_dof_shifts[idx] + ext_dof_perturbations)[:, :, None]\n                # unsqueezed_perturbed_ext_dof_shifts = perturbed_ext_dof_shift[:, :, None]\n                full_ext_tx1 = np.sum(unsqueezed_perturbed_ext_dof_shifts * external_dof1, axis=-2)\n                full_ext_tx2 = np.sum(unsqueezed_perturbed_ext_dof_shifts * external_dof2, axis=-2)\n                full_ext_tx_sum = full_ext_tx2 - full_ext_tx1\n\n            # Check for ASU clashes again\n            # Using the inverse transform of the model2 backbone and cb coordinates, check for clashes with the model1\n            # backbone and cb coordinates BallTree\n            ball_tree_kwargs = dict(binarytree=component1_backbone_cb_tree, clash_distance=clash_dist,\n                                    rotation=full_rotation2, translation=full_int_tx2,\n                                    rotation2=set_mat2, translation2=full_ext_tx_sum,\n                                    rotation3=inv_setting1,\n                                    translation3=None if full_int_tx1 is None else full_int_tx1 * -1,\n                                    rotation4=full_inv_rotation1)\n            # Create a fresh asu_clash_counts\n            asu_clash_counts = np.ones(number_perturbations_applied)\n            # clash_time_start = time.time()\n            overlap_return = perturb_check_tree_for_query_overlap(\n                **ball_tree_kwargs, return_containers={'overlap_counts': asu_clash_counts}, setup_args=(bb_cb_coords2,))\n            # logger.debug(f'Perturb clash took {time.time() - clash_time_start:8f}s')\n\n            # Extract the data\n            asu_clash_counts = overlap_return['overlap_counts']\n            logger.debug(f'Perturb expansion found asu_clash_counts:\\n{asu_clash_counts}')\n            passing_perturbations = np.flatnonzero(asu_clash_counts == 0)\n            # Check for symmetric clashes again\n            if not job.design.ignore_symmetric_clashes:\n                passing_symmetric_clash_indices_perturb = find_viable_symmetric_indices(passing_perturbations.tolist())\n            else:\n                passing_symmetric_clash_indices_perturb = slice(None)\n            # Index the passing ASU indices with the passing symmetric indices and keep all viable transforms\n            # Stack the viable perturbed transforms\n            stack_viable_transforms(passing_perturbations[passing_symmetric_clash_indices_perturb])\n\n        # Concatenate the stacked perturbations\n        full_rotation1 = np.concatenate(perturb_rotation1, axis=0)\n        full_rotation2 = np.concatenate(perturb_rotation2, axis=0)\n        number_of_transforms = len(full_rotation1)\n        logger.info(f'After perturbation, found {number_of_transforms} viable solutions')\n        if sym_entry.is_internal_tx1:\n            full_int_tx1 = np.zeros((number_of_transforms, 3), dtype=float)\n            # Add the translation to Z (axis=1)\n            full_int_tx1[:, -1] = perturb_int_tx1\n            # full_int_tx1 = stacked_internal_tx_vectors1\n\n        if sym_entry.is_internal_tx2:\n            full_int_tx2 = np.zeros((number_of_transforms, 3), dtype=float)\n            # Add the translation to Z (axis=1)\n            full_int_tx2[:, -1] = perturb_int_tx2\n            # full_int_tx2 = stacked_internal_tx_vectors2\n\n        if sym_entry.unit_cell:\n            # optimal_ext_dof_shifts[:, :, None] &lt;- None expands the axis to make multiplication accurate\n            full_optimal_ext_dof_shifts = np.concatenate(perturb_optimal_ext_dof_shifts, axis=0)\n            unsqueezed_optimal_ext_dof_shifts = full_optimal_ext_dof_shifts[:, :, None]\n            full_ext_tx1 = np.sum(unsqueezed_optimal_ext_dof_shifts * external_dof1, axis=-2)\n            full_ext_tx2 = np.sum(unsqueezed_optimal_ext_dof_shifts * external_dof2, axis=-2)\n\n        transform_hashes = create_transformation_hash()\n        logger.debug(f'Found the TransformHasher.translation_bin_width={model_transform_hasher.translation_bin_width}, '\n                     f'.rotation_bin_width={model_transform_hasher.rotation_bin_width}\\n'\n                     f'Current range of sampled translations={sum(translation_perturb_steps)}, '\n                     f'rotations={sum(rotation_steps)}')\n        # print(model_transform_hasher.translation_bin_width &gt; sum(translation_perturb_steps))\n        # print(model_transform_hasher.rotation_bin_width &gt; sum(rotation_steps))\n        if model_transform_hasher.translation_bin_width &gt; sum(translation_perturb_steps) or \\\n                model_transform_hasher.rotation_bin_width &gt; sum(rotation_steps):\n            # The translation/rotation is smaller than bins, so further exploration only possible without minimization\n            # Get the shape of the passing perturbations\n            perturbation_shape = [len(perturb) for perturb in perturb_rotation1]\n            # sorted_unique_transform_hashes = transform_hashes\n        else:\n            # Minimize perturbation space by unique transform hashes\n            # Using the current transforms, create a hash to uniquely label them and apply to the indices\n            sorted_unique_transform_hashes, unique_indices = np.unique(transform_hashes, return_index=True)\n            # Create array to mark which are unique\n            unique_transform_hashes = np.zeros_like(transform_hashes)\n            unique_transform_hashes[unique_indices] = 1\n            # Filter by unique_indices, sorting the indices to maintain the order of the transforms\n            # filter_transforms_by_indices(unique_indices)\n            unique_indices.sort()\n            filter_transforms_by_indices(unique_indices)\n            transform_hashes = transform_hashes[unique_indices]\n            # sorted_transform_hashes = np.sort(transform_hashes, axis=None)\n            # unique_sorted_transform_hashes = np.zeros_like(sorted_transform_hashes, dtype=bool)\n            # unique_sorted_transform_hashes[1:] = sorted_transform_hashes[1:] == sorted_transform_hashes[:-1]\n            # unique_sorted_transform_hashes[0] = True\n            # # Alternative\n            # unique_transform_hashes = pd.Index(transform_hashes).duplicated('first')\n\n            # total_number_of_perturbations = number_of_transforms * number_perturbations_applied\n            # Get the shape of the passing perturbations\n            perturbation_shape = []\n            # num_zeros = 0\n            last_perturb_start = 0\n            for perturb in perturb_rotation1:\n                perturb_end = last_perturb_start + len(perturb)\n                perturbation_shape.append(unique_transform_hashes[last_perturb_start:perturb_end].sum())\n                # Use if removing zero counts...\n                # shape = unique_transform_hashes[last_perturb_start:perturb_end].sum()\n                # if shape:\n                #     perturbation_shape.append(shape)\n                # else:\n                #     num_zeros += 1\n                last_perturb_start = perturb_end\n\n            number_of_transforms = len(full_rotation1)\n            logger.info(f'After culling duplicated transforms, found {number_of_transforms} viable solutions')\n            num_zeros = perturbation_shape.count(0)\n            if num_zeros:\n                logger.info(f'A total of {num_zeros} original transformations had no unique perturbations')\n                # Could use if removing zero counts... but probably less clear than above\n                # pop_zero_index = perturbation_shape.index(0)\n                # while pop_zero_index != -1:\n                #     perturbation_shape.pop(pop_zero_index)\n                #     pop_zero_index = perturbation_shape.index(0)\n\n        return transform_hashes, perturbation_shape, n_perturbed_dof\n\n    def optimize_found_transformations_by_metrics() -&gt; tuple[pd.DataFrame, list[int]]:\n        \"\"\"Perform a cycle of (optional) transformation perturbation, and then score and select those which are ranked\n        highest\n\n        Returns:\n            A tuple containing the DataFrame containing the selected metrics for selected Poses and the identifiers for\n                those Poses\n            # The mean value of the acquired metric for all found poses\n        \"\"\"\n        nonlocal poses_df, residues_df\n        nonlocal total_dof_perturbed\n        # total_dof_perturbed = sym_entry.total_dof\n\n        if any((sym_entry.number_dof_rotation, sym_entry.number_dof_translation)):\n            nonlocal rotation_steps, translation_perturb_steps\n            # Perform perturbations to the allowed degrees of freedom\n            # Modify the perturbation amount by half as the space is searched to completion\n            # Reduce rotation_step before as the original step size was already searched\n            rotation_steps = tuple(step * .5 for step in rotation_steps)\n            current_transformation_ids, number_of_perturbs_per_cluster, total_dof_perturbed = perturb_transformations()\n            # Sets number_perturbations_applied, number_of_transforms,\n            #  full_rotation1, full_rotation2, full_int_tx1, full_int_tx2,\n            #  full_optimal_ext_dof_shifts, full_ext_tx1, full_ext_tx2\n            # Reduce translation_perturb_steps after as the original step size was never searched\n            translation_perturb_steps = tuple(step * .5 for step in translation_perturb_steps)\n        # elif sym_entry.external_dof:  # The DOF are not such that perturbation would be of much benefit\n        else:\n            raise NotImplementedError(f\"Can't perturb external dof only quite yet\")\n\n            # Perform optimization by means of optimal_tx\n            def minimize_translations():\n                \"\"\"\"\"\"\n                logger.info(f'Optimizing transformations')\n                # The application of total_dof_perturbed might not be necessary as this optimizes fully\n                total_dof_perturbed = sym_entry.total_dof\n                # Remake the optimal shifts given each of the passing ghost fragment/surface fragment pairs\n                optimal_ext_dof_shifts = np.zeros((number_of_transforms, 3), dtype=float)\n                pose_residues = pose.residues\n                for idx in range(number_of_transforms):\n                    update_pose_coords(idx)\n                    add_fragments_to_pose()\n                    passing_ghost_coords = []\n                    passing_surf_coords = []\n                    reference_rmsds = []\n                    for entity_pair, fragment_info in pose.fragment_info_by_entity_pair:\n                        ghost_frag = pose_residues[fragment_info.paired]\n                        surf_frag = pose_residues[fragment_info.mapped]\n                        passing_ghost_coords.append(ghost_frag.guide_coords)\n                        passing_surf_coords.append(surf_frag.guide_coords)\n                        reference_rmsds.append(ghost_frag.rmsd)\n\n                    transform_passing_shifts = \\\n                        optimal_tx.solve_optimal_shifts(passing_ghost_coords, passing_surf_coords, reference_rmsds)\n                    mean_transform = transform_passing_shifts.mean(axis=0)\n\n                    # Inherent in minimize_translations() call due to DOF requirements of preceding else:\n                    if sym_entry.unit_cell:\n                        # Must take the optimal_ext_dof_shifts and multiply the column number by the corresponding row\n                        # in the sym_entry.external_dof#\n                        # optimal_ext_dof_shifts[0] scalar * sym_entry.group_external_dof[0] (1 row, 3 columns)\n                        # Repeat for additional DOFs, then add all up within each row.\n                        # For a single DOF, multiplication won't matter as only one matrix element will be available\n                        #\n                        # # Must find positive indices before external_dof1 multiplication in case negatives there\n                        # positive_indices = \\\n                        #     np.flatnonzero(np.all(transform_passing_shifts[:, :sym_entry.number_dof_external] &gt;= 0, axis=1))\n                        # number_passing_shifts = len(positive_indices)\n                        optimal_ext_dof_shifts[idx, :sym_entry.number_dof_external] = \\\n                            mean_transform[:sym_entry.number_dof_external]\n\n            # Using the current transforms, create a hash to uniquely label them and apply to the indices\n            current_transformation_ids = create_transformation_hash()\n            minimize_translations()\n\n        # Todo\n        #  enable precise metric acquisition\n        #  dock_metrics = collect_dock_metrics(score_functions, proteinmpnn_score=job.dock.proteinmpnn_score)\n        poses_df, residues_df = collect_dock_metrics(proteinmpnn_score=job.dock.proteinmpnn_score)\n        weighted_trajectory_df: pd.DataFrame = prioritize_transforms_by_selection(poses_df)\n        weighted_trajectory_df_index = weighted_trajectory_df.index\n\n        if number_perturbations_applied &gt; 1:\n            # Sort each perturbation cluster members by the prioritized metric\n            top_transform_cluster_indices: list[int] = []\n            perturb_passing_indices: list[list[int]] = []\n\n            # Used to progressively limit search as clusters deepen\n            if optimize_round == 1:\n                top_perturb_hits = total_dof_perturbed\n                logger.info(f'Selecting the top {top_perturb_hits} transformations from each perturbation')\n            else:\n                top_perturb_hits = 1\n                logger.info(f'Selecting the top transformation from each perturbation')\n            # # Round down the sqrt of the number_perturbations_applied\n            # top_perturb_hits = int(math.sqrt(number_perturbations_applied) + .5)\n            # top_perturb_hits = int(total_dof_perturbed/optimize_round + .5)\n\n            lower_perturb_idx = 0\n            for cluster_idx, number_of_perturbs in enumerate(number_of_perturbs_per_cluster):\n                if not number_of_perturbs:\n                    # All perturbations were culled due to overlap\n                    continue\n                # Set up the cluster range\n                upper_perturb_idx = lower_perturb_idx + number_of_perturbs\n                perturb_indices = list(range(lower_perturb_idx, upper_perturb_idx))\n                lower_perturb_idx = upper_perturb_idx\n                # Grab the cluster range indices\n                perturb_indexer = np.isin(weighted_trajectory_df_index, perturb_indices)\n                if perturb_indexer.any():\n                    if optimize_round == 1:\n                        # Slice the cluster range indices by the top hits\n                        selected_perturb_indices = \\\n                            weighted_trajectory_df_index[perturb_indexer][:top_perturb_hits].tolist()\n                        # if selected_perturb_indices:\n                        # Save the top transform and the top X transforms from each cluster\n                        top_transform_cluster_indices.append(selected_perturb_indices[0])\n                        perturb_passing_indices.append(selected_perturb_indices)\n                    else:  # Just grab the top hit\n                        top_transform_cluster_indices.append(weighted_trajectory_df_index[perturb_indexer][0])\n                else:  # Update that no perturb_indices present after filter\n                    number_of_perturbs_per_cluster[cluster_idx] = 0\n                #     perturb_passing_indices.append([])\n\n            if optimize_round == 1:\n                nonlocal round1_cluster_shape\n                round1_cluster_shape = [len(indices) for indices in perturb_passing_indices]\n            elif optimize_round == 2:\n                # This is only required if perturb_passing_indices.append([]) is used above\n                # Adjust the shape to account for any perturbations that were culled due to overlap\n                cluster_start = 0\n                for cluster_idx, cluster_shape in enumerate(round1_cluster_shape):\n                    cluster_end = cluster_start + cluster_shape\n                    for number_of_perturbs in number_of_perturbs_per_cluster[cluster_start:cluster_end]:\n                        if not number_of_perturbs:\n                            # All perturbations were culled due to overlap\n                            cluster_shape -= 1\n                    # for indices in perturb_passing_indices[cluster_start:cluster_end]:\n                    #     if not indices:\n                    #         cluster_shape -= 1\n                    # Set the cluster shape with the results of the perturbation trials\n                    round1_cluster_shape[cluster_idx] = cluster_shape\n                    cluster_start = cluster_end\n\n                number_top_indices = len(top_transform_cluster_indices)  # sum(round1_cluster_shape)\n                round1_number_of_clusters = len(round1_cluster_shape)\n                logger.info(f'Reducing round 1 expanded cluster search from {number_top_indices} '\n                            f'to {round1_number_of_clusters} transformations')\n                top_scores_s = weighted_trajectory_df.loc[top_transform_cluster_indices,\n                metrics.selection_weight_column]\n                # Filter down to the size of the original transforms from the cluster expansion\n                top_index_of_cluster = []\n                cluster_lower_bound = 0\n                for cluster_idx, cluster_shape in enumerate(round1_cluster_shape):\n                    if cluster_shape &gt; 0:\n                        cluster_upper_bound = cluster_lower_bound + cluster_shape\n                        top_cluster_score = top_scores_s.iloc[cluster_lower_bound:cluster_upper_bound].argmax()\n                        top_index_of_cluster.append(cluster_lower_bound + top_cluster_score)\n                        # Set new lower bound\n                        cluster_lower_bound = cluster_upper_bound\n\n                top_transform_cluster_indices = [top_transform_cluster_indices[idx] for idx in top_index_of_cluster]\n        else:\n            top_transform_cluster_indices = list(range(number_of_transforms))\n\n        # # Finally take from each of the top perturbation \"kernels\"\n        # # With each additional optimize_round, there is exponential increase in the number of transforms\n        # # unless there is filtering to take the top\n        # # Taking the sqrt (or equivalent function), needs to be incremented for each perturbation increase\n        # # so this doesn't get out of hand as the amount grows\n        # # For example, iteration 1 gets no sqrt, iteration 2 gets sqrt, 3 gets cube root\n        # # root_to_take = 1/iteration\n        # top_cluster_root_to_take = 1 / optimize_round\n        # # Take the metric at each of the top positions and sort these\n        # top_cluster_hits = int((number_of_transform_clusters**top_cluster_root_to_take) + .5)\n\n        # Operation NOTE:\n        # During the perturbation selection, this is the sqrt of the number_of_transform_clusters\n        # So, from 18 transforms (idx=1), expanded to 81 dof perturbs (idx=2), to get 1458 possible,\n        # 1392 didn't clash. 4 top_cluster_hits were selected and 9 transforms from each.\n        # This is a lot of sampling for the other 14 that were never chosen. They might've\n        # not been discovered without the perturb since the top score came from one of the 81\n        # possible perturb transforms\n        # if optimize_round &gt; 1:\n        #     cluster_divisor = (total_dof_perturbed / (optimize_round-1))\n        # else:\n        #     cluster_divisor = total_dof_perturbed\n        # Divide the clusters by the total applied degrees of freedom\n        # top_cluster_hits = int(number_of_transform_clusters/cluster_divisor + .5)\n        #\n        # # Grab the cluster range indices\n        # cluster_representative_indexer = np.isin(weighted_trajectory_s.index, top_transform_cluster_indices)\n        # selected_cluster_indices = \\\n        #     weighted_trajectory_s[cluster_representative_indexer][:top_cluster_hits].index.tolist()\n        # # selected_cluster_hits = top_transform_cluster_indices[:top_cluster_overall_hits]\n        # # # Use .loc here as we have a list used to index...\n        # # selected_cluster_indices = weighted_trajectory_s.loc[selected_cluster_hits].index.tolist()\n\n        # # Grab the top cluster indices in the order of the weighted_trajectory_df\n        # cluster_representative_indexer = np.isin(weighted_trajectory_df_index, top_transform_cluster_indices)\n        # selected_cluster_indices = weighted_trajectory_df_index[cluster_representative_indexer].tolist()\n        # if number_perturbations_applied &gt; 1 and optimize_round == 1:\n        #     # For each of the top perturbation clusters, add all the indices picked from the above logic\n        #     selected_indices = []\n        #     for selected_idx in selected_cluster_indices:\n        #         reference_idx = top_transform_cluster_indices.index(selected_idx)\n        #         selected_indices.extend(perturb_passing_indices[reference_idx])\n        # else:\n        #     selected_indices = selected_cluster_indices\n\n        # Grab the top cluster indices in the transformation order, not selected order\n        if number_perturbations_applied &gt; 1 and optimize_round == 1:\n            # For each of the top perturbation clusters, add all the indices picked from the above logic\n            selected_indices = []\n            for cluster_idx, top_index in enumerate(top_transform_cluster_indices):\n                selected_indices.extend(perturb_passing_indices[cluster_idx])\n        else:\n            selected_indices = top_transform_cluster_indices\n\n        # Handle results\n        # # Using the current transforms, create a hash to uniquely label them and apply to the indices\n        # current_transformation_ids = create_transformation_hash()\n\n        # Filter hits down in the order of the selected indices (i.e. transformation order)\n        filter_transforms_by_indices(selected_indices)\n        # Narrow down the metrics by the selected_indices. If this is the last cycle, they will be written\n        poses_df = poses_df.loc[selected_indices]\n        residues_df = residues_df.loc[selected_indices]\n        # Reset the DataFrame.index given new ranking\n        poses_df.index = residues_df.index = pd.RangeIndex(len(selected_indices))\n        # Get the transformations that were selected ready to return\n        # selected_transformation_ids = weighted_trajectory_df_index[selected_indices].tolist()\n        selected_transformation_ids = [current_transformation_ids[idx] for idx in selected_indices]\n\n        # Filter weighted_trajectory_df by selected_indices, and update the .index with ordered transformation_ids\n        selected_indexer = np.isin(weighted_trajectory_df_index, selected_indices)\n        weighted_trajectory_df.index = pd.Index(current_transformation_ids[weighted_trajectory_df_index])\n        weighted_trajectory_df = weighted_trajectory_df[selected_indexer]\n        # Todo?\n        #  Was returning this version of weighted_trajectory_df, but was only using the selected_transformation_ids\n        #  So, now just returning selected_indices\n        # # Filter down the current_transformation_ids by the indices passing job.dock.filter and update the .index\n        # weighted_current_transforms = current_transformation_ids[weighted_trajectory_df_index]\n        # weighted_trajectory_df.index = pd.Index(weighted_current_transforms)\n        # Todo use append_total_results() for global search? Was removed over memory considerations\n        # # Add the weighted_trajectory_df to the total_results_df to keep global results\n        # append_total_results(weighted_trajectory_df)\n\n        return weighted_trajectory_df, selected_transformation_ids\n\n    def create_transformation_hash() -&gt; np.ndarray:\n        \"\"\"Using the currently available transformation parameters for the two Model instances, create the\n        transformation hash to describe the orientation of the second model in relation to the first. This hash will be\n        unique over the sampling space when discrete differences exceed the TransformHasher.rotation_bin_width and\n        .translation_bin_width\n\n        Returns:\n            An integer hashing the currently active transforms to distinct orientational offset in the described space\n        \"\"\"\n        # Needs to be completed outside of individual naming function as it is stacked\n        # transforms = create_transformation_group()\n        # input(f'len(transforms[0][\"rotation\"]): {len(transforms[0][\"rotation\"])}')\n        guide_coordinates_model1, guide_coordinates_model2 = \\\n            cluster.apply_transform_groups_to_guide_coordinates(*create_transformation_group())\n        # input(guide_coordinates_model1[:3])\n        rotations = [None for _ in range(len(guide_coordinates_model1))]\n        # input(len(rotations))\n        translations = rotations.copy()\n        # Only turn the outermost array into a list. Keep the guide coordinate 3x3 arrays as arrays for superposition3d\n        for transform_idx, (guide_coord2, guide_coord1) in enumerate(\n                zip(list(guide_coordinates_model2), list(guide_coordinates_model1))):\n            # Reverse the orientation so that the rot, tx indicate the movement of guide_coord1 onto guide_coord2\n            rmsd, rot, tx = superposition3d(guide_coord2, guide_coord1)\n            rotations[transform_idx] = rot  # rotations.append(rot)\n            translations[transform_idx] = tx  # translations.append(tx)\n\n        # logger.debug(f'before rotations[:3]: {rotations[:3]}')\n        # logger.debug(f'before translations[:3]: {translations[:3]}')\n        hashed_transforms = model_transform_hasher.transforms_to_hash(rotations, np.array(translations))\n        # rotations, translations = model_transform_hasher.hash_to_transforms(hashed_transforms)\n        # logger.debug(f'after rotations[:3]: {rotations[:3]}')\n        # logger.debug(f'after translations[:3]: {translations[:3]}')\n\n        return hashed_transforms\n\n    def prioritize_transforms_by_selection(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Using the active transformations, measure the Pose metrics and filter/weight according to defaults/provided\n        parameters\n\n        Args:\n            df: The Pose DataFrame which should be prioritized from docking\n        Returns:\n            The DataFrame that has been sorted according to the specified filters/weights\n        \"\"\"\n        weighted_df = metrics.prioritize_design_indices(\n            df, filters=job.dock.filter, weights=job.dock.weight, default_weight=default_weight_metric)\n        # Set the metrics_of_interest to the default weighting metric name as well as any weights that are specified\n        metrics_of_interest = [metrics.selection_weight_column]\n        if job.dock.weight:\n            metrics_of_interest += list(job.dock.weight.keys())\n        #     weighted_df = weighted_trajectory_df.loc[:, list(job.dock.weight.keys())]\n        # else:\n        #     weighted_df = weighted_trajectory_df.loc[:, metrics.selection_weight_column]\n        weighted_df = weighted_df.loc[:, metrics_of_interest]\n        # weighted_trajectory_s = metrics.pareto_optimize_trajectories(poses_df, weights=job.dock.weight,\n        #                                                              default_sort=default_weight_metric)\n        # weighted_trajectory_s is sorted with best transform in index 0, regardless of whether it is ascending or not\n        return weighted_df\n\n    # Todo use append_total_results() for global search\n    # def append_total_results(additional_trajectory_df: pd.DataFrame) -&gt; pd.DataFrame:\n    #     \"\"\"Combine existing metrics with the new metrics\n    #     Args:\n    #         additional_trajectory_df: The additional DataFrame to add to the existing global metrics\n    #     Returns:\n    #         The full global metrics DataFrame\n    #     \"\"\"\n    #     nonlocal total_results_df\n    #     # Add new metrics to existing and keep the newly added if there are overlap\n    #     total_results_df = pd.concat([total_results_df, additional_trajectory_df], axis=0)\n    #     total_results_df = total_results_df[~total_results_df.index.duplicated(keep='last')]\n    #     return total_results_df\n\n    # def calculate_results_for_stopping(target_df: pd.DataFrame, indices: list[int | str]) -&gt; float | pd.Series:\n    #     \"\"\"Given a DataFrame with metrics from a round of optimization, calculate the optimization results to report on whether a stopping condition has been met\n    #\n    #     Args:\n    #         target_df: The DataFrame that resulted from the most recent optimization\n    #         indices: The indices which have been selected from the target_df\n    #     Returns:\n    #         The resulting values from the DataFrame based on the target metrics\n    #     \"\"\"\n    #     # Find the value of the new metrics in relation to the old to calculate the result from optimization\n    #     if job.dock.weight:\n    #         selected_columns = list(job.dock.weight.keys())\n    #     else:\n    #         selected_columns = metrics.selection_weight_column\n    #\n    #     selected_metrics_df = target_df.loc[indices, selected_columns]\n    #     # other_metrics_df = selected_metrics_df.drop(indices)\n    #     # Find the difference between the selected and the other\n    #     return selected_metrics_df.mean(axis=0)\n    #     # other_metrics_df.mean(axis=1)\n\n    # Initialize output DataFrames\n    # Todo\n    #  enable precise metric acquisition\n    #  dock_metrics = collect_dock_metrics(score_functions, proteinmpnn_score=job.dock.proteinmpnn_score)\n    poses_df, residues_df = collect_dock_metrics(proteinmpnn_score=job.dock.proteinmpnn_score)\n    weighted_trajectory_df = prioritize_transforms_by_selection(poses_df)\n    # # Get selected indices (sorted in original order)\n    # selected_indices = weighted_trajectory_df.index.sort_values().tolist()\n    # Get selected indices (sorted in weighted_trajectory_df order)\n    selected_indices = weighted_trajectory_df.index.tolist()\n\n    # Filter/sort transforms and metrics by the selected_indices\n    filter_transforms_by_indices(selected_indices)\n    poses_df = poses_df.loc[selected_indices]\n    residues_df = residues_df.loc[selected_indices]\n    # Get the hash of the current transforms\n    passing_transform_ids = create_transformation_hash()\n    weighted_trajectory_df.index = pd.Index(passing_transform_ids)\n\n    # -----------------------------------------------------------------------------------------------------------------\n    # Below creates perturbations to sampled transformations and iteratively optimizes scores the resulting Pose\n    # -----------------------------------------------------------------------------------------------------------------\n    # Set nonlocal perturbation/metric variables that are used in optimize_found_transformations_by_metrics()\n    number_of_transforms = number_of_original_transforms = len(full_rotation1)\n    number_perturbations_applied = 1\n    if job.dock.perturb_dof:\n        # Set the weighted_trajectory_df as total_results_df to keep a record of global results\n        # total_results_df = poses_df &lt;- this contains all filtered results too\n        total_results_df = weighted_trajectory_df\n        # Initialize docking score search\n        round1_cluster_shape = []\n        total_dof_perturbed = 1\n        optimize_round = 0\n        if job.dock.weight:\n            selected_columns = list(job.dock.weight.keys())\n        else:\n            selected_columns = [metrics.selection_weight_column]\n        result = total_results_df.loc[:, selected_columns].mean(axis=0)\n        # result = calculate_results_for_stopping(total_results_df, passing_transform_ids)\n        # threshold = 0.05  # 0.1 &lt;- not convergent # 1 &lt;- too lenient with pareto_optimize_trajectories\n        threshold_percent = 0.05\n        if isinstance(result, float):\n            def result_func(result_): return result_\n            thresholds = result * threshold_percent\n            last_result = 0.\n        else:  # pd.Series\n            def result_func(result_): return result_.values\n            result = result_func(result)\n            thresholds = tuple(result * threshold_percent)\n            last_result = tuple(0. for _ in thresholds)\n\n        # The condition sum(translation_perturb_steps) &lt; 0.1 is True after 4 optimization rounds...\n        # To ensure that the abs doesn't produce worse values, need to compare results in an unbounded scale\n        # (i.e. not between 0-1), which also indicates using a global scale. This way, iteration can tell if they are\n        # better. It is a feature of perturb_transformations() that the same transformation is always included in grid\n        # search at the moment, so the routine should never arrive at worse scores...\n        # Everything below could really be expedited with a Bayseian optimization search strategy\n        # while sum(translation_perturb_steps) &gt; 0.1 and all(tuple(abs(last_result - result) &gt; thresholds)):\n        # Todo the tuple(abs(last_result - result) &gt; thresholds)) with a float won't convert to an iterable\n        logger.info(f'Starting {optimize_found_transformations_by_metrics.__name__} of {number_of_transforms} '\n                    f'transformations with starting optimize target={result}')\n        while (optimize_round &lt; 2 or all(tuple(abs(last_result - result) &gt; thresholds))) \\\n                and sum(translation_perturb_steps) &gt; 0.1:  # model_transform_hasher.translation_bin_width:\n            #     and sum(rotation_steps) &gt; model_transform_hasher.rotation_bin_width:\n            optimize_round += 1\n            logger.info(f'{optimize_found_transformations_by_metrics.__name__} round {optimize_round}')\n            last_result = result\n            # Perform scoring and a possible iteration of dock perturbation\n            weighted_trajectory_df, passing_transform_ids = optimize_found_transformations_by_metrics()\n            # IMPORTANT:\n            # - weighted_trajectory_df.index is in the order of the job.dock.weight/default_selection_weight\n            # - passing_transform_ids are in the order of the current transformations\n            # - De-duplication of overlapping passing_transform_ids (in case of TransformHasher resolution collapse) is\n            #   handled after while loop\n            top_results_df = weighted_trajectory_df.loc[:, selected_columns]\n            # PREVIOUSLY, when weighted_trajectory_df contained all measurements passing filters from\n            #  optimize_found_transformations_by_metrics()\n            # - If the last optimize_found_transformations_by_metrics() sampled from a TransformHasher with no\n            #   resolvable bins (highest resolution reached), all the weighted_trajectory_df.index have the same\n            #   transformation id and loc/selection of them provides every match, not just passing_transform_ids\n            #   prescribed by optimization\n            #   - Mean value DOES NOT accurately reflect dataset, given the issue with selection of the same index...\n            # top_results_df = weighted_trajectory_df.loc[passing_transform_ids, selected_columns]\n            # Todo? Could also index the\n            #  top_results_df = total_results_df.loc[passing_transform_ids, selected_columns]\n\n            result = result_func(top_results_df.mean(axis=0))\n            # result = calculate_results_for_stopping(total_results_df, passing_transform_ids)\n            number_of_transforms = len(full_rotation1)\n            logger.info(f'Found {number_of_transforms} transformations after '\n                        f'{optimize_found_transformations_by_metrics.__name__} round {optimize_round} '\n                        f'with result={result}. last_result={last_result}')\n        else:\n            if optimize_round == 1:\n                number_top_indices = number_of_transforms\n                round1_number_of_clusters = len(round1_cluster_shape)\n                logger.info(f'Reducing round 1 expanded cluster search from {number_top_indices} '\n                            f'to {round1_number_of_clusters} transformations')\n                # Reduce the top_transform_cluster_indices to the best remaining in each optimize_round 1 cluster\n                # Important that indexing happens by the passing_transform_ids order, as these are in the order of\n                # the transformation search, while the weighted_trajectory_df is in the order of the job.dock.weight\n                top_scores_s = weighted_trajectory_df.loc[passing_transform_ids, metrics.selection_weight_column]\n                # Todo? Could also use\n                #  top_scores_s = total_results_df.loc[passing_transform_ids, metrics.selection_weight_column].mean(\n                #      axis=0)\n                # top_indices_of_cluster = []\n                # for i in range(round1_number_of_clusters):\n                #     # Slice by the expanded cluster amount\n                #     cluster_lower_bound = total_dof_perturbed * i\n                #     top_cluster_score = top_scores_s.iloc[cluster_lower_bound:\n                #                                           cluster_lower_bound + total_dof_perturbed].argmax()\n                #     top_indices_of_cluster.append(cluster_lower_bound + top_cluster_score)\n\n                # Filter down to the size of the original transforms from the cluster expansion\n                top_indices_of_cluster = []\n                cluster_lower_bound = 0\n                for cluster_idx, cluster_shape in enumerate(round1_cluster_shape):\n                    # This can't be less than 1 here...\n                    # if cluster_shape &gt; 0:\n                    cluster_upper_bound = cluster_lower_bound + cluster_shape\n                    top_cluster_score = top_scores_s.iloc[cluster_lower_bound:cluster_upper_bound].argmax()\n                    top_indices_of_cluster.append(cluster_lower_bound + top_cluster_score)\n                    # Set new lower bound\n                    cluster_lower_bound = cluster_upper_bound\n\n                passing_transform_ids = top_scores_s.iloc[top_indices_of_cluster].index.tolist()\n                # Filter hits down\n                filter_transforms_by_indices(top_indices_of_cluster)\n                # Narrow down the metrics by the selected_indices. If this is the last cycle, they will be written\n                poses_df = poses_df.loc[top_indices_of_cluster]\n                residues_df = residues_df.loc[top_indices_of_cluster]\n                # Reset the DataFrame.index\n                poses_df.index = residues_df.index = pd.RangeIndex(len(top_indices_of_cluster))\n                number_of_transforms = len(passing_transform_ids)\n\n            # Grab the passing_transform_ids according to the order of provided job.dock.weight, which follows the\n            # weighted_trajectory_df.index order, not transformations\n            # This sorts the final optimized indices and transfers this order to the pose outputs\n            # (current transform pool, poses_df, residues_df)\n            weighted_transform_ids = weighted_trajectory_df.index.tolist()\n            ordered_indices = [passing_transform_ids.index(_id) for _id in weighted_transform_ids]\n            # This calculation was used when weighted_trajectory_df contains extra measurements that aren't\n            # from passing_transform_ids. NOW all weighted_trajectory_df.index are passing_transform_ids\n            # passing_transform_indexer = np.isin(weighted_trajectory_df.index, passing_transform_ids)\n            # weighted_transform_ids = weighted_trajectory_df.index[passing_transform_indexer].tolist()\n            # # Check if there are degenerate transformation_hashes due to loss of resolution with the transform hasher\n            # if model_transform_hasher.translation_bin_width &gt; sum(translation_perturb_steps) or \\\n            #         model_transform_hasher.rotation_bin_width &gt; sum(rotation_steps):\n            #     logger.info(f\"Can't accurately describe each transformation due to resolution of the \"\n            #                 f\"{model_transform_hasher.__class__.__name__}\")\n            #     # Take the \"set\" of them\n            #     ordered_indices = utils.remove_duplicates(ordered_indices)\n            # Reorder hits and metrics by the ordered_indices\n            filter_transforms_by_indices(ordered_indices)\n            poses_df = poses_df.loc[ordered_indices]\n            residues_df = residues_df.loc[ordered_indices]\n\n        logger.info(f'Optimization complete, with {number_of_transforms} final transformations')\n    # Set the passing transformation identifiers as the trajectory metrics index\n    # These should all be the same order as w/ or w/o optimize_found_transformations_by_metrics() the order of\n    # passing_transform_ids is fetched from the order of the selected_indices and each _df is sorted accordingly\n    passing_index = pd.Index([f'{identifier:d}' for identifier in passing_transform_ids],\n                             name=sql.PoseMetrics.pose_id.name)\n    starting_transforms = len(passing_index)\n    # Deduplicate the indices by keeping the first instance\n    # The above sorting ensures that the first instance is the \"best\"\n    deduplicated_indices = ~passing_index.duplicated(keep='first')\n    # Filter data structures\n    passing_index = passing_index[deduplicated_indices]\n    poses_df = poses_df[deduplicated_indices]\n    residues_df = residues_df[deduplicated_indices]\n    filter_transforms_by_indices(np.flatnonzero(deduplicated_indices))\n    number_of_transforms = len(passing_index)\n    if starting_transforms != number_of_transforms:\n        logger.info(f'Removed {starting_transforms - number_of_transforms} due to transformation duplication')\n\n    # Finally, tabulate the ProteinMPNN metrics if they weren't already and are requested\n    if job.dock.proteinmpnn_score:\n        pass  # ProteinMPNN metrics already collected\n    elif job.use_proteinmpnn:  # Collect\n        logger.info(f'Measuring quality of docked interfaces with ProteinMPNN unconditional probabilities')\n        poses_df, residues_df = collect_dock_metrics(proteinmpnn_score=True)\n    poses_df.index = residues_df.index = passing_index\n    pose_names = poses_df.index.tolist()\n\n    def terminate(poses_df: pd.DataFrame, residues_df: pd.DataFrame) -&gt; list[PoseJob]:\n        \"\"\"Finalize any remaining work and return to the caller\"\"\"\n        # Extract transformation parameters for output\n        def populate_pose_metadata():\n            \"\"\"Add all required PoseJob information to output the created Pose instances for persistent storage\"\"\"\n            # Save all pose transformation information\n            # From here out, the transforms used should be only those of interest for outputting/sequence design\n            # filter_transforms_by_indices() &lt;- This is done above\n\n            # Format pose transformations for output\n            rotations1 = scipy.spatial.transform.Rotation.from_matrix(full_rotation1)\n            rotations2 = scipy.spatial.transform.Rotation.from_matrix(full_rotation2)\n            # Get all rotations in terms of the degree of rotation along the z-axis\n            # Using the x, y rotation to enforce the degeneracy matrix...\n            rotation_degrees_x1, rotation_degrees_y1, rotation_degrees_z1 = \\\n                zip(*rotations1.as_rotvec(degrees=True).tolist())\n            rotation_degrees_x2, rotation_degrees_y2, rotation_degrees_z2 = \\\n                zip(*rotations2.as_rotvec(degrees=True).tolist())\n\n            blank_parameter = list(repeat(None, number_of_transforms))\n            if sym_entry.is_internal_tx1:\n                nonlocal full_int_tx1\n                if len(full_int_tx1) &gt; 1:\n                    full_int_tx1 = full_int_tx1.squeeze()\n                z_heights1 = full_int_tx1[:, -1]\n            else:\n                z_heights1 = blank_parameter\n\n            if sym_entry.is_internal_tx2:\n                nonlocal full_int_tx2\n                if len(full_int_tx2) &gt; 1:\n                    full_int_tx2 = full_int_tx2.squeeze()\n                z_heights2 = full_int_tx2[:, -1]\n            else:\n                z_heights2 = blank_parameter\n\n            set_mat1_number, set_mat2_number, *_extra = sym_entry.setting_matrices_numbers\n            blank_parameters = list(repeat([None, None, None], number_of_transforms))\n            # if sym_entry.unit_cell:\n            #     full_uc_dimensions = full_uc_dimensions[passing_symmetric_clash_indices_perturb]\n            #     full_ext_tx1 = full_ext_tx1[:]\n            #     full_ext_tx2 = full_ext_tx2[:]\n            #     full_ext_tx_sum = full_ext_tx2 - full_ext_tx1\n            _full_ext_tx1 = blank_parameters if full_ext_tx1 is None else full_ext_tx1.squeeze()\n            _full_ext_tx2 = blank_parameters if full_ext_tx2 is None else full_ext_tx2.squeeze()\n\n            for idx, pose_job in enumerate(pose_jobs):\n                # Update the sql.EntityData with transformations\n                external_translation_x1, external_translation_y1, external_translation_z1 = _full_ext_tx1[idx]\n                external_translation_x2, external_translation_y2, external_translation_z2 = _full_ext_tx2[idx]\n                entity_transformations = [\n                    dict(\n                        rotation_x=rotation_degrees_x1[idx],\n                        rotation_y=rotation_degrees_y1[idx],\n                        rotation_z=rotation_degrees_z1[idx],\n                        internal_translation_z=z_heights1[idx],\n                        setting_matrix=set_mat1_number,\n                        external_translation_x=external_translation_x1,\n                        external_translation_y=external_translation_y1,\n                        external_translation_z=external_translation_z1),\n                    dict(\n                        rotation_x=rotation_degrees_x2[idx],\n                        rotation_y=rotation_degrees_y2[idx],\n                        rotation_z=rotation_degrees_z2[idx],\n                        internal_translation_z=z_heights2[idx],\n                        setting_matrix=set_mat2_number,\n                        external_translation_x=external_translation_x2,\n                        external_translation_y=external_translation_y2,\n                        external_translation_z=external_translation_z2)\n                ]\n\n                # Update sql.EntityData, sql.EntityMetrics, sql.EntityTransform\n                # pose_id = pose_job.id\n                # entity_data = []\n                # Todo the number of entities and the number of transformations could be different\n                entity_transforms = []\n                for entity, transform in zip(pose.entities, entity_transformations):\n                    transformation = sql.EntityTransform(**transform)\n                    entity_transforms.append(transformation)\n                    pose_job.entity_data.append(sql.EntityData(\n                        meta=entity.metadata,\n                        # metrics=entity.metrics,\n                        transform=transformation)\n                    )\n                # print('pose_job.entity_data', pose_job.entity_data)\n                # For whatever reason, this ^ print wouldn't print anything when above was written as:\n                # entity_data.append(sql.EntityData(pose=pose_job,\n                # And this warning occurred\n                # /home/kylemeador/symdesign/symdesign/protocols/fragdock.py:4394:\n                # SAWarning: Object of type &lt;EntityData&gt; not in session, add operation along 'EntityMetrics.entity'\n                # won't proceed\n                # It would print 4 objects, (2 of each EntityData) when this was written as:\n                # pose_job.entity_data.append(sql.EntityData(pose=pose_job,\n\n                session.add_all(entity_transforms)  # + entity_data)\n                # # Need to generate the EntityData.id\n                # session.flush()\n\n        pose_jobs = [PoseJob.from_name(pose_name, project=project, protocol=protocol_name)\n                     for pose_name in pose_names]\n\n        # Format output data, fix existing entries\n        # Populate the database with pose information. Has access to nonlocal session\n        populate_pose_metadata()\n        # Next, insert metadata information to database\n        pose_jobs = insert_pose_jobs(session, pose_jobs, project)\n\n        # For all new PoseJobs, insert them and their metrics into the database\n        remaining_pose_idx_name_pairs = []\n        for pose_job in pose_jobs:\n            name = pose_job.name\n            pose_name_index = pose_names.index(name)\n            if pose_name_index != -1:\n                remaining_pose_idx_name_pairs.append((pose_name_index, name))\n\n        # Finally, sort all the names to ensure that the indices from the first pass are accurate\n        # with the new set\n        remaining_indices, remaining_pose_names = zip(\n            *sorted(remaining_pose_idx_name_pairs, key=lambda name: name[0]))\n        # Select poses_df/residues_df by remaining remaining_pose_names\n        poses_df = poses_df.loc[remaining_pose_names, :]\n        residues_df = residues_df.loc[remaining_pose_names, :]\n        logger.debug(f'Reset the pose solutions with attributes:\\n'\n                     f'\\tpose names={remaining_pose_names}\\n'\n                     f'\\texisting transform indices={remaining_indices}\\n')\n        # number_of_transforms = len(remaining_pose_names)\n        filter_transforms_by_indices(list(remaining_indices))\n\n        # trajectory = TrajectoryMetadata(poses=pose_jobs, protocol=protocol)\n        # session.add(trajectory)\n\n        if job.db:\n            pose_ids = [pose_job.id for pose_job in pose_jobs]\n        else:\n            pose_ids = remaining_pose_names\n\n        if job.use_proteinmpnn:\n            # Explicitly set false as scoring the wild-type sequence isn't desired now\n            reset_use_proteinmpnn = True\n            job.use_proteinmpnn = False\n        else:\n            reset_use_proteinmpnn = False\n\n        for idx, pose_job in enumerate(pose_jobs):\n            # Add the next set of coordinates\n            update_pose_coords(idx)\n\n            if job.output_fragments:\n                add_fragments_to_pose()\n            if job.output_trajectory:\n                available_chain_ids = chain_id_generator()\n\n                def _exhaust_n_chain_ids(n: int) -&gt; str:\n                    for _ in range(n - 1):\n                        next(available_chain_ids)\n                    return next(available_chain_ids)\n\n                with open(os.path.join(project_dir, 'trajectory_oligomeric_models.pdb'), 'a') as f_traj:\n                    # pose.write(file_handle=f_traj, assembly=True)\n                    f_traj.write('{:9s}{:&gt;4d}\\n'.format('MODEL', idx + 1))\n                    model_specific_chain_id = next(available_chain_ids)\n                    for entity in pose.entities:\n                        starting_chain_id = entity.chain_id\n                        entity.chain_id = model_specific_chain_id\n                        entity.write(file_handle=f_traj, assembly=True)\n                        entity.chain_id = starting_chain_id\n                        model_specific_chain_id = _exhaust_n_chain_ids(entity.number_of_symmetry_mates)\n                    f_traj.write(f'ENDMDL\\n')\n\n            # Set the ASU, then write to a file\n            pose.set_contacting_asu(distance=cb_distance)\n            try:  # Remove existing cryst_record\n                del pose._cryst_record\n            except AttributeError:\n                pass\n            # pose.uc_dimensions\n            # if sym_entry.unit_cell:  # 2, 3 dimensions\n            #     cryst_record = generate_cryst1_record(full_uc_dimensions[idx], sym_entry.resulting_symmetry)\n            # else:\n            #     cryst_record = None\n            pose_job.pose = pose\n            pose_job.calculate_pose_design_metrics(session)\n            putils.make_path(pose_job.pose_directory)\n            pose_job.output_pose()\n            pose_job.source_path = pose_job.pose_path\n            pose_job.pose = None\n            logger.info(f'OUTPUT POSE: {pose_job.pose_directory}')\n\n            # Add metrics to the PoseJob then reset for next pose\n            for entity, data in zip(pose.entities, pose_job.entity_data):\n                data.metrics = entity.metrics\n                entity.clear_metrics()\n\n        # Output acquired metrics\n        if job.db:\n            # Update the poses_df and residues_df index to reflect the new pose_ids\n            poses_df.index = pd.Index(pose_ids, name=sql.PoseMetrics.pose_id.name)\n            # Write dataframes to the sql database\n            sql.write_dataframe(session, poses=poses_df)\n            output_residues = False\n            if output_residues:  # Todo job.metrics.residues\n                residues_df.index = pd.Index(pose_ids, name=sql.PoseResidueMetrics.pose_id.name)\n                sql.write_dataframe(session, pose_residues=residues_df)\n        else:  # Write to disk\n            residues_df.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)  # ascending=False\n            putils.make_path(job.all_scores)\n            residue_metrics_csv = os.path.join(job.all_scores, f'{building_blocks}_docked_poses_Residues.csv')\n            residues_df.to_csv(residue_metrics_csv)\n            logger.info(f'Wrote residue metrics to {residue_metrics_csv}')\n            trajectory_metrics_csv = \\\n                os.path.join(job.all_scores, f'{building_blocks}_docked_poses_Trajectories.csv')\n            job.dataframe = trajectory_metrics_csv\n            poses_df = pd.concat([poses_df], keys=[('dock', 'pose')], axis=1)\n            poses_df.columns = poses_df.columns.swaplevel(0, 1)\n            poses_df.sort_index(level=2, axis=1, inplace=True, sort_remaining=False)\n            poses_df.sort_index(level=1, axis=1, inplace=True, sort_remaining=False)\n            poses_df.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)\n            poses_df.to_csv(trajectory_metrics_csv)\n            logger.info(f'Wrote trajectory metrics to {trajectory_metrics_csv}')\n\n        # After pose output loop\n        # Explicitly set false as scoring the wildtype sequence isn't desired now\n        if reset_use_proteinmpnn:\n            job.use_proteinmpnn = True\n\n        # # Todo 2 modernize with the new SQL database and 6D transform aspirations\n        # # Cluster by perturbation if perturb_dof:\n        # if number_perturbations_applied &gt; 1:\n        #     perturbation_identifier = '-p_'\n        #     cluster_type_str = 'ByPerturbation'\n        #     seed_transforms = utils.remove_duplicates(\n        #         [pose_name.split(perturbation_identifier)[0] for pose_name in remaining_pose_names])\n        #     cluster_map = {seed_transform: remaining_pose_names[idx * number_perturbations_applied:\n        #                                               (idx + 1) * number_perturbations_applied]\n        #                    for idx, seed_transform in enumerate(seed_transforms)}\n        #     # for pose_name in remaining_pose_names:\n        #     #     seed_transform, *perturbation = pose_name.split(perturbation_identifier)\n        #     #     clustered_transformations[seed_transform].append(pose_name)\n        #\n        #     # Set the number of poses to cluster equal to the sqrt of the search area\n        #     job.cluster.number = math.sqrt(number_perturbations_applied)\n        # else:\n        #     cluster_type_str = 'ByTransformation'\n        #     cluster_map = cluster.cluster_by_transformations(*create_transformation_group(),\n        #                                                      values=project_pose_names)\n        # # Output clustering results\n        # job.cluster.map = utils.pickle_object(cluster_map,\n        #                                       name=putils.default_clustered_pose_file.format('', cluster_type_str),\n        #                                       out_path=project_dir)\n        # logger.info(f'Found {len(cluster_map)} unique clusters from {len(remaining_pose_names)} pose inputs. '\n        #             f'Wrote cluster map to {job.cluster.map}')\n\n        return pose_jobs\n\n    # Clean up, save data/output results\n    with job.db.session(expire_on_commit=False) as session:\n        pose_jobs = terminate(poses_df, residues_df)\n        session.commit()\n        metrics_stmt = select(PoseJob).where(PoseJob.id.in_([pose_job.id for pose_job in pose_jobs])) \\\n            .execution_options(populate_existing=True) \\\n            .options(selectinload(PoseJob.metrics))\n        pose_jobs = session.scalars(metrics_stmt).all()\n\n    logger.info(f'Total {building_blocks} dock trajectory took {time.time() - frag_dock_time_start:.2f}s')\n\n    return pose_jobs\n</code></pre>"},{"location":"reference/protocols/pose/","title":"pose","text":""},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory","title":"PoseDirectory","text":"<pre><code>PoseDirectory(root_directory: AnyStr = None, project: str = None, name: str = None, **kwargs)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>root_directory</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The base of the directory tree that houses all project directories</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the project where the files should be stored in the root_directory</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the pose, which becomes the final dirname where all files are stored</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def __init__(self, root_directory: AnyStr = None, project: str = None, name: str = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        root_directory: The base of the directory tree that houses all project directories\n        project: The name of the project where the files should be stored in the root_directory\n        name: The name of the pose, which becomes the final dirname where all files are stored\n    \"\"\"\n    self.info: dict = {}\n    \"\"\"Internal state info\"\"\"\n    self._info: dict = {}\n    \"\"\"Internal state info at load time\"\"\"\n\n    # try:\n    directory = os.path.join(root_directory, project, name)\n    # except TypeError:  # Can't pass None\n    #     missing_args = \", \".join((str_ for str_, arg in ((\"root_directory\", root_directory),\n    #                                                     (\"project\", project), (\"name\", name))\n    #                               if arg is None))\n    #     raise TypeError(f'{PoseDirectory.__name__} is missing the required arguments {missing_args}')\n    # if directory is not None:\n    self.pose_directory = directory\n    # PoseDirectory attributes. Set after finding correct path\n    self.log_path: str | Path = os.path.join(self.pose_directory, f'{name}.log')\n    self.designs_path: str | Path = os.path.join(self.pose_directory, 'designs')\n    # /root/Projects/project_Poses/design/designs\n    self.scripts_path: str | Path = os.path.join(self.pose_directory, 'scripts')\n    # /root/Projects/project_Poses/design/scripts\n    self.frags_path: str | Path = os.path.join(self.pose_directory, putils.frag_dir)\n    # /root/Projects/project_Poses/design/matching_fragments\n    self.flags: str | Path = os.path.join(self.scripts_path, 'flags')\n    # /root/Projects/project_Poses/design/scripts/flags\n    self.data_path: str | Path = os.path.join(self.pose_directory, putils.data)\n    # /root/Projects/project_Poses/design/data\n    self.scores_file: str | Path = os.path.join(self.data_path, f'{name}.sc')\n    # /root/Projects/project_Poses/design/data/name.sc\n    self.serialized_info: str | Path = os.path.join(self.data_path, 'info.pkl')\n    # /root/Projects/project_Poses/design/data/info.pkl\n    self.pose_path: str | Path = os.path.join(self.pose_directory, f'{name}.pdb')\n    self.asu_path: str | Path = os.path.join(self.pose_directory, 'asu.pdb')\n    # /root/Projects/project_Poses/design/asu.pdb\n    # self.asu_path: str | Path = os.path.join(self.pose_directory, f'{self.name}_{putils.asu}')\n    # # /root/Projects/project_Poses/design/design_name_asu.pdb\n    self.assembly_path: str | Path = os.path.join(self.pose_directory, f'{name}_assembly.pdb')\n    # /root/Projects/project_Poses/design/design_name_assembly.pdb\n    self.refine_pdb: str | Path = os.path.join(self.data_path, os.path.basename(self.pose_path))\n    # /root/Projects/project_Poses/design/data/design_name.pdb\n    self.consensus_pdb: str | Path = f'{os.path.splitext(self.pose_path)[0]}_for_consensus.pdb'\n    # /root/Projects/project_Poses/design/design_name_for_consensus.pdb\n    # self.consensus_design_pdb: str | Path = os.path.join(self.designs_path, os.path.basename(self.consensus_pdb))\n    # # /root/Projects/project_Poses/design/designs/design_name_for_consensus.pdb\n    self.design_profile_file: str | Path = os.path.join(self.data_path, 'design.pssm')\n    # /root/Projects/project_Poses/design/data/design.pssm\n    self.evolutionary_profile_file: str | Path = os.path.join(self.data_path, 'evolutionary.pssm')\n    # /root/Projects/project_Poses/design/data/evolutionary.pssm\n    self.fragment_profile_file: str | Path = os.path.join(self.data_path, 'fragment.pssm')\n    # /root/Projects/project_Poses/design/data/fragment.pssm\n    # These next two files may be present from NanohedraV1 outputs\n    # self.pose_file = os.path.join(self.pose_directory, putils.pose_file)\n    # self.frag_file = os.path.join(self.frags_path, putils.frag_text_file)\n    # These files are used as output from analysis protocols\n    # self.designs_metrics_csv = os.path.join(self.job.all_scores, f'{self}_Trajectories.csv')\n    self.designs_metrics_csv = os.path.join(self.data_path, f'designs.csv')\n    self.residues_metrics_csv = os.path.join(self.data_path, f'residues.csv')\n    # self.designed_sequences_file = os.path.join(self.job.all_scores, f'{self}_Sequences.pkl')\n    self.designed_sequences_file = os.path.join(self.designs_path, f'sequences.fasta')\n    self.current_script = None\n\n    if self.job.output_directory:\n        self.output_path = self.job.output_directory\n        self.output_modifier = f'{project}-{name}'\n        \"\"\"string with contents '{self.project}-{self.name}'\"\"\"\n        self.output_pose_path = os.path.join(self.output_path, f'{self.output_modifier}.pdb')\n        \"\"\"/output_path/{self.output_modifier}.pdb\"\"\"\n        # self.output_asu_path: str | Path = os.path.join(self.output_path, f'{self.output_modifier}_{putils.asu}')\n        # \"\"\"/output_path/{self.output_modifier}_asu.pdb\"\"\"\n        self.output_assembly_path: str | Path = \\\n            os.path.join(self.output_path, f'{self.output_modifier}_assembly.pdb')\n        \"\"\"/output_path/{self.output_modifier}_{putils.assembly}.pdb \"\"\"\n        # pose_directory = self.job.output_directory  # /output_directory &lt;- self.pose_directory/design.pdb\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.info","title":"info  <code>instance-attribute</code>","text":"<pre><code>info: dict = {}\n</code></pre> <p>Internal state info</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.output_modifier","title":"output_modifier  <code>instance-attribute</code>","text":"<pre><code>output_modifier = f'{project}-{name}'\n</code></pre> <p>string with contents '{self.project}-{self.name}'</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.output_pose_path","title":"output_pose_path  <code>instance-attribute</code>","text":"<pre><code>output_pose_path = join(output_path, f'{output_modifier}.pdb')\n</code></pre> <p>/output_path/{self.output_modifier}.pdb</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.output_assembly_path","title":"output_assembly_path  <code>instance-attribute</code>","text":"<pre><code>output_assembly_path: str | Path = join(output_path, f'{output_modifier}_assembly.pdb')\n</code></pre> <p>/output_path/{self.output_modifier}_{putils.assembly}.pdb</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.job","title":"job  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>job\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.refined","title":"refined  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>refined\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.refined_pdb","title":"refined_pdb  <code>property</code>","text":"<pre><code>refined_pdb: str\n</code></pre> <p>Access the file which holds refined coordinates. These are not necessaryily refined, but if a file exists in the designs_path with the pose name, it was generated from either Rosetta energy function minimization or from structure prediction, which is assumed to be refined. If none, exists, use the self.pose_path</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.symmetry_definition_files","title":"symmetry_definition_files  <code>property</code>","text":"<pre><code>symmetry_definition_files: list[AnyStr]\n</code></pre> <p>Retrieve the symmetry definition files name from PoseJob</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.designed_sequences","title":"designed_sequences  <code>property</code>","text":"<pre><code>designed_sequences: list[Sequence]\n</code></pre> <p>Return the designed sequences for the entire Pose associated with the PoseJob</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.get_pose_file","title":"get_pose_file","text":"<pre><code>get_pose_file() -&gt; AnyStr\n</code></pre> <p>Retrieve the pose file name from PoseJob</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def get_pose_file(self) -&gt; AnyStr:\n    \"\"\"Retrieve the pose file name from PoseJob\"\"\"\n    # glob_target = os.path.join(self.pose_directory, f'{self.name}*.pdb')\n    glob_target = self.pose_path\n    source = sorted(glob(glob_target))\n    if len(source) &gt; 1:\n        raise InputError(\n            f'Found {len(source)} files matching the path \"{glob_target}\". No more than one expected')\n    else:\n        try:\n            file = source[0]\n        except IndexError:  # glob found no files\n            self.log.debug(f\"Couldn't find any structural files matching the path '{glob_target}'\")\n            file = None\n            raise FileNotFoundError\n\n    return file\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseDirectory.get_design_files","title":"get_design_files","text":"<pre><code>get_design_files(design_type: str = '') -&gt; list[AnyStr]\n</code></pre> <p>Return the paths of all design files in a PoseJob</p> <p>Parameters:</p> <ul> <li> <code>design_type</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>Specify if a particular type of design should be selected by a \"type\" string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[AnyStr]</code>         \u2013          <p>The sorted design files found in the designs directory with an absolute path</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def get_design_files(self, design_type: str = '') -&gt; list[AnyStr]:\n    \"\"\"Return the paths of all design files in a PoseJob\n\n    Args:\n        design_type: Specify if a particular type of design should be selected by a \"type\" string\n\n    Returns:\n        The sorted design files found in the designs directory with an absolute path\n    \"\"\"\n    return sorted(glob(os.path.join(self.designs_path, f'*{design_type}*.pdb*')))\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData","title":"PoseData","text":"<pre><code>PoseData(name: str = None, project: str = None, source_path: AnyStr = None, pose: Pose = None, protocol: str = None, pose_source: DesignData = None, **kwargs)\n</code></pre> <p>             Bases: <code>PoseDirectory</code>, <code>PoseMetadata</code></p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The identifier</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project which this work belongs too</p> </li> <li> <code>source_path</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>If a path exists, where is structure files information stored?</p> </li> <li> <code>protocol</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a protocol was used to generate this Pose, which protocol?</p> </li> <li> <code>pose_source</code>             (<code>DesignData</code>, default:                 <code>None</code> )         \u2013          <p>If this is a descendant of another Design, which one?</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def __init__(self, name: str = None, project: str = None, source_path: AnyStr = None, pose: Pose = None,\n             protocol: str = None, pose_source: sql.DesignData = None,\n             # pose_transformation: Sequence[types.TransformationMapping] = None,\n             # entity_metadata: list[sql.EntityData] = None,\n             # entity_names: Sequence[str] = None,\n             # specific_designs: Sequence[str] = None, directives: list[dict[int, str]] = None,\n             **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        name: The identifier\n        project: The project which this work belongs too\n        source_path: If a path exists, where is structure files information stored?\n        protocol: If a protocol was used to generate this Pose, which protocol?\n        pose_source: If this is a descendant of another Design, which one?\n    \"\"\"\n    # PoseJob attributes\n    self.name = name\n    self.project = project\n    self.source_path = source_path\n\n    # Most __init__ code is called in __init_from_db__() according to sqlalchemy needs and DRY principles\n    self.__init_from_db__()\n    if pose is not None:  # It should be saved\n        self.initial_pose = pose\n        self.initial_pose.write(out_path=self.pose_path)\n        self.source_path = self.pose_path\n\n    # Save job variables to the state during initialization\n    sym_entry = kwargs.get('sym_entry')\n    if sym_entry:\n        self.sym_entry = sym_entry\n\n    if self.sym_entry:\n        self.symmetry_dimension = self.sym_entry.dimension\n        \"\"\"The dimension of the SymEntry\"\"\"\n        self.symmetry = self.sym_entry.resulting_symmetry\n        \"\"\"The resulting symmetry of the SymEntry\"\"\"\n        self.sym_entry_specification = self.sym_entry.specification\n        \"\"\"The specification string of the SymEntry\"\"\"\n        self.sym_entry_number = self.sym_entry.number\n        \"\"\"The SymEntry entry number\"\"\"\n\n    # Set up original DesignData entry for the pose baseline\n    if pose_source is None:\n        pose_source = sql.DesignData(name=name, pose=self, design_parent=None)  # , structure_path=source_path)\n    else:\n        pose_source = sql.DesignData(name=name, pose=self, design_parent=pose_source,\n                                     structure_path=pose_source.structure_path)\n    if protocol is not None:\n        pose_source.protocols.append(sql.DesignProtocol(protocol=protocol, job_id=self.job.id))\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.initial_pose","title":"initial_pose  <code>instance-attribute</code>","text":"<pre><code>initial_pose: Pose | None\n</code></pre> <p>Used if the structure has never been initialized previously</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.pose","title":"pose  <code>instance-attribute</code>","text":"<pre><code>pose: Pose | None\n</code></pre> <p>Contains the Pose object</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.protocol","title":"protocol  <code>instance-attribute</code>","text":"<pre><code>protocol: str | None\n</code></pre> <p>The name of the currently utilized protocol for file naming and metric results</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.symmetry_dimension","title":"symmetry_dimension  <code>instance-attribute</code>","text":"<pre><code>symmetry_dimension = dimension\n</code></pre> <p>The dimension of the SymEntry</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.symmetry","title":"symmetry  <code>instance-attribute</code>","text":"<pre><code>symmetry = resulting_symmetry\n</code></pre> <p>The resulting symmetry of the SymEntry</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.sym_entry_specification","title":"sym_entry_specification  <code>instance-attribute</code>","text":"<pre><code>sym_entry_specification = specification\n</code></pre> <p>The specification string of the SymEntry</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.sym_entry_number","title":"sym_entry_number  <code>instance-attribute</code>","text":"<pre><code>sym_entry_number = number\n</code></pre> <p>The SymEntry entry number</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.new_pose_identifier","title":"new_pose_identifier  <code>property</code>","text":"<pre><code>new_pose_identifier: str\n</code></pre> <p>Return the pose_identifier when the PoseData isn't part of the database</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.directives","title":"directives  <code>property</code> <code>writable</code>","text":"<pre><code>directives: list[dict[int, str]]\n</code></pre> <p>The design directives given to each design in current_designs to guide further sampling</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.current_designs","title":"current_designs  <code>property</code> <code>writable</code>","text":"<pre><code>current_designs: list[DesignData] | list\n</code></pre> <p>DesignData instances which were generated in the scope of this job and serve as a pool for additional work</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.structure_source","title":"structure_source  <code>property</code> <code>writable</code>","text":"<pre><code>structure_source: AnyStr\n</code></pre> <p>Return the source of the Pose structural information for the PoseJob</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.sym_entry","title":"sym_entry  <code>property</code> <code>writable</code>","text":"<pre><code>sym_entry: SymEntry | None\n</code></pre> <p>The SymEntry</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.job_kwargs","title":"job_kwargs  <code>property</code>","text":"<pre><code>job_kwargs: dict[str, Any]\n</code></pre> <p>Returns the keyword args that initialize the Pose given program input and PoseJob state</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.pose_kwargs","title":"pose_kwargs  <code>property</code>","text":"<pre><code>pose_kwargs: dict[str, Any]\n</code></pre> <p>Returns the keyword args that initialize the Pose given program input and PoseJob state</p>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.design_selector","title":"design_selector  <code>property</code> <code>writable</code>","text":"<pre><code>design_selector: PoseSpecification\n</code></pre> <p>Provide the design_selector parameters for the design in question</p> <p>Returns:</p> <ul> <li> <code>PoseSpecification</code>         \u2013          <p>A mapping of the selection criteria for StructureBase objects in the Pose</p> </li> </ul>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.refined","title":"refined  <code>property</code>","text":"<pre><code>refined: bool\n</code></pre> <p>Provide the state attribute regarding the source files status as \"previously refined\"</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>Whether refinement has occurred</p> </li> </ul>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.loop_modeled","title":"loop_modeled  <code>property</code>","text":"<pre><code>loop_modeled: bool\n</code></pre> <p>Provide the state attribute regarding the source files status as \"previously loop modeled\"</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>Whether loop modeling has occurred</p> </li> </ul>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_path","title":"from_path  <code>classmethod</code>","text":"<pre><code>from_path(path: str, project: str = None, **kwargs)\n</code></pre> <p>Load the PoseJob from a path with file types or a directory</p> <p>Parameters:</p> <ul> <li> <code>path</code>             (<code>str</code>)         \u2013          <p>The path where the PoseJob instance should load structural data</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project where the file should be included</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_path(cls, path: str, project: str = None, **kwargs):\n    \"\"\"Load the PoseJob from a path with file types or a directory\n\n    Args:\n        path: The path where the PoseJob instance should load structural data\n        project: The project where the file should be included\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(\n            f\"The specified {cls.__name__} path '{path}' wasn't found\")\n    filename, extension = os.path.splitext(path)\n    # if 'pdb' in extension or 'cif' in extension:  # Initialize from input file\n    if extension != '':  # Initialize from input file\n        project_path, name = os.path.split(filename)\n        if project is None:\n            remainder, project = os.path.split(project_path)\n            if project == '':\n                raise InputError(\n                    f\"Couldn't get the project from the path '{path}'. Please provide \"\n                    f\"project name with {flags.format_args(flags.project_name_args)}\")\n\n        return cls(name=name, project=project, source_path=path, **kwargs)\n    elif os.path.isdir(path):\n        # Same as from_directory. This is an existing pose_identifier that hasn't been initialized\n        try:\n            name, project, *_ = reversed(path.split(os.sep))\n        except ValueError:  # Only got 1 value during unpacking... This isn't a \"pose_directory\" identifier\n            raise InputError(\n                f\"Couldn't coerce {path} to a {cls.__name__}. The directory must contain the \"\n                f'\"project{os.sep}pose_name\" string')\n        return cls(name=name, project=project, **kwargs)\n    else:\n        raise InputError(\n            f\"{cls.__name__} couldn't load the specified source path '{path}'\")\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(file: str, project: str = None, **kwargs)\n</code></pre> <p>Load the PoseJob from a structural file including .pdb/.cif file types</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>str</code>)         \u2013          <p>The file where the PoseJob instance should load structure files</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project where the file should be included</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_file(cls, file: str, project: str = None, **kwargs):\n    \"\"\"Load the PoseJob from a structural file including .pdb/.cif file types\n\n    Args:\n        file: The file where the PoseJob instance should load structure files\n        project: The project where the file should be included\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    # file = os.path.abspath(file)\n    if not os.path.exists(file):\n        raise FileNotFoundError(\n            f\"The specified {cls.__name__} structure source file '{file}' wasn't found!\")\n    filename, extension = os.path.splitext(file)\n    # if 'pdb' in extension or 'cif' in extension:  # Initialize from input file\n    if extension != '':  # Initialize from input file\n        project_path, name = os.path.split(filename)\n    # elif os.path.isdir(design_path):  # This is a pose_name that hasn't been initialized\n    #     file = None\n    #     # self.project_path = os.path.dirname(design_path)\n    #     # self.project = os.path.basename(self.project_path)\n    #     # self.pose_identifier = f'{self.project}{os.sep}{self.name}'\n    else:\n        raise InputError(\n            f\"{cls.__name__} couldn't load the specified source file '{file}'\")\n\n    # if self.job.nanohedra_outputV1:\n    #     # path/to/design_symmetry/building_blocks/degen/rot/tx\n    #     # path_components[-4] are the oligomeric (building_blocks) names\n    #     name = '-'.join(path_components[-4:])\n    #     project = path_components[-5]  # if root is None else root\n    #     file = os.path.join(file, putils.asu)\n\n    if project is None:\n        remainder, project = os.path.split(project_path)\n        if project == '':\n            raise InputError(\n                f\"Couldn't get the project from the path '{file}'. Please provide \"\n                f\"project name with {flags.format_args(flags.project_name_args)}\")\n\n    return cls(name=name, project=project, source_path=file, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_directory","title":"from_directory  <code>classmethod</code>","text":"<pre><code>from_directory(source_path: str, **kwargs)\n</code></pre> <p>Assumes the PoseJob is constructed from the pose_name (project/pose_name) and job.projects</p> <p>Parameters:</p> <ul> <li> <code>source_path</code>             (<code>str</code>)         \u2013          <p>The path to the directory where PoseJob information is stored</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_directory(cls, source_path: str, **kwargs):\n    \"\"\"Assumes the PoseJob is constructed from the pose_name (project/pose_name) and job.projects\n\n    Args:\n        source_path: The path to the directory where PoseJob information is stored\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    try:\n        name, project, *_ = reversed(source_path.split(os.sep))\n    except ValueError:  # Only got 1 value during unpacking... This isn't a \"pose_directory\" identifier\n        raise InputError(\n            f\"Couldn't coerce {source_path} to a {cls.__name__}. The directory must contain the \"\n            f'\"project{os.sep}pose_name\" string')\n\n    return cls(name=name, project=project, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_name","title":"from_name  <code>classmethod</code>","text":"<pre><code>from_name(name: str = None, project: str = None, **kwargs)\n</code></pre> <p>Load the PoseJob from the name and project</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name to identify this PoseJob</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project where the file should be included</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_name(cls, name: str = None, project: str = None, **kwargs):\n    \"\"\"Load the PoseJob from the name and project\n\n    Args:\n        name: The name to identify this PoseJob\n        project: The project where the file should be included\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    return cls(name=name, project=project, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_pose_identifier","title":"from_pose_identifier  <code>classmethod</code>","text":"<pre><code>from_pose_identifier(pose_identifier: str, **kwargs)\n</code></pre> <p>Load the PoseJob from the name and project</p> <p>Parameters:</p> <ul> <li> <code>pose_identifier</code>             (<code>str</code>)         \u2013          <p>The project and the name concatenated that identify this PoseJob</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_pose_identifier(cls, pose_identifier: str, **kwargs):\n    \"\"\"Load the PoseJob from the name and project\n\n    Args:\n        pose_identifier: The project and the name concatenated that identify this PoseJob\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    try:\n        project, name = pose_identifier.split(os.sep)\n    except ValueError:  # We don't have a pose_identifier\n        raise InputError(\n            f\"Couldn't coerce {pose_identifier} to 'project' {os.sep} 'name'. Please ensure the \"\n            f'pose_identifier is passed with the \"project{os.sep}name\" string')\n    return cls(name=name, project=project, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.from_pose","title":"from_pose  <code>classmethod</code>","text":"<pre><code>from_pose(pose: Pose, project: str = None, **kwargs)\n</code></pre> <p>Load the PoseJob from an existing Pose</p> <p>Parameters:</p> <ul> <li> <code>pose</code>             (<code>Pose</code>)         \u2013          <p>The Pose to initialize the PoseJob with</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project where the file should be included</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>The PoseJob instance</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@classmethod\ndef from_pose(cls, pose: Pose, project: str = None, **kwargs):\n    \"\"\"Load the PoseJob from an existing Pose\n\n    Args:\n        pose: The Pose to initialize the PoseJob with\n        project: The project where the file should be included\n\n    Returns:\n        The PoseJob instance\n    \"\"\"\n    return cls(name=pose.name, project=project, pose=pose, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.__init_from_db__","title":"__init_from_db__","text":"<pre><code>__init_from_db__()\n</code></pre> <p>Initialize PoseData after the instance is \"initialized\", i.e. loaded from the database</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>@reconstructor\ndef __init_from_db__(self):\n    \"\"\"Initialize PoseData after the instance is \"initialized\", i.e. loaded from the database\"\"\"\n    # Design attributes\n    # self.current_designs = []\n    self.measure_evolution = self.measure_alignment = None\n    self.pose = self.initial_pose = self.protocol = None\n    # Get the main program options\n    self.job = resources.job.job_resources_factory.get()\n    # Symmetry attributes\n    # If a new sym_entry is provided it wouldn't be saved to the state but could be attempted to be used\n    if self.job.sym_entry is not None:\n        self.sym_entry = self.job.sym_entry\n\n    if self.job.design_selector:\n        self.design_selector = self.job.design_selector\n\n    # self.specific_designs_file_paths = []\n    # \"\"\"Contains the various file paths for each design of interest according to self.specific_designs\"\"\"\n\n    # These arguments are for PoseDirectory\n    # directory = os.path.join(self.job.projects, self.project, self.name)\n    super().__init__(root_directory=self.job.projects, project=self.project, name=self.name)  # Todo **kwargs)\n\n    putils.make_path(self.pose_directory)  # , condition=self.job.construct_pose)\n\n    # Initialize the logger for the PoseJob\n    if self.job.debug:\n        log_path = None\n        handler = level = 1  # Defaults to stdout, debug is level 1\n        # Don't propagate, emit ourselves\n        propagate = no_log_name = False\n    elif self.log_path:\n        log_path = self.log_path\n        handler = level = 2  # To a file\n        propagate = no_log_name = True\n    else:  # Log to the __main__ file logger\n        log_path = None\n        handler = level = 2  # To a file\n        propagate = no_log_name = False\n\n    if self.job.skip_logging:  # Set up null_logger\n        self.log = logging.getLogger('null')\n    else:\n        self.log = start_log(name=f'pose.{self.project}.{self.name}', handler=handler, level=level,\n                             location=log_path, no_log_name=no_log_name, propagate=propagate)\n        # propagate=True allows self.log to pass messages to 'pose' and 'project' logger\n\n    # Configure standard pose loading mechanism with self.source_path\n    if self.source_path is None:\n        try:\n            self.source_path = self.get_pose_file()\n        except FileNotFoundError:\n            if os.path.exists(self.asu_path):  # Standard mechanism of loading the pose\n                self.source_path = self.asu_path\n            else:\n                self.source_path = None\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.clear_state","title":"clear_state","text":"<pre><code>clear_state()\n</code></pre> <p>Set the current instance structural and sequence attributes to None to remove excess memory</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def clear_state(self):\n    \"\"\"Set the current instance structural and sequence attributes to None to remove excess memory\"\"\"\n    self.measure_evolution = self.measure_alignment = self.pose = self.initial_pose = None\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.use_specific_designs","title":"use_specific_designs","text":"<pre><code>use_specific_designs(designs: Sequence[str] = None, directives: list[dict[int, str]] = None, **kwargs)\n</code></pre> <p>Set up the instance with the names and instructions to perform further sequence design</p> <p>Parameters:</p> <ul> <li> <code>designs</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>The names of designs which to include in modules that support PoseJob designs</p> </li> <li> <code>directives</code>             (<code>list[dict[int, str]]</code>, default:                 <code>None</code> )         \u2013          <p>Instructions to guide further sampling of designs</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def use_specific_designs(self, designs: Sequence[str] = None, directives: list[dict[int, str]] = None,\n                         **kwargs):\n    \"\"\"Set up the instance with the names and instructions to perform further sequence design\n\n    Args:\n        designs: The names of designs which to include in modules that support PoseJob designs\n        directives: Instructions to guide further sampling of designs\n    \"\"\"\n    if designs:\n        self.current_designs = designs\n\n    if directives:\n        self.directives = directives\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.load_initial_pose","title":"load_initial_pose","text":"<pre><code>load_initial_pose()\n</code></pre> <p>Loads the Pose at the source_path attribute</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def load_initial_pose(self):\n    \"\"\"Loads the Pose at the source_path attribute\"\"\"\n    if self.pose:\n        self.initial_pose = self.pose\n    elif self.initial_pose is None:\n        if self.source_path is None:\n            raise InputError(\n                f\"Couldn't {self.load_initial_pose.__name__}() for {self.name} as there isn't a file specified\")\n        self.initial_pose = Pose.from_file(self.source_path, log=self.log)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.is_symmetric","title":"is_symmetric","text":"<pre><code>is_symmetric() -&gt; bool\n</code></pre> <p>Is the PoseJob symmetric?</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def is_symmetric(self) -&gt; bool:\n    \"\"\"Is the PoseJob symmetric?\"\"\"\n    return self.sym_entry is not None\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.get_designs_without_structure","title":"get_designs_without_structure","text":"<pre><code>get_designs_without_structure() -&gt; list[DesignData]\n</code></pre> <p>For each design, access whether there is a structure that exists for it. If not, return the design</p> <p>Returns:</p> <ul> <li> <code>list[DesignData]</code>         \u2013          <p>Each instance of the DesignData that is missing a structure</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def get_designs_without_structure(self) -&gt; list[sql.DesignData]:\n    \"\"\"For each design, access whether there is a structure that exists for it. If not, return the design\n\n    Returns:\n        Each instance of the DesignData that is missing a structure\n    \"\"\"\n    missing = []\n    for design in self.designs[1:]:  # Slice only real designs, not the pose_source\n        if design.structure_path and os.path.exists(design.structure_path):\n            continue\n        else:\n            missing.append(design)\n\n    return missing\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.transform_entities_to_pose","title":"transform_entities_to_pose","text":"<pre><code>transform_entities_to_pose(**kwargs) -&gt; list[Entity]\n</code></pre> <p>Take the set of entities involved in a pose composition and transform them from a standard reference frame to the Pose reference frame using the pose_transformation attribute</p> <p>Other Parameters:</p> <ul> <li> <code>refined</code>         \u2013          <p>bool = True - Whether to use refined models from the StructureDatabase</p> </li> <li> <code>oriented</code>         \u2013          <p>bool = False - Whether to use oriented models from the StructureDatabase</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def transform_entities_to_pose(self, **kwargs) -&gt; list[Entity]:\n    \"\"\"Take the set of entities involved in a pose composition and transform them from a standard reference frame to\n    the Pose reference frame using the pose_transformation attribute\n\n    Keyword Args:\n        refined: bool = True - Whether to use refined models from the StructureDatabase\n        oriented: bool = False - Whether to use oriented models from the StructureDatabase\n    \"\"\"\n    entities = self.get_entities(**kwargs)\n    if self.transformations:\n        entities = [entity.get_transformed_copy(**transformation)\n                    for entity, transformation in zip(entities, self.transformations)]\n        self.log.debug('Entities were transformed to the found docking parameters')\n    else:\n        # Todo change below to handle asymmetric cases...\n        self.log.error(missing_pose_transformation)\n\n    return entities\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.transform_structures_to_pose","title":"transform_structures_to_pose","text":"<pre><code>transform_structures_to_pose(structures: Iterable[StructureBase], **kwargs) -&gt; list[StructureBase]\n</code></pre> <p>Take a set of ContainsResidues instances and transform them from a standard reference frame to the Pose reference frame using the pose_transformation attribute</p> <p>Parameters:</p> <ul> <li> <code>structures</code>             (<code>Iterable[StructureBase]</code>)         \u2013          <p>The ContainsResidues instances to transform</p> </li> </ul> <p>Returns:     The transformed ContainsResidues instances if a transformation was possible</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def transform_structures_to_pose(\n    self, structures: Iterable[StructureBase], **kwargs\n) -&gt; list[StructureBase]:\n    \"\"\"Take a set of ContainsResidues instances and transform them from a standard reference frame to the Pose reference\n    frame using the pose_transformation attribute\n\n    Args:\n        structures: The ContainsResidues instances to transform\n    Returns:\n        The transformed ContainsResidues instances if a transformation was possible\n    \"\"\"\n    if self.transformations:\n        # Todo\n        #  Assumes a 1:1 correspondence between structures and transforms (component group numbers) CHANGE\n        structures = [structure.get_transformed_copy(**transformation)\n                      for structure, transformation in zip(structures, self.transformations)]\n        self.log.debug('Structures were transformed to the found docking parameters')\n    else:\n        self.log.error(missing_pose_transformation)\n        structures = list(structures)\n\n    return structures\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.get_entities","title":"get_entities","text":"<pre><code>get_entities(refined: bool = True, oriented: bool = False, **kwargs) -&gt; list[Entity]\n</code></pre> <p>Retrieve Entity files from the design Database using either the oriented directory, or the refined directory. If these don't exist, use the Pose directory, and load them into job for further processing</p> <p>Parameters:</p> <ul> <li> <code>refined</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to use the refined directory</p> </li> <li> <code>oriented</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use the oriented directory</p> </li> </ul> <p>Returns:     The list of Entity instances that belong to this PoseData</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def get_entities(self, refined: bool = True, oriented: bool = False, **kwargs) -&gt; list[Entity]:\n    \"\"\"Retrieve Entity files from the design Database using either the oriented directory, or the refined directory.\n    If these don't exist, use the Pose directory, and load them into job for further processing\n\n    Args:\n        refined: Whether to use the refined directory\n        oriented: Whether to use the oriented directory\n    Returns:\n        The list of Entity instances that belong to this PoseData\n    \"\"\"\n    # Todo change to rely on EntityData\n    source_preference = ['refined', 'oriented_asu', 'design']\n    # Todo once loop_model works 'full_models'\n\n    if refined:\n        source_idx = 0\n    elif oriented:\n        source_idx = 1\n    else:\n        source_idx = 2\n        self.log.info(f'Falling back on Entity instances present in the {self.__class__.__name__} structure_source')\n\n    entities = []\n    for data in self.entity_data:\n        name = data.name\n        source_preference_iter = iter(source_preference)\n        # Discard however many sources are unwanted (source_idx number)\n        for it in range(source_idx):\n            _ = next(source_preference_iter)\n\n        model = None\n        while not model:\n            try:\n                source = next(source_preference_iter)\n            except StopIteration:\n                raise SymDesignException(\n                    f\"{self.get_entities.__name__}: Couldn't locate the required files\")\n            source_datastore = getattr(self.job.structure_db, source, None)\n            # Todo this course of action isn't set up anymore. It should be depreciated...\n            if source_datastore is None:  # Try to get file from the PoseDirectory\n                raise SymDesignException(\n                    f\"Couldn't locate the specified Entity '{name}' from any Database sources\")\n\n            model = source_datastore.retrieve_data(name=name)\n            #  Error where the EntityID loaded from 2gtr_1.pdb was 2gtr_1_1\n            #  Below might help resolve this issue\n            # model_file = source_datastore.retrieve_file(name=name)\n            # if model_file:\n            #     model = Entity.from_file(model_file)\n            # else:\n            #     model = None\n            if isinstance(model, Structure):\n                self.log.info(f'Found Entity at {source} DataStore and loaded into job')\n            else:\n                self.log.warning(f\"Couldn't locate the Entity {name} at the Database source \"\n                                 f\"'{source_datastore.location}'\")\n\n        entities.extend([entity for entity in model.entities])\n    # Todo is this useful or is the ProteinMetadata already set elsewhere?\n    # if source_idx == 0:\n    #     self.refined = True\n    # if source_idx == 0:\n    #     self.loop_modeled = True\n\n    if len(entities) != len(self.entity_data):\n        raise SymDesignException(\n            f'Expected {len(entities)} entities, but found {len(self.entity_data)}')\n\n    return entities\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.report_exception","title":"report_exception","text":"<pre><code>report_exception(context: str = None) -&gt; None\n</code></pre> <p>Reports error traceback to the log file.</p> <p>Parameters:</p> <ul> <li> <code>context</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string describing the function or situation where the error occurred.</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def report_exception(self, context: str = None) -&gt; None:\n    \"\"\"Reports error traceback to the log file.\n\n    Args:\n        context: A string describing the function or situation where the error occurred.\n    \"\"\"\n    if context is None:\n        msg = ''\n    else:\n        msg = f'{context} resulted in the exception:\\n\\n'\n    self.log.exception(msg)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.format_see_log_msg","title":"format_see_log_msg","text":"<pre><code>format_see_log_msg() -&gt; str\n</code></pre> <p>Issue a standard informational message indicating the location of further error information</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def format_see_log_msg(self) -&gt; str:\n    \"\"\"Issue a standard informational message indicating the location of further error information\"\"\"\n    return f\"See the log '{self.log_path}' for further information\"\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.load_pose","title":"load_pose","text":"<pre><code>load_pose(file: str = None, entities: list[Structure] = None) -&gt; None\n</code></pre> <p>For the design info given by a PoseJob source, initialize the Pose with self.source_path file, self.symmetry, self.job, self.fragment_database, and self.log objects</p> <p>Handles Pose clash testing, writing the Pose, adding state variables to the pose</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The file path to a structure_source file</p> </li> <li> <code>entities</code>             (<code>list[Structure]</code>, default:                 <code>None</code> )         \u2013          <p>The Entities desired in the Pose</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def load_pose(self, file: str = None, entities: list[Structure] = None) -&gt; None:\n    \"\"\"For the design info given by a PoseJob source, initialize the Pose with self.source_path file,\n    self.symmetry, self.job, self.fragment_database, and self.log objects\n\n    Handles Pose clash testing, writing the Pose, adding state variables to the pose\n\n    Args:\n        file: The file path to a structure_source file\n        entities: The Entities desired in the Pose\n    \"\"\"\n    # Check to see if Pose is already loaded and nothing new provided\n    if self.pose and file is None and entities is None:\n        return\n    else:\n        if entities is not None:\n            self.structure_source = 'Protocol derived'\n            # Use the entities as provided\n        elif self.source_path is None or not os.path.exists(self.source_path):\n            # In case a design was initialized without a file\n            self.log.info(f\"No '.source_path' found. Fetching structure_source from \"\n                          f'{type(self.job.structure_db).__name__} and transforming to Pose')\n            # Minimize I/O with transform...\n            # Must add the instance to a session to load any self.transformations present\n            with self.job.db.session(expire_on_commit=False) as session:\n                session.add(self)\n                entities = self.transform_entities_to_pose()\n            # Todo should use the ProteinMetadata version if the constituent Entity coordinates aren't modified by\n            #  some small amount as this will ensure that files are refined and that loops are included...\n            # Collect the EntityTransform for these regardless of symmetry since the file will be coming from the\n            # oriented/origin position...\n            self.structure_source = 'Database'\n\n        # Initialize the Pose using provided PDB numbering so that design_selectors are respected\n        if entities:\n            # Chains should be renamed if the chain_id's are the same\n            if len({entity.chain_id for entity in entities}) != len(entities):\n                rename = True\n            else:\n                rename = False\n            self.pose = Pose.from_entities(entities, name=self.name, rename_chains=rename, **self.job_kwargs)\n        elif self.initial_pose:\n            # This is a fresh Model that was already loaded so reuse\n            # Careful, if processing has occurred to the initial_pose, then this may be wrong!\n            self.pose = Pose.from_structure(\n                self.initial_pose, name=self.name, entity_info=self.initial_pose.entity_info, **self.job_kwargs)\n            # Use entity_info from already parsed\n            self.structure_source = self.source_path\n        else:\n            self.structure_source = file if file else self.source_path\n            self.pose = Pose.from_file(self.structure_source, name=self.name, **self.pose_kwargs)\n\n        if self.design_selector:\n            self.pose.apply_design_selector(**self.design_selector)\n\n        try:\n            self.pose.is_clash(measure=self.job.design.clash_criteria,\n                               distance=self.job.design.clash_distance,\n                               warn=not self.job.design.ignore_clashes)\n        except ClashError:  # as error:\n            if self.job.design.ignore_pose_clashes:\n                self.report_exception(context='Clash checking')\n                self.log.warning(f\"The Pose from '{self.structure_source}' contains clashes. \"\n                                 f\"{self.format_see_log_msg()}\")\n            else:\n                # Todo get the message from error and raise a ClashError(\n                #      f'{message}. If you would like to proceed regardless, re-submit the job with '\n                #      f'{flags.format_args(flags.ignore_pose_clashes_args)}'\n                raise\n        if self.pose.is_symmetric():\n            if self.pose.symmetric_assembly_is_clash(measure=self.job.design.clash_criteria,\n                                                     distance=self.job.design.clash_distance):\n                if self.job.design.ignore_symmetric_clashes:\n                    # self.format_error_for_log()\n                    self.log.warning(\n                        f\"The Pose symmetric assembly from '{self.structure_source}' contains clashes.\")\n                    #     f\"{self.format_see_log_msg()}\")\n                else:\n                    raise ClashError(\n                        \"The symmetric assembly contains clashes and won't be considered. If you \"\n                        'would like to proceed regardless, re-submit the job with '\n                        f'{flags.format_args(flags.ignore_symmetric_clashes_args)}')\n\n            # If there is an empty list for the transformations, save the transformations identified from the Pose\n            try:\n                any_database_transformations = any(self.transformations)\n            except DetachedInstanceError:\n                any_database_transformations = False\n\n            if not any_database_transformations:\n                # Add the transformation data to the database\n                for data, transformation in zip(self.entity_data, self.pose.entity_transformations):\n                    # Make an empty EntityTransform\n                    data.transform = sql.EntityTransform()\n                    data.transform.transformation = transformation\n                # Ensure this information is persistent\n                self._update_db()\n\n    # Save the Pose asu\n    if not os.path.exists(self.pose_path) or self.job.overwrite or self.job.load_to_db:\n        # Set the pose_path as the source_path\n        self.source_path = out_path = self.pose_path\n        # # Propagate to the PoseJob parent DesignData\n        # self.source_path = out_path = self.pose_source.structure_path = self.pose_path\n        # Ensure this information is persistent\n        self._update_db()\n    else:  # Explicitly set None and write anything else requested by input options\n        out_path = None\n    self.output_pose(out_path=out_path)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.output_pose","title":"output_pose","text":"<pre><code>output_pose(out_path: AnyStr = 'POSE', force: bool = False)\n</code></pre> <p>Save a new Structure from multiple Chain or Entity objects including the Pose symmetry</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>'POSE'</code> )         \u2013          <p>The path to save the self.pose. All other outputs are done to the self.pose_directory or self.output_path</p> </li> <li> <code>force</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to force writing of the pose</p> </li> </ul> <p>Returns:     None</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def output_pose(self, out_path: AnyStr = 'POSE', force: bool = False):\n    \"\"\"Save a new Structure from multiple Chain or Entity objects including the Pose symmetry\n\n    Args:\n        out_path: The path to save the self.pose.\n            All other outputs are done to the self.pose_directory or self.output_path\n        force: Whether to force writing of the pose\n    Returns:\n        None\n    \"\"\"\n    # if self.job.pose_format:\n    #     self.pose.pose_numbering()\n\n    if self.job.fuse_chains:\n        # try:\n        for fusion_nterm, fusion_cterm in self.job.fuse_chains:\n            # rename_success = False\n            new_success, same_success = False, False\n            for idx, entity in enumerate(self.pose.entities):\n                if entity.chain_id == fusion_cterm:\n                    entity_new_chain_idx = idx\n                    new_success = True\n                if entity.chain_id == fusion_nterm:\n                    # entity_same_chain_idx = idx\n                    same_success = True\n                    # rename_success = True\n                    # break\n            # if not rename_success:\n            if not new_success and not same_success:\n                raise DesignError(\n                    f\"Your requested fusion of chain '{fusion_nterm}' with chain '{fusion_cterm}' didn't work\")\n                # self.log.critical('Your requested fusion of chain %s with chain %s didn\\'t work!' %\n                #                   (fusion_nterm, fusion_cterm))\n            else:  # Won't be accessed unless entity_new_chain_idx is set\n                self.pose.entities[entity_new_chain_idx].chain_id = fusion_nterm\n        # except AttributeError:\n        #     raise ValueError('One or both of the chain IDs %s were not found in the input model. Possible chain'\n        #                      ' ID\\'s are %s' % ((fusion_nterm, fusion_cterm), ','.join(new_asu.chain_ids)))\n\n    if self.pose.is_symmetric():\n        if self.job.output_assembly:\n            if self.job.output_to_directory:\n                assembly_path = self.output_assembly_path\n            else:\n                assembly_path = self.assembly_path\n            if not os.path.exists(assembly_path) or self.job.overwrite:\n                self.pose.write(assembly=True, out_path=assembly_path,\n                                increment_chains=self.job.increment_chains,\n                                surrounding_uc=self.job.output_surrounding_uc)\n                self.log.info(f\"Symmetric assembly written to: '{assembly_path}'\")\n        if self.job.output_oligomers:  # Write out Entity.assembly instances to the PoseJob\n            for idx, entity in enumerate(self.pose.entities):\n                if self.job.output_to_directory:\n                    oligomer_path = os.path.join(self.output_path, f'{entity.name}_oligomer.pdb')\n                else:\n                    oligomer_path = os.path.join(self.pose_directory, f'{entity.name}_oligomer.pdb')\n                if not os.path.exists(oligomer_path) or self.job.overwrite:\n                    entity.write(assembly=True, out_path=oligomer_path)\n                    self.log.info(f\"Entity {entity.name} oligomer written to: '{oligomer_path}'\")\n\n    if self.job.output_entities:  # Write out Entity instances to the PoseJob\n        for idx, entity in enumerate(self.pose.entities):\n            if self.job.output_to_directory:\n                entity_path = os.path.join(self.output_path, f'{entity.name}_entity_.pdb')\n            else:\n                entity_path = os.path.join(self.pose_directory, f'{entity.name}_entity.pdb')\n            if not os.path.exists(entity_path) or self.job.overwrite:\n                entity.write(out_path=entity_path)\n                self.log.info(f\"Entity {entity.name} written to: '{entity_path}'\")\n\n    if self.job.output_fragments:\n        # if not self.pose.fragment_pairs:\n        #     self.pose.generate_interface_fragments()\n        putils.make_path(self.frags_path)\n        if self.job.output_trajectory:\n            # Write all fragments as one trajectory\n            if self.sym_entry.unit_cell:\n                self.log.warning('No unit cell dimensions applicable to the Fragment trajectory file')\n\n        self.pose.write_fragment_pairs(out_path=self.frags_path, multimodel=self.job.output_trajectory)\n\n    if self.job.output_interface:\n        interface_structure = self.pose.get_interface()\n        if self.job.output_to_directory:\n            interface_path = os.path.join(self.output_path, f'{self.name}_interface.pdb')\n        else:\n            interface_path = os.path.join(self.pose_directory, f'{self.name}_interface.pdb')\n        interface_structure.write(out_path=interface_path)\n\n    if out_path == 'POSE':\n        out_path = self.pose_path\n\n    if out_path:\n        if not os.path.exists(out_path) or self.job.overwrite or force:\n            self.pose.write(out_path=out_path)\n            self.log.info(f\"Wrote Pose file to: '{out_path}'\")\n\n    if self.job.output_to_directory:\n        if not os.path.exists(self.output_pose_path) or self.job.overwrite or force:\n            out_path = self.output_pose_path\n            self.pose.write(out_path=out_path)\n            self.log.info(f\"Wrote Pose file to: '{out_path}'\")\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.set_up_evolutionary_profile","title":"set_up_evolutionary_profile","text":"<pre><code>set_up_evolutionary_profile(**kwargs)\n</code></pre> <p>Add evolutionary profile information for each Entity to the Pose</p> <p>Other Parameters:</p> <ul> <li> <code>warn_metrics</code>         \u2013          <p>Whether to warn the user about missing files for metric collection</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def set_up_evolutionary_profile(self, **kwargs):\n    \"\"\"Add evolutionary profile information for each Entity to the Pose\n\n    Keyword Args:\n        warn_metrics: Whether to warn the user about missing files for metric collection\n    \"\"\"\n    if self.job.use_evolution:\n        if self.measure_evolution is None and self.measure_alignment is None:\n            self.measure_evolution, self.measure_alignment = \\\n                load_evolutionary_profile(self.job.api_db, self.pose, **kwargs)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseData.generate_fragments","title":"generate_fragments","text":"<pre><code>generate_fragments(interface: bool = False, oligomeric_interfaces: bool = False, entities: bool = False)\n</code></pre> <p>For the design info given by a PoseJob source, initialize the Pose then generate interfacial fragment information between Entities. Aware of symmetry and design_selectors in fragment generation file</p> <p>Parameters:</p> <ul> <li> <code>interface</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to perform fragment generation on the interface</p> </li> <li> <code>oligomeric_interfaces</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to perform fragment generation on the oligomeric interface</p> </li> <li> <code>entities</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to perform fragment generation on each Entity</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def generate_fragments(self, interface: bool = False, oligomeric_interfaces: bool = False, entities: bool = False):\n    \"\"\"For the design info given by a PoseJob source, initialize the Pose then generate interfacial fragment\n    information between Entities. Aware of symmetry and design_selectors in fragment generation file\n\n    Args:\n        interface: Whether to perform fragment generation on the interface\n        oligomeric_interfaces: Whether to perform fragment generation on the oligomeric interface\n        entities: Whether to perform fragment generation on each Entity\n    \"\"\"\n    if interface:\n        self.pose.generate_interface_fragments(oligomeric_interfaces=oligomeric_interfaces,\n                                               distance=self.job.interface_distance)\n    if entities:\n        self.pose.generate_fragments(oligomeric_interfaces=oligomeric_interfaces,\n                                     distance=self.job.interface_distance)\n    if self.job.output_fragments:\n        self.output_pose()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol","title":"PoseProtocol","text":"<pre><code>PoseProtocol(name: str = None, project: str = None, source_path: AnyStr = None, pose: Pose = None, protocol: str = None, pose_source: DesignData = None, **kwargs)\n</code></pre> <p>             Bases: <code>PoseData</code></p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The identifier</p> </li> <li> <code>project</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The project which this work belongs too</p> </li> <li> <code>source_path</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>If a path exists, where is structure files information stored?</p> </li> <li> <code>protocol</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a protocol was used to generate this Pose, which protocol?</p> </li> <li> <code>pose_source</code>             (<code>DesignData</code>, default:                 <code>None</code> )         \u2013          <p>If this is a descendant of another Design, which one?</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def __init__(self, name: str = None, project: str = None, source_path: AnyStr = None, pose: Pose = None,\n             protocol: str = None, pose_source: sql.DesignData = None,\n             # pose_transformation: Sequence[types.TransformationMapping] = None,\n             # entity_metadata: list[sql.EntityData] = None,\n             # entity_names: Sequence[str] = None,\n             # specific_designs: Sequence[str] = None, directives: list[dict[int, str]] = None,\n             **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        name: The identifier\n        project: The project which this work belongs too\n        source_path: If a path exists, where is structure files information stored?\n        protocol: If a protocol was used to generate this Pose, which protocol?\n        pose_source: If this is a descendant of another Design, which one?\n    \"\"\"\n    # PoseJob attributes\n    self.name = name\n    self.project = project\n    self.source_path = source_path\n\n    # Most __init__ code is called in __init_from_db__() according to sqlalchemy needs and DRY principles\n    self.__init_from_db__()\n    if pose is not None:  # It should be saved\n        self.initial_pose = pose\n        self.initial_pose.write(out_path=self.pose_path)\n        self.source_path = self.pose_path\n\n    # Save job variables to the state during initialization\n    sym_entry = kwargs.get('sym_entry')\n    if sym_entry:\n        self.sym_entry = sym_entry\n\n    if self.sym_entry:\n        self.symmetry_dimension = self.sym_entry.dimension\n        \"\"\"The dimension of the SymEntry\"\"\"\n        self.symmetry = self.sym_entry.resulting_symmetry\n        \"\"\"The resulting symmetry of the SymEntry\"\"\"\n        self.sym_entry_specification = self.sym_entry.specification\n        \"\"\"The specification string of the SymEntry\"\"\"\n        self.sym_entry_number = self.sym_entry.number\n        \"\"\"The SymEntry entry number\"\"\"\n\n    # Set up original DesignData entry for the pose baseline\n    if pose_source is None:\n        pose_source = sql.DesignData(name=name, pose=self, design_parent=None)  # , structure_path=source_path)\n    else:\n        pose_source = sql.DesignData(name=name, pose=self, design_parent=pose_source,\n                                     structure_path=pose_source.structure_path)\n    if protocol is not None:\n        pose_source.protocols.append(sql.DesignProtocol(protocol=protocol, job_id=self.job.id))\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.identify_interface","title":"identify_interface","text":"<pre><code>identify_interface()\n</code></pre> <p>Find the interface(s) between each Entity in the Pose. Handles symmetric clash testing, writing the assembly</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def identify_interface(self):\n    \"\"\"Find the interface(s) between each Entity in the Pose. Handles symmetric clash testing, writing the assembly\n    \"\"\"\n    self.load_pose()\n    # Measure the interface by_distance if the pose is the result of known docking inputs and\n    # the associated sequence is non-sense\n    self.pose.find_and_split_interface(by_distance=(self.protocol == putils.nanohedra\n                                                    or self.protocol == putils.fragment_docking),\n                                       distance=self.job.interface_distance)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.orient","title":"orient","text":"<pre><code>orient(to_pose_directory: bool = True)\n</code></pre> <p>Orient the Pose with the prescribed symmetry at the origin and symmetry axes in canonical orientations job.symmetry is used to specify the orientation</p> <p>Parameters:</p> <ul> <li> <code>to_pose_directory</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to write the file to the pose_directory or to another source</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def orient(self, to_pose_directory: bool = True):\n    \"\"\"Orient the Pose with the prescribed symmetry at the origin and symmetry axes in canonical orientations\n    job.symmetry is used to specify the orientation\n\n    Args:\n        to_pose_directory: Whether to write the file to the pose_directory or to another source\n    \"\"\"\n    if not self.initial_pose:\n        self.load_initial_pose()\n\n    if self.symmetry:\n        # This may raise an error if symmetry is malformed\n        self.initial_pose.orient(symmetry=self.symmetry)\n\n        if to_pose_directory:\n            out_path = self.assembly_path\n        else:\n            putils.make_path(self.job.orient_dir)\n            out_path = os.path.join(self.job.orient_dir, f'{self.initial_pose.name}.pdb')\n\n        orient_file = self.initial_pose.write(out_path=out_path)\n        self.log.info(f'The oriented file was saved to {orient_file}')\n\n        # Now that symmetry is set, just set the pose attribute\n        self.pose = self.initial_pose\n        self.output_pose()\n    else:\n        raise SymmetryError(\n            warn_missing_symmetry % self.orient.__name__)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.prepare_rosetta_flags","title":"prepare_rosetta_flags","text":"<pre><code>prepare_rosetta_flags(symmetry_protocol: str = None, sym_def_file: str = None, pdb_out_path: str = None, out_dir: AnyStr = os.getcwd()) -&gt; str\n</code></pre> <p>Prepare a protocol specific Rosetta flags file with program specific variables</p> <p>Parameters:</p> <ul> <li> <code>symmetry_protocol</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The type of symmetric protocol (specifying design dimension) to use for Rosetta jobs</p> </li> <li> <code>sym_def_file</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A Rosetta specific file specifying the symmetry system</p> </li> <li> <code>pdb_out_path</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Disk location to write the resulting design files</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Disk location to write the flags file</p> </li> </ul> <p>Returns:     Disk location of the flags file</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def prepare_rosetta_flags(self, symmetry_protocol: str = None, sym_def_file: str = None, pdb_out_path: str = None,\n                          out_dir: AnyStr = os.getcwd()) -&gt; str:\n    \"\"\"Prepare a protocol specific Rosetta flags file with program specific variables\n\n    Args:\n        symmetry_protocol: The type of symmetric protocol (specifying design dimension) to use for Rosetta jobs\n        sym_def_file: A Rosetta specific file specifying the symmetry system\n        pdb_out_path: Disk location to write the resulting design files\n        out_dir: Disk location to write the flags file\n    Returns:\n        Disk location of the flags file\n    \"\"\"\n    # flag_variables (list(tuple)): The variable value pairs to be filed in the RosettaScripts XML\n    # Relies on PoseDirecotry attributes\n    # .designs_path\n    # .refined_pdb\n    # .scores_file\n    # .design_profile_file\n    # .fragment_profile_file\n    number_of_residues = self.pose.number_of_residues\n    self.log.info(f'Total number of residues in Pose: {number_of_residues}')\n\n    # Get ASU distance parameters\n    if self.symmetry_dimension:  # Check for None and dimension 0 simultaneously\n        # The furthest point from the ASU COM + the max individual Entity radius\n        distance = self.pose.radius + max([entity.radius for entity in self.pose.entities])  # all the radii\n        self.log.info(f'Expanding ASU into symmetry group by {distance:.2f} Angstroms')\n    else:\n        distance = 0\n\n    if not self.job.design.evolution_constraint:\n        constraint_percent, free_percent = 0, 1\n    else:\n        constraint_percent = 0.5\n        free_percent = 1 - constraint_percent\n\n    variables = rosetta.variables \\\n        + [('dist', distance), ('repack', 'yes'),\n           ('constrained_percent', constraint_percent), ('free_percent', free_percent)]\n    variables.extend([(putils.design_profile, self.design_profile_file)]\n                     if os.path.exists(self.design_profile_file) else [])\n    variables.extend([(putils.fragment_profile, self.fragment_profile_file)]\n                     if os.path.exists(self.fragment_profile_file) else [])\n\n    if self.pose.is_symmetric():\n        if symmetry_protocol is None:\n            symmetry_protocols = {0: 'make_point_group', 2: 'make_layer', 3: 'make_lattice',\n                                  None: 'asymmetric'}  # -1: 'asymmetric'\n            symmetry_protocol = symmetry_protocols[self.symmetry_dimension]\n        variables.append(('symmetry', symmetry_protocol))\n        # The current self.sym_entry can still be None if requested for this particular job\n        if sym_def_file is None and self.sym_entry is not None:\n            sym_def_file = self.sym_entry.sdf_lookup()\n            variables.append(('sdf', sym_def_file))\n\n        self.log.info(f'Symmetry Option: {symmetry_protocol}')\n        out_of_bounds_residue = number_of_residues*self.pose.number_of_symmetry_mates + 1\n    else:\n        if symmetry_protocol is not None:\n            variables.append(('symmetry', symmetry_protocol))\n        out_of_bounds_residue = number_of_residues + 1\n\n    interface_residue_ids = {}\n    for number, residues in self.pose.interface_residues_by_interface.items():\n        interface_residue_ids[f'interface{number}'] = \\\n            ','.join(f'{residue.number}{residue.chain_id}' for residue in residues)\n    # self.info['interface_residue_ids'] = self.interface_residue_ids\n    variables.extend([(interface, residues) if residues else (interface, out_of_bounds_residue)\n                      for interface, residues in interface_residue_ids.items()])\n\n    # Assign any additional designable residues\n    if self.pose.required_residues:\n        variables.extend(\n            [('required_residues', ','.join(f'{res.number}{res.chain_id}' for res in self.pose.required_residues))])\n    else:  # Get an out-of-bounds index\n        variables.extend([('required_residues', out_of_bounds_residue)])\n\n    # Allocate any \"core\" residues based on central fragment information\n    residues = self.pose.residues\n    fragment_residues = [residues[index] for index in self.pose.interface_fragment_residue_indices]\n    if fragment_residues:\n        variables.extend([('fragment_residues',\n                           ','.join([f'{res.number}{res.chain_id}' for res in fragment_residues]))])\n    else:  # Get an out-of-bounds index\n        variables.extend([('fragment_residues', out_of_bounds_residue)])\n    core_residues = self.pose.core_residues\n    if core_residues:\n        variables.extend([('core_residues', ','.join([f'{res.number}{res.chain_id}' for res in core_residues]))])\n    else:  # Get an out-of-bounds index\n        variables.extend([('core_residues', out_of_bounds_residue)])\n\n    rosetta_flags = rosetta.flags.copy()\n    if pdb_out_path:\n        rosetta_flags.extend([f'-out:path:pdb {pdb_out_path}', f'-scorefile {self.scores_file}'])\n    else:\n        rosetta_flags.extend([f'-out:path:pdb {self.designs_path}', f'-scorefile {self.scores_file}'])\n    rosetta_flags.append(f'-in:file:native {self.refined_pdb}')\n    rosetta_flags.append(f'-parser:script_vars {\" \".join(f\"{var}={val}\" for var, val in variables)}')\n\n    putils.make_path(out_dir)\n    out_file = os.path.join(out_dir, 'flags')\n    with open(out_file, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(rosetta_flags))\n    self.log.debug(f'Rosetta flags written to: {out_file}')\n\n    return out_file\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.generate_entity_metrics_commands","title":"generate_entity_metrics_commands","text":"<pre><code>generate_entity_metrics_commands(base_command) -&gt; list[list[str] | None]\n</code></pre> <p>Use the Pose state to generate metrics commands for each Entity instance</p> <p>Parameters:</p> <ul> <li> <code>base_command</code>         \u2013          <p>The base command to build Entity metric commands off of</p> </li> </ul> <p>Returns:     The formatted command for every Entity in the Pose</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def generate_entity_metrics_commands(self, base_command) -&gt; list[list[str] | None]:\n    \"\"\"Use the Pose state to generate metrics commands for each Entity instance\n\n    Args:\n        base_command: The base command to build Entity metric commands off of\n    Returns:\n        The formatted command for every Entity in the Pose\n    \"\"\"\n    # self.entity_names not dependent on Pose load\n    if len(self.entity_data) == 1:  # There is no unbound state to query as only one entity\n        return []\n    if len(self.symmetry_definition_files) != len(self.entity_data) or self.job.force:\n        putils.make_path(self.data_path)\n        for entity in self.pose.entities:\n            if entity.is_symmetric():  # Make symmetric energy in line with SymDesign energies v\n                entity.make_sdf(out_path=self.data_path,\n                                modify_sym_energy_for_cryst=True if self.sym_entry.dimension in [2, 3] else False)\n            # Todo monitor if Rosetta energy modifier changed from 2x for crystal set up and adjust accordingly\n            else:\n                shutil.copy(os.path.join(putils.symmetry_def_files, 'C1.sym'),\n                            os.path.join(self.data_path, f'{entity.name}.sdf'))\n\n    entity_metric_commands = []\n    for idx, data in enumerate(self.entity_data, 1):\n        name = data.name\n        if self.is_symmetric():\n            entity_sdf = f'sdf={os.path.join(self.data_path, f\"{name}.sdf\")}'\n            entity_sym = 'symmetry=make_point_group'\n        else:\n            entity_sdf, entity_sym = '', 'symmetry=asymmetric'\n        metric_cmd = base_command \\\n            + ['-parser:script_vars', 'repack=yes', f'entity={idx}', entity_sym] \\\n            + ([entity_sdf] if entity_sdf != '' else [])\n        self.log.info(f'Metrics command for Entity {name}: {list2cmdline(metric_cmd)}')\n        entity_metric_commands.append(metric_cmd)\n\n    return entity_metric_commands\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.get_cmd_process_rosetta_metrics","title":"get_cmd_process_rosetta_metrics","text":"<pre><code>get_cmd_process_rosetta_metrics() -&gt; list[list[str], ...]\n</code></pre> <p>Generate a list of commands compatible with subprocess.Popen()/subprocess.list2cmdline() to run process-rosetta-metrics</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def get_cmd_process_rosetta_metrics(self) -&gt; list[list[str], ...]:\n    \"\"\"Generate a list of commands compatible with subprocess.Popen()/subprocess.list2cmdline() to run\n    process-rosetta-metrics\n    \"\"\"\n    return [*putils.program_command_tuple, flags.process_rosetta_metrics, '--single', self.pose_directory] \\\n        + self.job.parsed_arguments\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.thread_sequences_to_backbone","title":"thread_sequences_to_backbone","text":"<pre><code>thread_sequences_to_backbone(sequences: dict[str, str] = None)\n</code></pre> <p>From the starting Pose, thread sequences onto the backbone, modifying relevant side chains i.e., mutate the Pose and build/pack using Rosetta FastRelax. If no sequences are provided, will use self.designed_sequences</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>dict[str, str]</code>, default:                 <code>None</code> )         \u2013          <p>A mapping of sequence alias to its sequence. These will be used for producing outputs and as the input sequence</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def thread_sequences_to_backbone(self, sequences: dict[str, str] = None):\n    \"\"\"From the starting Pose, thread sequences onto the backbone, modifying relevant side chains i.e., mutate the\n    Pose and build/pack using Rosetta FastRelax. If no sequences are provided, will use self.designed_sequences\n\n    Args:\n        sequences: A mapping of sequence alias to its sequence. These will be used for producing outputs and as\n            the input sequence\n    \"\"\"\n    if sequences is None:  # Gather all already designed sequences\n        raise NotImplementedError(\n            f'Must pass sequences to {self.thread_sequences_to_backbone.__name__}')\n        # Todo, this doesn't work!\n        # refine_sequences = unpickle(self.designed_sequences_file)\n        sequences = {seq: seq.seq for seq in read_fasta_file(self.designed_sequences_file)}\n\n    self.load_pose()\n    # Write each \"threaded\" structure out for further processing\n    number_of_residues = self.pose.number_of_residues\n    # Ensure that mutations to the Pose aren't saved to state\n    pose_copy = self.pose.copy()\n    design_files = []\n    for sequence_id, sequence in sequences.items():\n        if len(sequence) != number_of_residues:\n            raise DesignError(\n                f'The length of the sequence, {len(sequence)} != {number_of_residues}, '\n                f'the number of residues in the pose')\n        for res_idx, residue_type in enumerate(sequence):\n            pose_copy.mutate_residue(index=res_idx, to=residue_type)\n        # pre_threaded_file = os.path.join(self.data_path, f'{self.name}_{self.protocol}{seq_idx:04d}.pdb')\n        pre_threaded_file = os.path.join(self.data_path, f'{sequence_id}.pdb')\n        design_files.append(pose_copy.write(out_path=pre_threaded_file))\n\n    putils.make_path(self.scripts_path)\n    design_files_file = os.path.join(self.scripts_path, f'{starttime}_{self.protocol}_files.txt')\n\n    # # Modify each sequence score to reflect the new \"decoy\" name\n    # # Todo update as a consequence of new SQL\n    # sequence_ids = sequences.keys()\n    # design_scores = metrics.parse_rosetta_scores(self.scores_file)\n    # for design, scores in design_scores.items():\n    #     if design in sequence_ids:\n    #         # We previously saved data. Copy to the identifier that is present after threading\n    #         scores['decoy'] = f'{design}_{self.protocol}'\n    #         # write_json(_scores, self.scores_file)\n    #         with open(self.scores_file, 'a') as f_save:\n    #             json.dump(scores, f_save)  # , **kwargs)\n    #             # Ensure JSON lines are separated by newline\n    #             f_save.write('\\n')\n\n    if design_files:\n        with open(design_files_file, 'w') as f:\n            f.write('%s\\n' % '\\n'.join(design_files))\n    else:\n        raise DesignError(\n            f'{self.thread_sequences_to_backbone.__name__}: No designed sequences were located')\n\n    # self.refine(in_file_list=design_files_file)\n    self.refine(design_files=design_files)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.predict_structure","title":"predict_structure","text":"<pre><code>predict_structure()\n</code></pre> <p>Perform structure prediction on the .current_designs. If there are no .current_designs, will use any design that is missing a structure file. Additionally, add the Pose sequence to the prediction by using the job.predict.pose flag</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def predict_structure(self):\n    \"\"\"Perform structure prediction on the .current_designs. If there are no .current_designs, will use any\n    design that is missing a structure file. Additionally, add the Pose sequence to the prediction by using the\n    job.predict.pose flag\n    \"\"\"\n    with self.job.db.session(expire_on_commit=False) as session:\n        session.add(self)\n        if self.current_designs:\n            sequences = {design: design.sequence for design in self.current_designs}\n        else:\n            sequences = {design: design.sequence for design in self.get_designs_without_structure()}\n            self.current_designs.extend(sequences.keys())\n\n        if self.job.predict.pose:\n            # self.pose_source is loaded in above session through get_designs_without_structure()\n            pose_sequence = {self.pose_source: self.pose_source.sequence}\n            if self.pose_source.structure_path is None:\n                sequences = {**pose_sequence, **sequences}\n            elif self.job.overwrite:\n                sequences = {**pose_sequence, **sequences}\n            else:\n                logger.warning(f\"The flag --{flags.format_args(flags.predict_pose_args)} was specified, but the \"\n                               \"pose has already been predicted. If you meant to overwrite this pose, explicitly \"\n                               \"pass --overwrite\")\n    if not sequences:\n        raise DesignError(\n            f\"Couldn't find any sequences to {self.predict_structure.__name__}\")\n\n    # match self.job.predict.method:  # Todo python 3.10\n    #     case 'thread':\n    #         self.thread_sequences_to_backbone(sequences)\n    #     case 'alphafold':\n    #         # Sequences use within alphafold requires .fasta...\n    #         self.alphafold_predict_structure(sequences)\n    #     case _:\n    #         raise NotImplementedError(f\"For {self.predict_structure.__name__}, the method '\n    #                                   f\"{self.job.predict.method} isn't implemented yet\")\n\n    self.protocol = self.job.predict.method\n    if self.job.predict.method == 'thread':\n        self.thread_sequences_to_backbone(sequences)\n    elif self.job.predict.method == 'alphafold':\n        self.alphafold_predict_structure(sequences)\n    else:\n        raise NotImplementedError(\n            f\"For {self.predict_structure.__name__}, the method {self.job.predict.method} isn't implemented yet\")\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.alphafold_predict_structure","title":"alphafold_predict_structure","text":"<pre><code>alphafold_predict_structure(sequences: dict[DesignData, str], model_type: af_model_literal = 'monomer', **kwargs)\n</code></pre> <p>Use Alphafold to predict structures for sequences of interest. The sequences will be fit to the Pose parameters, however will have sequence features unique to that sequence. By default, no multiple sequence alignment will be used and the AlphafoldInitialGuess model will be used wherein the starting coordinates for the Pose will be supplied as the initial guess</p> <p>According to Deepmind Alphafold.ipynb (2/3/23), the usage of multimer with &gt; 3000 residues isn't validated. while a memory requirement of 4000 is the theoretical limit. I think it depends on the available memory Args:     sequences: The mapping for DesignData to sequence     model_type: The type of Alphafold model to use. Choose from 'monomer', 'monomer_casp14', 'monomer_ptm',         or 'multimer' Returns:     None</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def alphafold_predict_structure(self, sequences: dict[sql.DesignData, str],\n                                model_type: resources.ml.af_model_literal = 'monomer', **kwargs):\n    \"\"\"Use Alphafold to predict structures for sequences of interest. The sequences will be fit to the Pose\n    parameters, however will have sequence features unique to that sequence. By default, no multiple sequence\n    alignment will be used and the AlphafoldInitialGuess model will be used wherein the starting coordinates for\n    the Pose will be supplied as the initial guess\n\n    According to Deepmind Alphafold.ipynb (2/3/23), the usage of multimer with &gt; 3000 residues isn't validated.\n    while a memory requirement of 4000 is the theoretical limit. I think it depends on the available memory\n    Args:\n        sequences: The mapping for DesignData to sequence\n        model_type: The type of Alphafold model to use. Choose from 'monomer', 'monomer_casp14', 'monomer_ptm',\n            or 'multimer'\n    Returns:\n        None\n    \"\"\"\n    self.load_pose()\n    number_of_residues = self.pose.number_of_residues\n    logger.info(f'Performing structure prediction on {len(sequences)} sequences')\n    for design, sequence in sequences.items():\n        # Todo if differentially sized sequence inputs\n        self.log.debug(f'Found sequence {sequence}')\n        # max_sequence_length = max([len(sequence) for sequence in sequences.values()])\n        if len(sequence) != number_of_residues:\n            raise DesignError(\n                f'The sequence length {len(sequence)} != {number_of_residues}, the number of residues in the pose')\n    # Get the DesignData.ids for metrics output\n    design_ids = [design.id for design in sequences]\n\n    # Hardcode parameters\n    if model_type == 'monomer_casp14':\n        num_ensemble = 8\n    else:\n        num_ensemble = 1\n\n    number_of_entities = self.number_of_entities\n    heteromer = number_of_entities &gt; 1\n    run_multimer_system = heteromer or self.pose.number_of_chains &gt; 1\n    if run_multimer_system:\n        model_type = 'multimer'\n        self.log.info(f'The AlphaFold model was automatically set to {model_type} due to detected multimeric pose')\n\n    # # Todo enable compilation time savings by returning a precomputed model_factory. Padding the size of this may\n    # #  help quite a bit\n    # model_runners = resources.ml.alphafold_model_factory.get()\n    if self.job.predict.num_predictions_per_model is None:\n        if run_multimer_system:\n            # Default is 5, with 5 models for 25 outputs. Could do 1 to increase speed...\n            num_predictions_per_model = 5\n        else:\n            num_predictions_per_model = 1\n    else:\n        num_predictions_per_model = self.job.predict.num_predictions_per_model\n\n    # Set up the various model_runners to supervise the prediction task for each sequence\n    model_runners = resources.ml.set_up_model_runners(model_type=model_type,\n                                                      num_predictions_per_model=num_predictions_per_model,\n                                                      num_ensemble=num_ensemble,\n                                                      development=self.job.development)\n\n    def get_sequence_features_to_merge(seq_of_interest: str, multimer_length: int = None) -&gt; FeatureDict:\n        \"\"\"Set up a sequence that has similar features to the Pose, but different sequence, say from design output\n\n        Args:\n            seq_of_interest: The sequence of interest\n            multimer_length: The length of the multimer features, if the features are multimeric\n        Returns:\n            The Alphafold FeatureDict which is essentially a dictionary with dict[str, np.ndarray]\n        \"\"\"\n        # Set up scores for each model\n        sequence_length = len(seq_of_interest)\n        # Set the sequence up in the FeatureDict\n        # Todo the way that I am adding this here instead of during construction, seems that I should\n        #  symmetrize the sequence before passing to af_predict(). This would occur by entity, where the first\n        #  entity is combined, then the second entity is combined, etc. Any entity agnostic features such\n        #  as all_atom_mask would be able to be made here\n        _seq_features = af_pipeline.make_sequence_features(\n            sequence=seq_of_interest, description='', num_res=sequence_length)\n        # Always use the outer \"domain_name\" feature if there is one\n        _seq_features.pop('domain_name')\n        if multimer_length is not None:\n            # Remove \"object\" dtype arrays. This may be required for \"monomer\" runs too\n            # _seq_features.pop('domain_name')\n            _seq_features.pop('between_segment_residues')\n            _seq_features.pop('sequence')\n            _seq_features.pop('seq_length')\n            # The multimer model performs the one-hot operation itself. So processing gets the sequence as\n            # the idx encoded by this v argmax on the one-hot\n            _seq_features['aatype'] = np.argmax(_seq_features['aatype'], axis=-1).astype(np.int32)\n\n            multimer_number, remainder = divmod(multimer_length, sequence_length)\n            if remainder:\n                raise ValueError(\n                    'The multimer_sequence_length and the sequence_length must differ by an integer number. Found '\n                    f'multimer/monomer ({multimer_sequence_length})/({sequence_length}) with remainder {remainder}')\n            for key in ['aatype', 'residue_index']:\n                _seq_features[key] = np.tile(_seq_features[key], multimer_number)\n            # # For 'domain_name', 'sequence', and 'seq_length', transform the 1-D array to a scaler\n            # # np.asarray(np.array(['pope'.encode('utf-8')], dtype=np.object_)[0], dtype=np.object_)\n            # # Not sure why this transformation happens for multimer... as the multimer gets rid of them,\n            # # but they are ready for the monomer pipeline\n            # for key in ['domain_name', 'sequence']:\n            #     _seq_features[key] = np.asarray(_seq_features[key][0], dtype=np.object_)\n            # _seq_features['seq_length'] = np.asarray(sequence_length, dtype=np.int32)\n\n        # Todo ensure that the sequence is merged such as in the merge_and_pair subroutine\n        #  a good portion of which is below\n        # if feature_name_split in SEQ_FEATURES:\n        #     # Merges features along their length [MLKF...], [MLKF...] -&gt; [MLKF...MLKF...]\n        #     merged_example[feature_name] = np.concatenate(feats, axis=0)\n        #     # If sequence is the same length as Entity/Pose, then 'residue_index' should be correct and\n        #     # increasing for each sequence position, restarting at the beginning of a chain\n        # Make the atom positions according to the sequence\n        # Add all_atom_mask and dummy all_atom_positions based on aatype.\n        all_atom_mask = residue_constants.STANDARD_ATOM_MASK[_seq_features['aatype']]\n        _seq_features['all_atom_mask'] = all_atom_mask\n        _seq_features['all_atom_positions'] = np.zeros(list(all_atom_mask.shape) + [3])\n\n        # Todo check on 'seq_mask' introduction point for multimer...\n        # elif feature_name_split in TEMPLATE_FEATURES:\n        # DONT WORRY ABOUT THESE\n        #     merged_example[feature_name] = np.concatenate(feats, axis=1)\n        # elif feature_name_split in CHAIN_FEATURES:\n        #     merged_example[feature_name] = np.sum(x for x in feats).astype(np.int32)\n        # 'num_alignments' should be fine as msa incorporation has happened prior\n        # else:\n        #     # NO use at this time, these take the value of the first chain and eventually get thrown out\n        #     # 'domain_name', 'sequence', 'between_segment_residues'\n        #     merged_example[feature_name] = feats[0]\n        return _seq_features\n\n    def output_alphafold_structures(structure_types: dict[str, dict[str, str]], design_name: str = None):\n        \"\"\"From a PDB formatted string, output structures by design_name to self.designs_path/design_name\"\"\"\n        if design_name is None:\n            design_name = ''\n        # for design, design_scores in structures.items():\n        #     # # Output unrelaxed\n        #     # structures = design_scores['structures_unrelaxed']\n        for type_str in ['', 'un']:\n            structures = structure_types.get(f'{type_str}relaxed', [])\n            idx = count(1)\n            for model_name, structure in structures.items():\n                out_dir = os.path.join(self.designs_path, design_name)\n                putils.make_path(out_dir)\n                path = os.path.join(out_dir,\n                                    f'{design_name}-{model_name}_rank{next(idx)}-{type_str}relaxed.pdb')\n                with open(path, 'w') as f:\n                    f.write(structure)\n            # # Repeat for relaxed\n            # structures = design_scores['structures']\n            # idx = count(1)\n            # for model_name, structure in structures.items():\n            #     path = os.path.join(self.designs_path, f'{model_name}_rank{next(idx)}.pdb')\n            #     with open(path, 'w') as f:\n            #         f.write(structure)\n\n    def find_model_with_minimal_rmsd(models: dict[str, Structure], template_cb_coords) -&gt; tuple[list[float], str]:\n        \"\"\"Use the CB coords to calculate the RMSD and find the best Structure instance from a group of Alphafold\n        models and transform that model to the Pose coordinates\n\n        Returns:\n            The calculated rmsds, and the name of the model with the minimal rmsd\n        \"\"\"\n        min_rmsd = float('inf')\n        minimum_model = None\n        rmsds = []\n        for af_model_name, model in models.items():\n            rmsd, rot, tx = superposition3d(template_cb_coords, model.cb_coords)\n            rmsds.append(rmsd)\n            if rmsd &lt; min_rmsd:\n                min_rmsd = rmsd\n                minimum_model = af_model_name\n                # Move the Alphafold model into the Pose reference frame\n                model.transform(rotation=rot, translation=tx)\n\n        return rmsds, minimum_model\n\n    def combine_model_scores(_scores: Sequence[dict[str, np.ndarray | float]]) -&gt; dict[str, list[int | np.ndarray]]:\n        \"\"\"Add each of the different score types produced by each model to a dictionary with a list of model\n        folding_scores in each category in the order the models appear\n\n        Returns:\n            A dictionary with format {'score_type': [score1, score2, ...], } where score1 can have a shape float,\n            (n_residues,), or (n_residues, n_residues)\n        \"\"\"\n        return {score_type: [scores[score_type] for scores in _scores] for score_type in _scores[0].keys()}\n\n    def get_prev_pos_coords(sequence_: Iterable[str] = None, assembly: bool = False, entity: str = None) \\\n            -&gt; jnp.ndarray:\n        \"\"\"Using the PoseJob.pose instance, get coordinates compatible with AlphafoldInitialGuess\n\n        Args:\n            sequence_: The sequence to use as a template to generate the coordinates\n            assembly: Whether the coordinates should reflect the Pose Assembly\n            entity: If coordinates should come from a Pose Entity, which one?\n        Returns:\n            The alphafold formatted sequence coords in a JAX array\n        \"\"\"\n        pose_copy: Pose = self.pose.copy()\n        # Choose which Structure to iterate over residues\n        if entity is not None:\n            structure = pose_copy.get_entity(entity)\n        else:\n            structure = pose_copy\n\n        if sequence_ is None:\n            # Make an all Alanine structure backbone as the prev_pos\n            sequence_ = 'A' * structure.number_of_residues\n\n        # Mutate to a 'lame' version of sequence/structure, removing any side-chain atoms not present\n        for residue, residue_type in zip(structure.residues, sequence_):\n            deleted_indices = pose_copy.mutate_residue(index=residue.index, to=residue_type)\n\n        if assembly:\n            af_coords = structure.assembly.alphafold_coords\n        elif entity:\n            af_coords = structure.assembly.alphafold_coords\n        else:\n            af_coords = structure.alphafold_coords\n\n        return jnp.asarray(af_coords)\n\n    if self.job.predict.models_to_relax is not None:\n        relaxed = True\n    else:\n        relaxed = False\n\n    # Hard code in the use of only design based single sequence models\n    # if self.job.design:\n    #     no_msa = True\n    # else:\n    #     no_msa = False\n    no_msa = True\n    debug_entities_and_oligomers = False\n    # Ensure clashes aren't checked as these stop operation\n    pose_kwargs = self.pose_kwargs.copy()\n\n    # Get features for the Pose and predict\n    if self.job.predict.designs:\n        # For interface analysis the interface residues are needed\n        self.identify_interface()\n        if self.job.predict.assembly:\n            if self.pose.number_of_symmetric_residues &gt; resources.ml.MULTIMER_RESIDUE_LIMIT:\n                logger.critical(\n                    f\"Predicting on a symmetric input with {self.pose.number_of_symmetric_residues} isn't \"\n                    'recommended due to memory limitations')\n            features = self.pose.get_alphafold_features(symmetric=True, multimer=run_multimer_system, no_msa=no_msa)\n            # Todo may need to modify this if the pose isn't completely symmetric despite it being specified as such\n            number_of_residues = self.pose.number_of_symmetric_residues\n        else:\n            features = self.pose.get_alphafold_features(symmetric=False, multimer=run_multimer_system,\n                                                        no_msa=no_msa)\n\n        if run_multimer_system:  # Get the length\n            multimer_sequence_length = features['seq_length']\n        else:\n            multimer_sequence_length = None\n\n        putils.make_path(self.designs_path)\n        model_names = []\n        asu_design_structures = []  # structure_by_design = {}\n        asu_design_scores = {}  # []  # scores_by_design = {}\n        for design, sequence in sequences.items():\n            this_seq_features = get_sequence_features_to_merge(sequence, multimer_length=multimer_sequence_length)\n            logger.debug(f'Found this_seq_features:\\n\\t%s'\n                                  % \"\\n\\t\".join((f\"{k}={v}\" for k, v in this_seq_features.items())))\n            model_features = {'prev_pos': get_prev_pos_coords(sequence, assembly=self.job.predict.assembly)}\n            logger.info(f'Predicting Design {design.name} structure')\n            asu_structures, asu_scores = \\\n                resources.ml.af_predict({**features, **this_seq_features, **model_features}, model_runners,\n                                        gpu_relax=self.job.predict.use_gpu_relax,\n                                        models_to_relax=self.job.predict.models_to_relax)\n            if relaxed:\n                structures_to_load = asu_structures.get('relaxed', [])\n            else:\n                structures_to_load = asu_structures.get('unrelaxed', [])\n            # # Tod0 remove this after debug is done\n            # output_alphafold_structures(asu_structures, design_name=f'{design}-asu')\n            # asu_models = load_alphafold_structures(structures_to_load, name=str(design),  # Get '.name'\n            #                                        entity_info=self.pose.entity_info)\n            # Load the Model in while ignoring any potential clashes\n            # Todo should I limit the .splitlines by the number_of_residues? Assembly v asu considerations\n            asu_models = {model_name: Pose.from_pdb_lines(structure.splitlines(), name=design.name, **pose_kwargs)\n                          for model_name, structure in structures_to_load.items()}\n            # Because the pdb_lines aren't oriented, must handle orientation of incoming files to match sym_entry\n            # This is handled in find_model_with_minimal_rmsd(), however, the symmetry isn't set up correctly, i.e.\n            # we need to pose.make_oligomers() in the correct orientation\n            # Do this all at once after every design\n            if relaxed:  # Set b-factor data as relaxed get overwritten\n                for model_name, model in asu_models.items():\n                    model.set_b_factor_data(asu_scores[model_name]['plddt'][:number_of_residues])\n            # Check for the prediction rmsd between the backbone of the Entity Model and Alphafold Model\n            rmsds, minimum_model = find_model_with_minimal_rmsd(asu_models, self.pose.cb_coords)\n            # rmsds, minimum_model = find_model_with_minimal_rmsd(asu_models, self.pose.backbone_and_cb_coords)\n            if minimum_model is None:\n                self.log.critical(f\"Couldn't find the asu model with the minimal rmsd for Design {design}\")\n            # Append each ASU result to the full return\n            asu_design_structures.append(asu_models[minimum_model])\n            model_names.append(minimum_model)\n            # structure_by_design[design].append(asu_models[minimum_model])\n            # asu_design_scores.append({'rmsd_prediction_ensemble': rmsds, **asu_scores[minimum_model]})\n            # Average all models scores to get the ensemble of the predictions\n            combined_scores = combine_model_scores(list(asu_scores.values()))\n            asu_design_scores[design.name] = {'rmsd_prediction_ensemble': rmsds, **combined_scores}\n            # asu_design_scores[str(design)] = {'rmsd_prediction_ensemble': rmsds, **asu_scores[minimum_model]}\n            \"\"\"Each design in asu_design_scores contain the following features\n            {'predicted_aligned_error': [(n_residues, n_residues), ...]  # multimer/monomer_ptm\n             'plddt': [(n_residues,), ...]\n             'predicted_interface_template_modeling_score': [float, ...]  # multimer\n             'predicted_template_modeling_score': [float, ...]  # multimer/monomer_ptm\n             'rmsd_prediction_ensemble: [(number_of_models), ...]\n             }\n            \"\"\"\n\n        # Write the folded structure to designs_path and update DesignProtocols\n        for idx, (design_data, pose) in enumerate(zip(sequences.keys(), asu_design_structures)):\n            design_data.structure_path = \\\n                pose.write(out_path=os.path.join(self.designs_path, f'{design_data.name}.pdb'))\n            design_data.protocols.append(\n                sql.DesignProtocol(design_id=design_data.id, job_id=self.job.id, protocol=self.protocol,\n                                   alphafold_model=model_names[idx], file=design_data.structure_path)\n            )\n            # # This corrects the oligomeric specification for each Entity\n            # # by using the inherent _assign_pose_transformation()\n            # pose.make_oligomers()\n            # This would explicitly pass the transformation parameters which are correct for the PoseJob\n            # for entity in pose.entities:\n            #     entity.remove_mate_chains()\n            if debug_entities_and_oligomers:\n                pose.make_oligomers(transformations=self.transformations)\n                pose.write(out_path=os.path.join(self.designs_path, f'{pose.name}-asu-check.pdb'))\n                pose.write(assembly=True, out_path=os.path.join(self.designs_path, f'{pose.name}-assembly-check.pdb'))\n                for entity in pose.entities:\n                    entity.write(\n                        out_path=os.path.join(self.designs_path, f'{pose.name}{entity.name}-oligomer-asu-check.pdb'))\n                    entity.write(assembly=True,\n                                 out_path=os.path.join(self.designs_path,\n                                                       f'{pose.name}{entity.name}-oligomer-check.pdb'))\n\n        # Using the 2-fold aware pose.interface_residues_by_interface\n        interface_indices = tuple([residue.index for residue in residues]\n                                  for residues in self.pose.interface_residues_by_interface.values())\n        # All index are based on design.name\n        residues_df = self.analyze_residue_metrics_per_design(asu_design_structures)\n        designs_df = self.analyze_design_metrics_per_design(residues_df, asu_design_structures)\n        predict_designs_df, predict_residues_df = \\\n            self.analyze_alphafold_metrics(asu_design_scores, number_of_residues,\n                                           model_type=model_type, interface_indices=interface_indices)\n        residue_indices = list(range(number_of_residues))\n        # Set the index to use the design.id for each design instance\n        design_index = pd.Index(design_ids, name=sql.ResidueMetrics.design_id.name)\n        residue_sequences_df = pd.DataFrame([list(seq) for seq in sequences.values()],\n                                            index=predict_residues_df.index,\n                                            columns=pd.MultiIndex.from_product([residue_indices,\n                                                                                [sql.ResidueMetrics.type.name]]))\n        residues_df = residues_df.join([predict_residues_df, residue_sequences_df])\n        designs_df = designs_df.join(predict_designs_df)\n        designs_df.index = residues_df.index = design_index\n        # Output collected metrics\n        with self.job.db.session(expire_on_commit=False) as session:\n            self.output_metrics(session, designs=designs_df)\n            output_residues = False\n            if output_residues:\n                self.output_metrics(session, residues=residues_df)\n            # else:  # Only save the 'design_residue' columns\n            #     residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n            #     self.output_metrics(session, design_residues=residues_df)\n            # Commit the newly acquired metrics\n            session.commit()\n\n    # Prepare the features to feed to the model\n    if self.job.predict.entities:  # and self.number_of_entities &gt; 1:\n        # Get the features for each oligomeric Entity\n        # The folding_scores will all be the length of the gene Entity, not oligomer\n        # entity_scores_by_design = {design: [] for design in sequences}\n\n        # Sort the entity instances by their length to improve compile time.\n        entities = self.pose.entities\n        # The only compile should be the first prediction\n        entity_number_of_residues = [(entity.number_of_residues, idx) for idx, entity in enumerate(entities)]\n        entity_idx_sorted_residue_number_highest_to_lowest = \\\n            [idx for _, idx in sorted(entity_number_of_residues, key=lambda pair: pair[0], reverse=True)]\n        sorted_entities_and_data = [(entities[idx], self.entity_data[idx])\n                                    for idx in entity_idx_sorted_residue_number_highest_to_lowest]\n        entity_structure_by_design = {design: [] for design in sequences}\n        entity_design_dfs = []\n        entity_residue_dfs = []\n        for entity, entity_data in sorted_entities_and_data:\n            # Fold with symmetry True. If it isn't symmetric, symmetry won't be used\n            features = entity.get_alphafold_features(symmetric=True, no_msa=no_msa)\n            if run_multimer_system:  # Get the length\n                multimer_sequence_length = features['seq_length']\n                entity_number_of_residues = entity.assembly.number_of_residues\n            else:\n                multimer_sequence_length = None\n                entity_number_of_residues = entity.number_of_residues\n\n            logger.debug(f'Found oligomer with length: {entity_number_of_residues}')\n\n            # if run_multimer_system:\n            entity_cb_coords = np.concatenate([mate.cb_coords for mate in entity.chains])\n            # Todo\n            #  entity_backbone_and_cb_coords = entity.assembly.cb_coords\n            # else:\n            #     entity_cb_coords = entity.cb_coords\n\n            entity_interface_residues = \\\n                self.pose.get_interface_residues(entity1=entity, entity2=entity,\n                                                 distance=self.job.interface_distance, oligomeric_interfaces=True)\n            offset_index = entity.offset_index\n            entity_interface_indices = tuple([residue.index - offset_index for residue in residues]\n                                             for residues in entity_interface_residues)\n            entity_name = entity.name\n            this_entity_info = {entity_name: self.pose.entity_info[entity_name]}\n            entity_model_kwargs = dict(name=entity_name, entity_info=this_entity_info)\n            entity_slice = slice(entity.n_terminal_residue.index, 1 + entity.c_terminal_residue.index)\n            entity_scores_by_design = {}\n            # Iterate over provided sequences. Find the best structural model and it's folding_scores\n            for design, sequence in sequences.items():\n                sequence = sequence[entity_slice]\n                this_seq_features = \\\n                    get_sequence_features_to_merge(sequence, multimer_length=multimer_sequence_length)\n                logger.debug(f'Found this_seq_features:\\n\\t'\n                             '%s' % \"\\n\\t\".join((f\"{k}={v}\" for k, v in this_seq_features.items())))\n                # If not an oligomer, then get_prev_pos_coords() will just use the entity\n                model_features = {'prev_pos': get_prev_pos_coords(sequence, entity=entity_name)}\n                logger.info(f'Predicting Design {design.name} Entity {entity_name} structure')\n                entity_structures, entity_scores = \\\n                    resources.ml.af_predict({**features, **this_seq_features, **model_features}, model_runners)\n                # NOT using relaxation as these won't be output for design so their coarse features are all that\n                # are desired\n                #     gpu_relax=self.job.predict.use_gpu_relax, models_to_relax=self.job.predict.models_to_relax)\n                # if relaxed:\n                #     structures_to_load = entity_structures.get('relaxed', [])\n                # else:\n                structures_to_load = entity_structures.get('unrelaxed', [])\n\n                design_models = {model_name: Model.from_pdb_lines(structure.splitlines(), **entity_model_kwargs)\n                                 for model_name, structure in structures_to_load.items()}\n                # if relaxed:  # Set b-factor data as relaxed get overwritten\n                #     type_str = ''\n                #     for model_name, model in design_models.items():\n                #         model.set_b_factor_data(entity_scores[model_name]['plddt'][:entity_number_of_residues])\n                #     entity_structures['relaxed'] = \\\n                #         {model_name: model.get_atom_record() for model_name, model in design_models.items()}\n                # else:\n                type_str = 'un'\n\n                # output_alphafold_structures(entity_structures, design_name=f'{design}-{entity.name}')\n                # Check for the prediction rmsd between the backbone of the Entity Model and Alphafold Model\n                # Also, perform an alignment to the pose Entity\n                rmsds, minimum_model = find_model_with_minimal_rmsd(design_models, entity_cb_coords)\n                # rmsds, minimum_model = find_model_with_minimal_rmsd(design_models, entity_backbone_and_cb_coords)\n                if minimum_model is None:\n                    raise DesignError(\n                        f\"Couldn't find the Entity {entity.name} model with the minimal rmsd for Design {design}\")\n\n                # Put Entity Model into a directory in pose/designs/pose-design_id/entity.name.pdb\n                out_dir = os.path.join(self.designs_path, f'{design.name}')\n                putils.make_path(out_dir)\n                path = os.path.join(out_dir, f'{entity.name}-{minimum_model}-{type_str}relaxed.pdb')\n                minimum_entity = design_models[minimum_model]\n                minimum_entity.write(out_path=path)\n                # Append each Entity result to the full return\n                entity_structure_by_design[design].append(minimum_entity)\n                # Average all models scores to get the ensemble of the predictions\n                combined_scores = combine_model_scores(list(entity_scores.values()))\n                entity_scores_by_design[design.name] = {'rmsd_prediction_ensemble': rmsds, **combined_scores}\n                # entity_scores_by_design[str(design)] = \\\n                #     {'rmsd_prediction_ensemble': rmsds, **entity_scores[minimum_model]}\n                \"\"\"Each design in entity_scores_by_design contains the following features\n                {'predicted_aligned_error': [(n_residues, n_residues), ...]  # multimer/monomer_ptm\n                 'plddt': [(n_residues,), ...]\n                 'predicted_interface_template_modeling_score': [float, ...]  # multimer\n                 'predicted_template_modeling_score': [float, ...]  # multimer/monomer_ptm\n                 'rmsd_prediction_ensemble: [(number_of_models), ...]\n                 }\n                \"\"\"\n\n            # Todo\n            #  Ensure the sequence length is the size of the entity. If saving the entity_residues_df need to\n            #  change the column index to reflect the number of residues\n            entity_sequence_length = entity_slice.stop - entity_slice.start\n            entity_designs_df, entity_residues_df = \\\n                self.analyze_alphafold_metrics(entity_scores_by_design, entity_sequence_length,\n                                               model_type=model_type, interface_indices=entity_interface_indices)\n            # Set the index to use the design.id for each design instance and EntityData.id as an additional column\n            entity_designs_df.index = pd.MultiIndex.from_product(\n                [design_ids, [entity_data.id]], names=[sql.DesignEntityMetrics.design_id.name,\n                                                       sql.DesignEntityMetrics.entity_id.name])\n            entity_design_dfs.append(entity_designs_df)\n\n            # These aren't currently written...\n            entity_residue_dfs.append(entity_residues_df)\n\n        # Save the entity_designs_df DataFrames\n        with self.job.db.session(expire_on_commit=False) as session:\n            for entity_designs_df in entity_design_dfs:\n                sql.write_dataframe(session, entity_designs=entity_designs_df)\n            session.commit()\n\n        # Try to perform an analysis of the separated versus the combined prediction\n        if self.job.predict.designs:\n            # Reorder the entity structures\n            for design, entity_structs in entity_structure_by_design.items():\n                entity_structure_by_design[design] = \\\n                    [entity_structs[idx] for idx in entity_idx_sorted_residue_number_highest_to_lowest]\n            # Combine Entity structure to compare with the Pose prediction\n            entity_design_structures = [\n                Pose.from_entities([entity for model in entity_models for entity in model.entities],\n                                   name=design.name, **pose_kwargs)\n                for design, entity_models in entity_structure_by_design.items()\n            ]\n            # Combine Entity scores to compare with the Pose prediction\n            for residue_df, entity in zip(entity_residue_dfs, self.pose.entities):\n                # Rename the residue_indices along the top most column of DataFrame\n                residue_df.columns = \\\n                    residue_df.columns.set_levels(list(range(entity.n_terminal_residue.index,\n                                                             1 + entity.c_terminal_residue.index)),\n                                                  level=0)\n                # residue_df.rename(columns=dict(zip(range(entity.number_of_residues),\n                #                                    range(entity.n_terminal_residue.index,\n                #                                          entity.c_terminal_residue.index)\n                #                                    )))\n            entity_residues_df = pd.concat(entity_residue_dfs, axis=1)\n            try:\n                self.log.info('Testing the addition of entity_designs_df. They were not adding correctly without '\n                              '*unpack')\n                entity_designs_df, *extra_entity_designs_df = entity_design_dfs\n                for df in extra_entity_designs_df:\n                    entity_designs_df += df\n\n                entity_designs_df /= number_of_entities\n                # entity_designs_df = pd.concat(entity_design_dfs, axis=0)\n                # score_types_mean = ['rmsd_prediction_ensemble']\n                # if 'multimer' in model_type:\n                #     score_types_mean += ['predicted_interface_template_modeling_score',\n                #                          'predicted_template_modeling_score']\n                # elif 'ptm' in model_type:\n                #     score_types_mean += ['predicted_template_modeling_score']\n                #\n                # score_types_concat = ['predicted_aligned_error', 'plddt']\n                #\n                # entity_design_scores = []\n                # for design in sequences:\n                #     entity_scores = entity_scores_by_design[design]\n                #     logger.debug(f'Found entity_scores with contents:\\n{entity_scores}')\n                #     scalar_scores = {score_type: sum([sum(scores[score_type]) for scores in entity_scores])\n                #                      / number_of_entities\n                #                      for score_type in score_types_mean}\n                #     # 'predicted_aligned_error' won't concat correctly, so we average over each residue first\n                #     for scores in entity_scores:\n                #         scores['predicted_aligned_error'] = scores['predicted_aligned_error'].mean(axis=-1)\n                #     array_scores = {score_type: np.concatenate([scores[score_type] for scores in entity_scores])\n                #                     for score_type in score_types_concat}\n                #     scalar_scores.update(array_scores)\n                #     logger.debug(f'Found scalar_scores with contents:\\n{scalar_scores}')\n                #     entity_design_scores.append(scalar_scores)\n\n                # scores = {}\n                rmsds = []\n                for idx, design in enumerate(sequences):\n                    entity_pose = entity_design_structures[idx]\n                    asu_model = asu_design_structures[idx]\n                    # Find the RMSD between each type\n                    rmsd, rot, tx = superposition3d(asu_model.backbone_and_cb_coords,\n                                                    entity_pose.backbone_and_cb_coords)\n                    # score_deviation['rmsd'] = rmsd\n                    # scores[design] = score_deviation\n                    # self.log.critical(f'Found rmsd between separated entities and combined pose: {rmsd}')\n                    rmsds.append(rmsd)\n\n                # Compare all folding_scores\n                design_deviation_df = (predict_designs_df - entity_designs_df).abs()\n                design_deviation_df['rmsd_prediction_deviation'] = rmsds\n                design_deviation_file = \\\n                    os.path.join(self.data_path, f'{starttime}-af_pose-entity-designs-deviation_scores.csv')\n                design_deviation_df.to_csv(design_deviation_file)\n                logger.info('Wrote the design deviation file (between separate Entity instances and Pose)'\n                            f' to: {design_deviation_file}')\n                residue_deviation_df = (predict_residues_df - entity_residues_df).abs()\n                deviation_file = \\\n                    os.path.join(self.data_path, f'{starttime}-af_pose-entity-residues-deviation_scores.csv')\n                residue_deviation_df.to_csv(deviation_file)\n            except Exception as error:\n                raise DesignError(error)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.refine","title":"refine","text":"<pre><code>refine(to_pose_directory: bool = True, gather_metrics: bool = True, design_files: list[AnyStr] = None, in_file_list: AnyStr = None)\n</code></pre> <p>Refine the PoseJob.pose instance or design Model instances associated with this instance</p> <p>Parameters:</p> <ul> <li> <code>to_pose_directory</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the refinement should be saved to the PoseJob</p> </li> <li> <code>gather_metrics</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether metrics should be calculated for the Pose</p> </li> <li> <code>design_files</code>             (<code>list[AnyStr]</code>, default:                 <code>None</code> )         \u2013          <p>A list of files to perform refinement on</p> </li> <li> <code>in_file_list</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The path to a file containing a list of files to pass to Rosetta refinement</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def refine(self, to_pose_directory: bool = True, gather_metrics: bool = True,\n           design_files: list[AnyStr] = None, in_file_list: AnyStr = None):\n    \"\"\"Refine the PoseJob.pose instance or design Model instances associated with this instance\n\n    Args:\n        to_pose_directory: Whether the refinement should be saved to the PoseJob\n        gather_metrics: Whether metrics should be calculated for the Pose\n        design_files: A list of files to perform refinement on\n        in_file_list: The path to a file containing a list of files to pass to Rosetta refinement\n    \"\"\"\n    main_cmd = rosetta.script_cmd.copy()\n\n    suffix = []\n    generate_files_cmd = null_cmd\n    if to_pose_directory:  # Original protocol to refine a Nanohedra pose\n        flag_dir = self.scripts_path\n        pdb_out_path = self.designs_path\n        refine_pdb = self.refine_pdb\n        refined_pdb = self.refined_pdb\n        additional_flags = []\n    else:  # Protocol to refine input structure, place in a common location, then transform for many jobs to source\n        flag_dir = pdb_out_path = self.job.refine_dir\n        refine_pdb = self.source_path\n        refined_pdb = os.path.join(pdb_out_path, refine_pdb)\n        additional_flags = ['-no_scorefile', 'true']\n\n    flags_file = os.path.join(flag_dir, 'flags')\n    self.prepare_rosetta_flags(pdb_out_path=pdb_out_path, out_dir=flag_dir)\n    putils.make_path(pdb_out_path)\n\n    # Assign designable residues to interface1/interface2 variables, not necessary for non-complexed PDB jobs\n    if design_files is not None or in_file_list is not None:  # Run a list of files produced elsewhere\n        possible_refine_protocols = ['refine', 'thread']\n        if self.protocol in possible_refine_protocols:\n            switch = self.protocol\n        elif self.protocol is None:\n            switch = putils.refine\n        else:\n            switch = putils.refine\n            self.log.warning(f\"{self.refine.__name__}: The requested protocol '{self.protocol}', wasn't recognized \"\n                             f\"and is being treated as the standard '{switch}' protocol\")\n\n        # Create file output\n        designed_files_file = os.path.join(self.scripts_path, f'{starttime}_{switch}_files_output.txt')\n        if in_file_list:\n            design_files_file = in_file_list\n            generate_files_cmd = \\\n                ['python', putils.list_pdb_files, '-d', self.designs_path, '-o', designed_files_file, '-e', '.pdb',\n                 '-s', f'_{switch}']\n        elif design_files:\n            design_files_file = os.path.join(self.scripts_path, f'{starttime}_{self.protocol}_files.txt')\n            with open(design_files_file, 'w') as f:\n                f.write('%s\\n' % '\\n'.join(design_files))\n            # Write the designed_files_file with all \"tentatively\" designed file paths\n            out_file_string = f'%s{os.sep}{pdb_out_path}{os.sep}%s'\n            with open(designed_files_file, 'w') as f:\n                f.write('%s\\n' % '\\n'.join(os.path.join(pdb_out_path, os.path.basename(file))\n                                           for file in design_files))\n        else:\n            raise ValueError(\n                f\"Couldn't run {self.refine.__name__} without passing parameter 'design_files' or 'in_file_list'\")\n\n        # -in:file:native is here to block flag file version, not actually useful for refine\n        infile = ['-in:file:l', design_files_file, '-in:file:native', self.source_path]\n        metrics_pdb = ['-in:file:l', designed_files_file, '-in:file:native', self.source_path]\n        # generate_files_cmdline = [list2cmdline(generate_files_cmd)]\n    else:\n        self.protocol = switch = putils.refine\n        if self.job.interface_to_alanine:  # Mutate all design positions to Ala before the Refinement\n            # Ensure the mutations to the pose are wiped\n            pose_copy = self.pose.copy()\n            self.log.critical(f'In {self.refine.__name__}, ensure that the pose was copied correctly before '\n                              'trusting output. The residue in mutate_residue() should be equal after the copy... '\n                              'Delete this log if everything checks out')\n            for entity_pair, interface_residues_pair in self.pose.interface_residues_by_entity_pair.items():\n                # if interface_residues_pair[0]:  # Check that there are residues present\n                for entity, interface_residues in zip(entity_pair, interface_residues_pair):\n                    for residue in interface_residues:\n                        if residue.type != 'GLY':  # No mutation from GLY to ALA as Rosetta would build a CB\n                            pose_copy.mutate_residue(residue=residue, to='A')\n            # Change the name to reflect mutation so the self.pose_path isn't overwritten\n            refine_pdb = f'{os.path.splitext(refine_pdb)[0]}_ala_mutant.pdb'\n        # else:  # Do nothing and refine the source\n        #     pass\n\n        # # Set up self.refined_pdb by using a suffix\n        # suffix = ['-out:suffix', f'_{switch}']\n\n        self.pose.write(out_path=refine_pdb)\n        self.log.debug(f'Cleaned PDB for {switch}: \"{refine_pdb}\"')\n        # -in:file:native is here to block flag file version, not actually useful for refine\n        infile = ['-in:file:s', refine_pdb, '-in:file:native', refine_pdb]\n        metrics_pdb = ['-in:file:s', refined_pdb, '-in:file:native', refine_pdb]\n\n    # RELAX: Prepare command\n    if self.symmetry_dimension is not None and self.symmetry_dimension &gt; 0:\n        symmetry_definition = ['-symmetry_definition', 'CRYST1']\n    else:\n        symmetry_definition = []\n\n    # '-no_nstruct_label', 'true' comes from v\n    relax_cmd = main_cmd + rosetta.relax_flags_cmdline + additional_flags + symmetry_definition \\\n        + [f'@{flags_file}', '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, f'refine.xml'),\n           '-parser:script_vars', f'switch={switch}'] + infile + suffix \\\n        + (['-overwrite'] if self.job.overwrite else [])\n    self.log.info(f'{switch.title()} Command: {list2cmdline(relax_cmd)}')\n\n    if gather_metrics or self.job.metrics:\n        gather_metrics = True\n        if self.job.mpi &gt; 0:\n            main_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(self.job.mpi)] + main_cmd\n        # main_cmd += metrics_pdb\n        main_cmd += [f'@{flags_file}', '-out:file:score_only', self.scores_file,\n                     '-no_nstruct_label', 'true'] + metrics_pdb + ['-parser:protocol']\n        dev_label = '_DEV' if self.job.development else ''\n        metric_cmd_bound = main_cmd \\\n            + [os.path.join(putils.rosetta_scripts_dir, f'interface_metrics{dev_label}.xml')] \\\n            + symmetry_definition\n        entity_cmd = main_cmd + [os.path.join(putils.rosetta_scripts_dir, f'metrics_entity{dev_label}.xml')]\n        self.log.info(f'Metrics command for Pose: {list2cmdline(metric_cmd_bound)}')\n        metric_cmds = [metric_cmd_bound] \\\n            + self.generate_entity_metrics_commands(entity_cmd)\n    else:\n        metric_cmds = []\n\n    # Create executable/Run FastRelax on Clean ASU with RosettaScripts\n    if self.job.distribute_work:\n        analysis_cmd = self.get_cmd_process_rosetta_metrics()\n        self.current_script = distribute.write_script(\n            list2cmdline(relax_cmd), name=f'{starttime}_{self.protocol}.sh', out_path=flag_dir,\n            additional=[list2cmdline(generate_files_cmd)]\n            + [list2cmdline(command) for command in metric_cmds] + [list2cmdline(analysis_cmd)])\n    else:\n        relax_process = Popen(relax_cmd)\n        relax_process.communicate()  # Wait for command to complete\n        list_all_files_process = Popen(generate_files_cmd)\n        list_all_files_process.communicate()\n        if gather_metrics:\n            for metric_cmd in metric_cmds:\n                metrics_process = Popen(metric_cmd)\n                metrics_process.communicate()\n\n            # Gather metrics for each design produced from this procedure\n            if os.path.exists(self.scores_file):\n                self.process_rosetta_metrics()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.rosetta_interface_design","title":"rosetta_interface_design","text":"<pre><code>rosetta_interface_design()\n</code></pre> <p>For the basic process of sequence design between two halves of an interface, write the necessary files for refinement (FastRelax), redesign (FastDesign), and metrics collection (Filters &amp; SimpleMetrics)</p> <p>Stores job variables in a [stage]_flags file and the command in a [stage].sh file. Sets up dependencies based on the PoseJob</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def rosetta_interface_design(self):\n    \"\"\"For the basic process of sequence design between two halves of an interface, write the necessary files for\n    refinement (FastRelax), redesign (FastDesign), and metrics collection (Filters &amp; SimpleMetrics)\n\n    Stores job variables in a [stage]_flags file and the command in a [stage].sh file. Sets up dependencies based\n    on the PoseJob\n    \"\"\"\n    raise NotImplementedError(\n        f'There are multiple outdated dependencies that need to be updated to use Rosetta {flags.interface_design}'\n        f'with modern {putils.program_name}')\n    # Todo\n    #  Modify the way that files are generated/named and later listed for metrics. Right now, reliance on the file\n    #  suffix to get this right, but with the database, this is not needed and will result in inaccurate use\n    # Set up the command base (rosetta bin and database paths)\n    main_cmd = rosetta.script_cmd.copy()\n    if self.symmetry_dimension is not None and self.symmetry_dimension &gt; 0:\n        main_cmd += ['-symmetry_definition', 'CRYST1']\n\n    # Todo - Has this been solved?\n    #  must set up a blank -in:file:pssm in case the evolutionary matrix is not used. Design will fail!!\n    profile_cmd = ['-in:file:pssm', self.evolutionary_profile_file] \\\n        if os.path.exists(self.evolutionary_profile_file) else []\n\n    additional_cmds = []\n    out_file = []\n    design_files = os.path.join(self.scripts_path, f'{starttime}_design-files_{self.protocol}.txt')\n    if self.job.design.scout:\n        self.protocol = protocol_xml1 = putils.scout\n        # metrics_pdb = ['-in:file:s', self.scouted_pdb]\n        generate_files_cmd = \\\n            ['python', putils.list_pdb_files, '-d', self.designs_path, '-o', design_files, '-e', '.pdb',\n             '-s', f'_{self.protocol}']\n        metrics_pdb = ['-in:file:l', design_files]\n        # metrics_flags = 'repack=no'\n        nstruct_instruct = ['-no_nstruct_label', 'true']\n    else:\n        generate_files_cmd = \\\n            ['python', putils.list_pdb_files, '-d', self.designs_path, '-o', design_files, '-e', '.pdb',\n             '-s', f'_{self.protocol}']\n        metrics_pdb = ['-in:file:l', design_files]\n        # metrics_flags = 'repack=yes'\n        if self.job.design.structure_background:\n            self.protocol = protocol_xml1 = putils.structure_background\n            nstruct_instruct = ['-nstruct', str(self.job.design.number)]\n        elif self.job.design.hbnet:  # Run hbnet_design_profile protocol\n            self.protocol, protocol_xml1 = putils.hbnet_design_profile, 'hbnet_scout'\n            nstruct_instruct = ['-no_nstruct_label', 'true']\n            # Set up an additional command to perform interface design on hydrogen bond network from hbnet_scout\n            additional_cmds = \\\n                [[putils.hbnet_sort, os.path.join(self.data_path, 'hbnet_silent.o'),\n                  str(self.job.design.number)]] \\\n                + [main_cmd + profile_cmd\n                   + ['-in:file:silent', os.path.join(self.data_path, 'hbnet_selected.o'), f'@{self.flags}',\n                      '-in:file:silent_struct_type', 'binary',  # '-out:suffix', f'_{self.protocol}',\n                      # adding no_nstruct_label true as only hbnet uses this mechanism\n                      # hbnet_design_profile.xml could be just design_profile.xml\n                      '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, f'{self.protocol}.xml')] \\\n                   + nstruct_instruct]\n            # Set up additional out_file\n            out_file = ['-out:file:silent', os.path.join(self.data_path, 'hbnet_silent.o'),\n                        '-out:file:silent_struct_type', 'binary']\n            # silent_file = os.path.join(self.data_path, 'hbnet_silent.o')\n            # additional_commands = \\\n            #     [\n            #      # ['grep', '^SCORE', silent_file, '&gt;', os.path.join(self.data_path, 'hbnet_scores.sc')],\n            #      main_cmd + [os.path.join(self.data_path, 'hbnet_selected.o')]\n            #      [os.path.join(self.data_path, 'hbnet_selected.tags')]\n            #     ]\n        else:  # Run the legacy protocol\n            self.protocol = protocol_xml1 = flags.interface_design\n            nstruct_instruct = ['-nstruct', str(self.job.design.number)]\n\n    # DESIGN: Prepare command and flags file\n    self.prepare_rosetta_flags(out_dir=self.scripts_path)\n\n    if self.job.design.method == putils.consensus:\n        self.protocol = putils.consensus\n        design_cmd = main_cmd + rosetta.relax_flags_cmdline \\\n            + [f'@{self.flags}', '-in:file:s', self.consensus_pdb,\n               # '-in:file:native', self.refined_pdb,\n               '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, f'consensus.xml'),\n               '-out:suffix', f'_{self.protocol}', '-parser:script_vars', f'switch={putils.consensus}']\n    else:\n        design_cmd = main_cmd + profile_cmd + \\\n            [f'@{self.flags}', '-in:file:s', self.refined_pdb,\n             # self.scouted_pdb if os.path.exists(self.scouted_pdb) else self.refined_pdb,\n             '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, f'{protocol_xml1}.xml'),\n             '-out:suffix', f'_{self.protocol}'] + out_file + nstruct_instruct\n    if self.job.overwrite:\n        design_cmd += ['-overwrite']\n\n    # METRICS: Can remove if SimpleMetrics adopts pose metric caching and restoration\n    # Assumes all entity chains are renamed from A to Z for entities (1 to n)\n    entity_cmd = rosetta.script_cmd + metrics_pdb + \\\n        [f'@{self.flags}', '-out:file:score_only', self.scores_file, '-no_nstruct_label', 'true',\n         '-parser:protocol', os.path.join(putils.rosetta_scripts_dir, 'metrics_entity.xml')]\n\n    if self.job.mpi &gt; 0 and not self.job.design.scout:\n        design_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(self.job.mpi)] + design_cmd\n        entity_cmd = rosetta.run_cmds[putils.rosetta_extras] + [str(self.job.mpi)] + entity_cmd\n\n    self.log.info(f'{self.rosetta_interface_design.__name__} command: {list2cmdline(design_cmd)}')\n    metric_cmds = self.generate_entity_metrics_commands(entity_cmd)\n\n    # Create executable/Run FastDesign on Refined ASU with RosettaScripts. Then, gather Metrics\n    if self.job.distribute_work:\n        analysis_cmd = self.get_cmd_process_rosetta_metrics()\n        self.current_script = distribute.write_script(\n            list2cmdline(design_cmd), name=f'{starttime}_{self.protocol}', out_path=self.scripts_path,\n            additional=[list2cmdline(command) for command in additional_cmds]\n            + [list2cmdline(generate_files_cmd)]\n            + [list2cmdline(command) for command in metric_cmds] + [list2cmdline(analysis_cmd)])\n    else:\n        design_process = Popen(design_cmd)\n        design_process.communicate()  # Wait for command to complete\n        for command in additional_cmds:\n            process = Popen(command)\n            process.communicate()\n        list_all_files_process = Popen(generate_files_cmd)\n        list_all_files_process.communicate()\n        for metric_cmd in metric_cmds:\n            metrics_process = Popen(metric_cmd)\n            metrics_process.communicate()\n\n        # Gather metrics for each design produced from this procedure\n        if os.path.exists(self.scores_file):\n            self.process_rosetta_metrics()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.proteinmpnn_design","title":"proteinmpnn_design","text":"<pre><code>proteinmpnn_design(interface: bool = False, neighbors: bool = False) -&gt; None\n</code></pre> <p>Perform design based on the ProteinMPNN graph encoder/decoder network</p> Sets <p>self.protocol = 'proteinmpnn'</p> <p>Parameters:</p> <ul> <li> <code>interface</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to only specify the interface as designable, otherwise, use all residues</p> </li> <li> <code>neighbors</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to design interface neighbors</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def proteinmpnn_design(self, interface: bool = False, neighbors: bool = False) -&gt; None:\n    \"\"\"Perform design based on the ProteinMPNN graph encoder/decoder network\n\n    Sets:\n        self.protocol = 'proteinmpnn'\n\n    Args:\n        interface: Whether to only specify the interface as designable, otherwise, use all residues\n        neighbors: Whether to design interface neighbors\n\n    Returns:\n        None\n    \"\"\"\n    self.protocol = putils.proteinmpnn\n    self.log.info(f'Starting {self.protocol} design calculation with {self.job.design.number} '\n                  f'designs over each of the temperatures: {self.job.design.temperatures}')\n    # design_start = time.time()\n    sequences_and_scores: dict[str, np.ndarray | list] = \\\n        self.pose.design_sequences(number=self.job.design.number,\n                                   temperatures=self.job.design.temperatures,\n                                   # interface=interface, neighbors=neighbors,\n                                   interface=self.job.design.interface, neighbors=self.job.design.neighbors,\n                                   ca_only=self.job.design.ca_only,\n                                   model_name=self.job.design.proteinmpnn_model\n                                   )\n    # self.log.debug(f\"Took {time.time() - design_start:8f}s for design_sequences\")\n\n    # self.output_proteinmpnn_scores(design_names, sequences_and_scores)\n    # # Write every designed sequence to the sequences file...\n    # write_sequences(sequences_and_scores['sequences'], names=design_names, file_name=self.designed_sequences_file)\n    # Convert sequences to a plain string sequence representation\n    sequences_and_scores['sequences'] = \\\n        [''.join(sequence) for sequence in sequences_and_scores['sequences'].tolist()]\n\n    # Add protocol (job info) and temperature to sequences_and_scores\n    # number_of_new_designs = len(designs_metadata)\n    # sequences_and_scores[putils.protocol] = list(repeat(self.protocol, len(designs_metadata)))\n    # sequences_and_scores['temperatures'] = [temperature for temperature in self.job.design.temperatures\n    # protocols = list(repeat(self.protocol, len(designs_metadata)))\n    temperatures = [temperature for temperature in self.job.design.temperatures\n                    for _ in range(self.job.design.number)]\n\n    # # Write every designed sequence to an individual file...\n    # putils.make_path(self.designs_path)\n    # design_names = [design_data.name for design_data in designs_data]\n    # sequence_files = [\n    #     write_sequences(sequence, names=name, file_name=os.path.join(self.designs_path, name))\n    #     for name, sequence in zip(design_names, sequences_and_scores['sequences'])\n    # ]\n\n    # Update the Pose with the number of designs\n    with self.job.db.session(expire_on_commit=False) as session:\n        session.add(self)\n        designs_data = self.update_design_data(design_parent=self.pose_source)\n        session.add_all(designs_data)\n        session.flush()\n        design_ids = [design_data.id for design_data in designs_data]\n\n        # Update the Pose with the design protocols\n        for idx, design_data in enumerate(designs_data):\n            design_data.protocols.append(\n                sql.DesignProtocol(design=design_data,\n                                   job_id=self.job.id,\n                                   # design_id=design_ids[idx],\n                                   protocol=self.protocol,\n                                   temperature=temperatures[idx],\n                                   ))\n\n        # analysis_start = time.time()\n        designs_df, residues_df = self.analyze_proteinmpnn_metrics(design_ids, sequences_and_scores)\n        entity_designs_df = self.analyze_design_entities_per_residue(residues_df)\n        sql.write_dataframe(session, entity_designs=entity_designs_df)\n\n        # self.log.debug(f\"Took {time.time() - analysis_start:8f}s for analyze_proteinmpnn_metrics. \"\n        #                f\"{time.time() - design_start:8f}s total\")\n        self.output_metrics(session, designs=designs_df)\n        output_residues = False\n        if output_residues:\n            self.output_metrics(session, residues=residues_df)\n        else:  # Only save the 'design_residue' columns\n            residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n            self.output_metrics(session, design_residues=residues_df)\n        # Commit the newly acquired metrics\n        session.commit()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.update_design_protocols","title":"update_design_protocols","text":"<pre><code>update_design_protocols(design_ids: Sequence[str], protocols: Sequence[str] = None, temperatures: Sequence[float] = None, files: Sequence[AnyStr] = None) -&gt; list[DesignProtocol]\n</code></pre> <p>Associate newly created DesignData with DesignProtocol</p> <p>Parameters:</p> <ul> <li> <code>design_ids</code>             (<code>Sequence[str]</code>)         \u2013          <p>The identifiers for each DesignData</p> </li> <li> <code>protocols</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>The sequence of protocols to associate with DesignProtocol</p> </li> <li> <code>temperatures</code>             (<code>Sequence[float]</code>, default:                 <code>None</code> )         \u2013          <p>The temperatures to associate with DesignProtocol</p> </li> <li> <code>files</code>             (<code>Sequence[AnyStr]</code>, default:                 <code>None</code> )         \u2013          <p>The sequence of files to associate with DesignProtocol</p> </li> </ul> <p>Returns:     The new instances of the sql.DesignProtocol</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def update_design_protocols(self, design_ids: Sequence[str], protocols: Sequence[str] = None,\n                            temperatures: Sequence[float] = None, files: Sequence[AnyStr] = None) \\\n        -&gt; list[sql.DesignProtocol]:  # Unused\n    \"\"\"Associate newly created DesignData with DesignProtocol\n\n    Args:\n        design_ids: The identifiers for each DesignData\n        protocols: The sequence of protocols to associate with DesignProtocol\n        temperatures: The temperatures to associate with DesignProtocol\n        files: The sequence of files to associate with DesignProtocol\n    Returns:\n        The new instances of the sql.DesignProtocol\n    \"\"\"\n    metadata = [sql.DesignProtocol(\n        design_id=design_id, job_id=self.job.id, protocol=protocol, temperature=temperature, file=file)\n                for design_id, protocol, temperature, file in zip(design_ids, protocols, temperatures, files)]\n    return metadata\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.update_design_data","title":"update_design_data","text":"<pre><code>update_design_data(design_parent: DesignData, number: int = None) -&gt; list[DesignData]\n</code></pre> <p>Updates the PoseData with newly created design identifiers using DesignData</p> Sets <p>self.current_designs (list[DesignData]): Extends with the newly created DesignData instances</p> <p>Parameters:</p> <ul> <li> <code>design_parent</code>             (<code>DesignData</code>)         \u2013          <p>The design whom all new designs are based</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of designs. If not provided, set according to job.design.number * job.design.temperature</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DesignData]</code>         \u2013          <p>The new instances of the DesignData</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def update_design_data(self, design_parent: sql.DesignData, number: int = None) -&gt; list[sql.DesignData]:\n    \"\"\"Updates the PoseData with newly created design identifiers using DesignData\n\n    Sets:\n        self.current_designs (list[DesignData]): Extends with the newly created DesignData instances\n\n    Args:\n        design_parent: The design whom all new designs are based\n        number: The number of designs. If not provided, set according to job.design.number * job.design.temperature\n\n    Returns:\n        The new instances of the DesignData\n    \"\"\"\n    if number is None:\n        number = len(self.job.design.number * self.job.design.temperatures)\n\n    first_new_design_idx = self.number_of_designs  # + 1 &lt;- don't add 1 since the first design is .pose_source\n    # design_names = [f'{self.protocol}{seq_idx:04d}'  # f'{self.name}_{self.protocol}{seq_idx:04d}'\n    design_names = [f'{self.name}-{design_idx:04d}'\n                    for design_idx in range(first_new_design_idx, first_new_design_idx + number)]\n    designs = [sql.DesignData(name=name, pose_id=self.id, design_parent=design_parent)\n               for name in design_names]\n    # Set the PoseJob.current_designs for access by subsequent functions/protocols\n    self.current_designs.extend(designs)\n\n    return designs\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.output_metrics","title":"output_metrics","text":"<pre><code>output_metrics(session: Session = None, designs: DataFrame = None, design_residues: DataFrame = None, residues: DataFrame = None, pose_metrics: bool = False)\n</code></pre> <p>Format each possible DataFrame type for output via csv or SQL database</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>, default:                 <code>None</code> )         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>designs</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-design metric DataFrame where each index is the design id and the columns are design metrics</p> </li> <li> <code>design_residues</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, Boolean for design utilization)</p> </li> <li> <code>residues</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> <li> <code>pose_metrics</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the metrics being included are based on Pose (self.pose) measurements</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def output_metrics(self, session: Session = None, designs: pd.DataFrame = None,\n                   design_residues: pd.DataFrame = None, residues: pd.DataFrame = None, pose_metrics: bool = False):\n    \"\"\"Format each possible DataFrame type for output via csv or SQL database\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        designs: The typical per-design metric DataFrame where each index is the design id and the columns are\n            design metrics\n        design_residues: The typical per-residue metric DataFrame where each index is the design id and the columns\n            are (residue index, Boolean for design utilization)\n        residues: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n        pose_metrics: Whether the metrics being included are based on Pose (self.pose) measurements\n    \"\"\"\n    # Remove completely empty columns\n    if designs is not None:\n        designs.dropna(how='all', axis=1, inplace=True)\n    if residues is not None:\n        residues.dropna(how='all', axis=1, inplace=True)\n\n    if session is not None:\n        if designs is not None:\n            # design_index_names = ['pose', 'design']\n            # These are reliant on foreign keys...\n            # design_index_names = [sql.DesignMetrics.pose_id.name, sql.DesignMetrics.name.name]\n            # designs = pd.concat([designs], keys=[pose_identifier], axis=0)\n            # design_index_names = [sql.DesignMetrics.pose_id.name, sql.DesignMetrics.design_id.name]\n            # designs = pd.concat([designs], keys=[pose_identifier], axis=0)\n            # #                     names=design_index_names, axis=0)\n            # designs.index.set_names(design_index_names, inplace=True)\n            designs.index.set_names(sql.DesignMetrics.design_id.name, inplace=True)\n            # _design_ids = metrics.sql.write_dataframe(session, designs=designs)\n            sql.write_dataframe(session, designs=designs)\n\n        if residues is not None:\n            # residue_index_names = ['pose', 'design']\n            # These are reliant on foreign keys...\n            # residue_index_names = [sql.ResidueMetrics.pose_id.name, sql.ResidueMetrics.design_id.name, sql.ResidueMetrics.design_name.name]\n            # residues = pd.concat([residues], keys=list(zip(repeat(pose_identifier), _design_ids)), axis=0)\n            # residue_index_names = [sql.ResidueMetrics.pose_id.name, sql.ResidueMetrics.design_id.name]\n            # residues = pd.concat([residues], keys=[pose_identifier], axis=0)\n            # #                      names=residue_index_names, axis=0)\n            # residues.index.set_names(residue_index_names, inplace=True)\n            if pose_metrics:\n                index_name = sql.PoseResidueMetrics.pose_id.name\n                dataframe_kwargs = dict(pose_residues=residues)\n            else:\n                index_name = sql.ResidueMetrics.design_id.name\n                dataframe_kwargs = dict(residues=residues)\n\n            residues.index.set_names(index_name, inplace=True)\n            sql.write_dataframe(session, **dataframe_kwargs)\n\n        if design_residues is not None:\n            design_residues.index.set_names(sql.ResidueMetrics.design_id.name, inplace=True)\n            sql.write_dataframe(session, design_residues=design_residues)\n    else:\n        putils.make_path(self.data_path)\n        if residues is not None:\n            # Process dataframes for missing values NOT USED with SQL...\n            residues = residues.fillna(0.)\n            residues.sort_index(inplace=True)\n            residues.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)\n            # residue_metric_columns = residues.columns.levels[-1].tolist()\n            # self.log.debug(f'Residues metrics present: {residue_metric_columns}')\n\n            residues.to_csv(self.residues_metrics_csv)\n            self.log.info(f'Wrote Residues metrics to {self.residues_metrics_csv}')\n\n        if designs is not None:\n            designs.sort_index(inplace=True, axis=1)\n            # designs_metric_columns = designs.columns.tolist()\n            # self.log.debug(f'Designs metrics present: {designs_metric_columns}')\n\n            # if self.job.merge:\n            #     designs_df = pd.concat([designs_df], axis=1, keys=['metrics'])\n            #     designs_df = pd.merge(designs_df, residues_df, left_index=True, right_index=True)\n            # else:\n            designs.to_csv(self.designs_metrics_csv)\n            self.log.info(f'Wrote Designs metrics to {self.designs_metrics_csv}')\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.parse_rosetta_scores","title":"parse_rosetta_scores","text":"<pre><code>parse_rosetta_scores(scores: dict[str, dict[str, str | int | float]]) -&gt; tuple[DataFrame, DataFrame]\n</code></pre> <p>Process Rosetta scoring dictionary into suitable values</p> <p>Parameters:</p> <ul> <li> <code>scores</code>             (<code>dict[str, dict[str, str | int | float]]</code>)         \u2013          <p>The dictionary of scores to be parsed</p> </li> </ul> <p>Returns:     A tuple of DataFrame where each contains (         A per-design metric DataFrame where each index is the design id and the columns are design metrics,         A per-residue metric DataFrame where each index is the design id and the columns are             (residue index, residue metric)     )</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def parse_rosetta_scores(self, scores: dict[str, dict[str, str | int | float]]) \\\n        -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Process Rosetta scoring dictionary into suitable values\n\n    Args:\n        scores: The dictionary of scores to be parsed\n    Returns:\n        A tuple of DataFrame where each contains (\n            A per-design metric DataFrame where each index is the design id and the columns are design metrics,\n            A per-residue metric DataFrame where each index is the design id and the columns are\n                (residue index, residue metric)\n        )\n    \"\"\"\n    # Create protocol dataframe\n    scores_df = pd.DataFrame.from_dict(scores, orient='index')\n    # # Fill in all the missing values with that of the default pose_source\n    # scores_df = pd.concat([source_df, scores_df]).fillna(method='ffill')\n    # Gather all columns into specific types for processing and formatting\n    per_res_columns = [column for column in scores_df.columns.tolist() if 'res_' in column]\n\n    # Check proper input\n    metric_set = metrics.rosetta_required.difference(set(scores_df.columns))\n    if metric_set:\n        self.log.debug(f'Score columns present before required metric check: {scores_df.columns.tolist()}')\n        raise DesignError(\n            f'Missing required metrics: \"{\", \".join(metric_set)}\"')\n\n    # Remove unnecessary (old scores) as well as Rosetta pose score terms besides ref (has been renamed above)\n    # Todo learn know how to produce Rosetta score terms in output score file. Not in FastRelax...\n    remove_columns = metrics.rosetta_terms + metrics.unnecessary + per_res_columns\n    # Todo remove dirty when columns are correct (after P432)\n    #  and column tabulation precedes residue/hbond_processing\n    scores_df.drop(remove_columns, axis=1, inplace=True, errors='ignore')\n\n    # Todo implement this protocol if sequence data is taken at multiple points along a trajectory and the\n    #  sequence data along trajectory is a metric on it's own\n    # # Gather mutations for residue specific processing and design sequences\n    # for design, data in list(structure_design_scores.items()):  # make a copy as can be removed\n    #     sequence = data.get('final_sequence')\n    #     if sequence:\n    #         if len(sequence) &gt;= pose_length:\n    #             pose_sequences[design] = sequence[:pose_length]  # Todo won't work if design had insertions\n    #         else:\n    #             pose_sequences[design] = sequence\n    #     else:\n    #         self.log.warning('Design %s is missing sequence data, removing from design pool' % design)\n    #         structure_design_scores.pop(design)\n    # # format {entity: {design_name: sequence, ...}, ...}\n    # entity_sequences = \\\n    #     {entity: {design: sequence[entity.n_terminal_residue.number - 1:entity.c_terminal_residue.number]\n    #               for design, sequence in pose_sequences.items()} for entity in self.pose.entities}\n\n    # Drop designs where required data isn't present\n    if putils.protocol in scores_df.columns:\n        # Format protocol columns\n        # # Todo remove not DEV\n        # missing_group_indices = scores_df[putils.protocol].isna()\n        # scout_indices = [idx for idx in scores_df[missing_group_indices].index if 'scout' in idx]\n        # scores_df.loc[scout_indices, putils.protocol] = putils.scout\n        # structure_bkgnd_indices = [idx for idx in scores_df[missing_group_indices].index if 'no_constraint' in idx]\n        # scores_df.loc[structure_bkgnd_indices, putils.protocol] = putils.structure_background\n        # # Todo Done remove\n        missing_group_indices = scores_df[putils.protocol].isna()\n        # protocol_s.replace({'combo_profile': putils.design_profile}, inplace=True)  # ensure proper profile name\n\n        scores_df.drop(missing_group_indices, axis=0, inplace=True, errors='ignore')\n\n    # Replace empty strings with np.nan and convert remaining to float\n    scores_df.replace('', np.nan, inplace=True)\n    scores_df.fillna(dict(zip(metrics.protocol_specific_columns, repeat(0))), inplace=True)\n    # scores_df = scores_df.astype(float)  # , copy=False, errors='ignore')\n\n    # protocol_s.drop(missing_group_indices, inplace=True, errors='ignore')\n    viable_designs = scores_df.index.tolist()\n    if not viable_designs:\n        raise DesignError(\n            f'No viable designs remain after {self.process_rosetta_metrics.__name__} data processing steps')\n\n    self.log.debug(f'Viable designs with structures remaining after cleaning:\\n\\t{\", \".join(viable_designs)}')\n\n    # Take metrics for the pose_source\n    # entity_energies = [0. for _ in self.pose.entities]\n    # pose_source_residue_info = \\\n    #     {residue.index: {'complex': 0., 'bound': entity_energies.copy(), 'unbound': entity_energies.copy(),\n    #                      'solv_complex': 0., 'solv_bound': entity_energies.copy(),\n    #                      'solv_unbound': entity_energies.copy(), 'fsp': 0., 'cst': 0., 'hbond': 0}\n    #      for entity in self.pose.entities for residue in entity.residues}\n    # pose_source_id = self.pose_source.id\n    # residue_info = {pose_source_id: pose_source_residue_info}\n\n    # residue_info = {'energy': {'complex': 0., 'unbound': 0.}, 'type': None, 'hbond': 0}\n    # residue_info.update(self.pose.rosetta_residue_processing(structure_design_scores))\n    residue_info = self.pose.process_rosetta_residue_scores(scores)\n    # Can't use residue_processing (clean) ^ in the case there is a design without metrics... columns not found!\n    residue_info = metrics.process_residue_info(residue_info, hbonds=self.pose.rosetta_hbond_processing(scores))\n\n    # Process mutational frequencies, H-bond, and Residue energy metrics to dataframe\n    # which ends up with multi-index column with residue index as first (top) column index, metric as second index\n    rosetta_residues_df = pd.concat({design: pd.DataFrame(info) for design, info in residue_info.items()}) \\\n        .unstack()\n\n    return scores_df, rosetta_residues_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.rosetta_column_combinations","title":"rosetta_column_combinations","text":"<pre><code>rosetta_column_combinations(scores_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Calculate metrics from combinations of metrics with variable integer number metric names</p> <p>Parameters:</p> <ul> <li> <code>scores_df</code>             (<code>DataFrame</code>)         \u2013          <p>A DataFrame with Rosetta based metrics that should be combined with other metrics to produce new summary metrics</p> </li> </ul> <p>Returns:     A per-design metric DataFrame where each index is the design id and the columns are design metrics,</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def rosetta_column_combinations(self, scores_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate metrics from combinations of metrics with variable integer number metric names\n\n    Args:\n        scores_df: A DataFrame with Rosetta based metrics that should be combined with other metrics to produce new\n            summary metrics\n    Returns:\n        A per-design metric DataFrame where each index is the design id and the columns are design metrics,\n    \"\"\"\n    scores_columns = scores_df.columns.tolist()\n    self.log.debug(f'Metrics present: {scores_columns}')\n    summation_pairs = \\\n        {'buried_unsatisfied_hbonds_unbound':\n            list(filter(re.compile('buns[0-9]+_unbound$').match, scores_columns)),  # Rosetta\n         # 'interface_energy_bound':\n         #     list(filter(re.compile('interface_energy_[0-9]+_bound').match, scores_columns)),  # Rosetta\n         # 'interface_energy_unbound':\n         #     list(filter(re.compile('interface_energy_[0-9]+_unbound').match, scores_columns)),  # Rosetta\n         # 'interface_solvation_energy_bound':\n         #     list(filter(re.compile('solvation_energy_[0-9]+_bound').match, scores_columns)),  # Rosetta\n         # 'interface_solvation_energy_unbound':\n         #     list(filter(re.compile('solvation_energy_[0-9]+_unbound').match, scores_columns)),  # Rosetta\n         'interface_connectivity':\n             list(filter(re.compile('entity[0-9]+_interface_connectivity').match, scores_columns)),  # Rosetta\n         }\n    scores_df = metrics.columns_to_new_column(scores_df, summation_pairs)\n    # This is a nightmare as the column.map() turns all non-existing to np.nan need to fix upstream instead of\n    # changing here\n    # # Rename buns columns\n    # # This isn't stable long term given mapping of interface number (sql) and entity number (Rosetta)\n    # buns_columns = summation_pairs['buried_unsatisfied_hbonds_unbound']\n    # scores_df.columns = scores_df.columns.map(dict((f'buns{idx}_unbound', f'buried_unsatisfied_hbonds_unbound{idx}')\n    #                                                for idx in range(1, 1 + len(buns_columns))))\\\n    #     .fillna(scores_df.columns)\n    # Add number_residues_interface for div_pairs and int_comp_similarity\n    # scores_df['number_residues_interface'] = other_pose_metrics.pop('number_residues_interface')\n    scores_df = metrics.columns_to_new_column(scores_df, metrics.rosetta_delta_pairs, mode='sub')\n    scores_df = metrics.columns_to_new_column(scores_df, metrics.rosetta_division_pairs, mode='truediv')\n\n    scores_df.drop(metrics.clean_up_intermediate_columns, axis=1, inplace=True, errors='ignore')\n    repacking = scores_df.get('repacking')\n    if repacking is not None:\n        # Set interface_bound_activation_energy = np.nan where repacking is 0\n        # Currently is -1 for True (Rosetta Filter quirk...)\n        scores_df.loc[scores_df[repacking == 0].index, 'interface_bound_activation_energy'] = np.nan\n        scores_df.drop('repacking', axis=1, inplace=True)\n\n    return scores_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.process_rosetta_metrics","title":"process_rosetta_metrics","text":"<pre><code>process_rosetta_metrics()\n</code></pre> <p>From Rosetta based protocols, tally the resulting metrics and integrate with SymDesign metrics database</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def process_rosetta_metrics(self):\n    \"\"\"From Rosetta based protocols, tally the resulting metrics and integrate with SymDesign metrics database\"\"\"\n    self.log.debug(f'Found design scores in file: {self.scores_file}')  # Todo PoseJob(.path)\n    design_scores = metrics.parse_rosetta_scorefile(self.scores_file)\n\n    pose_design_scores = design_scores.pop(self.name, None)\n    if pose_design_scores:\n        # The pose_source is included in the calculations\n        self.calculate_pose_metrics(scores=pose_design_scores)\n\n    if not design_scores:\n        # No design scores were found\n        return\n\n    # Find all designs files\n    with self.job.db.session(expire_on_commit=False) as session:\n        session.add(self)\n        design_names = self.design_names\n    design_files = self.get_design_files()  # Todo PoseJob(.path)\n    scored_design_names = design_scores.keys()\n    new_design_paths_to_process = []\n    rosetta_provided_new_design_names = []\n    existing_design_paths_to_process = []\n    existing_design_indices = []\n    # Collect designs that we have metrics on (possibly again)\n    for idx, path in enumerate(design_files):\n        file_name, ext = os.path.splitext(os.path.basename(path))\n        if file_name in scored_design_names:\n            try:\n                design_data_index = design_names.index(file_name)\n            except ValueError:  # file_name not in design_names\n                # New, this file hasn't been processed\n                new_design_paths_to_process.append(path)\n                rosetta_provided_new_design_names.append(file_name)\n            else:\n                existing_design_paths_to_process.append(path)\n                existing_design_indices.append(design_data_index)\n\n    # Ensure that design_paths_to_process and self.current_designs have the same order\n    design_paths_to_process = existing_design_paths_to_process + new_design_paths_to_process\n    if not design_paths_to_process:\n        return\n\n    # # Find designs with scores but no structures\n    # structure_design_scores = {}\n    # for pose in designs:\n    #     try:\n    #         structure_design_scores[pose.name] = design_scores.pop(pose.name)\n    #     except KeyError:  # Structure wasn't scored, we will remove this later\n    #         pass\n\n    # Process the parsed scores to scores_df, rosetta_residues_df\n    scores_df, rosetta_residues_df = self.parse_rosetta_scores(design_scores)\n\n    # Format DesignData\n    # Get all existing\n    design_data = self.designs  # This is loaded above at 'design_names = self.design_names'\n    # Set current_designs to a fresh list\n    self._current_designs = [design_data[idx] for idx in existing_design_indices]\n    # Find parent info and remove from scores_df\n    if putils.design_parent in scores_df:\n        # Replace missing values with the pose_source DesignData\n        # This is loaded above at 'design_names = self.design_names'\n        parents = scores_df.pop(putils.design_parent)  # .fillna(self.pose_source)\n        logger.critical(\"Setting parents functionality hasn't been tested. Proceed with caution\")\n        for design, parent in parents.items():\n            if parent is np.nan:\n                parents[design] = self.pose_source\n            try:\n                design_name_index = design_names.index(parent)\n            except ValueError:  # This name isn't right\n                raise DesignError(\n                    f\"Couldn't find the design_parent for the design with name '{design}' and parent value of '\"\n                    f\"{parent}'. The available parents are:\\n\\t{', '.join(design_names)}\")\n            else:\n                parents[design] = design_data[design_name_index]\n    else:  # Assume this is an offspring of the pose\n        parents = {provided_name: self.pose_source for provided_name in rosetta_provided_new_design_names}\n\n    # Find protocol info and remove from scores_df\n    if putils.protocol in scores_df:\n        # Replace missing values with the pose_source DesignData\n        protocol_s = scores_df.pop(putils.protocol).fillna('metrics')\n        self.log.debug(f'Found \"protocol_s\" variable with dtype: {protocol_s.dtype}')\n    else:\n        protocol_s = {provided_name: 'metrics' for provided_name in rosetta_provided_new_design_names}\n\n    # Process all desired files to Pose\n    pose_kwargs = self.pose_kwargs\n    design_poses = [Pose.from_file(file, **pose_kwargs) for file in design_paths_to_process]\n    design_sequences = {pose.name: pose.sequence for pose in design_poses}\n    # sequences_df = self.analyze_sequence_metrics_per_design(sequences=design_sequences)\n\n    # The DataFrame.index needs to become design.id not design.name as it is here. Modify after processing\n    residues_df = self.analyze_residue_metrics_per_design(designs=design_poses)\n    # Join Rosetta per-residue DataFrame taking Structure analysis per-residue DataFrame index order\n    residues_df = residues_df.join(rosetta_residues_df)\n\n    designs_df = self.analyze_design_metrics_per_design(residues_df, design_poses)\n    # Join Rosetta per-design DataFrame taking Structure analysis per-design DataFrame index order\n    designs_df = designs_df.join(scores_df)\n\n    # Finish calculation of Rosetta scores with included metrics\n    designs_df = self.rosetta_column_combinations(designs_df)\n\n    # Score using proteinmpnn only if the design was created by Rosetta\n    if rosetta_provided_new_design_names:\n        score_sequences = [design_sequences.pop(new_file_name)\n                           for new_file_name in rosetta_provided_new_design_names]\n        sequences_and_scores = self.pose.score_sequences(\n            score_sequences, model_name=self.job.design.proteinmpnn_model)\n        # Set each position that was parsed as \"designable\"\n        # This includes packable residues from neighborhoods. How can we get only designable?\n        # Right now, it is only the interface residues that go into Rosetta\n        # Use simple reporting here until that changes...\n        # Todo get residues_df['design_indices'] worked out with set up using sql.DesignProtocol?\n        #  See self.analyze_pose_designs()\n        design_residues = residues_df.loc[rosetta_provided_new_design_names,\n                                          idx_slice[:, 'interface_residue']].to_numpy()\n        sequences_and_scores.update({'design_indices': design_residues})\n        mpnn_designs_df, mpnn_residues_df = \\\n            self.analyze_proteinmpnn_metrics(rosetta_provided_new_design_names, sequences_and_scores)\n        # Join DataFrames\n        designs_df = designs_df.join(mpnn_designs_df)\n        residues_df = residues_df.join(mpnn_residues_df)\n    # else:\n    #     mpnn_residues_df = pd.DataFrame()\n    #\n    # # Calculate sequence metrics for the remaining sequences\n    # sequences_df = self.analyze_sequence_metrics_per_design(sequences=design_sequences)\n    # sequences_df = sequences_df.join(mpnn_residues_df)\n\n    # Update the Pose.designs with DesignData for each of the new designs\n    with self.job.db.session(expire_on_commit=False) as session:\n        session.add(self)\n        new_designs_data = self.update_design_data(\n            design_parent=self.pose_source, number=len(new_design_paths_to_process))\n        session.add_all(new_designs_data)\n        # Generate ids for new entries\n        session.flush()\n        session.add_all(self.current_designs)\n\n        # Add attribute to DesignData to save the provided_name and design design_parent\n        for design_data, provided_name in zip(new_designs_data, rosetta_provided_new_design_names):\n            design_data.design_parent = parents[provided_name]\n            design_data.provided_name = provided_name\n\n        designs_path = self.designs_path\n        new_design_new_filenames = {data.provided_name: os.path.join(designs_path, f'{data.name}.pdb')\n                                    for data in new_designs_data}\n\n        # Update the Pose with the design protocols\n        for design in self.current_designs:\n            name_or_provided_name = getattr(design, 'provided_name', getattr(design, 'name'))\n            protocol_kwargs = dict(design_id=design.id,\n                                   job_id=self.job.id,\n                                   protocol=protocol_s[name_or_provided_name],\n                                   # temperature=temperatures[idx],)  # Todo from Rosetta?\n                                   )\n            new_filename = new_design_new_filenames.get(name_or_provided_name)\n            if new_filename:\n                protocol_kwargs['file'] = new_filename\n                # Set the structure_path for this DesignData\n                design.structure_path = new_filename\n            else:\n                protocol_kwargs['file'] = os.path.join(designs_path, f'{name_or_provided_name}.pdb')\n                if not design.structure_path:\n                    design.structure_path = protocol_kwargs['file']\n            design.protocols.append(sql.DesignProtocol(**protocol_kwargs))\n        # else:  # Assume that no design was done and only metrics were acquired\n        #     pass\n\n        # This is all done in update_design_data\n        # self.designs.append(new_designs_data)\n        # # Flush the newly acquired DesignData and DesignProtocol to generate .id primary keys\n        # self.job.current_session.flush()\n        # new_design_ids = [design_data.id for design_data in new_designs_data]\n\n        # Get the name/provided_name to design_id mapping\n        design_name_to_id_map = {\n            getattr(design, 'provided_name', getattr(design, 'name')): design.id\n            for design in self.current_designs}\n\n        # This call is redundant with the analyze_proteinmpnn_metrics(design_names, sequences_and_scores) above\n        # Todo remove from above the sequences portion..? Commenting out below for now\n        # designs_df = designs_df.join(self.analyze_design_metrics_per_residue(sequences_df))\n\n        # Rename all designs and clean up resulting metrics for storage\n        # In keeping with \"unit of work\", only rename once all data is processed incase we run into any errors\n        designs_df.index = designs_df.index.map(design_name_to_id_map)\n        residues_df.index = residues_df.index.map(design_name_to_id_map)\n        if rosetta_provided_new_design_names:\n            # Must move the entity_id to the columns for index.map to work\n            entity_designs_df = self.analyze_design_entities_per_residue(mpnn_residues_df)\n            entity_designs_df.reset_index(level=0, inplace=True)\n            entity_designs_df.index = entity_designs_df.index.map(design_name_to_id_map)\n\n        # Commit the newly acquired metrics to the database\n        # First check if the files are situated correctly\n        temp_count = count()\n        temp_files_to_move = {}\n        files_to_move = {}\n        for filename, new_filename in zip(new_design_paths_to_process, new_design_new_filenames.values()):\n            if filename == new_filename:\n                # These are the same file, proceed without processing\n                continue\n            elif os.path.exists(filename):\n                if not os.path.exists(new_filename):\n                    # Target file exists and nothing exists where it will be moved\n                    files_to_move[filename] = new_filename\n                else:\n                    # The new_filename already exists. Redirect the filename to a temporary file, then complete move\n                    dir_, base = os.path.split(new_filename)\n                    temp_filename = os.path.join(dir_, f'TEMP{next(temp_count)}')\n                    temp_files_to_move[filename] = temp_filename\n                    files_to_move[temp_filename] = new_filename\n            else:  # filename doesn't exist\n                raise DesignError(\n                    f\"The specified file {filename} doesn't exist\")\n\n        # If so, proceed with insert, file rename and commit\n        self.output_metrics(session, designs=designs_df)\n        if rosetta_provided_new_design_names:\n            sql.write_dataframe(session, entity_designs=entity_designs_df)\n        output_residues = False\n        if output_residues:\n            self.output_metrics(session, residues=residues_df)\n        else:  # Only save the 'design_residue' columns\n            if rosetta_provided_new_design_names:\n                residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n                self.output_metrics(session, design_residues=residues_df)\n        # Rename the incoming files to their prescribed names\n        for filename, temp_filename in temp_files_to_move.items():\n            shutil.move(filename, temp_filename)\n        for filename, new_filename in files_to_move.items():\n            shutil.move(filename, new_filename)\n\n        # Commit all new data\n        session.commit()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.calculate_pose_metrics","title":"calculate_pose_metrics","text":"<pre><code>calculate_pose_metrics(**kwargs)\n</code></pre> <p>Collect Pose metrics using the reference Pose</p> <p>Other Parameters:</p> <ul> <li> <code>scores</code>         \u2013          <p>dict[str, str | int | float] = None - Parsed Pose scores from Rosetta output</p> </li> <li> <code>novel_interface</code>         \u2013          <p>bool = True - Whether the pose interface is novel (i.e. docked) or from a bona-fide input</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def calculate_pose_metrics(self, **kwargs):\n    \"\"\"Collect Pose metrics using the reference Pose\n\n    Keyword Args:\n        scores: dict[str, str | int | float] = None - Parsed Pose scores from Rosetta output\n        novel_interface: bool = True - Whether the pose interface is novel (i.e. docked) or from a bona-fide input\n    \"\"\"\n    self.load_pose()\n    # self.identify_interface()\n    if not self.pose.fragment_info_by_entity_pair:\n        self.generate_fragments(interface=True)\n\n    def get_metrics():\n        _metrics = self.pose.metrics  # Also calculates entity.metrics\n        # Todo\n        # # Gather the docking metrics if not acquired from Nanohedra\n        # pose_residues_df = self.analyze_docked_metrics()\n        # self.output_metrics(residues=pose_residues_df, pose_metrics=True)\n        # Todo move into this mechanism PoseMetrics level calculations of the following:\n        #  dock_collapse_*,\n        #  dock_hydrophobicity,\n        #  proteinmpnn_v_evolution_probability_cross_entropy_loss\n        #\n        idx = 1\n        is_thermophilic = []\n        for idx, (entity, data) in enumerate(zip(self.pose.entities, self.entity_data), idx):\n            # Todo remove entity.thermophilicity once sql load more streamlined\n            # is_thermophilic.append(1 if entity.thermophilicity else 0)\n            is_thermophilic.append(entity.thermophilicity)\n            # Save entity.metrics to db\n            data.metrics = entity.metrics\n        _metrics.pose_thermophilicity = sum(is_thermophilic) / idx\n\n        return _metrics\n\n    # Check if PoseMetrics have been captured\n    if self.job.db:\n        if self.metrics is None or self.job.force:\n            with self.job.db.session(expire_on_commit=False) as session:\n                session.add(self)\n                metrics_ = get_metrics()\n                if self.metrics is None:\n                    self.metrics = metrics_\n                else:  # Update existing self.metrics\n                    current_metrics = self.metrics\n                    for attr, value in metrics_.__dict__.items():\n                        if attr == '_sa_instance_state':\n                            continue\n                        setattr(current_metrics, attr, value)\n                # Update the design_metrics for this Pose\n                self.calculate_pose_design_metrics(session, **kwargs)\n                session.commit()\n    else:\n        raise NotImplementedError(\n            f\"{self.calculate_pose_metrics.__name__} doesn't output anything yet when {type(self.job).__name__}.db\"\n            f\"={self.job.db}\")\n        raise NotImplementedError(f\"The reference=SymEntry.resulting_symmetry center_of_mass is needed as well\")\n        pose_df = self.pose.df  # Also performs entity.calculate_spatial_orientation_metrics()\n\n        entity_dfs = []\n        for entity in self.pose.entities:\n            entity_s = pd.Series(**entity.calculate_spatial_orientation_metrics())\n            entity_dfs.append(entity_s)\n\n        # Stack the Series on the columns to turn into a dataframe where the metrics are rows and entity are columns\n        entity_df = pd.concat(entity_dfs, keys=list(range(1, 1 + len(entity_dfs))), axis=1)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.calculate_pose_design_metrics","title":"calculate_pose_design_metrics","text":"<pre><code>calculate_pose_design_metrics(session: Session, scores: dict[str, str | int | float] = None, novel_interface: bool = True)\n</code></pre> <p>Collects 'design' and per-residue metrics on the reference Pose</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>An open transaction within sqlalchemy</p> </li> <li> <code>scores</code>             (<code>dict[str, str | int | float]</code>, default:                 <code>None</code> )         \u2013          <p>Parsed Pose scores from Rosetta output</p> </li> <li> <code>novel_interface</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the pose interface is novel (i.e. docked) or from a bona-fide input</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def calculate_pose_design_metrics(self, session: Session, scores: dict[str, str | int | float] = None, novel_interface: bool = True):\n    \"\"\"Collects 'design' and per-residue metrics on the reference Pose\n\n    Args:\n        session: An open transaction within sqlalchemy\n        scores: Parsed Pose scores from Rosetta output\n        novel_interface: Whether the pose interface is novel (i.e. docked) or from a bona-fide input\n    \"\"\"\n    pose = self.pose\n    pose_length = pose.number_of_residues\n    residue_indices = list(range(pose_length))\n\n    designs = [pose]\n    # Todo\n    #  This function call is sort of accomplished by the following code\n    #  residues_df = self.analyze_residue_metrics_per_design(designs=designs)\n    #  Only differences are inclusion of interface_residue (^) and the novel_interface flag\n\n    # novel_interface = False if not self.pose_source.protocols else True\n    # if novel_interface:  # The input structure wasn't meant to be together, take the errat measurement as such\n    #     source_errat = []\n    #     for idx, entity in enumerate(self.pose.entities):\n    #         _, oligomeric_errat = entity.assembly.errat(out_path=os.path.devnull)\n    #         source_errat.append(oligomeric_errat[:entity.number_of_residues])\n    #     # atomic_deviation[pose_source_name] = sum(source_errat_accuracy) / float(self.pose.number_of_entities)\n    #     pose_source_errat = np.concatenate(source_errat)\n    # else:\n    #     # pose_assembly_minimally_contacting = self.pose.assembly_minimally_contacting\n    #     # # atomic_deviation[pose_source_name], pose_per_residue_errat = \\\n    #     # _, pose_per_residue_errat = \\\n    #     #     pose_assembly_minimally_contacting.errat(out_path=os.path.devnull)\n    #     # pose_source_errat = pose_per_residue_errat[:pose_length]\n    #     # Get errat measurement\n    #     # per_residue_data[pose_source_name].update(self.pose.per_residue_errat())\n    #     pose_source_errat = self.pose.per_residue_errat()['errat_deviation']\n\n    interface_residue_indices = np.array([[residue.index for residue in pose.interface_residues]])\n    pose_name = pose.name\n    # pose_source_id = self.pose_source.id\n    # Collect reference Structure metrics\n    per_residue_data = {pose_name:\n                        {**pose.per_residue_interface_surface_area(),\n                         **pose.per_residue_contact_order(),\n                         # 'errat_deviation': pose_source_errat\n                         **pose.per_residue_spatial_aggregation_propensity()\n                         }}\n    # Convert per_residue_data into a dataframe matching residues_df orientation\n    residues_df = pd.concat({name: pd.DataFrame(data, index=residue_indices)\n                             for name, data in per_residue_data.items()}).unstack().swaplevel(0, 1, axis=1)\n    # Construct interface residue array\n    interface_residue_bool = np.zeros((len(designs), pose_length), dtype=int)\n    for idx, interface_indices in enumerate(list(interface_residue_indices)):\n        interface_residue_bool[idx, interface_indices] = 1\n    interface_residue_df = pd.DataFrame(data=interface_residue_bool, index=residues_df.index,\n                                        columns=pd.MultiIndex.from_product(\n                                            (residue_indices, ['interface_residue'])))\n    residues_df = residues_df.join(interface_residue_df)\n    # Make buried surface area (bsa) columns, and residue classification\n    residues_df = metrics.calculate_residue_buried_surface_area(residues_df)\n    residues_df = metrics.classify_interface_residues(residues_df)\n    # Todo same to here\n\n    designs_df = self.analyze_design_metrics_per_design(residues_df, designs)\n\n    # Score using proteinmpnn\n    if self.job.use_proteinmpnn:\n        sequences = [pose.sequence]  # Expected ASU sequence\n        sequences_and_scores = pose.score_sequences(\n            sequences, model_name=self.job.design.proteinmpnn_model)\n        # design_residues = np.zeros((1, pose_length), dtype=bool)\n        # design_residues[interface_residue_indices] = 1\n        sequences_and_scores['design_indices'] = np.zeros((1, pose_length), dtype=bool)\n        mpnn_designs_df, mpnn_residues_df = self.analyze_proteinmpnn_metrics([pose_name], sequences_and_scores)\n        entity_designs_df = self.analyze_design_entities_per_residue(mpnn_residues_df)\n        designs_df = designs_df.join(mpnn_designs_df)\n        # sequences_df = self.analyze_sequence_metrics_per_design(sequences=[self.pose.sequence],\n        #                                                         design_ids=[pose_source_id])\n        # designs_df = designs_df.join(self.analyze_design_metrics_per_residue(sequences_df))\n        # # Join per-residue like DataFrames\n        # # Each of these could have different index/column, so we use concat to perform an outer merge\n        # residues_df = residues_df.join([mpnn_residues_df, sequences_df])\n        residues_df = residues_df.join(mpnn_residues_df)\n    else:\n        entity_designs_df = pd.DataFrame()\n\n    if scores:\n        # pose_source_id = self.pose_source.id\n        scores_with_identifier = {pose_name: scores}\n        scores_df, rosetta_residues_df = self.parse_rosetta_scores(scores_with_identifier)\n        # Currently the metrics putils.protocol and putils.design_parent are not handle as this is the pose_source\n        # and no protocols should have been run on this, nor should it have a parent. They will be removed when\n        # output to the database\n        scores_df = scores_df.join(metrics.sum_per_residue_metrics(rosetta_residues_df))\n        # Finish calculation of Rosetta scores with included metrics\n        designs_df = self.rosetta_column_combinations(designs_df.join(scores_df))\n        residues_df = residues_df.join(rosetta_residues_df)\n\n    # with self.job.db.session(expire_on_commit=False) as session:\n    # Currently, self isn't producing any new information from database, session only important for metrics\n    # session.add(self)\n    # Correct the index of the DataFrame by changing from \"name\" to database ID\n    name_to_id_map = {pose_name: self.pose_source.id}\n    designs_df.index = designs_df.index.map(name_to_id_map)\n    # Must move the entity_id to the columns for index.map to work\n    entity_designs_df.reset_index(level=0, inplace=True)\n    entity_designs_df.index = entity_designs_df.index.map(name_to_id_map)\n    residues_df.index = residues_df.index.map(name_to_id_map)\n    self.output_metrics(session, designs=designs_df)\n    output_residues = False\n    if output_residues:\n        self.output_metrics(session, residues=residues_df)\n    else:  # Only save the 'design_residue' columns\n        try:\n            residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n        except KeyError:  # When self.job.use_proteinmpnn is false\n            pass\n        else:\n            self.output_metrics(session, design_residues=residues_df)\n    sql.write_dataframe(session, entity_designs=entity_designs_df)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_alphafold_metrics","title":"analyze_alphafold_metrics","text":"<pre><code>analyze_alphafold_metrics(folding_scores: dict[str, [dict[str, ndarray]]], pose_length: int, model_type: str = None, interface_indices: tuple[Iterable[int], Iterable[int]] = False) -&gt; tuple[DataFrame, DataFrame] | tuple[None, None]\n</code></pre> <p>From a set of folding metrics output by Alphafold (or possible other)</p> <p>Parameters:</p> <ul> <li> <code>folding_scores</code>             (<code>dict[str, [dict[str, ndarray]]]</code>)         \u2013          <p>Metrics which may contain the following features as single metric or list or metrics {'predicted_aligned_error': [(n_residues, n_residues), ...]  # multimer/monomer_ptm  'plddt': [(n_residues,), ...]  'predicted_interface_template_modeling_score': [float, ...]  # multimer  'predicted_template_modeling_score': [float, ...]  # multimer/monomer_ptm  'rmsd_prediction_ensemble: [(number_of_models), ...]  }</p> </li> <li> <code>pose_length</code>             (<code>int</code>)         \u2013          <p>The length of the scores to return for metrics with an array</p> </li> <li> <code>model_type</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The type of model used during prediction</p> </li> <li> <code>interface_indices</code>             (<code>tuple[Iterable[int], Iterable[int]]</code>, default:                 <code>False</code> )         \u2013          <p>The Residue instance of two sides of a predicted interface</p> </li> </ul> <p>Returns:     A tuple of DataFrame where each contains (         A per-design metric DataFrame where each index is the design id and the columns are design metrics,         A per-residue metric DataFrame where each index is the design id and the columns are             (residue index, residue metric)     )</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_alphafold_metrics(self, folding_scores: dict[str, [dict[str, np.ndarray]]], pose_length: int,\n                              model_type: str = None,\n                              interface_indices: tuple[Iterable[int], Iterable[int]] = False) \\\n        -&gt; tuple[pd.DataFrame, pd.DataFrame] | tuple[None, None]:\n    \"\"\"From a set of folding metrics output by Alphafold (or possible other)\n\n    Args:\n        folding_scores: Metrics which may contain the following features as single metric or list or metrics\n            {'predicted_aligned_error': [(n_residues, n_residues), ...]  # multimer/monomer_ptm\n             'plddt': [(n_residues,), ...]\n             'predicted_interface_template_modeling_score': [float, ...]  # multimer\n             'predicted_template_modeling_score': [float, ...]  # multimer/monomer_ptm\n             'rmsd_prediction_ensemble: [(number_of_models), ...]\n             }\n        pose_length: The length of the scores to return for metrics with an array\n        model_type: The type of model used during prediction\n        interface_indices: The Residue instance of two sides of a predicted interface\n    Returns:\n        A tuple of DataFrame where each contains (\n            A per-design metric DataFrame where each index is the design id and the columns are design metrics,\n            A per-residue metric DataFrame where each index is the design id and the columns are\n                (residue index, residue metric)\n        )\n    \"\"\"\n    if not folding_scores:\n        return None, None\n\n    if interface_indices:\n        if len(interface_indices) != 2:\n            raise ValueError(\n                f'The argument \"interface_indices\" must contain a pair of indices for each side of an interfaces. '\n                f'Found the number of interfaces, {len(interface_indices)} != 2, the number expected')\n        interface_indices1, interface_indices2 = interface_indices\n        # Check if this interface is 2-fold symetric. If so, the interface_indices2 will be empty\n        if not interface_indices2:\n            interface_indices2 = interface_indices1\n    else:\n        interface_indices1 = interface_indices2 = slice(None)\n\n    # def describe_metrics(array_like: Iterable[int] | np.ndarray) -&gt; dict[str, float]:\n    #     length = len(array_like)\n    #     middle, remain = divmod(length, 2)\n    #     if remain:  # Odd\n    #         median_div = 1\n    #     else:\n    #         middle = slice(middle, 1 + middle)\n    #         median_div = 2\n    #\n    #     return dict(\n    #         min=min(array_like),\n    #         max=max(array_like),\n    #         mean=sum(array_like) / length,\n    #         median=sum(list(sorted(array_like))[middle]) / median_div\n    #     )\n\n    score_types_mean = ['rmsd_prediction_ensemble']\n    if 'multimer' in model_type:\n        measure_pae = True\n        score_types_mean += ['predicted_interface_template_modeling_score',\n                             'predicted_template_modeling_score']\n    elif 'ptm' in model_type:\n        measure_pae = True\n        score_types_mean += ['predicted_template_modeling_score']\n    else:\n        measure_pae = False\n\n    # number_models, number_of_residues = len(representative_plddt_per_model[0])\n    # number_models = 1\n    per_residue_data = {}\n    design_scores = {}\n    for design_name, scores in folding_scores.items():\n        logger.debug(f'Found metrics with contents:\\n{scores}')\n        # This shouldn't fail as plddt should always be present\n        array_scores = {}\n        scalar_scores = {}\n        for score_type, score in scores.items():\n            # rmsd_metrics = describe_metrics(rmsds)\n            if score_type in score_types_mean:\n                if isinstance(score, list):\n                    score_len = len(score)\n                    scalar_scores[score_type] = mean_ = sum(score) / score_len\n                    if score_len &gt; 1:\n                        # Using the standard deviation of a sample\n                        deviation = sqrt(sum([(score_-mean_) ** 2 for score_ in score]) / (score_len-1))\n                    else:\n                        deviation = 0.\n                    scalar_scores[f'{score_type}_deviation'] = deviation\n                else:\n                    scalar_scores[score_type] = score\n            # Process 'predicted_aligned_error' when multimer/monomer_ptm. shape is (n_residues, n_residues)\n            elif measure_pae and score_type == 'predicted_aligned_error':\n                if isinstance(score, list):\n                    # First average over each residue, storing in a container\n                    number_models = len(score)\n                    pae: np.ndarray\n                    pae, *other_pae = score\n                    for pae_ in other_pae:\n                        pae += pae_\n                    if number_models &gt; 1:\n                        pae /= number_models\n                    # pae_container = np.zeros((number_models, pose_length), dtype=np.float32)\n                    # for idx, pae_ in enumerate(score):\n                    #     pae_container[idx, :] = pae_.mean(axis=0)[:pose_length]\n                    # # Next, average over each model\n                    # pae = pae_container.mean(axis=0)\n                else:\n                    pae = score\n                array_scores['predicted_aligned_error'] = pae.mean(axis=0)[:pose_length]\n\n                if interface_indices1:\n                    # Index the resulting pae to get the error at the interface residues in particular\n                    # interface_pae_means = [model_pae[interface_indices1][:, interface_indices2].mean()\n                    #                        for model_pae in scores['predicted_aligned_error']]\n                    # scalar_scores['predicted_aligned_error_interface'] = sum(interface_pae_means) / number_models\n                    self.log.critical(f'Found interface_indices1: {interface_indices1}')\n                    self.log.critical(f'Found interface_indices2: {interface_indices2}')\n                    interface_pae = pae[interface_indices1][:, interface_indices2]\n                    scalar_scores['predicted_aligned_error_interface'] = interface_pae.mean()\n                    scalar_scores['predicted_aligned_error_interface_deviation'] = interface_pae.std()\n            elif score_type == 'plddt':  # Todo combine with above\n                if isinstance(score, list):\n                    number_models = len(score)\n                    plddt: np.ndarray\n                    plddt, *other_plddt = score\n                    for plddt_ in other_plddt:\n                        plddt += plddt_\n                    if number_models &gt; 1:\n                        plddt /= number_models\n                else:\n                    plddt = score\n                array_scores['plddt'] = plddt[:pose_length]\n\n        logger.debug(f'Found scalar_scores with contents:\\n{scalar_scores}')\n        logger.debug(f'Found array_scores with contents:\\n{array_scores}')\n\n        per_residue_data[design_name] = array_scores\n        design_scores[design_name] = scalar_scores\n\n    designs_df = pd.DataFrame.from_dict(design_scores, orient='index')\n    # residues_df = pd.DataFrame.from_dict(per_residue_data, orient='index')\n    residue_indices = range(pose_length)\n    residues_df = pd.concat({name: pd.DataFrame(data, index=residue_indices)\n                             for name, data in per_residue_data.items()}).unstack().swaplevel(0, 1, axis=1)\n    designs_df = designs_df.join(metrics.sum_per_residue_metrics(residues_df))\n    designs_df['plddt'] /= pose_length\n    designs_df['plddt_deviation'] = residues_df.loc[:, idx_slice[:, 'plddt']].std(axis=1)\n    designs_df['predicted_aligned_error'] /= pose_length\n    designs_df['predicted_aligned_error_deviation'] = \\\n        residues_df.loc[:, idx_slice[:, 'predicted_aligned_error']].std(axis=1)\n\n    return designs_df, residues_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_design_entities_per_residue","title":"analyze_design_entities_per_residue","text":"<pre><code>analyze_design_entities_per_residue(residues_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Gather sequence metrics on a per-entity basis and write to the database</p> <p>Parameters:</p> <ul> <li> <code>residues_df</code>             (<code>DataFrame</code>)         \u2013          <p>A per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> </ul> <p>Returns:     A per-entity metric DataFrame where each index is a combination of (design_id, entity_id) and the columns         are design metrics</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_design_entities_per_residue(self, residues_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Gather sequence metrics on a per-entity basis and write to the database\n\n    Args:\n        residues_df: A per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n    Returns:\n        A per-entity metric DataFrame where each index is a combination of (design_id, entity_id) and the columns\n            are design metrics\n    \"\"\"\n    residue_indices = list(range(self.pose.number_of_residues))\n    mutation_df = residues_df.loc[:, idx_slice[:, 'mutation']]\n\n    entity_designs = {}\n    for entity_data, entity in zip(self.entity_data, self.pose.entities):\n        number_mutations = \\\n            mutation_df.loc[:, idx_slice[residue_indices[entity.n_terminal_residue.index:\n                                                         1 + entity.c_terminal_residue.index], :]]\\\n            .sum(axis=1)\n        entity_designs[entity_data.id] = dict(\n            number_mutations=number_mutations,\n            percent_mutations=number_mutations / entity.number_of_residues)\n\n    # Set up the DesignEntityMetrics dataframe for writing\n    entity_designs_df = pd.concat([pd.DataFrame(data) for data in entity_designs.values()],\n                                  keys=entity_designs.keys())\n    design_ids = residues_df.index.tolist()\n    entity_designs_df.index = entity_designs_df.index.set_levels(design_ids, level=-1)\n    # entity_designs_df = pd.concat([pd.DataFrame(data) for data in entity_designs.values()])\n    # design_name_to_id_map = dict(zip(entity_designs_df.index.get_level_values(-1), design_ids))\n    # # mapped_index = entity_designs_df.index.map(design_name_to_id_map)\n    # entity_designs_df.index = \\\n    #     pd.MultiIndex.from_tuples(zip(entity_designs.keys(),\n    #                                   entity_designs_df.index.map(design_name_to_id_map).tolist()))\n    # input(entity_designs_df)\n    entity_designs_df.index = entity_designs_df.index.rename(\n        [sql.DesignEntityMetrics.entity_id.name, sql.DesignEntityMetrics.design_id.name])\n    # entity_designs_df.reset_index(level=-1, inplace=True)\n    return entity_designs_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_design_metrics_per_design","title":"analyze_design_metrics_per_design","text":"<pre><code>analyze_design_metrics_per_design(residues_df: DataFrame, designs: Iterable[Pose] | Iterable[AnyStr]) -&gt; DataFrame\n</code></pre> <p>Take every design Model and perform design level structural analysis. Sums per-residue metrics (residues_df)</p> <p>Parameters:</p> <ul> <li> <code>residues_df</code>             (<code>DataFrame</code>)         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> <li> <code>designs</code>             (<code>Iterable[Pose] | Iterable[AnyStr]</code>)         \u2013          <p>The designs to perform analysis on. By default, fetches all available structures</p> </li> </ul> <p>Returns:     A per-design metric DataFrame where each index is the design id and the columns are design metrics     Including metrics 'interface_area_total' and 'number_residues_interface' which are used in other analysis         functions</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_design_metrics_per_design(self, residues_df: pd.DataFrame,\n                                      designs: Iterable[Pose] | Iterable[AnyStr]) -&gt; pd.DataFrame:\n    \"\"\"Take every design Model and perform design level structural analysis. Sums per-residue metrics (residues_df)\n\n    Args:\n        residues_df: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n        designs: The designs to perform analysis on. By default, fetches all available structures\n    Returns:\n        A per-design metric DataFrame where each index is the design id and the columns are design metrics\n        Including metrics 'interface_area_total' and 'number_residues_interface' which are used in other analysis\n            functions\n    \"\"\"\n    #     designs_df: The typical per-design metric DataFrame where each index is the design id and the columns are\n    #         design metrics\n    # Find all designs files\n    #   Todo fold these into Model(s) and attack metrics from Pose objects?\n    # if designs is None:\n    #     designs = []\n\n    # Compute structural measurements for all designs\n    pose_kwargs = self.pose_kwargs\n    interface_local_density = {}\n    # number_residues_interface = {}\n    for pose in designs:\n        try:\n            pose_name = pose.name\n        except AttributeError:  # This is likely a filepath\n            pose = Pose.from_file(pose, **pose_kwargs)\n            pose_name = pose.name\n        # Must find interface residues before measure local_density\n        pose.find_and_split_interface()\n        # number_residues_interface[pose.name] = len(pose.interface_residues)\n        interface_local_density[pose_name] = pose.local_density_interface()\n\n    # designs_df = pd.Series(interface_local_density, index=residues_df.index,\n    #                        name='interface_local_density').to_frame()\n    designs_df = metrics.sum_per_residue_metrics(residues_df)\n    interface_df = residues_df.loc[:, idx_slice[:, 'interface_residue']].droplevel(-1, axis=1)\n    designs_df['spatial_aggregation_propensity_interface'] = \\\n        ((residues_df.loc[:, idx_slice[:, 'spatial_aggregation_propensity_unbound']].droplevel(-1, axis=1)\n          - residues_df.loc[:, idx_slice[:, 'spatial_aggregation_propensity']].droplevel(-1, axis=1))\n         * interface_df).sum(axis=1)\n    # Divide by the number of interface residues\n    designs_df['spatial_aggregation_propensity_interface'] /= interface_df.sum(axis=1)\n\n    # Find the average for these summed design metrics\n    pose_length = self.pose.number_of_residues\n    designs_df['spatial_aggregation_propensity_unbound'] /= pose_length\n    designs_df['spatial_aggregation_propensity'] /= pose_length\n    # designs_df['number_residues_interface'] = pd.Series(number_residues_interface)\n    designs_df['interface_local_density'] = pd.Series(interface_local_density)\n\n    # self.load_pose()\n    # # Make designs_df errat_deviation that takes into account the pose_source sequence errat_deviation\n    # # Get per-residue errat scores from the residues_df\n    # errat_df = residues_df.loc[:, idx_slice[:, 'errat_deviation']].droplevel(-1, axis=1)\n    #\n    # # Todo improve efficiency by using precomputed. Something like:\n    # #  stmt = select(ResidueMetrics).where(ResidueMetrics.pose_id == self.id, ResidueMetrics.name == self.name)\n    # #  rows = session.scalars(stmt)\n    # #  row_dict = {row.index: row.errat_deviation for row in rows}\n    # #  pd.Series(row_dict, name='errat_Deviation')\n    # # pose_source_errat = errat_df.loc[self.pose.name, :]\n    # pose_source_errat = self.pose.per_residue_errat()['errat_deviation']\n    # # Include in errat_deviation if errat score is &lt; 2 std devs and isn't 0 to begin with\n    # source_errat_inclusion_boolean = \\\n    #     np.logical_and(pose_source_errat &lt; metrics.errat_2_sigma, pose_source_errat != 0.)\n    # # Find residues where designs deviate above wild-type errat scores\n    # errat_sig_df = errat_df.sub(pose_source_errat, axis=1) &gt; metrics.errat_1_sigma  # axis=1 is per-residue subtract\n    # # Then select only those residues which are expressly important by the inclusion boolean\n    # # This overwrites the metrics.sum_per_residue_metrics() value\n    # designs_df['errat_deviation'] = (errat_sig_df.loc[:, source_errat_inclusion_boolean] * 1).sum(axis=1)\n\n    # pose_df = self.pose.df\n    # # Todo should each have the same number_residues_interface? need each design specifics\n    # designs_df['number_residues_interface'] = pose_df['number_residues_interface']\n\n    # Find the proportion of the residue surface area that is solvent accessible versus buried in the interface\n    # if 'interface_area_total' in designs_df and 'area_total_complex' in designs_df:\n    interface_bsa_df = designs_df['interface_area_total']\n    designs_df['interface_area_to_residue_surface_ratio'] = \\\n        (interface_bsa_df / (interface_bsa_df + designs_df['area_total_complex']))\n\n    # designs_df['interface_area_total'] = pose_df['interface_area_total']\n    designs_df['number_of_interface_class'] = designs_df.loc[:, metrics.residue_classification].sum(axis=1)\n    designs_df = metrics.columns_to_new_column(designs_df, metrics.division_pairs, mode='truediv')\n    designs_df['interface_composition_similarity'] = \\\n        designs_df.apply(metrics.interface_composition_similarity, axis=1)\n    designs_df.drop('number_of_interface_class', axis=1, inplace=True)\n\n    return designs_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_design_metrics_per_residue","title":"analyze_design_metrics_per_residue","text":"<pre><code>analyze_design_metrics_per_residue(residues_df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Take every design in the residues_df and perform design level statistical analysis</p> <p>Parameters:</p> <ul> <li> <code>residues_df</code>             (<code>DataFrame</code>)         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> </ul> <p>Returns:     A per-design metric DataFrame where each index is the design id and the columns are design metrics</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_design_metrics_per_residue(self, residues_df: pd.DataFrame) -&gt; pd.DataFrame:\n    # designs_df: pd.DataFrame = None\n    \"\"\"Take every design in the residues_df and perform design level statistical analysis\n\n    Args:\n        residues_df: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n    Returns:\n        A per-design metric DataFrame where each index is the design id and the columns are design metrics\n    \"\"\"\n    #     designs_df: The typical per-design metric DataFrame where each index is the design id and the columns are\n    #         design metrics\n    self.load_pose()\n    # self.identify_interface()\n\n    # Load fragment_profile into the analysis\n    if not self.pose.fragment_info_by_entity_pair:\n        self.generate_fragments(interface=True)\n        self.pose.calculate_fragment_profile()\n\n    # CAUTION: Assumes each structure is the same length\n    pose_length = self.pose.number_of_residues\n    # residue_indices = list(range(pose_length))\n\n    designs_df = metrics.sum_per_residue_metrics(residues_df)\n\n    pose_df = self.pose.df\n    # Calculate mutational content\n    # designs_df['number_mutations'] = mutation_df.sum(axis=1)\n    designs_df['percent_mutations'] = designs_df['number_mutations'] / pose_length\n\n    designs_df['sequence_loss_design_per_residue'] = designs_df['sequence_loss_design'] / pose_length\n    # The per residue average loss compared to the design profile\n    designs_df['sequence_loss_evolution_per_residue'] = designs_df['sequence_loss_evolution'] / pose_length\n    # The per residue average loss compared to the evolution profile\n    # Todo modify this when fragments come from elsewhere, not just interface\n    designs_df['sequence_loss_fragment_per_residue'] = \\\n        designs_df['sequence_loss_fragment'] / pose_df['number_residues_interface_fragment_total']\n    # The per residue average loss compared to the fragment profile\n\n    # designs_df['collapse_new_positions'] /= pose_length\n    # designs_df['collapse_new_position_significance'] /= pose_length\n    designs_df['collapse_significance_by_contact_order_z_mean'] = \\\n        designs_df['collapse_significance_by_contact_order_z'] / \\\n        (residues_df.loc[:, idx_slice[:, 'collapse_significance_by_contact_order_z']] != 0).sum(axis=1)\n    # if self.measure_alignment:\n    # Todo THESE ARE NOW DIFFERENT SOURCE if not self.measure_alignment\n    collapse_increased_df = residues_df.loc[:, idx_slice[:, 'collapse_increased_z']]\n    total_increased_collapse = (collapse_increased_df != 0).sum(axis=1)\n    # designs_df['collapse_increase_significance_by_contact_order_z_mean'] = \\\n    #     designs_df['collapse_increase_significance_by_contact_order_z'] / total_increased_collapse\n    # designs_df['collapse_increased_z'] /= pose_length\n    designs_df['collapse_increased_z_mean'] = collapse_increased_df.sum(axis=1) / total_increased_collapse\n    designs_df['collapse_violation'] = designs_df['collapse_new_positions'] &gt; 0\n    designs_df['collapse_variance'] = designs_df['collapse_deviation_magnitude'] / pose_length\n    designs_df['collapse_sequential_peaks_z_mean'] = \\\n        designs_df['collapse_sequential_peaks_z'] / total_increased_collapse\n    designs_df['collapse_sequential_z_mean'] = designs_df['collapse_sequential_z'] / total_increased_collapse\n\n    # Ensure summed observed_types are taken as the average over the pose_length\n    for _type in observed_types:\n        try:\n            designs_df[f'observed_{_type}'] /= pose_length\n        except KeyError:\n            continue\n\n    return designs_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_pose_designs","title":"analyze_pose_designs","text":"<pre><code>analyze_pose_designs(designs: Iterable[DesignData] = None)\n</code></pre> <p>Retrieve all score information from a PoseJob and write results to .csv file</p> <p>Parameters:</p> <ul> <li> <code>designs</code>             (<code>Iterable[DesignData]</code>, default:                 <code>None</code> )         \u2013          <p>The DesignData instances to perform analysis on. By default, fetches all PoseJob.designs</p> </li> </ul> <p>Returns:     Series containing summary metrics for all designs</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_pose_designs(self, designs: Iterable[sql.DesignData] = None):\n    \"\"\"Retrieve all score information from a PoseJob and write results to .csv file\n\n    Args:\n        designs: The DesignData instances to perform analysis on. By default, fetches all PoseJob.designs\n    Returns:\n        Series containing summary metrics for all designs\n    \"\"\"\n    with self.job.db.session(expire_on_commit=False) as session:\n        if designs is None:\n            session.add(self)\n            if self.current_designs:\n                designs = self.current_designs\n            else:\n                # Slice off the self.pose_source from these\n                self.current_designs = designs = self.designs[1:]\n\n        if not designs:\n            return  # There is nothing to analyze\n\n        # Fetch data from the database\n        # Get the name/provided_name to design_id mapping\n        design_names = [design.name for design in designs]\n        design_ids = [design.id for design in designs]\n        design_name_to_id_map = dict(zip(design_names, design_ids))\n        design_sequences = [design.sequence for design in designs]\n        design_paths_to_process = [design.structure_path for design in designs]\n        select_stmt = select((sql.DesignResidues.design_id, sql.DesignResidues.index, sql.DesignResidues.design_residue))\\\n            .where(sql.DesignResidues.design_id.in_(design_ids))\n        index_col = sql.ResidueMetrics.index.name\n        design_residue_df = \\\n            pd.DataFrame.from_records(session.execute(select_stmt).all(),\n                                      columns=['id', index_col, sql.DesignResidues.design_residue.name])\n    design_residue_df = design_residue_df.set_index(['id', index_col]).unstack()\n    # Use simple reporting here until that changes...\n    # interface_residue_indices = [residue.index for residue in self.pose.interface_residues]\n    # pose_length = self.pose.number_of_residues\n    # design_residues = np.zeros((len(designs), pose_length), dtype=bool)\n    # design_residues[:, interface_residue_indices] = 1\n\n    # Score using proteinmpnn\n    # sequences_df = self.analyze_sequence_metrics_per_design(sequences=design_sequences)\n    # sequences_and_scores = self.pose.score(\n    sequences_and_scores = self.pose.score_sequences(\n        design_sequences, model_name=self.job.design.proteinmpnn_model)\n    sequences_and_scores['design_indices'] = design_residue_df.values\n\n    mpnn_designs_df, mpnn_residues_df = self.analyze_proteinmpnn_metrics(design_names, sequences_and_scores)\n    # The DataFrame.index needs to become the design.id not design.name. Modify after all processing\n    entity_designs_df = self.analyze_design_entities_per_residue(mpnn_residues_df)\n\n    # Process all desired files to Pose\n    pose_kwargs = self.pose_kwargs\n    designs_poses = \\\n        [Pose.from_file(file, **pose_kwargs) for file in design_paths_to_process if file is not None]\n\n    if designs_poses:\n        residues_df = self.analyze_residue_metrics_per_design(designs=designs_poses)\n        designs_df = self.analyze_design_metrics_per_design(residues_df, designs_poses)\n        # Join DataFrames\n        designs_df = designs_df.join(mpnn_designs_df)\n        residues_df = residues_df.join(mpnn_residues_df)\n    else:\n        designs_df = mpnn_designs_df\n        residues_df = mpnn_residues_df\n\n    # Each of these could have different index/column, so we use concat to perform an outer merge\n    # residues_df = pd.concat([residues_df, mpnn_residues_df, sequences_df], axis=1) WORKED!!\n    # residues_df = residues_df.join([mpnn_residues_df, sequences_df])\n    # Todo should this \"different index\" be allowed? be possible\n    #  residues_df = residues_df.join(rosetta_residues_df)\n\n    # Rename all designs and clean up resulting metrics for storage\n    # In keeping with \"unit of work\", only rename once all data is processed incase we run into any errors\n    designs_df.index = designs_df.index.map(design_name_to_id_map)\n    # Must move the entity_id to the columns for index.map to work\n    entity_designs_df.reset_index(level=0, inplace=True)\n    entity_designs_df.index = entity_designs_df.index.map(design_name_to_id_map)\n    residues_df.index = residues_df.index.map(design_name_to_id_map)\n\n    # Commit the newly acquired metrics to the database\n    with self.job.db.session(expire_on_commit=False) as session:\n        sql.write_dataframe(session, entity_designs=entity_designs_df)\n        self.output_metrics(session, designs=designs_df)\n        output_residues = False\n        if output_residues:\n            self.output_metrics(session, residues=residues_df)\n        # This function doesn't generate any 'design_residue'\n        # else:  # Only save the 'design_residue' columns\n        #     residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n        #     self.output_metrics(session, design_residues=residues_df)\n        # Commit the newly acquired metrics\n        session.commit()\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_pose_metrics_per_design","title":"analyze_pose_metrics_per_design","text":"<pre><code>analyze_pose_metrics_per_design(residues_df: DataFrame, designs_df: DataFrame = None, designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; Series\n</code></pre> <p>Perform Pose level analysis on every design produced from this Pose</p> <p>Parameters:</p> <ul> <li> <code>residues_df</code>             (<code>DataFrame</code>)         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> <li> <code>designs_df</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-design metric DataFrame where each index is the design id and the columns are design metrics</p> </li> <li> <code>designs</code>             (<code>Iterable[Pose] | Iterable[AnyStr]</code>, default:                 <code>None</code> )         \u2013          <p>The subsequent designs to perform analysis on</p> </li> </ul> <p>Returns:     Series containing summary metrics for all designs</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_pose_metrics_per_design(self, residues_df: pd.DataFrame, designs_df: pd.DataFrame = None,\n                                    designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; pd.Series:\n    \"\"\"Perform Pose level analysis on every design produced from this Pose\n\n    Args:\n        residues_df: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n        designs_df: The typical per-design metric DataFrame where each index is the design id and the columns are\n            design metrics\n        designs: The subsequent designs to perform analysis on\n    Returns:\n        Series containing summary metrics for all designs\n    \"\"\"\n    self.load_pose()\n\n    return pose_s\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_proteinmpnn_metrics","title":"analyze_proteinmpnn_metrics","text":"<pre><code>analyze_proteinmpnn_metrics(design_ids: Sequence[str], sequences_and_scores: dict[str, array]) -&gt; tuple[DataFrame, DataFrame]\n</code></pre> <p>Takes the sequences/scores ProteinMPNN features including 'design_indices', 'proteinmpnn_loss_complex', and 'proteinmpnn_loss_unbound' to format summary metrics. Performs sequence analysis with 'sequences' feature</p> <p>Parameters:</p> <ul> <li> <code>design_ids</code>             (<code>Sequence[str]</code>)         \u2013          <p>The associated design identifier for each corresponding entry in sequences_and_scores</p> </li> <li> <code>sequences_and_scores</code>             (<code>dict[str, array]</code>)         \u2013          <p>The mapping of ProteinMPNN score type to it's corresponding data</p> </li> </ul> <p>Returns:     A tuple of DataFrame where each contains (         A per-design metric DataFrame where each index is the design id and the columns are design metrics,         A per-residue metric DataFrame where each index is the design id and the columns are             (residue index, residue metric)     )</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_proteinmpnn_metrics(self, design_ids: Sequence[str], sequences_and_scores: dict[str, np.array])\\\n        -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    #                      designs: Iterable[Pose] | Iterable[AnyStr] = None\n    \"\"\"Takes the sequences/scores ProteinMPNN features including 'design_indices', 'proteinmpnn_loss_complex',\n    and 'proteinmpnn_loss_unbound' to format summary metrics. Performs sequence analysis with 'sequences' feature\n\n    Args:\n        design_ids: The associated design identifier for each corresponding entry in sequences_and_scores\n        sequences_and_scores: The mapping of ProteinMPNN score type to it's corresponding data\n    Returns:\n        A tuple of DataFrame where each contains (\n            A per-design metric DataFrame where each index is the design id and the columns are design metrics,\n            A per-residue metric DataFrame where each index is the design id and the columns are\n                (residue index, residue metric)\n        )\n    \"\"\"\n    #     designs: The designs to perform analysis on. By default, fetches all available structures\n    # Calculate metrics on input Pose before any manipulation\n    pose_length = self.pose.number_of_residues\n    residue_indices = list(range(pose_length))  # [residue.index for residue in self.pose.residues]\n    # residue_numbers = [residue.number for residue in self.pose.residues]\n\n    # metadata_df = pd.DataFrame(sequences_and_scores['temperatures'], index=design_ids, columns=['temperature'])\n    # metadata_df[putils.protocol] = sequences_and_scores[putils.protocol]\n    # numeric_sequences = sequences_and_scores['numeric_sequences']\n    # torch_numeric_sequences = torch.from_numpy(numeric_sequences)\n    # nan_blank_data = list(repeat(np.nan, pose_length))\n\n    # Construct residues_df\n    sequences = sequences_and_scores.pop('sequences')\n    sequences_and_scores['design_residue'] = sequences_and_scores.pop('design_indices')\n    # If sequences_and_scores gets any other keys in it this isn't explicitly enough\n    # proteinmpnn_data = {\n    #     'design_residue': sequences_and_scores['design_indices'],\n    #     'proteinmpnn_loss_complex': sequences_and_scores['proteinmpnn_loss_complex'],\n    #     'proteinmpnn_loss_unbound': sequences_and_scores['proteinmpnn_loss_unbound']\n    # }\n    proteinmpnn_residue_info_df = \\\n        pd.concat([pd.DataFrame(data, index=design_ids,\n                                columns=pd.MultiIndex.from_product([residue_indices, [metric]]))\n                   for metric, data in sequences_and_scores.items()], axis=1)\n\n    # Incorporate residue, design, and sequence metrics on every designed Pose\n    # Todo UPDATE These are now from a different collapse 'hydrophobicity' source, 'expanded'\n    sequences_df = self.analyze_sequence_metrics_per_design(sequences=sequences, design_ids=design_ids)\n    # Since no structure design completed, no residue_metrics is performed, but the pose source can be...\n    # # The residues_df here has the wrong .index. It needs to become the design.id not design.name\n    # residues_df = self.analyze_residue_metrics_per_design()  # designs=designs)\n    # # Join each per-residue like dataframe\n    # # Each of these can have difference index, so we use concat to perform an outer merge\n    # residues_df = pd.concat([residues_df, sequences_df, proteinmpnn_residue_info_df], axis=1)\n    # residues_df = pd.concat([sequences_df, proteinmpnn_residue_info_df], axis=1)\n    residues_df = sequences_df.join(proteinmpnn_residue_info_df)\n\n    designs_df = self.analyze_design_metrics_per_residue(residues_df)\n\n    designed_df = residues_df.loc[:, idx_slice[:, 'design_residue']].droplevel(-1, axis=1)\n    number_designed_residues_s = designed_df.sum(axis=1)\n\n    # designs_df[putils.protocol] = 'proteinmpnn'\n    designs_df['proteinmpnn_score_complex'] = designs_df['proteinmpnn_loss_complex'] / pose_length\n    designs_df['proteinmpnn_score_unbound'] = designs_df['proteinmpnn_loss_unbound'] / pose_length\n    designs_df['proteinmpnn_score_delta'] = \\\n        designs_df['proteinmpnn_score_complex'] - designs_df['proteinmpnn_score_unbound']\n    # Find the mean of each of these using the boolean like array and the sum\n    designs_df['proteinmpnn_score_complex_per_designed_residue'] = \\\n        (residues_df.loc[:, idx_slice[:, 'proteinmpnn_loss_complex']].droplevel(-1, axis=1)\n         * designed_df).sum(axis=1)\n    designs_df['proteinmpnn_score_complex_per_designed_residue'] /= number_designed_residues_s\n    designs_df['proteinmpnn_score_unbound_per_designed_residue'] = \\\n        (residues_df.loc[:, idx_slice[:, 'proteinmpnn_loss_unbound']].droplevel(-1, axis=1)\n         * designed_df).sum(axis=1)\n    designs_df['proteinmpnn_score_unbound_per_designed_residue'] /= number_designed_residues_s\n    designs_df['proteinmpnn_score_delta_per_designed_residue'] = \\\n        designs_df['proteinmpnn_score_complex_per_designed_residue'] \\\n        - designs_df['proteinmpnn_score_unbound_per_designed_residue']\n\n    # Make an array the length of the designs and size of the pose to calculate the interface residues\n    interface_residues = np.zeros((len(designed_df), pose_length), dtype=bool)\n    interface_residues[:, [residue.index for residue in self.pose.interface_residues]] = 1\n    number_interface_residues = interface_residues.sum(axis=1)\n    designs_df['proteinmpnn_score_complex_per_interface_residue'] = \\\n        (residues_df.loc[:, idx_slice[:, 'proteinmpnn_loss_complex']].droplevel(-1, axis=1)\n         * interface_residues).sum(axis=1)\n    designs_df['proteinmpnn_score_complex_per_interface_residue'] /= number_interface_residues\n    designs_df['proteinmpnn_score_unbound_per_interface_residue'] = \\\n        (residues_df.loc[:, idx_slice[:, 'proteinmpnn_loss_unbound']].droplevel(-1, axis=1)\n         * interface_residues).sum(axis=1)\n    designs_df['proteinmpnn_score_unbound_per_interface_residue'] /= number_interface_residues\n    designs_df['proteinmpnn_score_delta_per_interface_residue'] = \\\n        designs_df['proteinmpnn_score_complex_per_interface_residue'] \\\n        - designs_df['proteinmpnn_score_unbound_per_interface_residue']\n\n    # # Drop unused particular residues_df columns that have been summed\n    # per_residue_drop_columns = per_residue_energy_states + energy_metric_names + per_residue_sasa_states \\\n    #                            + collapse_metrics + residue_classification \\\n    #                            + ['errat_deviation', 'hydrophobic_collapse', 'contact_order'] \\\n    #                            + ['hbond', 'evolution', 'fragment', 'type'] + ['surface', 'interior']\n    # # Slice each of these columns as the first level residue number needs to be accounted for in MultiIndex\n    # residues_df = residues_df.drop(\n    #     list(residues_df.loc[:, idx_slice[:, per_residue_drop_columns]].columns),\n    #     errors='ignore', axis=1)\n\n    return designs_df, residues_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_residue_metrics_per_design","title":"analyze_residue_metrics_per_design","text":"<pre><code>analyze_residue_metrics_per_design(designs: Iterable[Pose] | Iterable[AnyStr]) -&gt; DataFrame\n</code></pre> <p>Perform per-residue analysis on design Model instances</p> <p>Parameters:</p> <ul> <li> <code>designs</code>             (<code>Iterable[Pose] | Iterable[AnyStr]</code>)         \u2013          <p>The designs to analyze. The StructureBase.name attribute is used for naming DataFrame indices</p> </li> </ul> <p>Returns:     A per-residue metric DataFrame where each index is the design id and the columns are         (residue index, residue metric)</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_residue_metrics_per_design(self, designs: Iterable[Pose] | Iterable[AnyStr]) -&gt; pd.DataFrame:\n    \"\"\"Perform per-residue analysis on design Model instances\n\n    Args:\n        designs: The designs to analyze. The StructureBase.name attribute is used for naming DataFrame indices\n    Returns:\n        A per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n    \"\"\"\n    # Compute structural measurements for all designs\n    per_residue_data: dict[str, dict[str, Any]] = {}\n    interface_residues = []\n    pose_kwargs = self.pose_kwargs\n    for pose in designs:\n        try:\n            name = pose.name\n        except AttributeError:  # This is likely a filepath\n            pose = Pose.from_file(pose, **pose_kwargs)\n            name = pose.name\n        # Get interface residues\n        pose.find_and_split_interface()\n        interface_residues.append([residue.index for residue in pose.interface_residues])\n        per_residue_data[name] = {\n            **pose.per_residue_interface_surface_area(),\n            **pose.per_residue_contact_order(),\n            # **pose.per_residue_errat()\n            **pose.per_residue_spatial_aggregation_propensity()\n        }\n\n    self.load_pose()\n\n    # CAUTION: Assumes each structure is the same length\n    pose_length = self.pose.number_of_residues\n    residue_indices = list(range(pose_length))\n    # residue_numbers = [residue.number for residue in self.pose.residues]\n    # design_residue_indices = [residue.index for residue in self.pose.design_residues]\n    # interface_residue_indices = [residue.index for residue in self.pose.interface_residues]\n\n    # Convert per_residue_data into a dataframe matching residues_df orientation\n    residues_df = pd.concat({name: pd.DataFrame(data, index=residue_indices)\n                            for name, data in per_residue_data.items()}).unstack().swaplevel(0, 1, axis=1)\n    # Construct interface residue array\n    interface_residue_bool = np.zeros((len(designs), pose_length), dtype=int)\n    for idx, interface_indices in enumerate(interface_residues):\n        interface_residue_bool[idx, interface_indices] = 1\n    interface_residue_df = pd.DataFrame(data=interface_residue_bool, index=residues_df.index,\n                                        columns=pd.MultiIndex.from_product((residue_indices,\n                                                                            ['interface_residue'])))\n    residues_df = residues_df.join(interface_residue_df)\n    # Make buried surface area (bsa) columns, and classify residue types\n    residues_df = metrics.calculate_residue_buried_surface_area(residues_df)\n    return metrics.classify_interface_residues(residues_df)\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.analyze_sequence_metrics_per_design","title":"analyze_sequence_metrics_per_design","text":"<pre><code>analyze_sequence_metrics_per_design(sequences: dict[str, Sequence[str]] | Sequence[Sequence[str]] = None, design_ids: Sequence[str] = None) -&gt; DataFrame\n</code></pre> <p>Gather metrics based on provided sequences in comparison to the Pose sequence</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>dict[str, Sequence[str]] | Sequence[Sequence[str]]</code>, default:                 <code>None</code> )         \u2013          <p>The sequences to analyze compared to the \"pose source\" sequence</p> </li> <li> <code>design_ids</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>If sequences isn't a mapping from identifier to sequence, the identifiers for each sequence</p> </li> </ul> <p>Returns:     A per-residue metric DataFrame where each index is the design id and the columns are         (residue index, residue metric)</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def analyze_sequence_metrics_per_design(self, sequences: dict[str, Sequence[str]] | Sequence[Sequence[str]] = None,\n                                        design_ids: Sequence[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Gather metrics based on provided sequences in comparison to the Pose sequence\n\n    Args:\n        sequences: The sequences to analyze compared to the \"pose source\" sequence\n        design_ids: If sequences isn't a mapping from identifier to sequence, the identifiers for each sequence\n    Returns:\n        A per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n    \"\"\"\n    # Ensure the pose.sequence (or reference sequence) is used as the first sequence during analysis\n    # if sequences is None:\n    #     # Todo handle design sequences from a read_fasta_file?\n    #     # Todo implement reference sequence from included file(s) or as with self.pose.sequence below\n    if isinstance(sequences, dict):\n        if sequences:\n            design_ids = list(sequences.keys())  # [self.pose.name] +\n            sequences = list(sequences.values())  # [self.pose.sequence] +\n        else:  # Nothing passed, return an empty DataFrame\n            return pd.DataFrame()\n    else:\n        if isinstance(design_ids, Sequence):\n            design_ids = list(design_ids)  # [self.pose.name] +\n        else:\n            raise ValueError(\n                f\"Can't perform {self.analyze_sequence_metrics_per_design.__name__} without argument \"\n                \"'design_ids' when 'sequences' isn't a dictionary\")\n        # All sequences must be string for Biopython\n        if isinstance(sequences, np.ndarray):\n            sequences = [''.join(sequence) for sequence in sequences.tolist()]  # [self.pose.sequence] +\n        elif isinstance(sequences, Sequence):\n            sequences = [''.join(sequence) for sequence in sequences]  # [self.pose.sequence] +\n        else:\n            design_sequences = design_ids = sequences = None\n            raise ValueError(\n                f\"Can't perform {self.analyze_sequence_metrics_per_design.__name__} with argument \"\n                f\"'sequences' as a {type(sequences).__name__}. Pass 'sequences' as a Sequence[str]\")\n    if len(design_ids) != len(sequences):\n        raise ValueError(\n            f\"The length of the design_ids ({len(design_ids)}) != sequences ({len(sequences)})\")\n\n    # Create numeric sequence types\n    number_of_sequences = len(sequences)\n    numeric_sequences = sequences_to_numeric(sequences)\n    torch_numeric_sequences = torch.from_numpy(numeric_sequences)\n\n    pose_length = self.pose.number_of_residues\n    residue_indices = list(range(pose_length))\n    nan_blank_data = np.tile(list(repeat(np.nan, pose_length)), (number_of_sequences, 1))\n\n    # Make requisite profiles\n    self.set_up_evolutionary_profile(warn_metrics=True)\n\n    # Try to add each of the profile types in observed_types to profile_background\n    profile_background = {}\n    if self.measure_evolution:\n        profile_background['evolution'] = evolutionary_profile_array = \\\n            pssm_as_array(self.pose.evolutionary_profile)\n        batch_evolutionary_profile = np.tile(evolutionary_profile_array, (number_of_sequences, 1, 1))\n        # torch_log_evolutionary_profile = torch.from_numpy(np.log(batch_evolutionary_profile))\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', category=RuntimeWarning)\n            # np.log causes -inf at 0, so they are corrected to a 'large' number\n            corrected_evol_array = np.nan_to_num(np.log(batch_evolutionary_profile), copy=False,\n                                                 nan=np.nan, neginf=metrics.zero_probability_evol_value)\n        torch_log_evolutionary_profile = torch.from_numpy(corrected_evol_array)\n        per_residue_evolutionary_profile_loss = \\\n            resources.ml.sequence_nllloss(torch_numeric_sequences, torch_log_evolutionary_profile)\n    else:\n        # Because self.pose.calculate_profile() is used below, need to ensure there is a null_profile attached\n        if not self.pose.evolutionary_profile:\n            self.pose.evolutionary_profile = self.pose.create_null_profile()\n        # per_residue_evolutionary_profile_loss = per_residue_design_profile_loss = nan_blank_data\n        per_residue_evolutionary_profile_loss = nan_blank_data\n\n    # Load fragment_profile into the analysis\n    # if self.job.design.term_constraint and not self.pose.fragment_queries:\n    # if self.job.design.term_constraint:\n    if not self.pose.fragment_info_by_entity_pair:\n        self.generate_fragments(interface=True)\n    if not self.pose.fragment_profile:\n        self.pose.calculate_fragment_profile()\n    profile_background['fragment'] = fragment_profile_array = self.pose.fragment_profile.as_array()\n    batch_fragment_profile = np.tile(fragment_profile_array, (number_of_sequences, 1, 1))\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', category=RuntimeWarning)\n        # np.log causes -inf at 0, so they are corrected to a 'large' number\n        corrected_frag_array = np.nan_to_num(np.log(batch_fragment_profile), copy=False,\n                                             nan=np.nan, neginf=metrics.zero_probability_frag_value)\n    per_residue_fragment_profile_loss = \\\n        resources.ml.sequence_nllloss(torch_numeric_sequences, torch.from_numpy(corrected_frag_array))\n    # else:\n    #     per_residue_fragment_profile_loss = nan_blank_data\n\n    # Set up \"design\" profile\n    self.pose.calculate_profile()\n    profile_background['design'] = design_profile_array = pssm_as_array(self.pose.profile)\n    batch_design_profile = np.tile(design_profile_array, (number_of_sequences, 1, 1))\n    if self.pose.fragment_info_by_entity_pair:\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', category=RuntimeWarning)\n            # np.log causes -inf at 0, so they are corrected to a 'large' number\n            corrected_design_array = np.nan_to_num(np.log(batch_design_profile), copy=False,\n                                                   nan=np.nan, neginf=metrics.zero_probability_evol_value)\n            torch_log_design_profile = torch.from_numpy(corrected_design_array)\n    else:\n        torch_log_design_profile = torch.from_numpy(np.log(batch_design_profile))\n    per_residue_design_profile_loss = \\\n        resources.ml.sequence_nllloss(torch_numeric_sequences, torch_log_design_profile)\n\n    if self.job.fragment_db is not None:\n        interface_bkgd = np.array(list(self.job.fragment_db.aa_frequencies.values()))\n        profile_background['interface'] = np.tile(interface_bkgd, (pose_length, 1))\n\n    # Format all sequence info to DataFrame\n    sequence_data = {\n        'type': [list(sequence) for sequence in sequences],  # Ensure 2D array like\n        'sequence_loss_design': per_residue_design_profile_loss,\n        'sequence_loss_evolution': per_residue_evolutionary_profile_loss,\n        'sequence_loss_fragment': per_residue_fragment_profile_loss\n    }\n\n    sequence_df = pd.concat([pd.DataFrame(data, index=design_ids,\n                                          columns=pd.MultiIndex.from_product([residue_indices, [metric]]))\n                             for metric, data in sequence_data.items()], axis=1)\n\n    # if profile_background:\n    # Todo This is pretty much already done!\n    #  pose_alignment = MultipleSequenceAlignment.from_array(sequences)\n    #  Make this capability\n    #   pose_alignment.tolist()\n    # Ensure sequences are strings as MultipleSequenceAlignment.from_dictionary() SeqRecord requires ids as strings\n    design_names = [str(id_) for id_ in design_ids]\n    design_sequences = dict(zip(design_names, sequences))\n    pose_alignment = MultipleSequenceAlignment.from_dictionary(design_sequences)\n    # Todo this must be calculated on the entire Designs batch\n    # # Calculate Jensen Shannon Divergence using design frequencies and different profile occurrence data\n    # per_residue_divergence_df = \\\n    #     pd.concat([pd.DataFrame(metrics.position_specific_divergence(pose_alignment.frequencies, background),\n    #                             index=design_ids,\n    #                             columns=pd.MultiIndex.from_product([residue_indices, [f'divergence_{profile}']]))\n    #                for profile, background in profile_background.items()])\n    # Perform a frequency extraction for each background profile\n    per_residue_background_frequency_df = \\\n        pd.concat([pd.DataFrame(pose_alignment.get_probabilities_from_profile(background), index=design_ids,\n                                columns=pd.MultiIndex.from_product([residue_indices, [f'observed_{profile}']]))\n                   for profile, background in profile_background.items()], axis=1)\n\n    sequences_by_entity: list[list[str]] = []\n    for entity in self.pose.entities:\n        entity_slice = slice(entity.n_terminal_residue.index, 1 + entity.c_terminal_residue.index)\n        sequences_by_entity.append([sequence[entity_slice] for sequence in sequences])\n\n    sequences_split_at_entity: list[tuple[str, ...]] = list(zip(*sequences_by_entity))\n\n    # Calculate hydrophobic collapse for each design\n    if self.measure_evolution:\n        hydrophobicity = 'expanded'\n    else:\n        hydrophobicity = 'standard'\n    contact_order_per_res_z, reference_collapse, collapse_profile = \\\n        self.pose.get_folding_metrics(hydrophobicity=hydrophobicity)\n    # collapse_significance_threshold = metrics.collapse_thresholds[hydrophobicity]\n    if self.measure_evolution:  # collapse_profile.size:  # Not equal to zero, use the profile instead\n        reference_collapse = collapse_profile\n    #     reference_mean = np.nanmean(collapse_profile, axis=-2)\n    #     reference_std = np.nanstd(collapse_profile, axis=-2)\n    # else:\n    #     reference_mean = reference_std = None\n    folding_and_collapse = \\\n        metrics.collapse_per_residue(sequences_split_at_entity, contact_order_per_res_z, reference_collapse)\n    per_residue_collapse_df = pd.concat({design_id: pd.DataFrame(data, index=residue_indices)\n                                         for design_id, data in zip(design_ids, folding_and_collapse)},\n                                        ).unstack().swaplevel(0, 1, axis=1)\n    # Calculate mutational content\n    # Make a mutational array, i.e. find those sites that have been mutated from the reference\n    # reference_numeric_sequence = numeric_sequences[0]\n    reference_numeric_sequence = self.pose.sequence_numeric\n    mutational_array = (numeric_sequences - reference_numeric_sequence != 0)\n    mutation_df = pd.DataFrame(mutational_array, index=design_ids,\n                               columns=pd.MultiIndex.from_product([residue_indices, ['mutation']]))\n    # Join all results\n    residues_df = sequence_df.join([  # per_residue_divergence_df,\n        # Make background_frequency dataframe according to whether indicated residue was allowed in profile\n        per_residue_background_frequency_df &gt; 0,\n        per_residue_collapse_df,\n        mutation_df])\n\n    # entity_alignment = multi_chain_alignment(entity_sequences)\n    # INSTEAD OF USING BELOW, split Pose.MultipleSequenceAlignment at entity.chain_break...\n    # entity_alignments = \\\n    #     {idx: MultipleSequenceAlignment.from_dictionary(designed_sequences)\n    #      for idx, designed_sequences in entity_sequences.items()}\n    # entity_alignments = \\\n    #     {idx: msa_from_dictionary(designed_sequences) for idx, designed_sequences in entity_sequences.items()}\n    # pose_collapse_ = pd.concat(pd.DataFrame(folding_and_collapse), axis=1, keys=[('sequence_design', 'pose')])\n    dca_design_residues_concat = []\n    dca_succeed = True\n    # dca_background_energies, dca_design_energies = [], []\n    dca_background_energies, dca_design_energies = {}, {}\n    for entity, entity_sequences in zip(self.pose.entities, sequences_by_entity):\n        try:  # Todo add these to the analysis\n            entity.h_fields = self.job.api_db.bmdca_fields.retrieve_data(name=entity.name)\n            entity.j_couplings = self.job.api_db.bmdca_couplings.retrieve_data(name=entity.name)\n            dca_background_residue_energies = entity.direct_coupling_analysis()\n            # Todo INSTEAD OF USING BELOW, split Pose.MultipleSequenceAlignment at entity.chain_break...\n            entity_alignment = \\\n                MultipleSequenceAlignment.from_dictionary(dict(zip(design_names, entity_sequences)))\n            # entity_alignment = msa_from_dictionary(entity_sequences[idx])\n            entity.msa = entity_alignment\n            dca_design_residue_energies = entity.direct_coupling_analysis()\n            dca_design_residues_concat.append(dca_design_residue_energies)\n            # dca_background_energies.append(dca_background_energies.sum(axis=1))\n            # dca_design_energies.append(dca_design_energies.sum(axis=1))\n            dca_background_energies[entity] = dca_background_residue_energies.sum(axis=1)  # Turns data to 1D\n            dca_design_energies[entity] = dca_design_residue_energies.sum(axis=1)\n        except AttributeError:\n            self.log.debug(f\"For {entity.name}, DCA analysis couldn't be performed. \"\n                           f\"Missing required parameter files\")\n            dca_succeed = False\n            break\n\n    if dca_succeed:\n        # concatenate along columns, adding residue index to column, design name to row\n        dca_concatenated_df = pd.DataFrame(np.concatenate(dca_design_residues_concat, axis=1),\n                                           index=design_ids, columns=residue_indices)\n        # dca_concatenated_df.columns = pd.MultiIndex.from_product([dca_concatenated_df.columns, ['dca_energy']])\n        dca_concatenated_df = pd.concat([dca_concatenated_df], keys=['dca_energy'], axis=1).swaplevel(0, 1, axis=1)\n        # Merge with residues_df\n        residues_df = pd.merge(residues_df, dca_concatenated_df, left_index=True, right_index=True)\n\n    return residues_df\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.PoseProtocol.interface_design_analysis","title":"interface_design_analysis","text":"<pre><code>interface_design_analysis(designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; Series\n</code></pre> <p>Retrieve all score information from a PoseJob and write results to .csv file</p> <p>Parameters:</p> <ul> <li> <code>designs</code>             (<code>Iterable[Pose] | Iterable[AnyStr]</code>, default:                 <code>None</code> )         \u2013          <p>The designs to perform analysis on. By default, fetches all available structures</p> </li> </ul> <p>Returns:     Series containing summary metrics for all designs</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def interface_design_analysis(self, designs: Iterable[Pose] | Iterable[AnyStr] = None) -&gt; pd.Series:\n    \"\"\"Retrieve all score information from a PoseJob and write results to .csv file\n\n    Args:\n        designs: The designs to perform analysis on. By default, fetches all available structures\n    Returns:\n        Series containing summary metrics for all designs\n    \"\"\"\n    raise NotImplementedError(\n        'This is in place for backward compatibility but is currently not debugged. Please'\n        f' consider using the module \"{flags.process_rosetta_metrics}\" instead, or debug '\n        f'{self.interface_design_analysis.__name__} from the module \"{flags.analysis}\"')\n    self.load_pose()\n    # self.identify_interface()\n\n    # Load fragment_profile into the analysis\n    if not self.pose.fragment_info_by_entity_pair:\n        self.generate_fragments(interface=True)\n        self.pose.calculate_fragment_profile()\n\n    # CAUTION: Assumes each structure is the same length\n    # Todo get residues_df['design_indices']\n    pose_length = self.pose.number_of_residues\n    residue_indices = list(range(pose_length))\n    # residue_numbers = [residue.number for residue in self.pose.residues]\n    # interface_residue_indices = [residue.index for residue in self.pose.interface_residues]\n\n    # Find all designs files\n    # Todo fold these into Model(s) and attack metrics from Pose objects?\n    if designs is None:\n        pose_kwargs = self.pose_kwargs\n        designs = [Pose.from_file(file, **pose_kwargs) for file in self.get_design_files()]  # Todo PoseJob(.path)\n\n    pose_sequences = {pose.name: pose.sequence for pose in designs}\n    sequences_df = self.analyze_sequence_metrics_per_design(pose_sequences)\n    # all_mutations = generate_mutations_from_reference(self.pose.sequence, pose_sequences,\n    #                                                   zero_index=True, return_to=True)\n\n    entity_energies = [0. for _ in self.pose.entities]\n    pose_source_residue_info = \\\n        {residue.index: {'complex': 0., 'bound': entity_energies.copy(), 'unbound': entity_energies.copy(),\n                         'solv_complex': 0., 'solv_bound': entity_energies.copy(),\n                         'solv_unbound': entity_energies.copy(), 'fsp': 0., 'cst': 0., 'hbond': 0}\n         for entity in self.pose.entities for residue in entity.residues}\n    pose_name = self.name\n    residue_info = {pose_name: pose_source_residue_info}\n\n    # Gather miscellaneous pose specific metrics\n    # other_pose_metrics = self.pose.calculate_metrics()\n    # Create metrics for the pose_source\n    empty_source = dict(\n        # **other_pose_metrics,\n        buried_unsatisfied_hbonds_complex=0,\n        # buried_unsatisfied_hbonds_unbound=0,\n        contact_count=0,\n        favor_residue_energy=0,\n        interaction_energy_complex=0,\n        interaction_energy_per_residue=0,\n        interface_separation=0,\n        number_hbonds=0,\n        rmsd_complex=0,  # Todo calculate this here instead of Rosetta using superposition3d\n        rosetta_reference_energy=0,\n        shape_complementarity=0,\n    )\n    job_key = 'no_energy'\n    empty_source[putils.protocol] = job_key\n    for idx, entity in enumerate(self.pose.entities, 1):\n        empty_source[f'buns{idx}_unbound'] = 0\n        empty_source[f'entity{idx}_interface_connectivity'] = 0\n\n    source_df = pd.DataFrame(empty_source, index=[pose_name])\n\n    # Get the metrics from the score file for each design\n    if os.path.exists(self.scores_file):  # Rosetta scores file is present  # Todo PoseJob(.path)\n        self.log.debug(f'Found design scores in file: {self.scores_file}')  # Todo PoseJob(.path)\n        design_was_performed = True\n        # # Get the scores from the score file on design trajectory metrics\n        # source_df = pd.DataFrame.from_dict({pose_name: {putils.protocol: job_key}}, orient='index')\n        # for idx, entity in enumerate(self.pose.entities, 1):\n        #     source_df[f'buns{idx}_unbound'] = 0\n        #     source_df[f'entity{idx}_interface_connectivity'] = 0\n        # source_df['buried_unsatisfied_hbonds_complex'] = 0\n        # # source_df['buried_unsatisfied_hbonds_unbound'] = 0\n        # source_df['contact_count'] = 0\n        # source_df['favor_residue_energy'] = 0\n        # # Used in sum_per_residue_df\n        # # source_df['interface_energy_complex'] = 0\n        # source_df['interaction_energy_complex'] = 0\n        # source_df['interaction_energy_per_residue'] = \\\n        #     source_df['interaction_energy_complex'] / len(self.pose.interface_residues)\n        # source_df['interface_separation'] = 0\n        # source_df['number_hbonds'] = 0\n        # source_df['rmsd_complex'] = 0\n        # source_df['rosetta_reference_energy'] = 0\n        # source_df['shape_complementarity'] = 0\n        design_scores = metrics.parse_rosetta_scorefile(self.scores_file)\n        self.log.debug(f'All designs with scores: {\", \".join(design_scores.keys())}')\n        # Find designs with scores and structures\n        structure_design_scores = {}\n        for pose in designs:\n            try:\n                structure_design_scores[pose.name] = design_scores.pop(pose.name)\n            except KeyError:  # Structure wasn't scored, we will remove this later\n                pass\n\n        # Create protocol dataframe\n        scores_df = pd.DataFrame.from_dict(structure_design_scores, orient='index')\n        # # Fill in all the missing values with that of the default pose_source\n        # scores_df = pd.concat([source_df, scores_df]).fillna(method='ffill')\n        # Gather all columns into specific types for processing and formatting\n        per_res_columns = []\n        # hbonds_columns = []\n        for column in scores_df.columns.tolist():\n            if 'res_' in column:  # if column.startswith('per_res_'):\n                per_res_columns.append(column)\n            # elif column.startswith('hbonds_res_selection'):\n            #     hbonds_columns.append(column)\n\n        # Check proper input\n        metric_set = metrics.rosetta_required.difference(set(scores_df.columns))\n        if metric_set:\n            raise DesignError(\n                f'Missing required metrics: \"{\", \".join(metric_set)}\"')\n\n        # Remove unnecessary (old scores) as well as Rosetta pose score terms besides ref (has been renamed above)\n        # Todo learn know how to produce Rosetta score terms in output score file. Not in FastRelax...\n        remove_columns = metrics.rosetta_terms + metrics.unnecessary + per_res_columns\n        # Todo remove dirty when columns are correct (after P432)\n        #  and column tabulation precedes residue/hbond_processing\n        # residue_info = {'energy': {'complex': 0., 'unbound': 0.}, 'type': None, 'hbond': 0}\n        residue_info.update(self.pose.process_rosetta_residue_scores(structure_design_scores))\n        # Can't use residue_processing (clean) ^ in the case there is a design without metrics... columns not found!\n        residue_info = metrics.process_residue_info(\n            residue_info, hbonds=self.pose.rosetta_hbond_processing(structure_design_scores))\n        # residue_info = metrics.incorporate_sequence_info(residue_info, pose_sequences)\n\n        # Drop designs where required data isn't present\n        # Format protocol columns\n        if putils.protocol in scores_df.columns:\n            missing_group_indices = scores_df[putils.protocol].isna()\n            # Todo remove not DEV\n            scout_indices = [idx for idx in scores_df[missing_group_indices].index if 'scout' in idx]\n            scores_df.loc[scout_indices, putils.protocol] = putils.scout\n            structure_bkgnd_indices = [idx for idx in scores_df[missing_group_indices].index\n                                       if 'no_constraint' in idx]\n            scores_df.loc[structure_bkgnd_indices, putils.protocol] = putils.structure_background\n            # Todo Done remove\n            # protocol_s.replace({'combo_profile': putils.design_profile}, inplace=True)  # Ensure proper name\n\n            scores_df.drop(missing_group_indices, axis=0, inplace=True, errors='ignore')\n            # protocol_s.drop(missing_group_indices, inplace=True, errors='ignore')\n\n        viable_designs = scores_df.index.tolist()\n        if not viable_designs:\n            raise DesignError('No viable designs remain after processing!')\n\n        self.log.debug(f'Viable designs with structures remaining after cleaning:\\n\\t{\", \".join(viable_designs)}')\n        pose_sequences = {design: sequence for design, sequence in pose_sequences.items() if\n                          design in viable_designs}\n\n        # Todo implement this protocol if sequence data is taken at multiple points along a trajectory and the\n        #  sequence data along trajectory is a metric on it's own\n        # # Gather mutations for residue specific processing and design sequences\n        # for design, data in list(structure_design_scores.items()):  # make a copy as can be removed\n        #     sequence = data.get('final_sequence')\n        #     if sequence:\n        #         if len(sequence) &gt;= pose_length:\n        #             pose_sequences[design] = sequence[:pose_length]  # Todo won't work if design had insertions\n        #         else:\n        #             pose_sequences[design] = sequence\n        #     else:\n        #         self.log.warning('Design %s is missing sequence data, removing from design pool' % design)\n        #         structure_design_scores.pop(design)\n        # # format {entity: {design_name: sequence, ...}, ...}\n        # entity_sequences = \\\n        #     {entity: {design: sequence[entity.n_terminal_residue.number - 1:entity.c_terminal_residue.number]\n        #               for design, sequence in pose_sequences.items()} for entity in self.pose.entities}\n    else:\n        self.log.debug(f'Missing design scores file at {self.scores_file}')  # Todo PoseJob(.path)\n        design_was_performed = False\n        # Todo add relevant missing scores such as those specified as 0 below\n        # Todo may need to put source_df in scores file alternative\n        # source_df = pd.DataFrame.from_dict({pose_name: {putils.protocol: job_key}}, orient='index')\n        # for idx, entity in enumerate(self.pose.entities, 1):\n        #     source_df[f'buns{idx}_unbound'] = 0\n        #     source_df[f'entity{idx}_interface_connectivity'] = 0\n        #     # residue_info = {'energy': {'complex': 0., 'unbound': 0.}, 'type': None, 'hbond': 0}\n        #     # design_info.update({residue.number: {'energy_delta': 0.,\n        #     #                                      'type': protein_letters_3to1.get(residue.type),\n        #     #                                      'hbond': 0} for residue in entity.residues})\n        # source_df['buried_unsatisfied_hbonds_complex'] = 0\n        # # source_df['buried_unsatisfied_hbonds_unbound'] = 0\n        # source_df['contact_count'] = 0\n        # source_df['favor_residue_energy'] = 0\n        # # source_df['interface_energy_complex'] = 0\n        # source_df['interaction_energy_complex'] = 0\n        # source_df['interaction_energy_per_residue'] = \\\n        #     source_df['interaction_energy_complex'] / len(self.pose.interface_residues)\n        # source_df['interface_separation'] = 0\n        # source_df['number_hbonds'] = 0\n        # source_df['rmsd_complex'] = 0\n        # source_df['rosetta_reference_energy'] = 0\n        # source_df['shape_complementarity'] = 0\n        scores_df = pd.DataFrame.from_dict({pose.name: {putils.protocol: job_key} for pose in designs},\n                                           orient='index')\n        # # Fill in all the missing values with that of the default pose_source\n        # scores_df = pd.concat([source_df, scores_df]).fillna(method='ffill')\n\n        remove_columns = metrics.rosetta_terms + metrics.unnecessary\n        residue_info.update({struct_name: pose_source_residue_info for struct_name in scores_df.index.tolist()})\n        # Todo generate energy scores internally which matches output from residue_processing\n        viable_designs = [pose.name for pose in designs]\n\n    scores_df.drop(remove_columns, axis=1, inplace=True, errors='ignore')\n\n    # Find protocols for protocol specific data processing removing from scores_df\n    protocol_s = scores_df.pop(putils.protocol).copy()\n    designs_by_protocol = protocol_s.groupby(protocol_s).groups\n    # unique_protocols = list(designs_by_protocol.keys())\n    # Remove refine and consensus if present as there was no design done over multiple protocols\n    # Todo change if we did multiple rounds of these protocols\n    designs_by_protocol.pop(putils.refine, None)\n    designs_by_protocol.pop(putils.consensus, None)\n    # Get unique protocols\n    unique_design_protocols = set(designs_by_protocol.keys())\n    self.log.info(f'Unique Design Protocols: {\", \".join(unique_design_protocols)}')\n\n    # Replace empty strings with np.nan and convert remaining to float\n    scores_df.replace('', np.nan, inplace=True)\n    scores_df.fillna(dict(zip(metrics.protocol_specific_columns, repeat(0))), inplace=True)\n    # scores_df = scores_df.astype(float)  # , copy=False, errors='ignore')\n    # Fill in all the missing values with that of the default pose_source\n    scores_df = pd.concat([source_df, scores_df]).fillna(method='ffill')\n\n    # atomic_deviation = {}\n    # pose_assembly_minimally_contacting = self.pose.assembly_minimally_contacting\n    # perform SASA measurements\n    # pose_assembly_minimally_contacting.get_sasa()\n    # assembly_asu_residues = pose_assembly_minimally_contacting.residues[:pose_length]\n    # per_residue_data['sasa_hydrophobic_complex'][pose_name] = \\\n    #     [residue.sasa_apolar for residue in assembly_asu_residues]\n    # per_residue_data['sasa_polar_complex'][pose_name] = [residue.sasa_polar for residue in assembly_asu_residues]\n    # per_residue_data['sasa_relative_complex'][pose_name] = \\\n    #     [residue.relative_sasa for residue in assembly_asu_residues]\n\n    # Grab metrics for the pose source. Checks if self.pose was designed\n    # Favor pose source errat/collapse on a per-entity basis if design occurred\n    # As the pose source assumes no legit interface present while designs have an interface\n    # per_residue_sasa_unbound_apolar, per_residue_sasa_unbound_polar, per_residue_sasa_unbound_relative = [], [], []\n    # source_errat_accuracy, inverse_residue_contact_order_z = [], []\n\n    per_residue_data: dict[str, dict[str, Any]] = \\\n        {pose_name: {**self.pose.per_residue_interface_surface_area(),\n                     **self.pose.per_residue_contact_order()}\n         }\n\n    number_of_entities = self.pose.number_of_entities\n    # if design_was_performed:  # The input structure was not meant to be together, treat as such\n    #     source_errat = []\n    #     for idx, entity in enumerate(self.pose.entities):\n    #         # Replace 'errat_deviation' measurement with uncomplexed entities\n    #         # oligomer_errat_accuracy, oligomeric_errat = entity_oligomer.errat(out_path=os.path.devnull)\n    #         # source_errat_accuracy.append(oligomer_errat_accuracy)\n    #         _, oligomeric_errat = entity.assembly.errat(out_path=os.path.devnull)\n    #         source_errat.append(oligomeric_errat[:entity.number_of_residues])\n    #     # atomic_deviation[pose_name] = sum(source_errat_accuracy) / float(number_of_entities)\n    #     pose_source_errat = np.concatenate(source_errat)\n    # else:\n    #     # Get errat measurement\n    #     # per_residue_data[pose_name].update(self.pose.per_residue_errat())\n    #     pose_source_errat = self.pose.per_residue_errat()['errat_deviation']\n    #\n    # per_residue_data[pose_name]['errat_deviation'] = pose_source_errat\n\n    # Compute structural measurements for all designs\n    interface_local_density = {pose_name: self.pose.local_density_interface()}\n    for pose in designs:  # Takes 1-2 seconds for Structure -&gt; assembly -&gt; errat\n        # Must find interface residues before measure local_density\n        pose.find_and_split_interface()\n        per_residue_data[pose.name] = pose.per_residue_interface_surface_area()\n        # Get errat measurement\n        # per_residue_data[pose.name].update(pose.per_residue_errat())\n        # Todo remove Rosetta\n        #  This is a measurement of interface_connectivity like from Rosetta\n        interface_local_density[pose.name] = pose.local_density_interface()\n\n    scores_df['interface_local_density'] = pd.Series(interface_local_density)\n\n    # Load profiles of interest into the analysis\n    self.set_up_evolutionary_profile(warn_metrics=True)\n\n    # self.generate_fragments() was already called\n    self.pose.calculate_profile()\n\n    profile_background = {'design': pssm_as_array(self.pose.profile),\n                          'evolution': pssm_as_array(self.pose.evolutionary_profile),\n                          'fragment': self.pose.fragment_profile.as_array()}\n    if self.job.fragment_db is not None:\n        interface_bkgd = np.array(list(self.job.fragment_db.aa_frequencies.values()))\n        profile_background['interface'] = np.tile(interface_bkgd, (self.pose.number_of_residues, 1))\n\n    # Calculate hydrophobic collapse for each design\n    # Include the pose_source in the measured designs\n    if self.measure_evolution:\n        # This is set for figures as well\n        hydrophobicity = 'expanded'\n    else:\n        hydrophobicity = 'standard'\n    contact_order_per_res_z, reference_collapse, collapse_profile = \\\n        self.pose.get_folding_metrics(hydrophobicity=hydrophobicity)\n    if self.measure_evolution:  # collapse_profile.size:  # Not equal to zero, use the profile instead\n        reference_collapse = collapse_profile\n    #     reference_mean = np.nanmean(collapse_profile, axis=-2)\n    #     reference_std = np.nanstd(collapse_profile, axis=-2)\n    # else:\n    #     reference_mean = reference_std = None\n    entity_sequences = []\n    for entity in self.pose.entities:\n        entity_slice = slice(entity.n_terminal_residue.index, 1 + entity.c_terminal_residue.index)\n        entity_sequences.append({design: sequence[entity_slice] for design, sequence in pose_sequences.items()})\n\n    all_sequences_split = [list(designed_sequences.values()) for designed_sequences in entity_sequences]\n    all_sequences_by_entity = list(zip(*all_sequences_split))\n\n    folding_and_collapse = \\\n        metrics.collapse_per_residue(all_sequences_by_entity, contact_order_per_res_z, reference_collapse)\n    per_residue_collapse_df = pd.concat({design_id: pd.DataFrame(data, index=residue_indices)\n                                         for design_id, data in zip(viable_designs, folding_and_collapse)},\n                                        ).unstack().swaplevel(0, 1, axis=1)\n\n    # Convert per_residue_data into a dataframe matching residues_df orientation\n    residues_df = pd.concat({name: pd.DataFrame(data, index=residue_indices)\n                            for name, data in per_residue_data.items()}).unstack().swaplevel(0, 1, axis=1)\n    # Fill in missing pose_source metrics for each design not calculated in rosetta\n    # residues_df.fillna(residues_df.loc[pose_name, idx_slice[:, 'contact_order']], inplace=True)\n    residues_df.fillna(residues_df.loc[pose_name, :], inplace=True)\n\n    residues_df = residues_df.join(per_residue_collapse_df)\n\n    # Process mutational frequencies, H-bond, and Residue energy metrics to dataframe\n    rosetta_residues_df = pd.concat({design: pd.DataFrame(info) for design, info in residue_info.items()}).unstack()\n    # returns multi-index column with residue number as first (top) column index, metric as second index\n    # during residues_df unstack, all residues with missing dicts are copied as nan\n    # Merge interface design specific residue metrics with total per residue metrics\n    # residues_df = pd.merge(residues_df, rosetta_residues_df, left_index=True, right_index=True)\n\n    # Join each residues_df like dataframe\n    # Each of these can have difference index, so we use concat to perform an outer merge\n    residues_df = pd.concat([residues_df, sequences_df, rosetta_residues_df], axis=1)\n    # # Join rosetta_residues_df and sequence metrics\n    # residues_df = residues_df.join([rosetta_residues_df, sequences_df])\n\n    if not profile_background:\n        divergence_s = pd.Series(dtype=float)\n    else:  # Calculate sequence statistics\n        # First, for entire pose\n        pose_alignment = MultipleSequenceAlignment.from_dictionary(pose_sequences)\n        observed, divergence = \\\n            metrics.calculate_sequence_observations_and_divergence(pose_alignment, profile_background)\n        observed_dfs = []\n        for profile, observed_values in observed.items():\n            # scores_df[f'observed_{profile}'] = observed_values.mean(axis=1)\n            observed_dfs.append(pd.DataFrame(observed_values, index=viable_designs,\n                                             columns=pd.MultiIndex.from_product([residue_indices,\n                                                                                 [f'observed_{profile}']]))\n                                )\n        # Add observation information into the residues_df\n        residues_df = residues_df.join(observed_dfs)\n        # Get pose sequence divergence\n        design_residue_indices = [residue.index for residue in self.pose.design_residues]\n        pose_divergence_s = pd.concat([pd.Series({f'{divergence_type}_per_residue':\n                                                  _divergence[design_residue_indices].mean()\n                                                  for divergence_type, _divergence in divergence.items()})],\n                                      keys=[('sequence_design', 'pose')])\n        # pose_divergence_s = pd.Series({f'{divergence_type}_per_residue': per_res_metric(stat)\n        #                                for divergence_type, stat in divergence.items()},\n        #                               name=('sequence_design', 'pose'))\n        if designs_by_protocol:  # were multiple designs generated with each protocol?\n            # find the divergence within each protocol\n            divergence_by_protocol = {protocol: {} for protocol in designs_by_protocol}\n            for protocol, designs in designs_by_protocol.items():\n                # Todo select from pose_alignment the indices of each design then pass to MultipleSequenceAlignment?\n                # protocol_alignment = \\\n                #     MultipleSequenceAlignment.from_dictionary({design: pose_sequences[design]\n                #                                                for design in designs})\n                protocol_alignment = MultipleSequenceAlignment.from_dictionary({design: pose_sequences[design]\n                                                                                for design in designs})\n                # protocol_mutation_freq = filter_dictionary_keys(protocol_alignment.frequencies,\n                #                                                 self.pose.interface_residues)\n                # protocol_mutation_freq = protocol_alignment.frequencies\n                protocol_divergence = {f'divergence_{profile}':\n                                       metrics.position_specific_divergence(protocol_alignment.frequencies, bgd)\n                                       for profile, bgd in profile_background.items()}\n                # if interface_bkgd is not None:\n                #     protocol_divergence['divergence_interface'] = \\\n                #         metrics.position_specific_divergence(protocol_alignment.frequencies, tiled_int_background)\n                # Get per residue divergence metric by protocol\n                divergence_by_protocol[protocol] = \\\n                    {f'{divergence_type}_per_residue': divergence[design_residue_indices].mean()\n                     for divergence_type, divergence in protocol_divergence.items()}\n            # new = dfd.columns.to_frame()\n            # new.insert(0, 'new2_level_name', new_level_values)\n            # dfd.columns = pd.MultiIndex.from_frame(new)\n            protocol_divergence_s = \\\n                pd.concat([pd.DataFrame(divergence_by_protocol).unstack()], keys=['sequence_design'])\n        else:\n            protocol_divergence_s = pd.Series(dtype=float)\n\n        # Get profile mean observed\n        # Perform a frequency extraction for each background profile\n        per_residue_background_frequency_df = \\\n            pd.concat([pd.DataFrame(pose_alignment.get_probabilities_from_profile(background), index=viable_designs,\n                                    columns=pd.MultiIndex.from_product([residue_indices, [f'observed_{profile}']]))\n                       for profile, background in profile_background.items()], axis=1)\n        # Todo\n        #  Ensure that only interface residues are selected, not only by those that are 0 as interface can be 0!\n        observed_frequencies_from_fragment_profile = profile_background.pop('fragment', None)\n        if observed_frequencies_from_fragment_profile:\n            observed_frequencies_from_fragment_profile[observed_frequencies_from_fragment_profile == 0] = np.nan\n            # Todo RuntimeWarning: Mean of empty slice\n            scores_df['observed_fragment_mean_probability'] = \\\n                np.nanmean(observed_frequencies_from_fragment_profile, axis=1)\n        for profile, background in profile_background.items():\n            scores_df[f'observed_{profile}_mean_probability'] = scores_df[f'observed_{profile}'] / pose_length\n            # scores_df['observed_evolution_mean_probability'] = scores_df['observed_evolution'] / pose_length\n\n        divergence_s = pd.concat([protocol_divergence_s, pose_divergence_s])\n\n    # entity_alignment = multi_chain_alignment(entity_sequences)\n    # INSTEAD OF USING BELOW, split Pose.MultipleSequenceAlignment at entity.chain_break...\n    # entity_alignments = \\\n    #     {idx: MultipleSequenceAlignment.from_dictionary(designed_sequences)\n    #      for idx, designed_sequences in entity_sequences.items()}\n    # entity_alignments = \\\n    #     {idx: msa_from_dictionary(designed_sequences) for idx, designed_sequences in entity_sequences.items()}\n    # pose_collapse_ = pd.concat(pd.DataFrame(folding_and_collapse), axis=1, keys=[('sequence_design', 'pose')])\n    dca_design_residues_concat = []\n    dca_succeed = True\n    # dca_background_energies, dca_design_energies = [], []\n    dca_background_energies, dca_design_energies = {}, {}\n    for idx, entity in enumerate(self.pose.entities):\n        try:  # Todo add these to the analysis\n            entity.h_fields = self.job.api_db.bmdca_fields.retrieve_data(name=entity.name)\n            entity.j_couplings = self.job.api_db.bmdca_couplings.retrieve_data(name=entity.name)\n            dca_background_residue_energies = entity.direct_coupling_analysis()\n            # Todo\n            #  Instead of below, split Pose.MultipleSequenceAlignment at entity.chain_break...\n            entity_alignment = MultipleSequenceAlignment.from_dictionary(entity_sequences[idx])\n            # entity_alignment = msa_from_dictionary(entity_sequences[idx])\n            entity.msa = entity_alignment\n            dca_design_residue_energies = entity.direct_coupling_analysis()\n            dca_design_residues_concat.append(dca_design_residue_energies)\n            # dca_background_energies.append(dca_background_energies.sum(axis=1))\n            # dca_design_energies.append(dca_design_energies.sum(axis=1))\n            dca_background_energies[entity] = dca_background_residue_energies.sum(axis=1)  # turns data to 1D\n            dca_design_energies[entity] = dca_design_residue_energies.sum(axis=1)\n        except AttributeError:\n            self.log.warning(f\"For {entity.name}, DCA analysis couldn't be performed. \"\n                             f\"Missing required parameter files\")\n            dca_succeed = False\n\n    if dca_succeed:\n        # concatenate along columns, adding residue index to column, design name to row\n        dca_concatenated_df = pd.DataFrame(np.concatenate(dca_design_residues_concat, axis=1),\n                                           index=viable_designs, columns=residue_indices)\n        # dca_concatenated_df.columns = pd.MultiIndex.from_product([dca_concatenated_df.columns, ['dca_energy']])\n        dca_concatenated_df = pd.concat([dca_concatenated_df], keys=['dca_energy'], axis=1).swaplevel(0, 1, axis=1)\n        # Merge with residues_df\n        residues_df = pd.merge(residues_df, dca_concatenated_df, left_index=True, right_index=True)\n\n    # Make buried surface area (bsa) columns, and residue classification\n    residues_df = metrics.calculate_residue_buried_surface_area(residues_df)\n    residues_df = metrics.classify_interface_residues(residues_df)\n\n    # Calculate new metrics from combinations of other metrics\n    # Add design residue information to scores_df such as how many core, rim, and support residues were measured\n    mean_columns = ['hydrophobicity']  # , 'sasa_relative_bound', 'sasa_relative_complex']\n    scores_df = scores_df.join(metrics.sum_per_residue_metrics(residues_df, mean_metrics=mean_columns))\n\n    # scores_df['collapse_new_positions'] /= pose_length\n    # scores_df['collapse_new_position_significance'] /= pose_length\n    scores_df['collapse_significance_by_contact_order_z_mean'] = \\\n        scores_df['collapse_significance_by_contact_order_z'] / \\\n        (residues_df.loc[:, idx_slice[:, 'collapse_significance_by_contact_order_z']] != 0).sum(axis=1)\n    # if self.measure_alignment:\n    # Todo THESE ARE NOW DIFFERENT SOURCE if not self.measure_alignment\n    collapse_increased_df = residues_df.loc[:, idx_slice[:, 'collapse_increased_z']]\n    total_increased_collapse = (collapse_increased_df != 0).sum(axis=1)\n    # scores_df['collapse_increase_significance_by_contact_order_z_mean'] = \\\n    #     scores_df['collapse_increase_significance_by_contact_order_z'] / total_increased_collapse\n    # scores_df['collapse_increased_z'] /= pose_length\n    scores_df['collapse_increased_z_mean'] = \\\n        collapse_increased_df.sum(axis=1) / total_increased_collapse\n    scores_df['collapse_variance'] = scores_df['collapse_deviation_magnitude'] / pose_length\n    scores_df['collapse_sequential_peaks_z_mean'] = \\\n        scores_df['collapse_sequential_peaks_z'] / total_increased_collapse\n    scores_df['collapse_sequential_z_mean'] = \\\n        scores_df['collapse_sequential_z'] / total_increased_collapse\n\n    # scores_df['interface_area_total'] = bsa_assembly_df = \\\n    #     scores_df['interface_area_polar'] + scores_df['interface_area_hydrophobic']\n    # Find the proportion of the residue surface area that is solvent accessible versus buried in the interface\n    bsa_assembly_df = scores_df['interface_area_total']\n    scores_df['interface_area_to_residue_surface_ratio'] = \\\n        (bsa_assembly_df / (bsa_assembly_df + scores_df['area_total_complex']))\n\n    # # Make scores_df errat_deviation that takes into account the pose_source sequence errat_deviation\n    # # Include in errat_deviation if errat score is &lt; 2 std devs and isn't 0 to begin with\n    # # Get per-residue errat scores from the residues_df\n    # errat_df = residues_df.loc[:, idx_slice[:, 'errat_deviation']].droplevel(-1, axis=1)\n    #\n    # pose_source_errat = errat_df.loc[pose_name, :]\n    # source_errat_inclusion_boolean = \\\n    #     np.logical_and(pose_source_errat &lt; metrics.errat_2_sigma, pose_source_errat != 0.)\n    # # Find residues where designs deviate above wild-type errat scores\n    # errat_sig_df = errat_df.sub(pose_source_errat, axis=1) &gt; metrics.errat_1_sigma  # axis=1 is per-residue subtract\n    # # Then select only those residues which are expressly important by the inclusion boolean\n    # # This overwrites the metrics.sum_per_residue_metrics() value\n    # scores_df['errat_deviation'] = (errat_sig_df.loc[:, source_errat_inclusion_boolean] * 1).sum(axis=1)\n\n    # Calculate mutational content\n    mutation_df = residues_df.loc[:, idx_slice[:, 'mutation']]\n    # scores_df['number_mutations'] = mutation_df.sum(axis=1)\n    scores_df['percent_mutations'] = scores_df['number_mutations'] / pose_length\n\n    idx = 1\n    # prior_slice = 0\n    for idx, entity in enumerate(self.pose.entities, idx):\n        # entity_n_terminal_residue_index = entity.n_terminal_residue.index\n        # entity_c_terminal_residue_index = entity.c_terminal_residue.index\n        scores_df[f'entity{idx}_number_mutations'] = \\\n            mutation_df.loc[:, idx_slice[residue_indices[entity.n_terminal_residue.index:  # prior_slice\n                                                         1 + entity.c_terminal_residue.index], :]].sum(axis=1)\n        # prior_slice = entity_c_terminal_residue_index\n        scores_df[f'entity{idx}_percent_mutations'] = \\\n            scores_df[f'entity{idx}_number_mutations'] / entity.number_of_residues\n\n    # scores_df['number_mutations'] = \\\n    #     pd.Series({design: len(mutations) for design, mutations in all_mutations.items()})\n    # scores_df['percent_mutations'] = scores_df['number_mutations'] / pose_length\n\n    # Check if any columns are &gt; 50% interior (value can be 0 or 1). If so, return True for that column\n    # interior_residue_df = residues_df.loc[:, idx_slice[:, 'interior']]\n    # interior_residue_numbers = \\\n    #     interior_residues.loc[:, interior_residues.mean(axis=0) &gt; 0.5].columns.remove_unused_levels().levels[0].\n    #     to_list()\n    # if interior_residue_numbers:\n    #     self.log.info(f'Design Residues {\",\".join(map(str, sorted(interior_residue_numbers)))}\n    #                   'are located in the interior')\n\n    # This shouldn't be much different from the state variable self.interface_residue_numbers\n    # perhaps the use of residue neighbor energy metrics adds residues which contribute, but not directly\n    # interface_residues = set(residues_df.columns.levels[0].unique()).difference(interior_residue_numbers)\n\n    # Add design residue information to scores_df such as how many core, rim, and support residues were measured\n    # for residue_class in metrics.residue_classification:\n    #     scores_df[residue_class] = residues_df.loc[:, idx_slice[:, residue_class]].sum(axis=1)\n\n    # Calculate metrics from combinations of metrics with variable integer number metric names\n    scores_columns = scores_df.columns.tolist()\n    self.log.debug(f'Metrics present: {scores_columns}')\n    # sum columns using list[0] + list[1] + list[n]\n    # residues_df = residues_df.drop([column\n    #                                 for columns in [complex_df.columns, bound_df.columns, unbound_df.columns,\n    #                                                 solvation_complex_df.columns, solvation_bound_df.columns,\n    #                                                 solvation_unbound_df.columns]\n    #                                 for column in columns], axis=1)\n    summation_pairs = \\\n        {'buried_unsatisfied_hbonds_unbound': list(filter(re.compile('buns[0-9]+_unbound$').match, scores_columns)),  # Rosetta\n         # 'interface_energy_bound':\n         #     list(filter(re.compile('interface_energy_[0-9]+_bound').match, scores_columns)),  # Rosetta\n         # 'interface_energy_unbound':\n         #     list(filter(re.compile('interface_energy_[0-9]+_unbound').match, scores_columns)),  # Rosetta\n         # 'interface_solvation_energy_bound':\n         #     list(filter(re.compile('solvation_energy_[0-9]+_bound').match, scores_columns)),  # Rosetta\n         # 'interface_solvation_energy_unbound':\n         #     list(filter(re.compile('solvation_energy_[0-9]+_unbound').match, scores_columns)),  # Rosetta\n         'interface_connectivity':\n             list(filter(re.compile('entity[0-9]+_interface_connectivity').match, scores_columns)),  # Rosetta\n         }\n\n    scores_df = metrics.columns_to_new_column(scores_df, summation_pairs)\n    scores_df = metrics.columns_to_new_column(scores_df, metrics.rosetta_delta_pairs, mode='sub')\n    # Add number_residues_interface for div_pairs and int_comp_similarity\n    # scores_df['number_residues_interface'] = other_pose_metrics.pop('number_residues_interface')\n    scores_df = metrics.columns_to_new_column(scores_df, metrics.rosetta_division_pairs, mode='truediv')\n    scores_df['interface_composition_similarity'] = \\\n        scores_df.apply(metrics.interface_composition_similarity, axis=1)\n    scores_df.drop(metrics.clean_up_intermediate_columns, axis=1, inplace=True, errors='ignore')\n    repacking = scores_df.get('repacking')\n    if repacking is not None:\n        # Set interface_bound_activation_energy = np.nan where repacking is 0\n        # Currently is -1 for True (Rosetta Filter quirk...)\n        scores_df.loc[scores_df[repacking == 0].index, 'interface_bound_activation_energy'] = np.nan\n        scores_df.drop('repacking', axis=1, inplace=True)\n\n    # Process dataframes for missing values and drop refine trajectory if present\n    # refine_index = scores_df[scores_df[putils.protocol] == putils.refine].index\n    # scores_df.drop(refine_index, axis=0, inplace=True, errors='ignore')\n    # residues_df.drop(refine_index, axis=0, inplace=True, errors='ignore')\n    # residue_info.pop(putils.refine, None)  # Remove refine from analysis\n    # residues_no_frags = residues_df.columns[residues_df.isna().all(axis=0)].remove_unused_levels().levels[0]\n    # Remove completely empty columns such as obs_interface\n    residues_df.dropna(how='all', inplace=True, axis=1)\n    residues_df = residues_df.fillna(0.).copy()\n    # residue_indices_no_frags = residues_df.columns[residues_df.isna().all(axis=0)]\n\n    # POSE ANALYSIS\n    # scores_df = pd.concat([scores_df, proteinmpnn_df], axis=1)\n    scores_df.dropna(how='all', inplace=True, axis=1)  # Remove completely empty columns\n    # Refine is not considered sequence design and destroys mean. remove v\n    # designs_df = scores_df.sort_index().drop(putils.refine, axis=0, errors='ignore')\n    # Consensus cst_weights are very large and destroy the mean.\n    # Remove this drop for consensus or refine if they are run multiple times\n    designs_df = \\\n        scores_df.drop([pose_name, putils.refine, putils.consensus], axis=0, errors='ignore').sort_index()\n\n    # Get total design statistics for every sequence in the pose and every protocol specifically\n    scores_df[putils.protocol] = protocol_s\n    protocol_groups = scores_df.groupby(putils.protocol)\n    # numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    # print(designs_df.select_dtypes(exclude=numerics))\n\n    pose_stats, protocol_stats = [], []\n    for idx, stat in enumerate(stats_metrics):\n        # Todo both groupby calls have this warning\n        #  FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version,\n        #  it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid\n        #  columns or specify the value of numeric_only to silence this warning.\n        pose_stats.append(getattr(designs_df, stat)().rename(stat))\n        protocol_stats.append(getattr(protocol_groups, stat)())\n\n    # Add the number of observations of each protocol\n    protocol_stats[stats_metrics.index(mean)]['observations'] = protocol_groups.size()\n    # Format stats_s for final pose_s Series\n    protocol_stats_s = pd.concat([stat_df.T.unstack() for stat_df in protocol_stats], keys=stats_metrics)\n    pose_stats_s = pd.concat(pose_stats, keys=list(zip(stats_metrics, repeat('pose'))))\n    pose_stats_s[('mean', 'pose', 'observations')] = len(viable_designs)\n    stat_s = pd.concat([protocol_stats_s.dropna(), pose_stats_s.dropna()])  # dropna removes NaN metrics\n\n    # Change statistic names for all df that are not groupby means for the final trajectory dataframe\n    for idx, stat in enumerate(stats_metrics):\n        if stat != mean:\n            protocol_stats[idx] = protocol_stats[idx].rename(index={protocol: f'{protocol}_{stat}'\n                                                                    for protocol in unique_design_protocols})\n    # Remove std rows if there is no stdev\n    # number_of_trajectories = len(designs_df) + len(protocol_groups) + 1  # 1 for the mean\n    # final_trajectory_indices = designs_df.index.tolist() + unique_protocols + [mean]\n    designs_df = pd.concat([designs_df]\n                           + [df.dropna(how='all', axis=0) for df in protocol_stats]  # v don't add if nothing\n                           + [pd.to_numeric(s).to_frame().T for s in pose_stats if not all(s.isna())])\n    # This concat puts back pose_name, refine, consensus index as protocol_stats is calculated on scores_df\n    # # Add all pose information to each trajectory\n    # pose_metrics_df = pd.DataFrame.from_dict({idx: other_pose_metrics for idx in final_trajectory_indices},\n    #                                          orient='index')\n    # designs_df = pd.concat([designs_df, pose_metrics_df], axis=1)\n    designs_df = designs_df.fillna({'observations': 1})\n\n    # Calculate protocol significance\n    pvalue_df = pd.DataFrame()\n    scout_protocols = list(filter(re.compile(f'.*{putils.scout}').match,\n                                  protocol_s[~protocol_s.isna()].unique().tolist()))\n    similarity_protocols = unique_design_protocols.difference([putils.refine, job_key] + scout_protocols)\n    sim_series = []\n    if putils.structure_background not in unique_design_protocols:\n        self.log.info(f\"Missing background protocol '{putils.structure_background}'. No protocol significance \"\n                      f'measurements available for this pose')\n    elif len(similarity_protocols) == 1:  # measure significance\n        self.log.info(\"Can't measure protocol significance, only one protocol of interest\")\n    else:  # Test significance between all combinations of protocols by grabbing mean entries per protocol\n        for prot1, prot2 in combinations(sorted(similarity_protocols), 2):\n            select_df = \\\n                designs_df.loc[[design for designs in [designs_by_protocol[prot1], designs_by_protocol[prot2]]\n                                for design in designs], metrics.significance_columns]\n            # prot1/2 pull out means from designs_df by using the protocol name\n            difference_s = \\\n                designs_df.loc[prot1, metrics.significance_columns].sub(\n                    designs_df.loc[prot2, metrics.significance_columns])\n            pvalue_df[(prot1, prot2)] = metrics.df_permutation_test(select_df, difference_s, compare='mean',\n                                                                    group1_size=len(designs_by_protocol[prot1]))\n        pvalue_df = pvalue_df.T  # transpose significance pairs to indices and significance metrics to columns\n        designs_df = pd.concat([designs_df, pd.concat([pvalue_df], keys=['similarity']).swaplevel(0, 1)])\n\n        # Compute residue energy/sequence differences between each protocol\n        residue_energy_df = residues_df.loc[:, idx_slice[:, 'energy_delta']]\n\n        scaler = skl.preprocessing.StandardScaler()\n        res_pca = skl.decomposition.PCA(resources.config.default_pca_variance)\n        residue_energy_np = scaler.fit_transform(residue_energy_df.values)\n        residue_energy_pc = res_pca.fit_transform(residue_energy_np)\n\n        seq_pca = skl.decomposition.PCA(resources.config.default_pca_variance)\n        designed_sequence_modifications = residues_df.loc[:, idx_slice[:, 'type']].sum(axis=1).tolist()\n        pairwise_sequence_diff_np = scaler.fit_transform(all_vs_all(designed_sequence_modifications,\n                                                                    sequence_difference))\n        seq_pc = seq_pca.fit_transform(pairwise_sequence_diff_np)\n        # Make principal components (PC) DataFrame\n        residue_energy_pc_df = pd.DataFrame(residue_energy_pc, index=residue_energy_df.index,\n                                            columns=[f'pc{idx}' for idx in range(1, len(res_pca.components_) + 1)])\n        seq_pc_df = pd.DataFrame(seq_pc, index=list(residue_info.keys()),\n                                 columns=[f'pc{idx}' for idx in range(1, len(seq_pca.components_) + 1)])\n        # Compute the euclidean distance\n        # pairwise_pca_distance_np = pdist(seq_pc)\n        # pairwise_pca_distance_np = SDUtils.all_vs_all(seq_pc, euclidean)\n\n        # Merge PC DataFrames with labels\n        # seq_pc_df = pd.merge(protocol_s, seq_pc_df, left_index=True, right_index=True)\n        seq_pc_df[putils.protocol] = protocol_s\n        # residue_energy_pc_df = pd.merge(protocol_s, residue_energy_pc_df, left_index=True, right_index=True)\n        residue_energy_pc_df[putils.protocol] = protocol_s\n        # Next group the labels\n        sequence_groups = seq_pc_df.groupby(putils.protocol)\n        residue_energy_groups = residue_energy_pc_df.groupby(putils.protocol)\n        # Measure statistics for each group\n        # All protocol means have pairwise distance measured to access similarity\n        # Gather protocol similarity/distance metrics\n        sim_measures = {'sequence_distance': {}, 'energy_distance': {}}\n        # sim_stdev = {}  # 'similarity': None, 'seq_distance': None, 'energy_distance': None}\n        # grouped_pc_seq_df_dict, grouped_pc_energy_df_dict, similarity_stat_dict = {}, {}, {}\n        for stat in stats_metrics:\n            grouped_pc_seq_df = getattr(sequence_groups, stat)()\n            grouped_pc_energy_df = getattr(residue_energy_groups, stat)()\n            similarity_stat = getattr(pvalue_df, stat)(axis=1)  # protocol pair : stat Series\n            if stat == mean:\n                # for each measurement in residue_energy_pc_df, need to take the distance between it and the\n                # structure background mean (if structure background, is the mean is useful too?)\n                background_distance = \\\n                    cdist(residue_energy_pc,\n                          grouped_pc_energy_df.loc[putils.structure_background, :].values[np.newaxis, :])\n                designs_df = \\\n                    pd.concat([designs_df,\n                               pd.Series(background_distance.flatten(), index=residue_energy_pc_df.index,\n                                         name=f'energy_distance_from_{putils.structure_background}_mean')], axis=1)\n\n                # if renaming is necessary\n                # protocol_stats_s[stat].index = protocol_stats_s[stat].index.to_series().map(\n                #     {protocol: protocol + '_' + stat for protocol in sorted(unique_design_protocols)})\n                # find the pairwise distance from every point to every other point\n                seq_pca_mean_distance_vector = pdist(grouped_pc_seq_df)\n                energy_pca_mean_distance_vector = pdist(grouped_pc_energy_df)\n                # protocol_indices_map = list(tuple(condensed_to_square(k, len(seq_pca_mean_distance_vector)))\n                #                             for k in seq_pca_mean_distance_vector)\n                # find similarity between each protocol by taking row average of all p-values for each metric\n                # mean_pvalue_s = pvalue_df.mean(axis=1)  # protocol pair : mean significance Series\n                # mean_pvalue_s.index = pd.MultiIndex.from_tuples(mean_pvalue_s.index)\n                # sim_measures['similarity'] = mean_pvalue_s\n                similarity_stat.index = pd.MultiIndex.from_tuples(similarity_stat.index)\n                sim_measures['similarity'] = similarity_stat\n\n                # for vector_idx, seq_dist in enumerate(seq_pca_mean_distance_vector):\n                #     i, j = condensed_to_square(vector_idx, len(grouped_pc_seq_df.index))\n                #     sim_measures['sequence_distance'][(grouped_pc_seq_df.index[i],\n                #                                        grouped_pc_seq_df.index[j])] = seq_dist\n\n                for vector_idx, (seq_dist, energy_dist) in enumerate(zip(seq_pca_mean_distance_vector,\n                                                                         energy_pca_mean_distance_vector)):\n                    i, j = condensed_to_square(vector_idx, len(grouped_pc_energy_df.index))\n                    sim_measures['sequence_distance'][(grouped_pc_seq_df.index[i],\n                                                       grouped_pc_seq_df.index[j])] = seq_dist\n                    sim_measures['energy_distance'][(grouped_pc_energy_df.index[i],\n                                                     grouped_pc_energy_df.index[j])] = energy_dist\n            elif stat == std:\n                # sim_stdev['similarity'] = similarity_stat_dict[stat]\n                pass\n                # # Todo need to square each pc, add them up, divide by the group number, then take the sqrt\n                # sim_stdev['sequence_distance'] = grouped_pc_seq_df\n                # sim_stdev['energy_distance'] = grouped_pc_energy_df\n\n        # Find the significance between each pair of protocols\n        protocol_sig_s = pd.concat([pvalue_df.loc[[pair], :].squeeze() for pair in pvalue_df.index.tolist()],\n                                   keys=[tuple(pair) for pair in pvalue_df.index.tolist()])\n        # squeeze turns the column headers into series indices. Keys appends to make a multi-index\n\n        # Find total protocol similarity for different metrics\n        # for measure, values in sim_measures.items():\n        #     # measure_s = pd.Series({pair: similarity for pair, similarity in values.items()})\n        #     # measure_s = pd.Series(values)\n        #     similarity_sum['protocol_%s_sum' % measure] = pd.Series(values).sum()\n        similarity_sum = {f'protocol_{measure}_sum': pd.Series(values).sum()\n                          for measure, values in sim_measures.items()}\n        similarity_sum_s = pd.concat([pd.Series(similarity_sum)], keys=[('sequence_design', 'pose')])\n\n        # Process similarity between protocols\n        sim_measures_s = pd.concat([pd.Series(values) for values in sim_measures.values()],\n                                   keys=list(sim_measures.keys()))\n        # # Todo test\n        # sim_stdev_s = pd.concat(list(sim_stdev.values()),\n        #                         keys=list(zip(repeat('std'), sim_stdev.keys()))).swaplevel(1, 2)\n        # sim_series = [protocol_sig_s, similarity_sum_s, sim_measures_s, sim_stdev_s]\n        sim_series = [protocol_sig_s, similarity_sum_s, sim_measures_s]\n\n        # if self.job.figures:  # Todo ensure output is as expected then move below\n        #     protocols_by_design = {design: protocol for protocol, designs in designs_by_protocol.items()\n        #                            for design in designs}\n        #     _path = os.path.join(self.job.all_scores, str(self))\n        #     # Set up Labels &amp; Plot the PC data\n        #     protocol_map = {protocol: i for i, protocol in enumerate(designs_by_protocol)}\n        #     integer_map = {i: protocol for (protocol, i) in protocol_map.items()}\n        #     pc_labels_group = [protocols_by_design[design] for design in residue_info]\n        #     # pc_labels_group = np.array([protocols_by_design[design] for design in residue_info])\n        #     pc_labels_int = [protocol_map[protocols_by_design[design]] for design in residue_info]\n        #     fig = plt.figure()\n        #     # ax = fig.add_subplot(111, projection='3d')\n        #     ax = Axes3D(fig, rect=[0, 0, .7, 1], elev=48, azim=134)\n        #     # plt.cla()\n        #\n        #     # for color_int, label in integer_map.items():  # zip(pc_labels_group, pc_labels_int):\n        #     #     ax.scatter(seq_pc[pc_labels_group == label, 0],\n        #     #                seq_pc[pc_labels_group == label, 1],\n        #     #                seq_pc[pc_labels_group == label, 2],\n        #     #                c=color_int, cmap=plt.cm.nipy_spectral, edgecolor='k')\n        #     scatter = ax.scatter(seq_pc[:, 0], seq_pc[:, 1], seq_pc[:, 2], c=pc_labels_int, cmap='Spectral',\n        #                          edgecolor='k')\n        #     # handles, labels = scatter.legend_elements()\n        #     # # print(labels)  # ['$\\\\mathdefault{0}$', '$\\\\mathdefault{1}$', '$\\\\mathdefault{2}$']\n        #     # ax.legend(handles, labels, loc='upper right', title=protocol)\n        #     # # ax.legend(handles, [integer_map[label] for label in labels], loc=\"upper right\", title=protocol)\n        #     # # plt.axis('equal') # not possible with 3D graphs\n        #     # plt.legend()  # No handles with labels found to put in legend.\n        #     colors = [scatter.cmap(scatter.norm(i)) for i in integer_map.keys()]\n        #     custom_lines = [plt.Line2D([], [], ls='', marker='.', mec='k', mfc=c, mew=.1, ms=20)\n        #                     for c in colors]\n        #     ax.legend(custom_lines, [j for j in integer_map.values()], loc='center left',\n        #               bbox_to_anchor=(1.0, .5))\n        #     # # Add group mean to the plot\n        #     # for name, label in integer_map.items():\n        #     #     ax.scatter(seq_pc[pc_labels_group == label, 0].mean(),\n        #     #                seq_pc[pc_labels_group == label, 1].mean(),\n        #     #                seq_pc[pc_labels_group == label, 2].mean(), marker='x')\n        #     ax.set_xlabel('PC1')\n        #     ax.set_ylabel('PC2')\n        #     ax.set_zlabel('PC3')\n        #     # plt.legend(pc_labels_group)\n        #     plt.savefig('%s_seq_pca.png' % _path)\n        #     plt.clf()\n        #     # Residue PCA Figure to assay multiple interface states\n        #     fig = plt.figure()\n        #     # ax = fig.add_subplot(111, projection='3d')\n        #     ax = Axes3D(fig, rect=[0, 0, .7, 1], elev=48, azim=134)\n        #     scatter = ax.scatter(residue_energy_pc[:, 0], residue_energy_pc[:, 1], residue_energy_pc[:, 2],\n        #                          c=pc_labels_int,\n        #                          cmap='Spectral', edgecolor='k')\n        #     colors = [scatter.cmap(scatter.norm(i)) for i in integer_map.keys()]\n        #     custom_lines = [plt.Line2D([], [], ls='', marker='.', mec='k', mfc=c, mew=.1, ms=20) for c in\n        #                     colors]\n        #     ax.legend(custom_lines, [j for j in integer_map.values()], loc='center left',\n        #               bbox_to_anchor=(1.0, .5))\n        #     ax.set_xlabel('PC1')\n        #     ax.set_ylabel('PC2')\n        #     ax.set_zlabel('PC3')\n        #     plt.savefig('%s_res_energy_pca.png' % _path)\n\n    # Format output and save Trajectory, Residue DataFrames, and PDB Sequences\n    # if self.job.save:\n    # residues_df[(putils.protocol, putils.protocol)] = protocol_s\n    # residues_df.sort_index(inplace=True, key=lambda x: x.str.isdigit())  # put wt entry first\n    with self.job.db.session(expire_on_commit=False) as session:\n        self.output_metrics(session, designs=designs_df)\n        output_residues = False\n        if output_residues:\n            self.output_metrics(session, residues=residues_df)\n        else:  # Only save the 'design_residue' columns\n            residues_df = residues_df.loc[:, idx_slice[:, sql.DesignResidues.design_residue.name]]\n            self.output_metrics(session, design_residues=residues_df)\n        # Commit the newly acquired metrics\n        session.commit()\n\n    # pickle_object(pose_sequences, self.designed_sequences_file, out_path='')  # Todo PoseJob(.path)\n    write_sequences(pose_sequences, file_name=self.designed_sequences_file)\n\n    # Create figures\n    if self.job.figures:  # For plotting collapse profile, errat data, contact order\n        interface_residue_indices = [residue.index for residue in self.pose.interface_residues]\n        # Plot: Format the collapse data with residues as index and each design as column\n        # collapse_graph_df = pd.DataFrame(per_residue_data['hydrophobic_collapse'])\n        collapse_graph_df = residues_df.loc[:, idx_slice[:, 'hydrophobic_collapse']].droplevel(-1, axis=1)\n        reference_collapse = [entity.hydrophobic_collapse() for entity in self.pose.entities]\n        reference_collapse_concatenated_s = \\\n            pd.Series(np.concatenate(reference_collapse), name=putils.reference_name)\n        collapse_graph_df[putils.reference_name] = reference_collapse_concatenated_s\n        # collapse_graph_df.columns += 1  # offset index to residue numbering\n        # collapse_graph_df.sort_index(axis=1, inplace=True)\n        # graph_collapse = sns.lineplot(data=collapse_graph_df)\n        # g = sns.FacetGrid(tip_sumstats, col=\"sex\", row=\"smoker\")\n        # graph_collapse = sns.relplot(data=collapse_graph_df, kind='line')  # x='Residue Number'\n\n        # Set the base figure aspect ratio for all sequence designs\n        figure_aspect_ratio = (pose_length / 25., 20)  # 20 is arbitrary size to fit all information in figure\n        color_cycler = cycler(color=large_color_array)\n        plt.rc('axes', prop_cycle=color_cycler)\n        fig = plt.figure(figsize=figure_aspect_ratio)\n        # legend_fill_value = int(15 * pose_length / 100)\n\n        # sharex=True allows the axis to be shared across plots\n        # collapse_ax, contact_ax, errat_ax = fig.subplots(3, 1, sharex=True)\n        # collapse_ax, errat_ax = fig.subplots(2, 1, sharex=True)\n        collapse_ax = fig.subplots(1, 1, sharex=True)\n        # Add the contact order to the same plot with a separate axis\n        contact_ax = collapse_ax.twinx()\n        contact_order_df = residues_df.loc[pose_name, idx_slice[:, 'contact_order']].droplevel(-1, axis=1)\n        # source_contact_order_s = pd.Series(source_contact_order, index=residue_indices, name='contact_order')\n        contact_ax.plot(contact_order_df, label='Contact Order',\n                        color='#fbc0cb', lw=1, linestyle='-')  # pink\n        # contact_ax.scatter(residue_indices, source_contact_order_s, color='#fbc0cb', marker='o')  # pink\n        # wt_contact_order_concatenated_min_s = source_contact_order_s.min()\n        # wt_contact_order_concatenated_max_s = source_contact_order_s.max()\n        # wt_contact_order_range = wt_contact_order_concatenated_max_s - wt_contact_order_concatenated_min_s\n        # scaled_contact_order = ((source_contact_order_s - wt_contact_order_concatenated_min_s)\n        #                         / wt_contact_order_range)  # / wt_contact_order_range)\n        # graph_contact_order = sns.relplot(data=errat_graph_df, kind='line')  # x='Residue Number'\n        # collapse_ax1.plot(scaled_contact_order)\n        # contact_ax.vlines(self.pose.chain_breaks, 0, 1, transform=contact_ax.get_xaxis_transform(),\n        #                   label='Entity Breaks', colors='#cccccc')  # , grey)\n        # contact_ax.vlines(interface_residue_indices, 0, 0.05, transform=contact_ax.get_xaxis_transform(),\n        #                   label='Design Residues', colors='#f89938', lw=2)  # , orange)\n        contact_ax.set_ylabel('Contact Order')\n        # contact_ax.set_xlim(0, pose_length)\n        contact_ax.set_ylim(0, None)\n        # contact_ax.figure.savefig(os.path.join(self.data_path, 'hydrophobic_collapse+contact.png'))\n        # collapse_ax1.figure.savefig(os.path.join(self.data_path, 'hydrophobic_collapse+contact.png'))\n\n        # Get the plot of each collapse profile into a matplotlib axes\n        # collapse_ax = collapse_graph_df.plot.line(legend=False, ax=collapse_ax, figsize=figure_aspect_ratio)\n        # collapse_ax = collapse_graph_df.plot.line(legend=False, ax=collapse_ax)\n        collapse_ax.plot(collapse_graph_df.T.values, label=collapse_graph_df.index)\n        # collapse_ax = collapse_graph_df.plot.line(ax=collapse_ax)\n        collapse_ax.xaxis.set_major_locator(MultipleLocator(20))\n        collapse_ax.xaxis.set_major_formatter('{x:.0f}')\n        # For the minor ticks, use no labels; default NullFormatter.\n        collapse_ax.xaxis.set_minor_locator(MultipleLocator(5))\n        collapse_ax.set_xlim(0, pose_length)\n        collapse_ax.set_ylim(0, 1)\n        # # CAN'T SET FacetGrid object for most matplotlib elements...\n        # ax = graph_collapse.axes\n        # ax = plt.gca()  # gca &lt;- get current axis\n        # labels = [fill(index, legend_fill_value) for index in collapse_graph_df.index]\n        # collapse_ax.legend(labels, loc='lower left', bbox_to_anchor=(0., 1))\n        # collapse_ax.legend(loc='lower left', bbox_to_anchor=(0., 1))\n        # Plot the chain break(s) and design residues\n        # linestyles={'solid', 'dashed', 'dashdot', 'dotted'}\n        collapse_ax.vlines(self.pose.chain_breaks, 0, 1, transform=collapse_ax.get_xaxis_transform(),\n                           label='Entity Breaks', colors='#cccccc')  # , grey)\n        collapse_ax.vlines(interface_residue_indices, 0, 0.05, transform=collapse_ax.get_xaxis_transform(),\n                           label='Design Residues', colors='#f89938', lw=2)  # , orange)\n        # Plot horizontal significance\n        collapse_significance_threshold = metrics.collapse_thresholds[hydrophobicity]\n        collapse_ax.hlines([collapse_significance_threshold], 0, 1, transform=collapse_ax.get_yaxis_transform(),\n                           label='Collapse Threshold', colors='#fc554f', linestyle='dotted')  # tomato\n        # collapse_ax.set_xlabel('Residue Number')\n        collapse_ax.set_ylabel('Hydrophobic Collapse Index')\n        # collapse_ax.set_prop_cycle(color_cycler)\n        # ax.autoscale(True)\n        # collapse_ax.figure.tight_layout()  # no standardization\n        # collapse_ax.figure.savefig(os.path.join(self.data_path, 'hydrophobic_collapse.png'))  # no standardization\n\n        # Plot: Collapse description of total profile against each design\n        entity_collapse_mean, entity_collapse_std = [], []\n        for entity in self.pose.entities:\n            if entity.msa:\n                collapse = entity.collapse_profile()\n                entity_collapse_mean.append(collapse.mean(axis=-2))\n                entity_collapse_std.append(collapse.std(axis=-2))\n            else:\n                break\n        else:  # Only execute if we successfully looped\n            profile_mean_collapse_concatenated_s = \\\n                pd.concat([entity_collapse_mean[idx] for idx in range(number_of_entities)], ignore_index=True)\n            profile_std_collapse_concatenated_s = \\\n                pd.concat([entity_collapse_std[idx] for idx in range(number_of_entities)], ignore_index=True)\n            profile_mean_collapse_concatenated_s.index += 1  # offset index to residue numbering\n            profile_std_collapse_concatenated_s.index += 1  # offset index to residue numbering\n            collapse_graph_describe_df = pd.DataFrame({\n                'std_min': profile_mean_collapse_concatenated_s - profile_std_collapse_concatenated_s,\n                'std_max': profile_mean_collapse_concatenated_s + profile_std_collapse_concatenated_s,\n            })\n            collapse_graph_describe_df.index += 1  # offset index to residue numbering\n            collapse_graph_describe_df['Residue Number'] = collapse_graph_describe_df.index\n            collapse_ax.vlines('Residue Number', 'std_min', 'std_max', data=collapse_graph_describe_df,\n                               color='#e6e6fa', linestyle='-', lw=1, alpha=0.8)  # lavender\n            # collapse_ax.figure.savefig(os.path.join(self.data_path, 'hydrophobic_collapse_versus_profile.png'))\n\n        # # Plot: Errat Accuracy\n        # # errat_graph_df = pd.DataFrame(per_residue_data['errat_deviation'])\n        # # errat_graph_df = residues_df.loc[:, idx_slice[:, 'errat_deviation']].droplevel(-1, axis=1)\n        # # errat_graph_df = errat_df\n        # # wt_errat_concatenated_s = pd.Series(np.concatenate(list(source_errat.values())), name='clean_asu')\n        # # errat_graph_df[pose_name] = pose_source_errat\n        # # errat_graph_df.columns += 1  # offset index to residue numbering\n        # errat_df.sort_index(axis=0, inplace=True)\n        # # errat_ax = errat_graph_df.plot.line(legend=False, ax=errat_ax, figsize=figure_aspect_ratio)\n        # # errat_ax = errat_graph_df.plot.line(legend=False, ax=errat_ax)\n        # # errat_ax = errat_graph_df.plot.line(ax=errat_ax)\n        # errat_ax.plot(errat_df.T.values, label=collapse_graph_df.index)\n        # errat_ax.xaxis.set_major_locator(MultipleLocator(20))\n        # errat_ax.xaxis.set_major_formatter('{x:.0f}')\n        # # For the minor ticks, use no labels; default NullFormatter.\n        # errat_ax.xaxis.set_minor_locator(MultipleLocator(5))\n        # # errat_ax.set_xlim(0, pose_length)\n        # errat_ax.set_ylim(0, None)\n        # # graph_errat = sns.relplot(data=errat_graph_df, kind='line')  # x='Residue Number'\n        # # Plot the chain break(s) and design residues\n        # # labels = [fill(column, legend_fill_value) for column in errat_graph_df.columns]\n        # # errat_ax.legend(labels, loc='lower left', bbox_to_anchor=(0., 1.))\n        # # errat_ax.legend(loc='lower center', bbox_to_anchor=(0., 1.))\n        # errat_ax.vlines(self.pose.chain_breaks, 0, 1, transform=errat_ax.get_xaxis_transform(),\n        #                 label='Entity Breaks', colors='#cccccc')  # , grey)\n        # errat_ax.vlines(interface_residue_indices, 0, 0.05, transform=errat_ax.get_xaxis_transform(),\n        #                 label='Design Residues', colors='#f89938', lw=2)  # , orange)\n        # # Plot horizontal significance\n        # errat_ax.hlines([metrics.errat_2_sigma], 0, 1, transform=errat_ax.get_yaxis_transform(),\n        #                 label='Significant Error', colors='#fc554f', linestyle='dotted')  # tomato\n        # errat_ax.set_xlabel('Residue Number')\n        # errat_ax.set_ylabel('Errat Score')\n        # # errat_ax.autoscale(True)\n        # # errat_ax.figure.tight_layout()\n        # # errat_ax.figure.savefig(os.path.join(self.data_path, 'errat.png'))\n        collapse_handles, collapse_labels = collapse_ax.get_legend_handles_labels()\n        contact_handles, contact_labels = contact_ax.get_legend_handles_labels()\n        # errat_handles, errat_labels = errat_ax.get_legend_handles_labels()\n        # print(handles, labels)\n        handles = collapse_handles + contact_handles\n        labels = collapse_labels + contact_labels\n        # handles = errat_handles + contact_handles\n        # labels = errat_labels + contact_labels\n        labels = [label.replace(f'{self.name}_', '') for label in labels]\n        # plt.legend(loc='upper right', bbox_to_anchor=(1, 1))  #, ncol=3, mode='expand')\n        # print(labels)\n        # plt.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -1.), ncol=3)  # , mode='expand'\n        # v Why the hell doesn't this work??\n        # fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.), ncol=3,  # , mode='expand')\n        # fig.subplots_adjust(bottom=0.1)\n        plt.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -1.), ncol=3)  # , mode='expand')\n        #            bbox_transform=plt.gcf().transFigure)  # , bbox_transform=collapse_ax.transAxes)\n        fig.tight_layout()\n        fig.savefig(os.path.join(self.data_path, 'DesignMetricsPerResidues.png'))  # Todo PoseJob(.path)\n\n    # After parsing data sources\n    # other_pose_metrics['observations'] = len(viable_designs)\n    # interface_metrics_s = pd.concat([pd.Series(other_pose_metrics)], keys=[('dock', 'pose')])\n\n    # CONSTRUCT: Create pose series and format index names\n    # pose_s = pd.concat([interface_metrics_s, stat_s, divergence_s] + sim_series).swaplevel(0, 1)\n    pose_s = pd.concat([stat_s, divergence_s] + sim_series).swaplevel(0, 1)\n    # Remove pose specific metrics from pose_s, sort, and name protocol_mean_df\n    pose_s.drop([putils.protocol], level=2, inplace=True, errors='ignore')\n    pose_s.sort_index(level=2, inplace=True, sort_remaining=False)\n    pose_s.sort_index(level=1, inplace=True, sort_remaining=False)\n    pose_s.sort_index(level=0, inplace=True, sort_remaining=False)\n    pose_s.name = str(self)\n\n    return pose_s\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.load_evolutionary_profile","title":"load_evolutionary_profile","text":"<pre><code>load_evolutionary_profile(api_db: APIDatabase, model: ContainsEntities, warn_metrics: bool = False) -&gt; tuple[bool, bool]\n</code></pre> <p>Add evolutionary profile information to the provided Entity</p> <p>Parameters:</p> <ul> <li> <code>api_db</code>             (<code>APIDatabase</code>)         \u2013          <p>The database which store all information pertaining to evolutionary information</p> </li> <li> <code>model</code>             (<code>ContainsEntities</code>)         \u2013          <p>The ContainsEntities instance to check for Entity instances to load evolutionary information</p> </li> <li> <code>warn_metrics</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to warn the user about missing files for metric collection</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>A tuple of boolean values of length two indicating if, 1-evolutionary and 2-alignment information was added to</p> </li> <li> <code>bool</code>         \u2013          <p>the Entity instances</p> </li> </ul> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def load_evolutionary_profile(api_db: resources.wrapapi.APIDatabase, model: ContainsEntities,\n                              warn_metrics: bool = False) -&gt; tuple[bool, bool]:\n    \"\"\"Add evolutionary profile information to the provided Entity\n\n    Args:\n        api_db: The database which store all information pertaining to evolutionary information\n        model: The ContainsEntities instance to check for Entity instances to load evolutionary information\n        warn_metrics: Whether to warn the user about missing files for metric collection\n\n    Returns:\n        A tuple of boolean values of length two indicating if, 1-evolutionary and 2-alignment information was added to\n        the Entity instances\n    \"\"\"\n    # Assume True given this function call and set False if not possible for one of the Entities\n    measure_evolution = measure_alignment = True\n    warn = False\n    for entity in model.entities:\n        if entity.evolutionary_profile:\n            continue\n\n        # if len(entity.uniprot_ids) &gt; 1:\n        #     raise SymDesignException(\n        #         f\"Can't set the profile for an {entity.__class__.__name__} with number of UniProtIDs \"\n        #         f\"({len(entity.uniprot_ids)}) &gt; 1. Please remove this or update the code\")\n        # for idx, uniprot_id in enumerate(entity.uniprot_ids):\n        evolutionary_profile = {}\n        for uniprot_id in entity.uniprot_ids:\n            profile = api_db.hhblits_profiles.retrieve_data(name=uniprot_id)\n            if not profile:\n                null_entries = entity.create_null_entries(range(entity.number_of_residues))\n                for entry, residue in zip(null_entries.values(), entity.residues):\n                    entry['type'] = residue.type1\n\n                evolutionary_profile.update(null_entries)\n                # # Try and add... This would be better at the program level due to memory issues\n                # entity.add_evolutionary_profile(out_dir=self.job.api_db.hhblits_profiles.location)\n            else:\n                if evolutionary_profile:\n                    # Renumber the profile based on the current length\n                    profile = {entry_number: entry\n                               for entry_number, entry in enumerate(profile.values(), len(evolutionary_profile))}\n                evolutionary_profile.update(profile)\n\n        if not evolutionary_profile:\n            measure_evolution = False\n            warn = True\n        else:\n            logger.debug(f'Adding {entity.name}.evolutionary_profile')\n            entity.evolutionary_profile = evolutionary_profile\n\n        if entity.msa:\n            continue\n\n        # Fetch the multiple sequence alignment for further processing\n        msas = []\n        for uniprot_id in entity.uniprot_ids:\n            msa = api_db.alignments.retrieve_data(name=uniprot_id)\n            if not msa:\n                measure_alignment = False\n                warn = True\n            else:\n                logger.debug(f'Adding {entity.name}.msa')\n                msas.append(msa)\n                # Todo\n                #  The alignment of concatenated evolutionary profiles is sensitive to the length of the internal\n                #  gaps when there is an extend penalty\n                #  modify_alignment_algorithm = True\n                #  query_internal_extend_gap_score=0\n\n        if msas:\n            # Combine all\n            max_alignment_size = max([msa_.length for msa_ in msas])\n            msa, *other_msas = msas\n            combined_alignment = msa.alignment\n            # modify_alignment_algorithm = False\n            msa_: MultipleSequenceAlignment\n            for msa_ in other_msas:\n                length_difference = max_alignment_size - msa_.length\n                if length_difference:  # Not 0\n                    msa_.pad_alignment(length_difference)\n                combined_alignment += msa_.alignment\n            # To create the full MultipleSequenceAlignment\n            entity.msa = MultipleSequenceAlignment(combined_alignment)\n\n    if warn_metrics and warn:\n        if not measure_evolution and not measure_alignment:\n            logger.info(\"Metrics relying on evolution aren't being collected as the required files weren't \"\n                        f'found. These include: {\", \".join(metrics.all_evolutionary_metrics)}')\n        elif not measure_alignment:\n            logger.info('Metrics relying on a multiple sequence alignment including: '\n                        f'{\", \".join(metrics.multiple_sequence_alignment_dependent_metrics)}'\n                        \"are being calculated with the reference sequence as there was no MSA found\")\n        else:\n            logger.info(\"Metrics relying on an evolutionary profile aren't being collected as there was no profile \"\n                        f'found. These include: {\", \".join(metrics.profile_dependent_metrics)}')\n\n    # if measure_evolution:\n    model.evolutionary_profile = \\\n        concatenate_profile([entity.evolutionary_profile for entity in model.entities])\n    # else:\n    #     self.pose.evolutionary_profile = self.pose.create_null_profile()\n    return measure_evolution, measure_alignment\n</code></pre>"},{"location":"reference/protocols/pose/#protocols.pose.insert_pose_jobs","title":"insert_pose_jobs","text":"<pre><code>insert_pose_jobs(session: Session, pose_jobs: Iterable[PoseJob], project: str) -&gt; list[PoseJob]\n</code></pre> <p>Add PoseJobs to the database accounting for existing entries</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>An open sqlalchemy session</p> </li> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances which should be inserted</p> </li> <li> <code>project</code>             (<code>str</code>)         \u2013          <p>The name of the project which the <code>pose_jobs</code> belong</p> </li> </ul> <p>Raises:     sqlalchemy.exc.IntegrityError Returns:     The PoseJob instances that are already present</p> Source code in <code>symdesign/protocols/pose.py</code> <pre><code>def insert_pose_jobs(session: Session, pose_jobs: Iterable[PoseJob], project: str) -&gt; list[PoseJob]:\n    \"\"\"Add PoseJobs to the database accounting for existing entries\n\n    Args:\n        session: An open sqlalchemy session\n        pose_jobs: The PoseJob instances which should be inserted\n        project: The name of the project which the `pose_jobs` belong\n    Raises:\n        sqlalchemy.exc.IntegrityError\n    Returns:\n        The PoseJob instances that are already present\n    \"\"\"\n    error_count = count(1)\n    # logger.debug(f\"Start: {getattr(pose_jobs[0], 'id', 'No id')}\")\n    while True:\n        pose_name_to_pose_jobs = {pose_job.name: pose_job for pose_job in pose_jobs}\n        session.add_all(pose_jobs)\n        # logger.debug(f\"ADD ALL: {getattr(pose_jobs[0], 'id', 'No id')}\")\n        try:  # Flush PoseJobs to the current session to generate ids\n            session.flush()\n        except IntegrityError:  # PoseJob.project/.name already inserted\n            # logger.debug(f\"FLUSH: {getattr(pose_jobs[0], 'id', 'No id')}\")\n            session.rollback()\n            # logger.debug(f\"ROLLBACK: {getattr(pose_jobs[0], 'id', 'No id')}\")\n            number_flush_attempts = next(error_count)\n            logger.debug(f'rollback() #{number_flush_attempts}')\n\n            # Find the actual pose_jobs_to_commit and place in session\n            pose_names = list(pose_name_to_pose_jobs.keys())\n            fetch_jobs_stmt = select(PoseJob).where(PoseJob.project.is_(project)) \\\n                .where(PoseJob.name.in_(pose_names))\n            existing_pose_jobs = session.scalars(fetch_jobs_stmt).all()\n            # Note: Values are sorted by alphanumerical, not numerical\n            # ex, design 11 is processed before design 2\n            existing_pose_names = {pose_job_.name for pose_job_ in existing_pose_jobs}\n            new_pose_names = set(pose_names).difference(existing_pose_names)\n            if not new_pose_names:  # No new PoseJobs\n                return existing_pose_jobs\n            else:\n                pose_jobs = [pose_name_to_pose_jobs[pose_name] for pose_name in new_pose_names]\n                # Set each of the primary id to None so they are updated again during the .flush()\n                for pose_job in pose_jobs:\n                    pose_job.id = None\n                # logger.debug(f\"RESET: {getattr(pose_jobs[0], 'id', 'No id')}\")\n\n                if number_flush_attempts == 1:\n                    logger.debug(\n                        f'From {len(pose_names)} pose_jobs:\\n{sorted(pose_names)}\\n'\n                        f'Found {len(new_pose_names)} new_pose_jobs:\\n{sorted(new_pose_names)}\\n'\n                        f'Removing existing docked poses from output: {\", \".join(existing_pose_names)}')\n                elif number_flush_attempts == 2:\n                    # Try to attach existing protein_metadata.entity_id\n                    # possibly_new_uniprot_to_prot_metadata = {}\n                    possibly_new_uniprot_to_prot_metadata = defaultdict(list)\n                    # pose_name_to_prot_metadata = defaultdict(list)\n                    for pose_job in pose_jobs:\n                        for entity_data in pose_job.entity_data:\n                            possibly_new_uniprot_to_prot_metadata[\n                                entity_data.meta.uniprot_ids].append(entity_data.meta)\n\n                    all_uniprot_id_to_prot_data = sql.initialize_metadata(\n                        session, possibly_new_uniprot_to_prot_metadata)\n\n                    # logger.debug([[data.meta.entity_id for data in pose_job.entity_data] for pose_job in pose_jobs])\n                    # Get all uniprot_entities, and fix ProteinMetadata that is already loaded\n                    for pose_name, pose_job in pose_name_to_pose_jobs.items():\n                        for entity_data in pose_job.entity_data:\n                            entity_id = entity_data.meta.entity_id\n                            # Search the updated ProteinMetadata\n                            for protein_metadata in all_uniprot_id_to_prot_data.values():\n                                for data in protein_metadata:\n                                    if entity_id == data.entity_id:\n                                        # Set with the valid ProteinMetadata\n                                        entity_data.meta = data\n                                        break\n                                else:  # No break occurred, continue with outer loop\n                                    continue\n                                break  # outer loop too\n                            else:\n                                insp = inspect(entity_data)\n                                logger.warning(\n                                    f'Missing the {sql.ProteinMetadata.__name__} instance for {entity_data} with '\n                                    f'entity_id {entity_id}')\n                                logger.debug(f'\\tThis instance is transient? {insp.transient}, pending?'\n                                             f' {insp.pending}, persistent? {insp.persistent}')\n                    logger.debug(f'Found the newly added Session instances:\\n{session.new}')\n                elif number_flush_attempts == 3:\n                    attrs_of_interest = \\\n                        ['id', 'entity_id', 'reference_sequence', 'thermophilicity', 'symmetry_group', 'model_source']\n                    properties = []\n                    for pose_job in pose_jobs:\n                        for entity_data in pose_job.entity_data:\n                            properties.append('\\t'.join([f'{attr}={getattr(entity_data.meta, attr)}'\n                                                         for attr in attrs_of_interest]))\n                    pose_job_properties = '\\n\\t'.join(properties)\n                    logger.warning(f\"The remaining PoseJob instances have the following \"\n                                   f\"{sql.ProteinMetadata.__name__} properties:\\n\\t{pose_job_properties}\")\n                    # This is another error\n                    raise\n        else:\n            break\n\n    return pose_jobs\n</code></pre>"},{"location":"reference/protocols/select/","title":"select","text":""},{"location":"reference/protocols/select/#protocols.select.load_total_dataframe","title":"load_total_dataframe","text":"<pre><code>load_total_dataframe(pose_jobs: Iterable[PoseJob], pose: bool = False) -&gt; DataFrame\n</code></pre> <p>Return a pandas DataFrame with the trajectories of every PoseJob loaded and formatted according to the design directory and design on the index</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which metrics are desired</p> </li> <li> <code>pose</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the total dataframe should contain the mean metrics from the pose or each individual design</p> </li> </ul> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_total_dataframe(pose_jobs: Iterable[PoseJob], pose: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Return a pandas DataFrame with the trajectories of every PoseJob loaded and formatted according to the\n    design directory and design on the index\n\n    Args:\n        pose_jobs: The PoseJob instances for which metrics are desired\n        pose: Whether the total dataframe should contain the mean metrics from the pose or each individual design\n    \"\"\"\n    all_dfs = []  # None for design in pose_jobs]\n    for idx, pose_job in enumerate(pose_jobs):\n        try:\n            all_dfs.append(pd.read_csv(pose_job.designs_metrics_csv, index_col=0, header=[0]))\n        except FileNotFoundError:  # as error\n            # results[idx] = error\n            logger.warning(f'{pose_job}: No trajectory analysis file found. Skipping')\n\n    if pose:\n        for pose_job, df in zip(pose_jobs, all_dfs):\n            df.fillna(0., inplace=True)  # Shouldn't be necessary if saved files were formatted correctly\n            # try:\n            df.drop([index for index in df.index.tolist() if isinstance(index, float)], inplace=True)\n            # Get rid of all individual trajectories and std, not mean\n            pose_name = pose_job.name\n            df.drop([index for index in df.index.tolist() if pose_name in index or 'std' in index], inplace=True)\n            # except TypeError:\n            #     for index in df.index.tolist():\n            #         print(index, type(index))\n    else:  # designs\n        for pose_job, df in zip(pose_jobs, all_dfs):\n            # Get rid of all statistic entries, mean, std, etc.\n            pose_name = pose_job.name\n            df.drop([index for index in df.index.tolist() if pose_name not in index], inplace=True)\n\n    # Add pose directory str as MultiIndex\n    try:\n        df = pd.concat(all_dfs, keys=[str(pose_job) for pose_job in pose_jobs])\n    except ValueError:  # No objects to concatenate\n        raise RuntimeError(f\"Didn't find any trajectory information in the provided PoseDirectory instances\")\n    df.replace({False: 0, True: 1, 'False': 0, 'True': 1}, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_and_format","title":"load_and_format","text":"<pre><code>load_and_format(session: Session, stmt: Select, selected_column_names: Iterable[str]) -&gt; DataFrame\n</code></pre> <p>From a SELECTable query, fetch the requested columns/attributes from the database, load into a DataFrame, and clean</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>stmt</code>             (<code>Select</code>)         \u2013          <p>The SELECTable query statement</p> </li> <li> <code>selected_column_names</code>             (<code>Iterable[str]</code>)         \u2013          <p>The column names to use during DataFrame construction</p> </li> </ul> <p>Returns:     The specified columns/attributes formatted as DataFrame.columns and their rows as the DataFrame.index</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_and_format(session: Session, stmt: Select, selected_column_names: Iterable[str]) -&gt; pd.DataFrame:\n    \"\"\"From a SELECTable query, fetch the requested columns/attributes from the database, load into a DataFrame,\n    and clean\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        stmt: The SELECTable query statement\n        selected_column_names: The column names to use during DataFrame construction\n    Returns:\n        The specified columns/attributes formatted as DataFrame.columns and their rows as the DataFrame.index\n    \"\"\"\n    # Apply join condition(s) between each element to resolve.\n    df = pd.DataFrame.from_records(session.execute(stmt).all(), columns=selected_column_names)\n    logger.debug(f'Loaded DataFrame with primary_id keys: '\n                 f'{[key for key in selected_column_names if \"id\" in key and \"residue\" not in key]}')\n\n    # Format the dataframe and set the index\n    # df = df.sort_index(axis=1).set_index('design_id')\n    # Remove completely empty columns such as obs_interface\n    df.dropna(how='all', inplace=True, axis=1)\n    df.replace({False: 0, True: 1, 'False': 0, 'True': 1}, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_all_metrics_dataframe","title":"load_sql_all_metrics_dataframe","text":"<pre><code>load_sql_all_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance's, PoseMetrics, EntityMetrics, DesignMetrics, and DesignEntityMetrics for each associated design</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A DataFrame formatted with every metric in PoseMetrics, EntityMetrics, and DesignMetrics. The final DataFrame         will have an as many entries corresponding to each Entity in EntityData for a total of DesignData's X number         of Entities entries</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_all_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None,\n                                   design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format every PoseJob instance's, PoseMetrics, EntityMetrics, DesignMetrics, and DesignEntityMetrics for\n    each associated design\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        A DataFrame formatted with every metric in PoseMetrics, EntityMetrics, and DesignMetrics. The final DataFrame\n            will have an as many entries corresponding to each Entity in EntityData for a total of DesignData's X number\n            of Entities entries\n    \"\"\"\n    pm_c = [c for c in sql.PoseMetrics.__table__.columns if not c.primary_key]\n    pm_names = [c.name for c in pm_c]\n    dm_c = [c for c in sql.DesignMetrics.__table__.columns if not c.primary_key]\n    dm_names = [c.name for c in dm_c]\n    # entity_metadata_c = [sql.ProteinMetadata.n_terminal_helix,\n    #                      sql.ProteinMetadata.c_terminal_helix,\n    #                      sql.ProteinMetadata.thermophilicity]\n    em_c = [c for c in (*sql.EntityMetrics.__table__.columns,\n                        # *entity_metadata_c,\n                        *sql.DesignEntityMetrics.__table__.columns)\n            if not c.primary_key]\n    # Remove design_id (its duplicated?)\n    em_c.pop(em_c.index(sql.DesignEntityMetrics.design_id))\n    # Remove entity_id, it's duplicated\n    em_c.pop(em_c.index(sql.DesignEntityMetrics.entity_id))\n    em_names = [f'entity_{c.name}' if c.name != 'entity_id' else c.name for c in em_c]\n    selected_columns = (*pm_c, *dm_c, *em_c)\n    selected_column_names = (*pm_names, *dm_names, *em_names)\n    # # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    # join_stmt = select(selected_columns).select_from(sql.PoseMetrics)\\\n    #     .join(sql.EntityData, sql.EntityData.pose_id == sql.PoseMetrics.pose_id)\\\n    #     .join(sql.EntityMetrics, sql.EntityMetrics.entity_id == sql.EntityData.id)\\\n    #     .join(sql.DesignData, sql.DesignData.pose_id == sql.PoseMetrics.pose_id, ).join(sql.DesignMetrics)\\\n    #     .join(sql.DesignEntityMetrics, sql.DesignEntityMetrics.design_id == sql.DesignData.id)\n\n    join_stmt = select(selected_columns).select_from(sql.EntityData) \\\n        .join(sql.PoseMetrics, sql.PoseMetrics.pose_id == sql.EntityData.pose_id) \\\n        .join(sql.EntityMetrics, sql.EntityMetrics.entity_id == sql.EntityData.id) \\\n        .join(sql.DesignEntityMetrics, sql.DesignEntityMetrics.entity_id == sql.EntityData.id) \\\n        .join(sql.DesignMetrics, sql.DesignMetrics.design_id == sql.DesignEntityMetrics.design_id)\n\n    if pose_ids:\n        stmt = join_stmt.where(sql.PoseMetrics.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    if design_ids:\n        stmt = stmt.where(sql.DesignMetrics.design_id.in_(design_ids))\n    else:\n        stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_poses_dataframe","title":"load_sql_poses_dataframe","text":"<pre><code>load_sql_poses_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance's, PoseMetrics and EntityMetrics</p> <p>Optionally limit those loaded to certain PoseJob.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A DataFrame formatted with every metric in PoseMetrics and EntityMetrics. The final DataFrame will have an entry         corresponding to each Entity in EntityData for a total of PoseJob's X number of Entities entries</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_poses_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    # , design_ids: Iterable[int] = None\n    \"\"\"Load and format every PoseJob instance's, PoseMetrics and EntityMetrics\n\n    Optionally limit those loaded to certain PoseJob.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n    Returns:\n        A DataFrame formatted with every metric in PoseMetrics and EntityMetrics. The final DataFrame will have an entry\n            corresponding to each Entity in EntityData for a total of PoseJob's X number of Entities entries\n    \"\"\"\n    #     design_ids: DesignData instance identifiers for which metrics are desired\n    # Accessing only the PoseMetrics and EntityMetrics\n    pm_c = [c for c in sql.PoseMetrics.__table__.columns if not c.primary_key]\n    pm_names = [c.name for c in pm_c]\n    # entity_metadata_c = [sql.ProteinMetadata.n_terminal_helix,\n    #                      sql.ProteinMetadata.c_terminal_helix,\n    #                      sql.ProteinMetadata.thermophilicity]\n    # em_c = [c for c in (*sql.EntityMetrics.__table__.columns, *entity_metadata_c) if not c.primary_key]\n    em_c = [c for c in sql.EntityMetrics.__table__.columns if not c.primary_key]\n    em_names = [f'entity_{c.name}' if c.name != 'entity_id' else c.name for c in em_c]\n    # em_c = [c for c in sql.EntityMetrics.__table__.columns if not c.primary_key]\n    # em_names = [f'entity_{c.name}' if c.name != 'entity_id' else c.name for c in em_c]\n    selected_columns = (*pm_c, *em_c)\n    selected_column_names = (*pm_names, *em_names)\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.EntityData) \\\n        .join(sql.PoseMetrics, sql.PoseMetrics.pose_id == sql.EntityData.pose_id) \\\n        .join(sql.EntityMetrics, sql.EntityMetrics.entity_id == sql.EntityData.id)\n\n    if pose_ids:\n        stmt = join_stmt.where(sql.PoseMetrics.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_pose_metrics_dataframe","title":"load_sql_pose_metrics_dataframe","text":"<pre><code>load_sql_pose_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance's, PoseMetrics</p> <p>Optionally limit those loaded to certain PoseJob.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A DataFrame formatted with every metric in PoseMetrics. The final DataFrame will have an entry corresponding to         each PoseJob</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_pose_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format every PoseJob instance's, PoseMetrics\n\n    Optionally limit those loaded to certain PoseJob.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n    Returns:\n        A DataFrame formatted with every metric in PoseMetrics. The final DataFrame will have an entry corresponding to\n            each PoseJob\n    \"\"\"\n    # Accessing only the PoseMetrics\n    pm_c = [c for c in sql.PoseMetrics.__table__.columns if not c.primary_key]\n    # pm_names = [c.name for c in pm_c]\n    selected_columns = (*pm_c,)\n    selected_column_names = [c.name for c in selected_columns]  # (*pm_names,)\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.PoseMetrics)\n    if pose_ids:\n        stmt = join_stmt.where(sql.PoseMetrics.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_entity_metrics_dataframe","title":"load_sql_entity_metrics_dataframe","text":"<pre><code>load_sql_entity_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance's, EntityMetrics/DesignEntityMetrics</p> <p>Optionally limit those loaded to certain PoseJob.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A DataFrame formatted with the pose_id, EntityMetrics, and DesignEntityMetrics. The final DataFrame will have an         entry corresponding to each Entity in EntityData for a total of PoseJob's X number of entities entries</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_entity_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None,\n                                      design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format every PoseJob instance's, EntityMetrics/DesignEntityMetrics\n\n    Optionally limit those loaded to certain PoseJob.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        A DataFrame formatted with the pose_id, EntityMetrics, and DesignEntityMetrics. The final DataFrame will have an\n            entry corresponding to each Entity in EntityData for a total of PoseJob's X number of entities entries\n    \"\"\"\n    # Accessing only the PoseJob.id and EntityMetrics\n    pose_id_c = sql.EntityData.pose_id\n    # entity_metadata_c = [sql.ProteinMetadata.n_terminal_helix,\n    #                      sql.ProteinMetadata.c_terminal_helix,\n    #                      sql.ProteinMetadata.thermophilicity]\n    # em_c = [c for c in (*sql.EntityMetrics.__table__.columns, *entity_metadata_c) if not c.primary_key]\n    em_c = [c for c in (*sql.EntityMetrics.__table__.columns,\n                        # *entity_metadata_c,\n                        *sql.DesignEntityMetrics.__table__.columns)\n            if not c.primary_key]\n    # # Remove design_id\n    # em_c.pop(em_c.index(sql.DesignEntityMetrics.design_id))\n    # Remove entity_id as entity_id is duplicated\n    em_c.pop(em_c.index(sql.DesignEntityMetrics.entity_id))\n    em_names = [f'entity_{c.name}' if c.name not in ['entity_id', 'design_id'] else c.name for c in em_c]\n    selected_columns = (pose_id_c, *em_c,)\n    selected_column_names = (pose_id_c.name, *em_names,)\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.EntityData)\\\n        .join(sql.EntityMetrics, sql.EntityMetrics.entity_id == sql.EntityData.id) \\\n        .join(sql.DesignEntityMetrics, sql.DesignEntityMetrics.entity_id == sql.EntityData.id)\n\n    if pose_ids:\n        stmt = join_stmt.where(sql.EntityData.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    if design_ids:\n        stmt = stmt.where(sql.DesignEntityMetrics.design_id.in_(design_ids))\n    else:\n        stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_design_metrics_dataframe","title":"load_sql_design_metrics_dataframe","text":"<pre><code>load_sql_design_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format DesignMetrics/DesignEntityMetrics for each design associated with the PoseJob</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A pandas DataFrame formatted with every metric in DesignMetrics/DesignEntityMetrics. The final DataFrame will         have an entry for each DesignEntity for each DesignData</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_design_metrics_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) \\\n        -&gt; pd.DataFrame:\n    \"\"\"Load and format DesignMetrics/DesignEntityMetrics for each design associated with the PoseJob\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        A pandas DataFrame formatted with every metric in DesignMetrics/DesignEntityMetrics. The final DataFrame will\n            have an entry for each DesignEntity for each DesignData\n    \"\"\"\n    # dd_c = [sql.DesignData.pose_id, sql.DesignData.design_id]\n    dd_c = (sql.DesignData.pose_id,)\n    dd_names = [c.name for c in dd_c]\n    dm_c = [c for c in sql.DesignMetrics.__table__.columns if not c.primary_key]\n    dm_names = [c.name for c in dm_c]\n    # em_c = [c for c in sql.DesignEntityMetrics.__table__.columns if not c.primary_key]\n    # # Remove design_id\n    # em_c.pop(em_c.index(sql.DesignEntityMetrics.design_id))\n    # em_names = [f'entity_{c.name}' if c.name != 'entity_id' else c.name for c in em_c]\n    selected_columns = (*dd_c, *dm_c)  # , *em_c)\n    selected_column_names = (*dd_names, *dm_names)  # , *em_names)\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.DesignData)\\\n        .join(sql.DesignMetrics)\n    if pose_ids:\n        stmt = join_stmt.where(sql.DesignData.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    if design_ids:\n        stmt = stmt.where(sql.DesignData.id.in_(design_ids))\n    else:\n        stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_design_entities_dataframe","title":"load_sql_design_entities_dataframe","text":"<pre><code>load_sql_design_entities_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format DesignEntityMetrics for each design associated with the PoseJob</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A pandas DataFrame formatted with every metric in DesignMetrics/DesignEntityMetrics. The final DataFrame will         have an entry for each DesignEntity for each DesignData</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_design_entities_dataframe(session: Session, pose_ids: Iterable[int] = None,\n                                       design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format DesignEntityMetrics for each design associated with the PoseJob\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        A pandas DataFrame formatted with every metric in DesignMetrics/DesignEntityMetrics. The final DataFrame will\n            have an entry for each DesignEntity for each DesignData\n    \"\"\"\n    # dd_c = [sql.DesignData.pose_id, sql.DesignData.design_id]\n    dd_c = (sql.DesignData.pose_id,)\n    dd_names = [c.name for c in dd_c]\n    # dm_c = [c for c in sql.DesignMetrics.__table__.columns if not c.primary_key]\n    # dm_names = [c.name for c in dm_c]\n    em_c = [c for c in sql.DesignEntityMetrics.__table__.columns if not c.primary_key]\n    # Remove design_id\n    em_c.pop(em_c.index(sql.DesignEntityMetrics.design_id))\n    em_names = [f'entity_{c.name}' if c.name != 'entity_id' else c.name for c in em_c]\n    selected_columns = (*dd_c, *em_c)  # *dm_c,\n    selected_column_names = (*dd_names, *em_names)  # *dm_names,\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.DesignData)\\\n        .join(sql.DesignEntityMetrics)  # .join(PoseJob)\n    if pose_ids:\n        stmt = join_stmt.where(sql.DesignData.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    if design_ids:\n        stmt = stmt.where(sql.DesignData.id.in_(design_ids))\n    else:\n        stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_pose_metadata_dataframe","title":"load_sql_pose_metadata_dataframe","text":"<pre><code>load_sql_pose_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance associated metadata including protocol information</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>Not used, but here for API. DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will         have an entry for each DesignData</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_pose_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None,\n                                     design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format every PoseJob instance associated metadata including protocol information\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: Not used, but here for API. DesignData instance identifiers for which metrics are desired\n    Returns:\n        The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will\n            have an entry for each DesignData\n    \"\"\"\n    selected_columns = PoseJob.__table__.columns\n    selected_column_names = ['pose_id' if c.name == 'id' else c.name for c in selected_columns]\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(PoseJob)\n    if pose_ids:\n        stmt = join_stmt.where(PoseJob.id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    # if design_ids:\n    #     stmt = stmt.where(sql.DesignData.id.in_(design_ids))\n    # else:\n    #     stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_design_metadata_dataframe","title":"load_sql_design_metadata_dataframe","text":"<pre><code>load_sql_design_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None, design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format requested identifiers DesignData/DesignProtocol</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will         have an entry for each DesignData</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_design_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None,\n                                       design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Load and format requested identifiers DesignData/DesignProtocol\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will\n            have an entry for each DesignData\n    \"\"\"\n    dd_c = [c for c in sql.DesignData.__table__.columns if not c.primary_key]\n    dd_c.pop(dd_c.index(sql.DesignData.name))\n    # dd_names = [c.name for c in dd_c]\n    # name REMOVE\n    # pose_id NEED\n    # design_parent_id\n    # structure_path\n    # sequence\n    dp_c = [c for c in sql.DesignProtocol.__table__.columns if not c.primary_key]\n    # dp_names = [c.name for c in dp_c]\n    # protocol\n    # job_id JOIN\n    # design_id NEED\n    # file\n    # temperature\n    # alphafold_model\n\n    job_c = [c for c in sql.JobProtocol.__table__.columns if not c.primary_key]\n    # job_names = [c.name for c in job_c]\n    selected_columns = (*dp_c, *dd_c, *job_c)\n    # selected_column_names = (*dp_names, *dd_names, *job_names)\n    selected_column_names = [c.name for c in selected_columns]\n\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.DesignData) \\\n        .join(sql.DesignProtocol).join(sql.JobProtocol)\n    if pose_ids:\n        stmt = join_stmt.where(sql.DesignData.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    if design_ids:\n        stmt = stmt.where(sql.DesignData.id.in_(design_ids))\n    else:\n        stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_sql_entity_metadata_dataframe","title":"load_sql_entity_metadata_dataframe","text":"<pre><code>load_sql_entity_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Load and format every PoseJob instance associated metadata including protocol information</p> <p>Optionally limit those loaded to certain PoseJob.id's and DesignData.id's</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>#</code>             (<code>design_ids</code>)         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will         have an entry for each DesignData</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_sql_entity_metadata_dataframe(session: Session, pose_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n                                       # design_ids: Iterable[int] = None\n    \"\"\"Load and format every PoseJob instance associated metadata including protocol information\n\n    Optionally limit those loaded to certain PoseJob.id's and DesignData.id's\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        # design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        The pandas DataFrame formatted with the every metric in DesignMetrics. The final DataFrame will\n            have an entry for each DesignData\n    \"\"\"\n    # pj_c = [PoseJob.id]\n    # pj_names = [c.name for c in pj_c]\n    pose_id_c = [sql.EntityData.pose_id, sql.EntityData.id]\n    pose_id_name = ['entity_id' if c.name == 'id' else c.name for c in pose_id_c]\n    em_c = [sql.ProteinMetadata.n_terminal_helix,\n            sql.ProteinMetadata.c_terminal_helix,\n            sql.ProteinMetadata.entity_id,  # The name of the Entity\n            sql.ProteinMetadata.symmetry_group,\n            sql.ProteinMetadata.refined,\n            sql.ProteinMetadata.loop_modeled,\n            # sql.ProteinMetadata.uniprot_ids,  # This is a property...\n            sql.ProteinMetadata.thermophilicity]\n    # This named entity_name as the external access is marked as entity_name while database access uses entity_id\n    em_names = [f'entity_{c.name}' if c.name != 'entity_id' else 'entity_name' for c in em_c]\n    uni_c = [sql.UniProtProteinAssociation.uniprot_id]\n    uni_names = [c.name for c in uni_c]\n\n    selected_columns = (*pose_id_c, *em_c, *uni_c)\n    selected_column_names = (*pose_id_name, *em_names, *uni_names)\n    # Construct the SQL query\n    # Todo CAUTION Deprecated API features detected for 2.0! # Error issued for the below line\n    join_stmt = select(selected_columns).select_from(sql.EntityData) \\\n        .join(sql.ProteinMetadata) \\\n        .join(sql.UniProtProteinAssociation)\n\n    if pose_ids:\n        stmt = join_stmt.where(sql.EntityData.pose_id.in_(pose_ids))\n    else:\n        stmt = join_stmt\n\n    # if design_ids:\n    #     stmt = stmt.where(sql.DesignData.id.in_(design_ids))  # Maybe can optimize joins\n    # else:\n    #     stmt = stmt\n\n    return load_and_format(session, stmt, selected_column_names)\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.poses","title":"poses","text":"<pre><code>poses(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]\n</code></pre> <p>Select PoseJob instances based on filters and weighting of all design summary metrics</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> </ul> <p>Returns:     The matching PoseJob instances</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def poses(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]:\n    \"\"\"Select PoseJob instances based on filters and weighting of all design summary metrics\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n    Returns:\n        The matching PoseJob instances\n    \"\"\"\n    job = job_resources_factory.get()\n    default_weight_metric = config.default_weight_parameter[job.design.method]\n\n    if job.specification_file:  # Figure out poses from a specification file, filters, and weights\n        loc_result = [(pose_job, design) for pose_job in pose_jobs\n                      for design in pose_job.current_designs]\n        total_df = load_total_dataframe(pose_jobs, pose=True)\n        selected_poses_df = \\\n            metrics.prioritize_design_indices(total_df.loc[loc_result, :], filters=job.filter, weights=job.weight,\n                                              protocols=job.protocol, default_weight=default_weight_metric,\n                                              function=job.weight_function)\n        # Remove excess pose instances\n        number_chosen = 0\n        selected_indices, selected_poses = [], set()\n        for pose_job, design in selected_poses_df.index.tolist():\n            if pose_job not in selected_poses:\n                selected_poses.add(pose_job)\n                selected_indices.append((pose_job, design))\n                number_chosen += 1\n                if number_chosen == job.select_number:\n                    break\n\n        # Specify the result order according to any filtering and weighting\n        # Drop the specific design for the dataframe. If they want the design, they should run select_sequences\n        save_poses_df = \\\n            selected_poses_df.loc[selected_indices, :].droplevel(-1)  # .droplevel(0, axis=1).droplevel(0, axis=1)\n        # # convert selected_poses to PoseJob objects\n        # selected_poses = [pose_job for pose_job in pose_jobs if pose_job_name in selected_poses]\n    else:  # if job.total:  # Figure out poses from file/directory input, filters, and weights\n        total_df = load_total_dataframe(pose_jobs, pose=True)\n        if job.protocol:  # Todo adapt to protocol column not in Trajectories right now...\n            group_df = total_df.groupby(putils.protocol)\n            df = pd.concat([group_df.get_group(x) for x in group_df.groups], axis=1,\n                           keys=list(zip(group_df.groups, repeat('mean'))))\n        else:\n            df = pd.concat([total_df], axis=1, keys=['pose', 'metric'])\n        # Figure out designs from dataframe, filters, and weights\n        selected_poses_df = metrics.prioritize_design_indices(df, filters=job.filter, weights=job.weight,\n                                                              protocols=job.protocol,\n                                                              default_weight=default_weight_metric,\n                                                              function=job.weight_function)\n        # Remove excess pose instances\n        number_chosen = 0\n        selected_indices, selected_poses = [], set()\n        for pose_job, design in selected_poses_df.index.tolist():\n            if pose_job not in selected_poses:\n                selected_poses.add(pose_job)\n                selected_indices.append((pose_job, design))\n                number_chosen += 1\n                if number_chosen == job.select_number:\n                    break\n\n        # Specify the result order according to any filtering and weighting\n        # Drop the specific design for the dataframe. If they want the design, they should run select_sequences\n        save_poses_df = \\\n            selected_poses_df.loc[selected_indices, :].droplevel(-1)  # .droplevel(0, axis=1).droplevel(0, axis=1)\n        # # convert selected_poses to PoseJob objects\n        # selected_poses = [pose_job for pose_job in pose_jobs if pose_job_name in selected_poses]\n    # else:\n    #     logger.critical('Missing a required method to provide or find metrics from %s. If you meant to gather '\n    #                     'metrics from every pose in your input specification, ensure you include the --global '\n    #                     'argument' % putils.program_output)\n    #     sys.exit()\n\n    # Format selected poses for output\n    putils.make_path(job.output_directory)\n    logger.info(f'Relevant files will be saved in the output directory: {job.output_directory}')\n\n    if job.save_total:\n        total_df = total_df[~total_df.index.duplicated()]\n        total_df_filename = os.path.join(job.output_directory, 'TotalPoseMetrics.csv')\n        total_df.to_csv(total_df_filename)\n        logger.info(f'Total Pose DataFrame was written to: {total_df_filename}')\n\n    logger.info(f'{len(save_poses_df)} Poses were selected')\n    if len(save_poses_df) != len(total_df):\n        if job.filter or job.weight:\n            new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-{\"Filtered\" if job.filter else \"\"}'\n                                                               f'{\"Weighted\" if job.weight else \"\"}PoseMetrics.csv')\n        else:\n            new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-PoseMetrics.csv')\n        save_poses_df.to_csv(new_dataframe)\n        logger.info(f'New DataFrame with selected poses was written to: {new_dataframe}')\n\n    # # Select by clustering analysis\n    # if job.cluster:\n    # Sort results according to clustered poses if clustering exists\n    if job.cluster.map:\n        if os.path.exists(job.cluster.map):\n            cluster_map = utils.unpickle(job.cluster.map)\n        else:\n            raise FileNotFoundError(f'No --{flags.cluster_map} \"{job.cluster.map}\" file was found')\n\n        # Make the selected_poses into strings\n        selected_pose_strs = list(map(str, selected_poses))\n        # Check if the cluster map is stored as PoseDirectories or strings and convert\n        representative_representative = next(iter(cluster_map))\n        if not isinstance(representative_representative, PoseJob):\n            # Make the cluster map based on strings\n            for representative in list(cluster_map.keys()):\n                # Remove old entry and convert all arguments to pose_id strings, saving as pose_id strings\n                cluster_map[str(representative)] = [str(member) for member in cluster_map.pop(representative)]\n\n        final_pose_indices = select_from_cluster_map(selected_pose_strs, cluster_map, number=job.cluster.number)\n        final_poses = [selected_poses[idx] for idx in final_pose_indices]\n        logger.info(f'Selected {len(final_poses)} poses after clustering')\n    else:  # Try to generate the cluster_map?\n        # raise utils.InputError(f'No --{flags.cluster_map} was provided. To cluster poses, specify:'\n        logger.info(f'No --{flags.cluster_map} was provided. To {flags.cluster_poses}, specify:'\n                    f'\"{putils.program_command} {flags.cluster_poses}\" or '\n                    f'\"{putils.program_command} {flags.protocol} '\n                    f'--{flags.modules} {flags.cluster_poses} {flags.select_poses}\"')\n        logger.info('Grabbing all selected poses')\n        final_poses = selected_poses\n        # cluster_map: dict[str | PoseJob, list[str | PoseJob]] = {}\n        # # {pose_string: [pose_string, ...]} where key is representative, values are matching designs\n        # # OLD -&gt; {composition: {pose_string: cluster_representative}, ...}\n        # compositions: dict[tuple[str, ...], list[PoseJob]] = \\\n        #     protocols.cluster.group_compositions(selected_poses)\n        # if job.multi_processing:\n        #     mp_results = utils.mp_map(protocols.cluster.cluster_pose_by_transformations, compositions.values(),\n        #                               processes=job.cores)\n        #     for result in mp_results:\n        #         cluster_map.update(result.items())\n        # else:\n        #     for composition_group in compositions.values():\n        #         cluster_map.update(protocols.cluster.cluster_pose_by_transformations(composition_group))\n        #\n        # cluster_map_file = \\\n        #     os.path.join(job.clustered_poses, putils.default_clustered_pose_file.format(utils.starttime, location))\n        # pose_cluster_file = utils.pickle_object(cluster_map, name=cluster_map_file, out_path='')\n        # logger.info(f'Found {len(cluster_map)} unique clusters from {len(pose_jobs)} pose inputs. '\n        #             f'All clusters stored in {pose_cluster_file}')\n    # else:\n    #     logger.info('Grabbing all selected poses')\n    #     final_poses = selected_poses\n\n    if len(final_poses) &gt; job.select_number:\n        final_poses = final_poses[:job.select_number]\n        logger.info(f'Found {len(final_poses)} poses after applying your select-number criteria')\n\n    return final_poses\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.select_from_cluster_map","title":"select_from_cluster_map","text":"<pre><code>select_from_cluster_map(selected_pose_jobs: Sequence[Any], cluster_map: dict[Any, list[Any]], number: int = 1) -&gt; list[int]\n</code></pre> <p>From a mapping of cluster representatives to their members, select members based on their ranking in the selected_members sequence</p> <p>Parameters:</p> <ul> <li> <code>selected_pose_jobs</code>             (<code>Sequence[Any]</code>)         \u2013          <p>A sorted list of members that are members of the cluster_map</p> </li> <li> <code>cluster_map</code>             (<code>dict[Any, list[Any]]</code>)         \u2013          <p>A mapping of cluster representatives to their members</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of members to select</p> </li> </ul> <p>Returns:     The indices of selected_members, trimmed and retrieved according to cluster_map membership</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def select_from_cluster_map(selected_pose_jobs: Sequence[Any], cluster_map: dict[Any, list[Any]], number: int = 1) \\\n        -&gt; list[int]:\n    \"\"\"From a mapping of cluster representatives to their members, select members based on their ranking in the\n    selected_members sequence\n\n    Args:\n        selected_pose_jobs: A sorted list of members that are members of the cluster_map\n        cluster_map: A mapping of cluster representatives to their members\n        number: The number of members to select\n    Returns:\n        The indices of selected_members, trimmed and retrieved according to cluster_map membership\n    \"\"\"\n    # Make the selected_poses into strings\n    selected_pose_identifiers = list(map(str, selected_pose_jobs))\n    # Check if the cluster map is stored as PoseDirectories or strings and convert\n    representative_representative = next(iter(cluster_map))\n    if not isinstance(representative_representative, PoseJob):\n        # Make the cluster map based on strings\n        for representative in list(cluster_map.keys()):\n            # Remove old entry and convert all arguments to pose_id strings, saving as pose_id strings\n            cluster_map[str(representative)] = [str(member) for member in cluster_map.pop(representative)]\n\n    membership_representative_map = cluster.invert_cluster_map(cluster_map)\n    representative_found: dict[Any, list[Any]] = defaultdict(list)\n    not_found = []\n    for idx, member in enumerate(selected_pose_identifiers):\n        try:\n            cluster_representative = membership_representative_map[member]\n        except KeyError:\n            not_found.append(idx)\n        else:\n            representative_found[cluster_representative].append(idx)\n\n    # Only include the highest ranked pose in the output as it provides info on all occurrences\n    final_member_indices = []\n    for member_indices in representative_found.values():\n        final_member_indices.extend(member_indices[:number])\n\n    if not_found:\n        logger.warning(f\"Couldn't locate the following members:\\n\\t%s\\nAdding all of these to your selection...\" %\n                       '\\n\\t'.join(selected_pose_identifiers[idx] for idx in not_found))\n        # 'Was {flags.cluster_poses} only run on a subset of the poses that were selected?\n        final_member_indices.extend(not_found)\n\n    return final_member_indices\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.designs","title":"designs","text":"<pre><code>designs(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]\n</code></pre> <p>Select PoseJob instances based on filters and weighting of all design summary metrics</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> </ul> <p>Returns:     The matching PoseJob instances mapped to design name</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def designs(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]:\n    \"\"\"Select PoseJob instances based on filters and weighting of all design summary metrics\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n    Returns:\n        The matching PoseJob instances mapped to design name\n    \"\"\"\n    job = job_resources_factory.get()\n    default_weight_metric = config.default_weight_parameter[job.design.method]\n    if job.specification_file:  # Figure out designs from a specification file, filters, and weights\n        loc_result = [(pose_job, design) for pose_job in pose_jobs\n                      for design in pose_job.current_designs]\n        total_df = load_total_dataframe(pose_jobs)\n        selected_poses_df = \\\n            metrics.prioritize_design_indices(total_df.loc[loc_result, :], filters=job.filter, weights=job.weight,\n                                              protocols=job.protocol, default_weight=default_weight_metric,\n                                              function=job.weight_function)\n        # Specify the result order according to any filtering, weighting, and number\n        selected_poses = {}\n        for pose_job, design in selected_poses_df.index.tolist()[:job.select_number]:\n            _designs = selected_poses.get(pose_job, None)\n            if _designs:\n                _designs.append(design)\n            else:\n                selected_poses[pose_job] = [design]\n\n        # results = selected_poses\n        save_poses_df = selected_poses_df.droplevel(0)  # .droplevel(0, axis=1).droplevel(0, axis=1)\n        # Convert to PoseJob objects\n        # results = {pose_job: results[str(pose_job)] for pose_job in pose_jobs\n        #            if str(pose_job) in results}\n    else:  # if job.total:\n        total_df = load_total_dataframe(pose_jobs)\n        if job.protocol:\n            group_df = total_df.groupby(putils.protocol)\n            df = pd.concat([group_df.get_group(x) for x in group_df.groups], axis=1,\n                           keys=list(zip(group_df.groups, repeat('mean'))))\n        else:\n            df = pd.concat([total_df], axis=1, keys=['pose', 'metric'])\n        # Figure out designs from dataframe, filters, and weights\n        selected_poses_df = metrics.prioritize_design_indices(df, filters=job.filter, weights=job.weight,\n                                                              protocols=job.protocol,\n                                                              default_weight=default_weight_metric,\n                                                              function=job.weight_function)\n        selected_designs = selected_poses_df.index.tolist()\n        job.select_number = \\\n            len(selected_designs) if len(selected_designs) &lt; job.select_number else job.select_number\n        # if job.allow_multiple_poses:\n        #     logger.info(f'Choosing {job.select_number} designs, from the top ranked designs regardless of pose')\n        #     loc_result = selected_designs[:job.select_number]\n        #     results = {pose_job: design for pose_job, design in loc_result}\n        # else:  # elif job.designs_per_pose:\n        designs_per_pose = job.designs_per_pose\n        logger.info(f'Choosing up to {job.select_number} Designs, with {designs_per_pose} Design(s) per pose')\n        number_chosen = count(1)\n        selected_poses = {}\n        for pose_job, design in selected_designs:\n            _designs = selected_poses.get(pose_job, None)\n            if _designs:\n                if len(_designs) &gt;= designs_per_pose:\n                    # We already have too many, continue with search. No need to check as no addition\n                    continue\n                _designs.append(design)\n            else:\n                selected_poses[pose_job] = [design]\n\n            if next(number_chosen) == job.select_number:\n                break\n\n        # results = selected_poses\n        loc_result = [(pose_job, design) for pose_job, _designs in selected_poses.items() for design in _designs]\n\n        # Include only the found index names to the saved dataframe\n        save_poses_df = selected_poses_df.loc[loc_result, :]  # .droplevel(0).droplevel(0, axis=1).droplevel(0, axis=1)\n        # Convert to PoseJob objects\n        # results = {pose_job: results[str(pose_job)] for pose_job in pose_jobs\n        #            if str(pose_job) in results}\n    # else:  # Select designed sequences from each PoseJob.pose provided\n    #     from . import select_sequences\n    #     sequence_metrics = []  # Used to get the column headers\n    #     sequence_filters = sequence_weights = None\n    #\n    #     if job.filter or job.weight:\n    #         try:\n    #             representative_pose_job = next(iter(pose_jobs))\n    #         except StopIteration:\n    #             raise RuntimeError('Missing the required argument pose_jobs. It must be passed to continue')\n    #         example_trajectory = representative_pose_job.designs_metrics_csv\n    #         trajectory_df = pd.read_csv(example_trajectory, index_col=0, header=[0])\n    #         sequence_metrics = set(trajectory_df.columns.get_level_values(-1).tolist())\n    #\n    #     if job.filter == list():\n    #         sequence_filters = metrics.query_user_for_metrics(sequence_metrics, mode='filter', level='sequence')\n    #\n    #     if job.weight == list():\n    #         sequence_weights = metrics.query_user_for_metrics(sequence_metrics, mode='weight', level='sequence')\n    #\n    #     results: dict[PoseJob, list[str]]\n    #     if job.multi_processing:\n    #         # sequence_weights = {'buns_per_ang': 0.2, 'observed_evolution': 0.3, 'shape_complementarity': 0.25,\n    #         #                     'int_energy_res_summary_delta': 0.25}\n    #         zipped_args = zip(pose_jobs, repeat(sequence_filters), repeat(sequence_weights),\n    #                           repeat(job.designs_per_pose), repeat(job.protocol))\n    #         # result_mp = zip(*utils.mp_starmap(select_sequences, zipped_args, processes=job.cores))\n    #         result_mp = utils.mp_starmap(select_sequences, zipped_args, processes=job.cores)\n    #         results = {pose_job: _designs for pose_job, _designs in zip(pose_jobs, result_mp)}\n    #     else:\n    #         results = {pose_job: select_sequences(\n    #                              pose_job, filters=sequence_filters, weights=sequence_weights,\n    #                              number=job.designs_per_pose, protocols=job.protocol)\n    #                    for pose_job in pose_jobs}\n    #\n    #     # Todo there is no sort here so the number isn't really doing anything\n    #     results = dict(list(results.items())[:job.select_number])\n    #     loc_result = [(pose_job, design) for pose_job, _designs in results.items() for design in _designs]\n    #     total_df = load_total_dataframe(pose_jobs)\n    #     save_poses_df = total_df.loc[loc_result, :].droplevel(0).droplevel(0, axis=1).droplevel(0, axis=1)\n\n    logger.info(f'{len(selected_poses)} Poses were selected')\n    logger.info(f'{len(save_poses_df)} Designs were selected')\n    # Format selected sequences for output\n    putils.make_path(job.output_directory)\n    logger.info(f'Relevant files will be saved in the output directory: {job.output_directory}')\n\n    if job.save_total:\n        total_df = total_df[~total_df.index.duplicated()]\n        total_df_filename = os.path.join(job.output_directory, 'TotalDesignMetrics.csv')\n        total_df.to_csv(total_df_filename)\n        logger.info(f'Total Pose/Designs DataFrame was written to: {total_df}')\n\n    if job.filter or job.weight:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-{\"Filtered\" if job.filter else \"\"}'\n                                                           f'{\"Weighted\" if job.weight else \"\"}DesignMetrics.csv')\n    else:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-DesignMetrics.csv')\n    save_poses_df.to_csv(new_dataframe)\n    logger.info(f'New DataFrame with selected designs was written to: {new_dataframe}')\n\n    # Create new output of designed PDB's  # Todo attach the state to these files somehow for further use\n    exceptions = []\n    for pose_job, _designs in selected_poses.items():\n        pose_job.current_designs = _designs\n        for design in _designs:\n            file_path = os.path.join(pose_job.designs_path, f'*{design}*')\n            file = sorted(glob(file_path))\n            if not file:  # Add to exceptions\n                exceptions.append((pose_job, f'No file found for \"{file_path}\"'))\n                continue\n            out_path = os.path.join(job.output_directory, f'{pose_job}_design_{design}.pdb')\n            if not os.path.exists(out_path):\n                shutil.copy(file[0], out_path)  # [i])))\n                # shutil.copy(pose_job.designs_metrics_csv,\n                #     os.path.join(outdir_traj, os.path.basename(pose_job.designs_metrics_csv)))\n                # shutil.copy(pose_job.residues_metrics_csv,\n                #     os.path.join(outdir_res, os.path.basename(pose_job.residues_metrics_csv)))\n        # try:\n        #     # Create symbolic links to the output PDB's\n        #     os.symlink(file[0], os.path.join(job.output_directory,\n        #                                      '%s_design_%s.pdb' % (str(pose_job), design)))  # [i])))\n        #     os.symlink(pose_job.designs_metrics_csv,\n        #                os.path.join(outdir_traj, os.path.basename(pose_job.designs_metrics_csv)))\n        #     os.symlink(pose_job.residues_metrics_csv,\n        #                os.path.join(outdir_res, os.path.basename(pose_job.residues_metrics_csv)))\n        # except FileExistsError:\n        #     pass\n\n    return list(selected_poses.keys())\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.sequences","title":"sequences","text":"<pre><code>sequences(pose_jobs: list[PoseJob]) -&gt; list[PoseJob]\n</code></pre> <p>Perform design selection followed by sequence formatting on those designs</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>list[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> </ul> <p>Returns:     The matching PoseJob instances</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def sequences(pose_jobs: list[PoseJob]) -&gt; list[PoseJob]:\n    \"\"\"Perform design selection followed by sequence formatting on those designs\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n    Returns:\n        The matching PoseJob instances\n    \"\"\"\n    from dnachisel.DnaOptimizationProblem.NoSolutionError import NoSolutionError\n    job = job_resources_factory.get()\n    results = designs(pose_jobs)\n    # Set up output_file pose_jobs for __main__.terminate()\n    return_pose_jobs = list(results.keys())\n    job.output_file = os.path.join(job.output_directory, 'SelectedDesigns.poses')\n\n    if job.multicistronic:\n        intergenic_sequence = job.multicistronic_intergenic_sequence\n    else:\n        intergenic_sequence = ''\n\n    # Format sequences for expression\n    tag_sequences, final_sequences, inserted_sequences, nucleotide_sequences = {}, {}, {}, {}\n    codon_optimization_errors = {}\n    for pose_job, _designs in results.items():\n        pose_job.load_pose()\n        number_of_entities = pose_job.number_of_entities\n        number_of_tags, tag_index = solve_tags(number_of_entities, job.tag_entities)\n        pose_job.pose.rename_chains()\n        for design in _designs:\n            file_glob = f'{pose_job.designs_path}{os.sep}*{design}*'\n            file = sorted(glob(file_glob))\n            if not file:\n                logger.error(f'No file found for {file_glob}')\n                continue\n            design_pose = Pose.from_file(file[0], log=pose_job.log, entity_names=pose_job.entity_names)\n            designed_atom_sequences = [entity.sequence for entity in design_pose.entities]\n\n            # Container of booleans whether each Entity has been tagged\n            missing_tags = [1 for _ in range(number_of_entities)]\n            prior_offset = 0\n            # all_missing_residues = {}\n            # mutations = []\n            sequences_and_tags = {}\n            entity_termini_availability, entity_helical_termini = {}, {}\n            for idx, (source_entity, design_entity) in enumerate(zip(pose_job.pose.entities, design_pose.entities)):\n                # source_entity.retrieve_api_metadata()\n                # source_entity.reference_sequence\n                sequence_id = f'{pose_job}_{source_entity.name}'\n                design_string = f'{design}_{source_entity.name}'\n                termini_availability = pose_job.pose.get_termini_accessibility(source_entity)\n                logger.debug(f'Designed Entity {sequence_id} has the following termini accessible for tags: '\n                             f'{termini_availability}')\n                if job.avoid_tagging_helices:\n                    termini_helix_availability = \\\n                        pose_job.pose.get_termini_accessibility(source_entity, report_if_helix=True)\n                    logger.debug(f'Designed Entity {sequence_id} has the following helical termini available: '\n                                 f'{termini_helix_availability}')\n                    termini_availability = {'n': termini_availability['n'] and not termini_helix_availability['n'],\n                                            'c': termini_availability['c'] and not termini_helix_availability['c']}\n                    entity_helical_termini[design_string] = termini_helix_availability\n                logger.debug(f'The termini {termini_availability} are available for tagging')\n                entity_termini_availability[design_string] = termini_availability\n                true_termini = [term for term, is_true in termini_availability.items() if is_true]\n\n                # Find sequence specified attributes required for expression formatting\n                # disorder = generate_mutations(source_entity.sequence, source_entity.reference_sequence,\n                #                               only_gaps=True)\n                # disorder = source_entity.disorder\n                source_offset = source_entity.offset_index\n                indexed_disordered_residues = {res_number + source_offset + prior_offset: mutation\n                                               for res_number, mutation in source_entity.disorder.items()}\n                # Todo, moved below indexed_disordered_residues on 7/26, ensure correct!\n                prior_offset += len(indexed_disordered_residues)\n                # Generate the source TO design mutations before any disorder handling\n                mutations = generate_mutations(source_entity.sequence, design_entity.sequence, offset=False)\n                # Insert the disordered residues into the design pose\n                for residue_index, mutation in indexed_disordered_residues.items():\n                    logger.debug(f'Inserting {mutation[\"from\"]} into position {residue_index} on chain '\n                                 f'{source_entity.chain_id}')\n                    design_pose.insert_residue_type(residue_index, mutation['from'], chain_id=source_entity.chain_id)\n                    # adjust mutations to account for insertion\n                    for mutation_index in sorted(mutations.keys(), reverse=True):\n                        if mutation_index &lt; residue_index:\n                            break\n                        else:  # mutation should be incremented by one\n                            mutations[mutation_index + 1] = mutations.pop(mutation_index)\n\n                # Check for expression tag addition to the designed sequences after disorder addition\n                inserted_design_sequence = design_entity.sequence\n                selected_tag = {}\n                available_tags = expression.find_expression_tags(inserted_design_sequence)\n                if available_tags:  # look for existing tag to remove from sequence and save identity\n                    tag_names, tag_termini, _ = \\\n                        zip(*[(tag['name'], tag['termini'], tag['sequence']) for tag in available_tags])\n                    try:\n                        preferred_tag_index = tag_names.index(job.preferred_tag)\n                        if tag_termini[preferred_tag_index] in true_termini:\n                            selected_tag = available_tags[preferred_tag_index]\n                    except ValueError:\n                        pass\n                    pretag_sequence = expression.remove_terminal_tags(inserted_design_sequence, tag_names)\n                else:\n                    pretag_sequence = inserted_design_sequence\n                logger.debug(f'The pretag sequence is:\\n{pretag_sequence}')\n\n                # Find the open reading frame offset using the structure sequence after insertion\n                offset = find_orf_offset(pretag_sequence, mutations)\n                formatted_design_sequence = pretag_sequence[offset:]\n                logger.debug(f'The open reading frame offset index is {offset}')\n                logger.debug(f'The formatted_design sequence is:\\n{formatted_design_sequence}')\n\n                if number_of_tags == 0:  # Don't solve tags\n                    sequences_and_tags[design_string] = {'sequence': formatted_design_sequence, 'tag': {}}\n                    continue\n\n                if not selected_tag:\n                    # Find compatible tags from matching PDB observations\n                    possible_matching_tags = []\n                    for uniprot_id in source_entity.uniprot_ids:\n                        uniprot_id_matching_tags = tag_sequences.get(uniprot_id, None)\n                        if uniprot_id_matching_tags is None:\n                            uniprot_id_matching_tags = \\\n                                expression.find_matching_expression_tags(uniprot_id=uniprot_id)\n                            tag_sequences[uniprot_id] = uniprot_id_matching_tags\n                        possible_matching_tags.extend(uniprot_id_matching_tags)\n\n                    if possible_matching_tags:\n                        tag_names, tag_termini, _ = \\\n                            zip(*[(tag['name'], tag['termini'], tag['sequence'])\n                                  for tag in possible_matching_tags])\n                    else:\n                        tag_names, tag_termini, _ = [], [], []\n\n                    iteration = 0\n                    while iteration &lt; len(tag_names):\n                        try:\n                            preferred_tag_index_2 = tag_names[iteration:].index(job.preferred_tag)\n                            if tag_termini[preferred_tag_index_2] in true_termini:\n                                selected_tag = uniprot_id_matching_tags[preferred_tag_index_2]\n                                break\n                        except ValueError:\n                            selected_tag = \\\n                                expression.select_tags_for_sequence(sequence_id,\n                                                                    uniprot_id_matching_tags,\n                                                                    preferred=job.preferred_tag,\n                                                                    **termini_availability)\n                            break\n                        iteration += 1\n\n                if selected_tag.get('name'):\n                    missing_tags[idx] = 0\n                    logger.debug(f'The pre-existing, identified tag is:\\n{selected_tag}')\n                sequences_and_tags[design_string] = {'sequence': formatted_design_sequence, 'tag': selected_tag}\n\n            # After selecting all tags, consider tagging the design as a whole\n            if number_of_tags &gt; 0:\n                number_of_found_tags = number_of_entities - sum(missing_tags)\n                if number_of_tags &gt; number_of_found_tags:\n                    print(f'There were {number_of_tags} requested tags for {pose_job} design {design.name} and '\n                          f'{number_of_found_tags} were found')\n                    current_tag_options = \\\n                        '\\n\\t'.join([f'{i} - {entity_name}\\n'\n                                     f'\\tAvailable Termini: {entity_termini_availability[entity_name]}'\n                                     f'\\n\\t\\t   TAGS: {tag_options[\"tag\"]}'\n                                     for i, (entity_name, tag_options) in enumerate(sequences_and_tags.items(), 1)])\n                    print(f'Current Tag Options:\\n\\t{current_tag_options}')\n                    if job.avoid_tagging_helices:\n                        print('Helical Termini:\\n\\t%s'\n                              % '\\n\\t'.join(f'{entity_name}\\t{availability}'\n                                            for entity_name, availability in entity_helical_termini.items()))\n                    satisfied = input(\"If this is acceptable, enter 'continue', otherwise, \"\n                                      f'you can modify the tagging options with any other input.{input_string}')\n                    if satisfied == 'continue':\n                        number_of_found_tags = number_of_tags\n\n                    iteration_idx = 0\n                    while number_of_tags != number_of_found_tags:\n                        if iteration_idx == len(missing_tags):\n                            print(f'You have seen all options, but the number of requested tags ({number_of_tags}) '\n                                  f\"doesn't equal the number selected ({number_of_found_tags})\")\n                            satisfied = input('If you are satisfied with this, enter \"continue\", otherwise enter '\n                                              'anything and you can view all remaining options starting from the '\n                                              f'first entity{input_string}')\n                            if satisfied == 'continue':\n                                break\n                            else:\n                                iteration_idx = 0\n                        for idx, entity_missing_tag in enumerate(missing_tags[iteration_idx:]):\n                            sequence_id = f'{pose_job}_{pose_job.pose.entities[idx].name}'\n                            if entity_missing_tag and tag_index[idx]:  # isn't tagged but could be\n                                print(f'Entity {sequence_id} is missing a tag. Would you like to tag this entity?')\n                                if not boolean_choice():\n                                    continue\n                            else:\n                                continue\n                            if job.preferred_tag:\n                                tag = job.preferred_tag\n                                while True:\n                                    termini = input('Your preferred tag will be added to one of the termini. Which '\n                                                    f'termini would you prefer? [n/c]{input_string}')\n                                    if termini.lower() in ['n', 'c']:\n                                        break\n                                    else:\n                                        print(f\"'{termini}' is an invalid input. One of 'n' or 'c' is required\")\n                            else:\n                                while True:\n                                    tag_input = input('What tag would you like to use? Enter the number of the '\n                                                      f'below options.\\n\\t%s\\n{input_string}' %\n                                                      '\\n\\t'.join(\n                                                          [f'{i} - {tag}' for i, tag in enumerate(expression.tags, 1)]))\n                                    if tag_input.isdigit():\n                                        tag_input = int(tag_input)\n                                        if tag_input &lt;= len(expression.tags):\n                                            tag = list(expression.tags.keys())[tag_input - 1]\n                                            break\n                                    print(\"Input doesn't match available options. Please try again\")\n                                while True:\n                                    termini = input('Your tag will be added to one of the termini. Which termini '\n                                                    f'would you prefer? [n/c]{input_string}')\n                                    if termini.lower() in ['n', 'c']:\n                                        break\n                                    else:\n                                        print(f'\"{termini}\" is an invalid input. One of \"n\" or \"c\" is required')\n\n                            selected_entity = list(sequences_and_tags.keys())[idx]\n                            if termini == 'n':\n                                new_tag_sequence = \\\n                                    expression.tags[tag] + 'SG' \\\n                                    + sequences_and_tags[selected_entity]['sequence'][:12]\n                            else:  # termini == 'c'\n                                new_tag_sequence = \\\n                                    sequences_and_tags[selected_entity]['sequence'][-12:] \\\n                                    + 'GS' + expression.tags[tag]\n                            sequences_and_tags[selected_entity]['tag'] = {'name': tag, 'sequence': new_tag_sequence}\n                            missing_tags[idx] = 0\n                            break\n\n                        iteration_idx += 1\n                        number_of_found_tags = number_of_entities - sum(missing_tags)\n\n                elif number_of_tags &lt; number_of_found_tags:  # when more than the requested number of tags were id'd\n                    print(f'There were only {number_of_tags} requested tags for design {pose_job} and '\n                          f'{number_of_found_tags} were found')\n                    while number_of_tags != number_of_found_tags:\n                        tag_input = input(f'Which tag would you like to remove? Enter the number of the currently '\n                                          'configured tag option that you would like to remove. If you would like '\n                                          f\"to keep all, specify 'keep'\\n\\t%s\\n{input_string}\"\n                                          % '\\n\\t'.join([f'{i} - {entity_name}\\n\\t\\t{tag_options[\"tag\"]}'\n                                                         for i, (entity_name, tag_options)\n                                                         in enumerate(sequences_and_tags.items(), 1)]))\n                        if tag_input == 'keep':\n                            break\n                        elif tag_input.isdigit():\n                            tag_input = int(tag_input)\n                            if tag_input &lt;= len(sequences_and_tags):\n                                missing_tags[tag_input - 1] = 1\n                                selected_entity = list(sequences_and_tags.keys())[tag_input - 1]\n                                sequences_and_tags[selected_entity]['tag'] = \\\n                                    {'name': None, 'termini': None, 'sequence': None}\n                                # tag = list(expression.tags.keys())[tag_input - 1]\n                                break\n                            else:\n                                print(\"Input doesn't match an integer from the available options. Please try again\")\n                        else:\n                            print(f\"'{tag_input}' is an invalid input. Try again\")\n                        number_of_found_tags = number_of_entities - sum(missing_tags)\n\n            # Apply all tags to the sequences\n            # Todo indicate the linkers that will be used!\n            #  Request a new one if not ideal!\n            cistronic_sequence = ''\n            for idx, (design_string, sequence_tag) in enumerate(sequences_and_tags.items()):\n                tag, sequence = sequence_tag['tag'], sequence_tag['sequence']\n                # print('TAG:\\n', tag.get('sequence'), '\\nSEQUENCE:\\n', sequence)\n                design_sequence = expression.add_expression_tag(tag.get('sequence'), sequence)\n                if tag.get('sequence') and design_sequence == sequence:  # tag exists and no tag added\n                    tag_sequence = expression.tags[tag.get('name')]\n                    if tag.get('termini') == 'n':\n                        if design_sequence[0] == 'M':  # remove existing Met to append tag to n-term\n                            design_sequence = design_sequence[1:]\n                        design_sequence = tag_sequence + 'SG' + design_sequence\n                    else:  # termini == 'c'\n                        design_sequence = design_sequence + 'GS' + tag_sequence\n\n                # If no MET start site, include one\n                if design_sequence[0] != 'M':\n                    design_sequence = f'M{design_sequence}'\n\n                # If there is an unrecognized amino acid, modify\n                if 'X' in design_sequence:\n                    logger.critical(f'An unrecognized amino acid was specified in the sequence {design_string}. '\n                                    'This requires manual intervention.')\n                    # idx = 0\n                    seq_length = len(design_sequence)\n                    while True:\n                        idx = design_sequence.find('X')\n                        if idx == -1:  # Todo clean\n                            break\n                        idx_range = (idx - 6 if idx - 6 &gt; 0 else 0,\n                                     idx + 6 if idx + 6 &lt; seq_length else seq_length)\n                        while True:\n                            new_amino_acid = input(\"What amino acid should be swapped for 'X' in this sequence \"\n                                                   f'context?\\n\\t{idx_range[0] + 1}'\n                                                   f'{\" \" * (len(range(*idx_range)) - (len(str(idx_range[0])) + 1))}'\n                                                   f'{idx_range[1] + 1}'\n                                                   f'\\n\\t{design_sequence[idx_range[0]:idx_range[1]]}'\n                                                   f'{input_string}').upper()\n                            if new_amino_acid in protein_letters_alph1:\n                                design_sequence = design_sequence[:idx] + new_amino_acid + design_sequence[idx + 1:]\n                                break\n                            else:\n                                print(f\"{new_amino_acid} doesn't match a single letter canonical amino acid. \"\n                                      \"Please try again\")\n\n                # For a final manual check of sequence generation, find sequence additions compared to the design\n                # model and save to view where additions lie on sequence. Cross these additions with design\n                # structure to check if insertions are compatible\n                all_insertions = {residue: {'to': aa} for residue, aa in enumerate(design_sequence, 1)}\n                all_insertions.update(generate_mutations(design_sequence, designed_atom_sequences[idx],\n                                                         keep_gaps=True))\n                # Reduce to sequence only\n                inserted_sequences[design_string] = \\\n                    f'{\"\".join([res[\"to\"] for res in all_insertions.values()])}\\n{design_sequence}'\n                logger.info(f'Formatted sequence comparison:\\n{inserted_sequences[design_string]}')\n                final_sequences[design_string] = design_sequence\n                if job.nucleotide:\n                    try:\n                        nucleotide_sequence = \\\n                            optimize_protein_sequence(design_sequence, species=job.optimize_species)\n                    except NoSolutionError:  # add the protein sequence?\n                        logger.warning(f'Optimization of {design_string} was not successful!')\n                        codon_optimization_errors[design_string] = design_sequence\n                        break\n\n                    if job.multicistronic:\n                        if idx &gt; 0:\n                            cistronic_sequence += intergenic_sequence\n                        cistronic_sequence += nucleotide_sequence\n                    else:\n                        nucleotide_sequences[design_string] = nucleotide_sequence\n            if job.multicistronic:\n                nucleotide_sequences[str(pose_job)] = cistronic_sequence\n\n    # Report Errors\n    if codon_optimization_errors:\n        # Todo utilize errors\n        error_file = \\\n            write_sequences(codon_optimization_errors, csv=job.csv,\n                            file_name=os.path.join(job.output_directory,\n                                                   f'OptimizationErrorProteinSequences'))\n    # Write output sequences to fasta file\n    seq_file = write_sequences(final_sequences, csv=job.csv,\n                               file_name=os.path.join(job.output_directory, 'SelectedSequences'))\n    logger.info(f'Final Design protein sequences written to: {seq_file}')\n    seq_comparison_file = \\\n        write_sequences(inserted_sequences, csv=job.csv,\n                        file_name=os.path.join(job.output_directory, 'SelectedSequencesExpressionAdditions'))\n    logger.info(f'Final Expression sequence comparison to Design sequence written to: {seq_comparison_file}')\n    # check for protein or nucleotide output\n    if job.nucleotide:\n        nucleotide_sequence_file = \\\n            write_sequences(nucleotide_sequences, csv=job.csv,\n                            file_name=os.path.join(job.output_directory, 'SelectedSequencesNucleotide'))\n        logger.info(f'Final Design nucleotide sequences written to: {nucleotide_sequence_file}')\n\n    return return_pose_jobs\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.format_save_df","title":"format_save_df","text":"<pre><code>format_save_df(session: Session, designs_df: DataFrame, pose_ids: Iterable[int], design_ids: Iterable[int] = None) -&gt; DataFrame\n</code></pre> <p>Given a DataFrame with Pose/Design information, clean Pose and Entity information for readable output</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>designs_df</code>             (<code>DataFrame</code>)         \u2013          <p>A DataFrame with design metrics. Must contain a column corresponding to PoseJob.id named \"pose_id\"</p> </li> <li> <code>pose_ids</code>             (<code>Iterable[int]</code>)         \u2013          <p>PoseJob instance identifiers for which metrics are desired</p> </li> <li> <code>design_ids</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>DesignData instance identifiers for which metrics are desired</p> </li> </ul> <p>Returns:     A DataFrame formatted with the PoseMetrics, EntityMetrics, and DesignEntityMetrics. The final DataFrame will         have an entry for each PoseJob with separate metric columns grouped by 'structure_entity', i.e. Pose and         Entity metrics</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def format_save_df(session: Session, designs_df: pd.DataFrame, pose_ids: Iterable[int],\n                   design_ids: Iterable[int] = None) -&gt; pd.DataFrame:\n    \"\"\"Given a DataFrame with Pose/Design information, clean Pose and Entity information for readable output\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        designs_df: A DataFrame with design metrics. Must contain a column corresponding to PoseJob.id named \"pose_id\"\n        pose_ids: PoseJob instance identifiers for which metrics are desired\n        design_ids: DesignData instance identifiers for which metrics are desired\n    Returns:\n        A DataFrame formatted with the PoseMetrics, EntityMetrics, and DesignEntityMetrics. The final DataFrame will\n            have an entry for each PoseJob with separate metric columns grouped by 'structure_entity', i.e. Pose and\n            Entity metrics\n    \"\"\"\n    structure_entity = 'structure_entity'\n    pose_id = 'pose_id'\n    entity_id = 'entity_id'\n    pose_metrics_df = load_sql_pose_metrics_dataframe(session, pose_ids=pose_ids)\n    pose_metrics_df.set_index(pose_id, inplace=True)\n    logger.debug(f'pose_metrics_df:\\n{pose_metrics_df}')\n    # save_df = pose_metrics_df.join(designs_df)  # , on='pose_id')\n\n    pose_metadata_df = load_sql_pose_metadata_dataframe(session, pose_ids=pose_ids)\n    pose_metrics_df = pose_metrics_df.join(pose_metadata_df.set_index(pose_id), rsuffix='_DROP')\n    logger.debug(f'pose_metrics_df after metadata join:\\n{pose_metrics_df}')\n\n    # Join the designs_df (which may not have pose_id as index, but must have pose_id as a column)\n    # with the pose_id indexed pose_metrics_df. This keeps the designs_df index in save_df\n    save_df = designs_df.join(pose_metrics_df, on=pose_id, rsuffix='_DROP')\n    save_df.drop(save_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n    save_df.columns = pd.MultiIndex.from_product([['pose'], save_df.columns.tolist()],\n                                                 names=[structure_entity, 'metric'])\n    logger.debug(f'save_df:\\n{save_df}')\n    # Get EntityMetrics\n    entity_metrics_df = load_sql_entity_metrics_dataframe(session, pose_ids=pose_ids, design_ids=design_ids)\n    logger.debug(f'entity_metrics_df:\\n{entity_metrics_df}')\n    # entity_metrics_df.set_index(pose_id, inplace=True)\n    # Manipulate to combine with Pose data for the final format:\n    # structure_entity        1        2 |    pose\n    # metric            go fish  go fish | go fish\n    # pose_id1           3    4   3    3 |  6  3.5\n    # pose_id2           5    3   3    3 |  8    3\n    # ...\n    entity_metadata_df = load_sql_entity_metadata_dataframe(session, pose_ids=pose_ids)\n    logger.debug(f'entity_metadata_df:\\n{entity_metadata_df}')\n    # entity_metadata_df.set_index(pose_id, inplace=True)\n    # entity_metrics_df = entity_metrics_df.join(entity_metadata_df.set_index(pose_id), on=pose_id, rsuffix='_DROP')\n    if entity_metrics_df.empty:\n        # In the case there are no designs and therefore no design_entity_metrics entries\n        entity_metrics_df = entity_metadata_df\n    else:\n        entity_metrics_df = entity_metrics_df.join(entity_metadata_df.set_index([pose_id, entity_id]),\n                                                   on=[pose_id, entity_id], rsuffix='_DROP')\n    entity_metrics_df.drop(entity_metrics_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n    logger.debug(f'entity_metrics_df after metadata.join:\\n{entity_metrics_df}')\n    # Get the first return from factorize since we just care about the unique \"code\" values\n    entity_metrics_df[structure_entity] = \\\n        entity_metrics_df.groupby(pose_id).entity_id.transform(lambda x: pd.factorize(x)[0]) + 1\n    # Todo add numeric_only=True? to groupby ops\n    # entity_metrics_df[structure_entity] = entity_metrics_df.groupby(pose_id).entity_id.cumcount() + 1\n    # entity_metrics_df[structure_entity] = \\\n    #     (entity_metrics_df.groupby('pose_id').entity_id.cumcount() + 1).apply(lambda x: f'entity_{x}')\n    entity_metrics_df = entity_metrics_df.drop_duplicates([pose_id, structure_entity])\n    logger.debug(f'entity_metrics_df AFTER factorize and deduplication:\\n{entity_metrics_df}')\n    # Make the stacked entity df and use the pose_id index to join with the above df\n    pose_oriented_entity_df = entity_metrics_df.set_index([pose_id, structure_entity]).unstack().swaplevel(axis=1)\n    # pose_oriented_entity_df.index = pd.MultiIndex.from_product([['pose'], pose_oriented_entity_df.index])\n    # pose_oriented_entity_df = entity_metrics_df.unstack().swaplevel(axis=1)\n    logger.debug(f'pose_oriented_entity_df:\\n{pose_oriented_entity_df}')\n    save_df = save_df.join(pose_oriented_entity_df, on=[('pose', pose_id)])  # , rsuffix='_DROP')  # , on=pose_id\n    # save_df.drop(save_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n    logger.debug(f'Final save_df:\\n{save_df}')\n\n    return save_df\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.load_pose_job_from_id","title":"load_pose_job_from_id","text":"<pre><code>load_pose_job_from_id(session: Session, ids: Sequence[int]) -&gt; list[PoseJob]\n</code></pre> <p>Given pose identifiers, either directory strings, or database ids, load PoseJobs</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          </li> <li> <code>ids</code>             (<code>Sequence[int]</code>)         \u2013          </li> </ul> <p>Returns:     The matching PoseJobs</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def load_pose_job_from_id(session: Session, ids: Sequence[int]) -&gt; list[PoseJob]:\n    \"\"\"Given pose identifiers, either directory strings, or database ids, load PoseJobs\n\n    Args:\n        session:\n        ids:\n    Returns:\n        The matching PoseJobs\n    \"\"\"\n    # if job.module in flags.select_modules:\n    #     pose_job_stmt = select(PoseJob).options(\n    #         lazyload(PoseJob.entity_data),\n    #         lazyload(PoseJob.metrics))\n    # else:  # Load all attributes\n    pose_job_stmt = select(PoseJob)\n    try:  # To convert the identifier to an integer\n        int(ids[0])\n    except ValueError:  # Can't convert to integer, identifiers_are_database_id = False\n        fetch_jobs_stmt = pose_job_stmt.where(PoseJob.pose_identifier.in_(ids))\n    else:\n        fetch_jobs_stmt = pose_job_stmt.where(PoseJob.id.in_(ids))\n\n    return session.scalars(fetch_jobs_stmt).all()\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.sql_poses","title":"sql_poses","text":"<pre><code>sql_poses(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]\n</code></pre> <p>Select PoseJob instances based on filters and weighting of all design summary metrics</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> </ul> <p>Returns:     The selected PoseJob instances</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def sql_poses(pose_jobs: Iterable[PoseJob]) -&gt; list[PoseJob]:\n    \"\"\"Select PoseJob instances based on filters and weighting of all design summary metrics\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n    Returns:\n        The selected PoseJob instances\n    \"\"\"\n    job = job_resources_factory.get()\n    default_weight_metric = config.default_weight_parameter[job.design.method]\n\n    # Select poses from a starting pool and provided filters and weights\n    pose_ids = [pose_job.id for pose_job in pose_jobs]\n    # design_ids = [design.id for pose_job in pose_jobs for design in pose_job.current_designs]\n    #     total_df = load_sql_poses_dataframe(session, pose_ids=pose_ids, design_ids=design_ids)\n    #     selected_poses_df = \\\n    #         metrics.prioritize_design_indices(total_df, filters=job.filter, weights=job.weight,\n    #                                           protocols=job.protocol, function=job.weight_function)\n    #     # Remove excess pose instances\n    #     number_chosen = 0\n    #     selected_indices, selected_poses = [], set()\n    #     for pose_job, design in selected_poses_df.index.tolist():\n    #         if pose_job not in selected_poses:\n    #             selected_poses.add(pose_job)\n    #             selected_indices.append((pose_job, design))\n    #             number_chosen += 1\n    #             if number_chosen == job.select_number:\n    #                 break\n    #\n    #     # Specify the result order according to any filtering and weighting\n    #     # Drop the specific design for the dataframe. If they want the design, they should run select_sequences\n    #     save_poses_df = \\\n    #         selected_poses_df.loc[selected_indices, :].droplevel(-1)  # .droplevel(0, axis=1).droplevel(0, axis=1)\n    #     # # convert selected_poses to PoseJob objects\n    #     # selected_poses = [pose_job for pose_job in pose_jobs if pose_job_name in selected_poses]\n    # else:  # if job.total:  # Figure out poses from file/directory input, filters, and weights\n    #     pose_ids = design_ids = None\n    #     # total_df = load_sql_poses_dataframe(session)\n    #\n    #     # if job.protocol:  # Todo adapt to protocol column not in Trajectories right now...\n    #     #     group_df = total_df.groupby(putils.protocol)\n    #     #     df = pd.concat([group_df.get_group(x) for x in group_df.groups], axis=1,\n    #     #                    keys=list(zip(group_df.groups, repeat('mean'))))\n    #     # else:\n    #     #     df = pd.concat([total_df], axis=1, keys=['pose', 'metric'])\n\n    pose_id = 'pose_id'\n    entity_id = 'entity_id'\n    design_id = 'design_id'\n    with job.db.session(expire_on_commit=False) as session:\n        # Figure out designs from dataframe, filters, and weights\n        total_df = load_sql_poses_dataframe(session, pose_ids=pose_ids)  # , design_ids=design_ids)\n        if total_df.empty:\n            raise utils.MetricsError(\n                f\"For the input PoseJobs, there aren't metrics collected. Use the '{flags.analysis}' module or perform \"\n                \"some design module before selection\")\n        # # Todo\n        # job_metadata_df = load_sql_pose_job_metadata_dataframe(session, pose_ids=pose_ids)\n        pose_metadata_df = load_sql_pose_metadata_dataframe(session, pose_ids=pose_ids)\n        entity_metadata_df = load_sql_entity_metadata_dataframe(session, pose_ids=pose_ids)\n        logger.debug(f'entity_metadata_df:\\n{entity_metadata_df}')\n        total_df = total_df.join(pose_metadata_df.set_index(pose_id), on=pose_id, rsuffix='_DROP')\n        total_df = \\\n            total_df.join(entity_metadata_df.set_index([pose_id, entity_id]), on=[pose_id, entity_id], rsuffix='_DROP')\n        total_df.drop(total_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n        # logger.debug(f'total_df: {total_df.columns.tolist()}')\n\n        designs_df = load_sql_design_metrics_dataframe(session, pose_ids=pose_ids)  # , design_ids=design_ids)\n        if designs_df.empty:\n            pose_designs_mean_df = pd.DataFrame()\n            # print(total_df)\n            # raise NotImplementedError(f\"Can't proceed without at least the PoseJob.pose_source\")\n        else:\n            # designs_df has a multiplicity of number_of_entities from DesignEntityMetrics table join\n            # Use the pose_id index to join to the total_df\n            # Todo ensure non-numeric are here as well\n            designs_df.drop(design_id, axis=1, inplace=True)\n            pose_designs_mean_df = designs_df.groupby(pose_id).mean(numeric_only=True)\n            total_df = total_df.join(pose_designs_mean_df, on=pose_id, rsuffix='_DROP')\n\n            # # Todo JobMetadata\n            # design_ids = total_df[design_id].unique().tolist()\n            # design_metadata_df = load_sql_design_metadata_dataframe(session, design_ids=design_ids)\n            # total_df = total_df.join(design_metadata_df.set_index(design_id), on=design_id, rsuffix='_DROP')\n\n            entity_designs_df = load_sql_design_entities_dataframe(session, pose_ids=pose_ids)  # design_ids=design_ids)\n            # logger.debug(f'entity_designs_df: {entity_designs_df}')\n            pose_design_entities_mean_df = entity_designs_df.groupby([pose_id, entity_id]).mean(numeric_only=True)\n            logger.debug(f'pose_design_entities_mean_df: {pose_design_entities_mean_df}')\n            # # Drop unused designs columns\n            # entity_columns = \\\n            #     [c.name for c in sql.DesignEntityMetrics.__table__.columns if c.name in designs_df.columns]\n            # entity_designs_df = designs_df.loc[:, ['pose_id'] + entity_columns]\n            # designs_df.drop([design_id] + entity_columns, axis=1, inplace=True)\n            # This will create a total_df that is the number_of_entities X larger than the number of poses\n            total_df = total_df.join(pose_design_entities_mean_df, on=[pose_id, entity_id], rsuffix='_DROP')\n            total_df.drop(total_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n            logger.debug(f'total_df:\\n{total_df}')\n\n        if job.filter or job.protocol:\n            entity_multiplicity = len(entity_metadata_df) / len(pose_metadata_df)\n            # Todo still not accurate, got 13914 from 4241 designs\n            logger.warning('Filtering statistics have an increased representation due to included Entity metrics. '\n                           f'Values reported for each filter will be {entity_multiplicity}x over those actually '\n                           f'present')\n        # Ensure the pose_id is the index to prioritize\n        total_df.set_index(pose_id, inplace=True)\n        # Perform selection using provided arguments\n        if not job.filter and not job.weight and not job.protocol and default_weight_metric not in total_df.columns:\n            # Nothing to filter/weight\n            selected_poses_df = total_df\n        else:  # Filter/weight\n            selected_poses_df = \\\n                metrics.prioritize_design_indices(total_df, filters=job.filter, weights=job.weight,\n                                                  protocols=job.protocol, default_weight=default_weight_metric,\n                                                  function=job.weight_function)\n        # Remove excess pose instances\n        selected_pose_ids = utils.remove_duplicates(selected_poses_df.index.tolist())[:job.select_number]\n\n        # Select by clustering analysis\n        if job.cluster_selection or job.cluster.map:\n            pose_jobs = load_pose_job_from_id(session, selected_pose_ids)\n            if job.cluster_selection:\n                pose_jobs = cluster.cluster_poses(pose_jobs)\n\n            # Sort results according to clustered poses\n            # cluster_map: dict[str | PoseJob, list[str | PoseJob]] = {}\n            if os.path.exists(job.cluster.map):\n                cluster_map = utils.unpickle(job.cluster.map)\n            else:\n                raise FileNotFoundError(\n                    f'No \"{job.cluster.map}\" file was found')\n\n            final_pose_indices = select_from_cluster_map(pose_jobs, cluster_map, number=job.cluster.number)\n            final_poses = [pose_jobs[idx] for idx in final_pose_indices]\n            logger.info(f'Selected {len(final_poses)} poses after clustering')\n            selected_pose_ids = [pose_job.id for pose_job in final_poses]\n\n        if len(selected_pose_ids) &gt; job.select_number:\n            selected_pose_ids = selected_pose_ids[:job.select_number]\n            logger.info(f'Found {len(selected_pose_ids)} Poses after applying your --select-number criteria')\n\n        # Format selected PoseJob ids for output, including all additional metrics/metadata\n        if not pose_designs_mean_df.empty:\n            save_poses_df = pose_designs_mean_df.loc[selected_pose_ids].reset_index()\n        else:\n            save_poses_df = pd.DataFrame(zip(selected_pose_ids, range(len(selected_pose_ids))),\n                                         columns=[pose_id, 'idx_DROP'])\n            # save_poses_df.index = pd.Index(selected_pose_ids, name=pose_id)\n            # save_poses_df = pd.Series(selected_pose_ids, name=pose_id).to_frame()\n        save_poses_df = format_save_df(session, save_poses_df, selected_pose_ids)\n\n        putils.make_path(job.output_directory)\n        logger.info(f'Relevant files will be saved in the output directory: {job.output_directory}')\n        if job.output_structures:\n            logger.info(f'Copying Pose files...')\n            # Create new output of designed PDB's\n            final_pose_id_to_identifier = {}\n            for pose_id_ in tqdm(selected_pose_ids, bar_format=TQDM_BAR_FORMAT, leave=False):\n                pose_job = session.get(PoseJob, pose_id_)\n                final_pose_id_to_identifier[pose_id_] = pose_job.pose_identifier\n                structure_path = pose_job.get_pose_file()\n                if structure_path and os.path.exists(structure_path):\n                    out_path = os.path.join(job.output_directory, f'{pose_job.project}-{pose_job.name}.pdb')\n                    # Todo attach the program state to these files for downstream use?\n                    shutil.copy(structure_path, out_path)\n                else:\n                    pose_job.log.error(f\"Expected file '{structure_path}' wasn't found for {pose_job.pose_identifier}\")\n        else:\n            final_pose_id_to_identifier = load_pose_identifier_from_id(session, selected_pose_ids)\n\n        if job.save_total:\n            out_total_df = total_df[~total_df[pose_id].duplicated()]\n            total_pose_ids = out_total_df[pose_id].tolist()\n            total_pose_id_to_identifier = load_pose_identifier_from_id(session, total_pose_ids)\n            # Map the names to existing identifiers\n            out_total_df['pose_identifier'] = out_total_df[pose_id].map(total_pose_id_to_identifier)\n            out_total_df.set_index('pose_identifier', inplace=True)\n            out_total_df.index.rename('pose_identifier', inplace=True)\n            # Write\n            total_df_filename = os.path.join(job.output_directory, 'TotalPoseMetrics.csv')\n            out_total_df.to_csv(total_df_filename)\n            logger.info(f'Total Pose DataFrame written to: {total_df_filename}')\n            del out_total_df\n    # End session\n\n    # No need to rename as the index aren't design_id\n    # save_poses_df.reset_index(col_fill='pose', col_level=-1, inplace=True)\n    # Rename the identifiers to human-readable names\n    save_poses_df.set_index(\n        save_poses_df[('pose', pose_id)].map(final_pose_id_to_identifier).rename('pose_identifier'), inplace=True)\n\n    # Format selected poses for output\n    logger.info(f'{len(save_poses_df)} Poses were selected')\n    if job.filter or job.weight:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-{\"Filtered\" if job.filter else \"\"}'\n                                                           f'{\"Weighted\" if job.weight else \"\"}PoseMetrics.csv')\n    else:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-PoseMetrics.csv')\n    save_poses_df.to_csv(new_dataframe)\n    logger.info(f'New DataFrame with selected poses written to: {new_dataframe}')\n\n    return final_pose_id_to_identifier.values()\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.sql_designs","title":"sql_designs","text":"<pre><code>sql_designs(pose_jobs: Iterable[PoseJob], return_pose_jobs: bool = False) -&gt; list[PoseJob]\n</code></pre> <p>Select PoseJob instances based on filters and weighting of all design summary metrics</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>Iterable[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> <li> <code>return_pose_jobs</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to force the creation of PoseJob instances and load selected designs into PoseJob.current_designs</p> </li> </ul> <p>Returns:     The selected PoseJob instances with selected designs stored in the .current_designs attribute</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def sql_designs(pose_jobs: Iterable[PoseJob], return_pose_jobs: bool = False) -&gt; list[PoseJob]:\n    \"\"\"Select PoseJob instances based on filters and weighting of all design summary metrics\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n        return_pose_jobs: Whether to force the creation of PoseJob instances and load selected designs into\n            PoseJob.current_designs\n    Returns:\n        The selected PoseJob instances with selected designs stored in the .current_designs attribute\n    \"\"\"\n    job = job_resources_factory.get()\n    default_weight_metric = config.default_weight_parameter[job.design.method]\n\n    # Select designs from a starting pool and provided filters and weights\n    pose_ids = [pose_job.id for pose_job in pose_jobs]\n    design_ids = [design.id for pose_job in pose_jobs for design in pose_job.current_designs]\n    #     total_df = load_sql_design_metrics_dataframe(session, pose_ids=pose_ids, design_ids=design_ids)\n    #     selected_poses_df = \\\n    #         metrics.prioritize_design_indices(total_df, filters=job.filter, weights=job.weight,\n    #                                           protocols=job.protocol, function=job.weight_function)\n    #     # Specify the result order according to any filtering, weighting, and number\n    #     results = {}\n    #     for pose_id, design in selected_poses_df.index.tolist()[:job.select_number]:\n    #         if pose_id in results:\n    #             results[pose_id].append(design)\n    #         else:\n    #             results[pose_id] = [design]\n    #\n    #     save_designs_df = selected_poses_df.droplevel(0)  # .droplevel(0, axis=1).droplevel(0, axis=1)\n    #     # Convert to PoseJob objects\n    #     # results = {pose_id: results[str(pose_id)] for pose_id in pose_jobs\n    #     #            if str(pose_id) in results}\n    # else:  # if job.total:  # Figure out poses from file/directory input, filters, and weights\n    #     pose_ids = design_ids = None\n    #     # total_df = load_total_dataframe(pose_jobs)\n    #     total_df = load_sql_design_metrics_dataframe(session)\n    #     if job.protocol:\n    #         group_df = total_df.groupby('protocol')\n    #         df = pd.concat([group_df.get_group(x) for x in group_df.groups], axis=1,\n    #                        keys=list(zip(group_df.groups, repeat('mean'))))\n    #     else:\n    #         df = pd.concat([total_df], axis=1, keys=['pose', 'metric'])\n    #     # Figure out designs from dataframe, filters, and weights\n    #     selected_poses_df = metrics.prioritize_design_indices(df, filters=job.filter, weights=job.weight,\n    #                                                           protocols=job.protocol, function=job.weight_function)\n    #     selected_designs = selected_poses_df.index.tolist()\n    #     job.select_number = \\\n    #         len(selected_designs) if len(selected_designs) &lt; job.select_number else job.select_number\n    #\n    #     # Include only the found index names to the saved dataframe\n    #     save_designs_df = selected_poses_df.loc[loc_result, :]  # droplevel(0).droplevel(0, axis=1).droplevel(0, axis=1)\n    #     # Convert to PoseJob objects\n    #     # results = {pose_id: results[str(pose_id)] for pose_id in pose_jobs\n    #     #            if str(pose_id) in results}\n\n    pose_id = 'pose_id'\n    entity_id = 'entity_id'\n    design_id = 'design_id'\n    with job.db.session(expire_on_commit=False) as session:\n        # Figure out designs from dataframe, filters, and weights\n        total_df = load_sql_all_metrics_dataframe(session, pose_ids=pose_ids, design_ids=design_ids)\n        pose_metadata_df = load_sql_pose_metadata_dataframe(session, pose_ids=pose_ids)\n        entity_metadata_df = load_sql_entity_metadata_dataframe(session, pose_ids=pose_ids)\n        logger.debug(f'entity_metadata_df:\\n{entity_metadata_df}')\n        try:\n            design_ids = total_df[design_id].unique().tolist()\n        except KeyError:  # No design_id key, probably no design_id present\n            # logger.critical(f\"Couldn't find any '{design_id}' from the selection. \"\n            raise utils.InputError(\n                f\"Couldn't find any '{design_id}' from the selection. \"\n                f\"Make sure designs are produced before {flags.select_designs} is used\")\n        design_metadata_df = load_sql_design_metadata_dataframe(session, design_ids=design_ids)\n        # logger.info(f'design_metadata_df:\\n{design_metadata_df}')\n        # logger.info(f'columns:\\n{sorted(design_metadata_df.columns.tolist())}')\n        if design_metadata_df.empty:\n            pass\n        else:\n            total_df = total_df.join(design_metadata_df.set_index(design_id), on=design_id, rsuffix='_DROP')\n        total_df = total_df.join(pose_metadata_df.set_index(pose_id), on=pose_id, rsuffix='_DROP')\n        total_df = \\\n            total_df.join(entity_metadata_df.set_index([pose_id, entity_id]), on=[pose_id, entity_id], rsuffix='_DROP')\n        total_df.drop(total_df.filter(regex='_DROP$').columns.tolist(), axis=1, inplace=True)\n        logger.debug(f'total_df:\\n{total_df}')\n        # logger.debug(f'total_df: {total_df.columns.tolist()}')\n        if total_df.empty:\n            raise utils.MetricsError(\n                f\"For the input PoseJobs, there aren't metrics collected. Use the '{flags.analysis}' module or perform \"\n                f\"some design module before {job.module}\")\n        if job.filter or job.protocol:\n            entity_multiplicity = len(entity_metadata_df) / len(pose_metadata_df)\n            logger.warning('Filtering statistics have an increased representation due to included Entity metrics. '\n                           f'Values reported for each filter will be {entity_multiplicity}x over those actually '\n                           f'present')\n        # Ensure the design_id is the index to prioritize, though both pose_id and design_id are grabbed below\n        total_df.set_index(design_id, inplace=True)\n        # Perform selection using provided arguments\n        if not job.filter and not job.weight and not job.protocol and default_weight_metric not in total_df.columns:\n            # Nothing to filter/weight\n            selected_designs_df = total_df\n        else:  # Filter/weight\n            selected_designs_df = \\\n                metrics.prioritize_design_indices(total_df, filters=job.filter, weights=job.weight,\n                                                  protocols=job.protocol, default_weight=default_weight_metric,\n                                                  function=job.weight_function)\n\n        # Drop duplicated values keeping the order of the DataFrame\n        selected_designs_df = selected_designs_df[~selected_designs_df.index.duplicated()]\n\n        # Select by clustering analysis\n        if job.cluster_selection or job.cluster.map:\n            selected_pose_ids = selected_designs_df[pose_id].tolist()\n            pose_jobs = load_pose_job_from_id(session, selected_pose_ids)\n            if job.cluster_selection:\n                pose_jobs = cluster.cluster_poses(pose_jobs)\n\n            # Sort results according to clustered poses\n            # cluster_map: dict[str | PoseJob, list[str | PoseJob]] = {}\n            if os.path.exists(job.cluster.map):\n                cluster_map = utils.unpickle(job.cluster.map)\n            else:\n                raise FileNotFoundError(\n                    f'No \"{job.cluster.map}\" file was found')\n\n            final_pose_indices = select_from_cluster_map(pose_jobs, cluster_map, number=job.cluster.number)\n            final_poses = [pose_jobs[idx] for idx in final_pose_indices]\n            logger.info(f'Selected {len(final_poses)} poses after clustering')\n            selected_pose_ids = [pose_job.id for pose_job in final_poses]\n            selected_designs_df = selected_designs_df[selected_designs_df[pose_id].isin(selected_pose_ids)]\n\n        # Specify the result order according to any filtering, weighting, and number\n        number_selected = len(selected_designs_df)\n        job.select_number = number_selected if number_selected &lt; job.select_number else job.select_number\n        designs_per_pose = job.designs_per_pose\n        logger.info(f'Choosing up to {job.select_number} Designs, with {designs_per_pose} Design(s) per Pose')\n\n        # Get the pose_id and the design_id for each found design\n        selected_design_ids = selected_designs_df.index.tolist()\n        selected_pose_ids = selected_designs_df[pose_id].tolist()\n        selected_designs = list(zip(selected_pose_ids, selected_design_ids))\n        selected_designs_iter = iter(selected_designs)\n        number_chosen = count()\n        chosen = next(number_chosen)\n        # selected_pose_id_to_design_ids = defaultdict(list)  # Alt way\n        selected_pose_id_to_design_ids = {}\n        try:\n            while chosen &lt; job.select_number:\n                pose_id_, design_id_ = next(selected_designs_iter)\n                # Alt way, but doesn't count designs_per_pose\n                # selected_pose_id_to_design_ids[pose_id].append(design_id)\n                _designs = selected_pose_id_to_design_ids.get(pose_id_, None)\n                if _designs:\n                    if len(_designs) &lt; designs_per_pose:\n                        _designs.append(design_id_)\n                    else:  # Number of designs already satisfied for this pose\n                        continue\n                else:\n                    selected_pose_id_to_design_ids[pose_id_] = [design_id_]\n                chosen = next(number_chosen)\n        except StopIteration:  # We exhausted selected_designs_iter\n            pass\n\n        logger.info(f'{len(selected_pose_id_to_design_ids)} Poses were selected')\n        putils.make_path(job.output_directory)\n        logger.info(f'Relevant files will be saved in the output directory: {job.output_directory}')\n        if job.save_total:\n            # Remove duplicate entries\n            out_total_df = total_df[~total_df.index.duplicated()].copy()\n            total_design_ids = out_total_df.index.tolist()\n            total_pose_ids, total_design_ids, total_design_identifier = \\\n                zip(*load_design_identifier_from_id(session, total_design_ids))\n            total_design_id_to_identifier = dict(zip(total_design_ids, total_design_identifier))\n            total_pose_id_to_identifier = load_pose_identifier_from_id(session, set(total_pose_ids))\n            # Map the names to existing identifiers\n            out_total_df['pose_identifier'] = out_total_df[pose_id].map(total_pose_id_to_identifier)\n            # Put the design_ids to a column\n            out_total_df.reset_index(inplace=True)\n            out_total_df['design_name'] = out_total_df[design_id].map(total_design_id_to_identifier)\n            out_total_df.set_index(['pose_identifier', 'design_name'], inplace=True)\n            out_total_df.index.rename(['pose_identifier', 'design_name'], inplace=True)\n            # Write\n            total_df_filename = os.path.join(job.output_directory, 'TotalDesignMetrics.csv')\n            out_total_df.to_csv(total_df_filename)\n            logger.info(f'Total Pose/Designs DataFrame written to: {total_df_filename}')\n            del out_total_df\n\n        # Format selected designs for output\n        selected_design_ids = []\n        for design_ids in selected_pose_id_to_design_ids.values():\n            selected_design_ids.extend(design_ids)\n\n        if job.output_structures:\n            logger.info(f'Copying Design files...')\n            # Create new output of designed PDB's\n            pose_id_to_identifier = {}\n            design_id_to_identifier = {}\n            results = []\n            for pose_id_, design_ids in tqdm(\n                    selected_pose_id_to_design_ids.items(), bar_format=TQDM_BAR_FORMAT, leave=False):\n                pose_job = session.get(PoseJob, pose_id_)\n                pose_id_to_identifier[pose_id_] = pose_job.pose_identifier\n                current_designs = []\n                for design_id_ in design_ids:\n                    design = session.get(sql.DesignData, design_id_)\n                    design_name = design.name\n                    design_id_to_identifier[design_id_] = design_name\n                    design_structure_path = design.structure_path\n                    if design_structure_path:\n                        out_path = os.path.join(job.output_directory, f'{pose_job.project}-{design_name}.pdb')\n                        if os.path.exists(design_structure_path):\n                            # Todo attach the program state to these files for downstream use?\n                            shutil.copy(design_structure_path, out_path)\n                        else:\n                            pose_job.log.error(f\"Expected file '{design_structure_path}' wasn't found for \"\n                                               f\"{design_structure_path}\")\n                        continue\n                    else:\n                        pose_job.log.error(f'No structure found for \"{design}\"')\n\n                    current_designs.append(design)\n\n                pose_job.current_designs = current_designs\n                results.append(pose_job)\n        else:\n            pose_ids, design_ids, design_identifier = zip(*load_design_identifier_from_id(session, selected_design_ids))\n            design_id_to_identifier = dict(zip(design_ids, design_identifier))\n            unique_pose_ids = utils.remove_duplicates(pose_ids)\n            pose_id_to_identifier = load_pose_identifier_from_id(session, unique_pose_ids)\n            if return_pose_jobs:\n                pose_job_stmt = select(PoseJob).where(PoseJob.id.in_(unique_pose_ids))\n                results = session.scalars(pose_job_stmt).all()\n                pose_id_to_design_ids = defaultdict(list)\n                for idx, pose_id_ in enumerate(pose_ids):\n                    pose_id_to_design_ids[pose_id_].append(design_ids[idx])\n\n                for pose_job in results:\n                    pose_job.current_designs = pose_id_to_design_ids[pose_job.id]\n            else:\n                results = pose_id_to_identifier.values()\n\n        # Todo incorporate design_metadata_df\n        design_metrics_df = load_sql_design_metrics_dataframe(session, design_ids=selected_design_ids)\n        design_metadata_df = load_sql_design_metadata_dataframe(session, design_ids=selected_design_ids)\n        if design_metadata_df.empty:\n            pass\n        else:\n            # designs_df has a multiplicity of number_of_entities from DesignEntityMetrics table join\n            design_metrics_df = \\\n                design_metadata_df.join(design_metrics_df.set_index(design_id), on=design_id, rsuffix='_DROP')\n        # Format selected PoseJob with metrics for output\n        # save_designs_df = selected_designs_df\n        save_designs_df = format_save_df(session, design_metrics_df,\n                                         selected_pose_id_to_design_ids.keys(),\n                                         design_ids=selected_design_ids\n                                         )\n    # End session\n\n    # No need to rename as the index aren't design_id\n    # save_designs_df.reset_index(col_fill='pose', col_level=-1, inplace=True)\n    # Rename the identifiers to human-readable names\n    save_designs_df[('pose', 'design_name')] = save_designs_df[('pose', design_id)].map(design_id_to_identifier)\n    # print('AFTER design_name', save_designs_df)\n    save_designs_df[('pose', 'pose_identifier')] = save_designs_df[('pose', pose_id)].map(pose_id_to_identifier)\n    # print('AFTER pose_identifier', save_designs_df)\n    save_designs_df.set_index([('pose', 'pose_identifier'), ('pose', 'design_name')], inplace=True)\n    save_designs_df.index.rename(['pose_identifier', 'design_name'], inplace=True)\n    # print('AFTER set_index', save_designs_df)\n\n    if job.filter or job.weight:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-{\"Filtered\" if job.filter else \"\"}'\n                                                           f'{\"Weighted\" if job.weight else \"\"}DesignMetrics.csv')\n    else:\n        new_dataframe = os.path.join(job.output_directory, f'{utils.starttime}-DesignMetrics.csv')\n    save_designs_df.to_csv(new_dataframe)\n    logger.info(f'New DataFrame with selected designs written to: {new_dataframe}')\n\n    return results  # , exceptions\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.solve_tags","title":"solve_tags","text":"<pre><code>solve_tags(n_of_tags: int, tag_entities: tagging_literal = None) -&gt; tuple[int, list[bool]]\n</code></pre> <p>Set up mechanism to solve sequence tagging preferences</p> <p>Parameters:</p> <ul> <li> <code>n_of_tags</code>             (<code>int</code>)         \u2013          <p>The number of taggable entities</p> </li> <li> <code>tag_entities</code>             (<code>tagging_literal</code>, default:                 <code>None</code> )         \u2013          <p>A specification of how the tagable entities could be tagged</p> </li> </ul> <p>Returns:     The number of tags requested and the indices in which the tags could be applied</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def solve_tags(n_of_tags: int, tag_entities: flags.tagging_literal = None) -&gt; tuple[int, list[bool]]:\n    \"\"\"Set up mechanism to solve sequence tagging preferences\n\n    Args:\n        n_of_tags: The number of taggable entities\n        tag_entities: A specification of how the tagable entities could be tagged\n    Returns:\n        The number of tags requested and the indices in which the tags could be applied\n    \"\"\"\n    if tag_entities is None:\n        boolean_tags = [False for _ in range(n_of_tags)]\n        n_of_tags = 0\n    elif tag_entities == 'all':\n        boolean_tags = [True for _ in range(n_of_tags)]\n    elif tag_entities == 'single':\n        boolean_tags = [True for _ in range(n_of_tags)]\n        n_of_tags = 1\n    else:\n        boolean_tags = []\n        for tag_specification in map(str.strip, job.tag_entities.split(',')):\n            # Remove non-numeric stuff\n            if tag_specification == '':  # Probably a trailing ',' ...\n                continue\n            else:\n                tag_specification.translate(utils.keep_digit_table)\n\n            try:  # To convert to an integer\n                boolean_tags.append(True if int(tag_specification) == 1 else False)\n            except ValueError:  # Not an integer False\n                boolean_tags.append(False)\n\n        # Add any missing arguments to the tagging scheme\n        for _ in range(n_of_tags - len(boolean_tags)):\n            boolean_tags.append(False)\n        n_of_tags = sum(boolean_tags)\n\n    return n_of_tags, boolean_tags\n</code></pre>"},{"location":"reference/protocols/select/#protocols.select.sql_sequences","title":"sql_sequences","text":"<pre><code>sql_sequences(pose_jobs: list[PoseJob]) -&gt; list[PoseJob]\n</code></pre> <p>Perform design selection followed by sequence formatting on those designs</p> <p>Parameters:</p> <ul> <li> <code>pose_jobs</code>             (<code>list[PoseJob]</code>)         \u2013          <p>The PoseJob instances for which selection is desired</p> </li> </ul> <p>Returns:     The matching PoseJob instances</p> Source code in <code>symdesign/protocols/select.py</code> <pre><code>def sql_sequences(pose_jobs: list[PoseJob]) -&gt; list[PoseJob]:\n    \"\"\"Perform design selection followed by sequence formatting on those designs\n\n    Args:\n        pose_jobs: The PoseJob instances for which selection is desired\n    Returns:\n        The matching PoseJob instances\n    \"\"\"\n    from dnachisel.DnaOptimizationProblem.NoSolutionError import NoSolutionError\n    job = job_resources_factory.get()\n    pose_jobs = sql_designs(pose_jobs, return_pose_jobs=True)\n    # Ensure each design has relevant database features loaded\n    with job.db.session(expire_on_commit=False) as session:\n        session.add_all(pose_jobs)\n        for pose_job in pose_jobs:\n            # for entity_data in pose_job.entity_data:\n            #     entity_data.metrics.number_of_residues\n            session.add_all(pose_job.current_designs)\n            for design in pose_job.current_designs:\n                design.metrics.sequence\n    # Set up output_file pose_jobs for __main__.terminate()\n    job.output_file = os.path.join(job.output_directory, 'SelectedDesigns.poses')\n\n    if job.multicistronic:\n        intergenic_sequence = job.multicistronic_intergenic_sequence\n    else:\n        intergenic_sequence = ''\n    if job.tag_entities:\n        if job.tag_linker:\n            tag_linker = job.tag_linker\n        else:\n            tag_linker = constants.default_tag_linker\n        logger.info(f\"Using the sequence '{tag_linker}' to link each protein sequence and the specified tag\")\n\n    # Format sequences for expression\n    alignment_length = 40\n    metrics_sequences = {}\n    tag_sequences = {}\n    final_sequences = {}\n    inserted_sequences = {}\n    nucleotide_sequences = {}\n    codon_optimization_errors = {}\n    for pose_job in pose_jobs:\n        pose_job.load_pose()\n        # Create the source_gap_mutations which provide mutation style dict for each gep\n        # from the reference to the structure sequence\n        entity_sequences = [entity.sequence for entity in pose_job.pose.entities]\n        source_gap_mutations = [generate_mutations(entity.reference_sequence, entity.sequence,\n                                                   zero_index=True, only_gaps=True)\n                                for entity in pose_job.pose.entities]\n        number_of_entities = pose_job.number_of_entities\n        number_of_tags_requested, entity_taggable_indices = solve_tags(number_of_entities, job.tag_entities)\n\n        # Find termini data\n        logger.info('Searching for solvent accessible termini')\n        entity_termini_availability = []\n        entity_helical_termini = []\n        entity_true_termini = []\n        for entity in pose_job.pose.entities:\n            pose_entity_id = f'{pose_job}_{entity.name}'\n            termini_availability = pose_job.pose.get_termini_accessibility(entity)\n            logger.debug(f'Designed Entity {pose_entity_id} has the accessible termini: {termini_availability}')\n            if job.avoid_tagging_helices:\n                termini_helix_availability = \\\n                    pose_job.pose.get_termini_accessibility(entity, report_if_helix=True)\n                logger.debug(f'Designed Entity {pose_entity_id} has helical termini available: '\n                             f'{termini_helix_availability}')\n                termini_availability = {'n': termini_availability['n'] and not termini_helix_availability['n'],\n                                        'c': termini_availability['c'] and not termini_helix_availability['c']}\n                entity_helical_termini.append(termini_helix_availability)\n\n            # Report and finalize for this Entity\n            logger.debug(f'Designed Entity {pose_entity_id} has the termini available for tagging: '\n                         f'{termini_availability}')\n            entity_termini_availability.append(termini_availability)\n            entity_true_termini.append([term for term, is_true in termini_availability.items() if is_true])\n\n        metrics_sequences[pose_job] = []\n        for design in pose_job.current_designs:\n            design_sequence = design.metrics.sequence\n            entity_number_residues_begin = entity_number_residues_end = 0\n            designed_atom_sequences = []\n            # Todo ensure can use without structure and Pose incase of sequence length change\n            # for entity_data in pose_job.entity_data:\n            for entity in pose_job.pose.entities:\n                # entity_number_residues_end += entity_data.metrics.number_of_residues\n                entity_number_residues_end += entity.number_of_residues\n                designed_atom_sequences.append(design_sequence[entity_number_residues_begin:entity_number_residues_end])\n                entity_number_residues_begin = entity_number_residues_end\n\n            # Loop over each Entity\n            entity_names = []\n            entity_sequence_and_tags = []\n            # Container of booleans, initialized where each Entity is missing a tag\n            entity_missing_tags = [True for _ in range(number_of_entities)]\n            for entity_idx, (data, source_sequence, design_sequence) in \\\n                    enumerate(zip(pose_job.entity_data, entity_sequences, designed_atom_sequences)):\n                # Generate the design TO source mutations before any disorder handling\n                # This will place design sequence identities in the 'from' position of mutations dictionary\n                entity_name = data.name\n                entity_names.append(entity_name)\n                mutations = generate_mutations(''.join(design_sequence), source_sequence, zero_index=True)\n                logger.debug(f'Found mutations: {mutations}')\n                # Make sequence as list instead of string to use list.insert()\n                inserted_design_sequence = list(design_sequence)\n                # Insert the disordered residues into the design sequence\n                for residue_index, mutation in source_gap_mutations[entity_idx].items():\n                    # residue_index is zero indexed\n                    new_aa_type = mutation['from']\n                    logger.debug(f'Inserting {new_aa_type} into index {residue_index} on Entity {entity_name}')\n                    # design_pose.insert_residue_type(residue_index, new_aa_type, chain_id=entity.chain_id)\n                    inserted_design_sequence.insert(residue_index, new_aa_type)\n                    # Adjust mutations to account for insertion\n                    for mutation_index in sorted(mutations.keys(), reverse=True):\n                        if mutation_index &lt; residue_index:\n                            break\n                        else:  # Mutation should be incremented by one\n                            mutations[mutation_index + 1] = mutations.pop(mutation_index)\n\n                # Check for expression tag addition to the designed sequences after disorder addition\n                inserted_design_sequence = ''.join(inserted_design_sequence)\n                logger.debug(f'The inserted design sequence is:\\n{inserted_design_sequence}')\n                selected_tag = {}\n                available_tags = expression.find_expression_tags(inserted_design_sequence)\n                if available_tags:\n                    # Look for existing tags, save and possibly select a tag\n                    tag_names, tag_termini, _ = \\\n                        zip(*[(tag['name'], tag['termini'], tag['sequence']) for tag in available_tags])\n                    try:\n                        preferred_tag_index = tag_names.index(job.preferred_tag)\n                    except ValueError:\n                        pass\n                    else:\n                        if tag_termini[preferred_tag_index] in entity_true_termini[entity_idx]:\n                            selected_tag = available_tags[preferred_tag_index]\n                    # Remove existing tags from sequence\n                    pretag_sequence = expression.remove_terminal_tags(inserted_design_sequence, tag_names)\n                    logger.debug(f'The sequence cleaned of tags is:\\n{pretag_sequence}')\n                else:\n                    pretag_sequence = inserted_design_sequence\n                    logger.debug(f'The pre-tagged sequence is the same as the inserted design sequence')\n\n                # Find the open reading frame offset using the structure sequence after insertion\n                offset = find_orf_offset(pretag_sequence, mutations)\n                logger.debug(f'The open reading frame offset index is {offset}')\n                if offset &gt;= 0:\n                    formatted_design_sequence = pretag_sequence[offset:]\n                    logger.debug(f'The formatted_design sequence is:\\n{formatted_design_sequence}')\n                else:  # Subtract the offset from the mutations\n                    # for mutation_index in sorted(mutations.keys(), reverse=True):\n                    #     mutations[mutation_index + offset] = mutations.pop(mutation_index)\n                    logger.debug('The offset is negative indicating non-reference sequence (such as tag linker '\n                                 'residues), were added to the n-termini')\n                    formatted_design_sequence = pretag_sequence\n\n                # Figure out tagging specification\n                if number_of_tags_requested == 0:  # Don't solve tags\n                    selected_tag = {}\n                # elif job.preferred_tag:\n                # else:\n                #     if not selected_tag:\n                #         # Find compatible tags from matching PDB observations\n                #         possible_matching_tags = []\n                #         id_matching_tags = tag_sequences.get(entity_name)\n                #         if id_matching_tags is None:\n                #             tag_sequences[entity_name] = id_matching_tags = \\\n                #                 expression.find_matching_expression_tags(entity_id=entity_name,\n                #                                                          alignment_length=alignment_length)\n                #         possible_matching_tags.extend(id_matching_tags)\n                #\n                #         for uniprot_id in data.uniprot_ids:\n                #             id_matching_tags = tag_sequences.get(uniprot_id)\n                #             if id_matching_tags is None:\n                #                 tag_sequences[uniprot_id] = id_matching_tags = \\\n                #                     expression.find_matching_expression_tags(uniprot_id=uniprot_id,\n                #                                                              alignment_length=alignment_length)\n                #             possible_matching_tags.extend(id_matching_tags)\n                #\n                #         if possible_matching_tags:\n                #             tag_names, tag_termini, _ = \\\n                #                 zip(*[(tag['name'], tag['termini'], tag['sequence'])\n                #                       for tag in possible_matching_tags])\n                #         else:\n                #             tag_names, tag_termini, _ = [], [], []\n                #\n                #         while True:\n                #             # Using the while loop to enable break and avoid expression.select_tags_for_sequence()\n                #             try:\n                #                 preferred_tag_index_2 = tag_names.index(job.preferred_tag)\n                #             except ValueError:  # job.preferred_tag not indexed\n                #                 pass\n                #             else:\n                #                 if tag_termini[preferred_tag_index_2] in entity_true_termini[entity_idx]:\n                #                     selected_tag = possible_matching_tags[preferred_tag_index_2]\n                #                     break\n                #             design_entity_id = f'{design.name}-{entity_name}'\n                #             selected_tag = \\\n                #                 expression.select_tags_for_sequence(design_entity_id,\n                #                                                     possible_matching_tags,\n                #                                                     preferred=job.preferred_tag,\n                #                                                     **entity_termini_availability[entity_idx])\n                #             break\n                #\n                #     if selected_tag.get('name'):\n                #         entity_missing_tags[entity_idx] = False\n                #         logger.debug(f'The pre-existing, identified tag is:\\n{selected_tag}')\n                entity_sequence_and_tags.append({'sequence': formatted_design_sequence, 'tag': selected_tag})\n\n            # After selecting individual Entity tags, consider tagging the whole Design\n            if number_of_tags_requested &gt; 0:\n                number_of_found_tags = number_of_entities - sum(entity_missing_tags)\n                # When fewer than the requested number of tags were identified\n                if number_of_tags_requested &gt; number_of_found_tags:\n                    print(f'There were {number_of_tags_requested} requested tags for {pose_job} Design {design.name} '\n                          f'and {number_of_found_tags} were found')\n                    header = 'Index', 'Name', 'Selected tag', 'Available termini'\n                    if job.avoid_tagging_helices:\n                        header += ('Helical termini',)\n                        helical_info = [(','.join(term for term, available in term_availablity.items() if available),)\n                                        for term_availablity in entity_helical_termini]\n                    else:\n                        helical_info = tuple()\n                    current_tag_options = \\\n                        '\\n\\t'.join(utils.pretty_format_table(\n                            [(idx + 1, entity_names[idx],\n                              seq_tag_options['tag'] if seq_tag_options['tag'].get('name') else None,\n                              ','.join(term for term, available in entity_termini_availability[idx].items()\n                                       if available)) + helical_info[idx]\n                             for idx, seq_tag_options in enumerate(entity_sequence_and_tags)],\n                            header=header))\n                    print(f'Existing Entity tagging options:\\n\\t{current_tag_options}')\n                    satisfied = utils.validate_input(\n                        \"Enter 'p' (proceed) to accept this tagging scheme, or 'c' (configure) to configure tags\",\n                        ['p', 'c'])\n                    if satisfied == 'p':\n                        number_of_found_tags = number_of_tags_requested\n\n                    iteration = count()\n                    while number_of_tags_requested != number_of_found_tags:\n                        iteration_idx = next(iteration)\n                        if iteration_idx == number_of_entities:\n                            print(\"You've seen all options, but the number of tags requested, \"\n                                  f'{number_of_tags_requested} != {number_of_found_tags}, the number of tags found')\n                            satisfied = utils.validate_input(\n                                \"If you are satisfied with this scheme, enter 'p' (proceed), otherwise enter 'c' \"\n                                '(configure), and you can view all options again starting with the first entity',\n                                ['p', 'c'])\n                            if satisfied == 'p':\n                                break\n                            else:  # Start over\n                                iteration = count()\n                                continue\n                        for entity_idx, entity_missing_tag in enumerate(entity_missing_tags[iteration_idx:]):\n                            entity_name = entity_names[entity_idx]\n                            if entity_missing_tag and entity_taggable_indices[entity_idx]:  # Isn't tagged but could be\n                                print(f'Entity {pose_job}_{entity_name} is missing a tag. '\n                                      f'Would you like to tag this entity?')\n                                if not boolean_choice():\n                                    continue\n                            else:\n                                continue\n                            # Solve by preferred_tag or user input\n                            if job.preferred_tag:\n                                tag = job.preferred_tag\n                            else:\n                                print('Tag options include:\\n\\t%s' %\n                                      '\\n\\t'.join([f'{idx} - {tag}' for idx, tag in enumerate(expression.tags, 1)]))\n                                tag_input = validate_input('Which of the above tags would you like to use? Enter the '\n                                                           'number of your preferred option',\n                                                           list(map(str, range(1, 1 + len(expression.tags)))))\n                                # Adjust for python indexing\n                                tag_index = int(tag_input) - 1\n                                tag = list(expression.tags.keys())[tag_index]\n                            # termini = validate_input(f\"Which termini should the selected tag '{tag}', be added to?\",\n                            #                          ['n', 'c'])\n                            # Find compatible tags from matching PDB observations\n                            possible_matching_tags = []\n                            id_matching_tags = tag_sequences.get(entity_name)\n                            if id_matching_tags is None:\n                                tag_sequences[entity_name] = id_matching_tags = \\\n                                    expression.find_matching_expression_tags(entity_id=entity_name,\n                                                                             alignment_length=alignment_length)\n                            possible_matching_tags.extend(id_matching_tags)\n\n                            for uniprot_id in pose_job.entity_data[entity_idx].uniprot_ids:\n                                id_matching_tags = tag_sequences.get(uniprot_id)\n                                if id_matching_tags is None:\n                                    tag_sequences[uniprot_id] = id_matching_tags = \\\n                                        expression.find_matching_expression_tags(uniprot_id=uniprot_id,\n                                                                                 alignment_length=alignment_length)\n                                possible_matching_tags.extend(id_matching_tags)\n                            termini = expression.report_termini_availability(possible_matching_tags,\n                                                                             **entity_termini_availability[entity_idx])\n                            if termini == 'skip':\n                                continue\n\n                            selected_sequence_and_tag = entity_sequence_and_tags[entity_idx]\n                            if termini == 'n':\n                                new_tag_sequence = expression.tags[tag] \\\n                                    + tag_linker + selected_sequence_and_tag['sequence'][:alignment_length]\n                            else:  # termini == 'c'\n                                new_tag_sequence = selected_sequence_and_tag['sequence'][-alignment_length:] \\\n                                    + tag_linker + expression.tags[tag]\n                            selected_sequence_and_tag['tag'] = \\\n                                {'name': tag, 'termini': termini, 'sequence': new_tag_sequence}\n                            entity_missing_tags[entity_idx] = False\n                            break\n\n                        number_of_found_tags = number_of_entities - sum(entity_missing_tags)\n                # When more than the requested number of tags were identified\n                elif number_of_tags_requested &lt; number_of_found_tags:\n                    print(f'There were only {number_of_tags_requested} requested tags for design {pose_job}, however, '\n                          f'{number_of_found_tags} were found')\n                    print('Configured tags:')\n                    print('\\t%s' % '\\n\\t'.join([f'{idx + 1} - {entity_names[idx]}\\n\\t\\t{tag_options[\"tag\"]}'\n                                                for idx, tag_options in enumerate(entity_sequence_and_tags)]))\n                    while number_of_tags_requested != number_of_found_tags:\n                        tag_input = utils.validate_input(\n                            'Which tag would you like to remove? Enter a number from the above tag options or, if you '\n                            \"would like to keep all, specify 'keep'\",\n                            list(map(str, range(1, 1 + number_of_found_tags))) + ['keep'])\n                        if tag_input == 'keep':\n                            break\n                        else:  # if tag_input.isdigit():\n                            tag_index = int(tag_input) - 1\n                            # if tag_input &lt;= len(entity_sequence_and_tags):\n                            # Set that this entity is now missing a tag\n                            entity_missing_tags[tag_index] = True\n                            entity_sequence_and_tags[tag_index]['tag'] = \\\n                                {'name': None, 'termini': None, 'sequence': None}\n\n                        number_of_found_tags = number_of_entities - sum(entity_missing_tags)\n\n            # Apply all tags to the sequences\n            cistronic_sequence = ''\n            sequences_for_metrics = []\n            for idx, (entity_name, sequence_tag) in enumerate(zip(entity_names, entity_sequence_and_tags)):\n                design_string = f'{design.name}_{entity_name}'\n                sequence = sequence_tag['sequence']\n                tag = sequence_tag['tag']\n                chimeric_tag_sequence = tag.get('sequence')\n\n                # tagged_sequence = expression.add_expression_tag(chimeric_tag_sequence, sequence)\n                if chimeric_tag_sequence:  # A tag exists\n                    # if tagged_sequence == sequence:  # No tag added\n                    #     tag_sequence = expression.tags[tag['name']]\n                    #     if tag.get('termini') == 'n':\n                    #         if tagged_sequence[0] == 'M':  # Remove existing n-term Met to append tag to n-term\n                    #             tagged_sequence = tagged_sequence[1:]\n                    #         tagged_sequence = tag_sequence + tag_linker + tagged_sequence\n                    #     else:  # termini == 'c'\n                    #         tagged_sequence = tagged_sequence + tag_linker + tag_sequence\n                    # else:\n                    logger.debug(f'Cleaning chimeric tag sequence: {chimeric_tag_sequence}')\n                    tag_termini = tag['termini']\n                    chimeric_tag_sequence = expression.remove_terminal_tags(chimeric_tag_sequence, termini=tag_termini)\n                    tag_sequence = expression.tags[tag['name']]\n                    if tag_termini == 'n':\n                        chimeric_tag_sequence = tag_sequence + chimeric_tag_sequence\n                    else:\n                        chimeric_tag_sequence += tag_sequence\n\n                    logger.debug(f'Applying cleaned chimeric tag sequence: {chimeric_tag_sequence}')\n                    if tag_linker:  # and tag_linker not in chimeric_tag_sequence:\n                        # Add the linker between the tag and designed sequence\n                        tag_insert_index = chimeric_tag_sequence.find(tag_sequence)\n                        slice_count = count(1)\n                        slice_idx = next(slice_count)\n                        if tag_termini == 'n':\n                            # Insert the index from the c-term side\n                            tag_insert_index += len(tag_sequence)\n                            # for i in range(1, len(tag_linker)):\n                            #     if chimeric_tag_sequence[tag_insert_index:].startswith(tag_linker[-i:]):\n                            while chimeric_tag_sequence[tag_insert_index:].startswith(tag_linker[-slice_idx:]):\n                                slice_idx = next(slice_count)\n                            else:  # Subtract 1 from the index and slice the tag_linker\n                                slice_idx = (slice_idx-1) * -1 if slice_idx &gt; 1 else None\n                                this_tag_linker = tag_linker[:slice_idx]\n                        else:\n                            while chimeric_tag_sequence[:tag_insert_index].endswith(tag_linker[:slice_idx]):\n                                slice_idx = next(slice_count)\n                            else:  # Subtract 1 from the index and slice the tag_linker\n                                this_tag_linker = tag_linker[slice_idx - 1:]\n\n                        chimeric_tag_sequence = chimeric_tag_sequence[:tag_insert_index] + this_tag_linker \\\n                            + chimeric_tag_sequence[tag_insert_index:]\n                        logger.debug(f'Formatted the chimeric tag sequence with the specified linker:'\n                                     f' {chimeric_tag_sequence}')\n\n                    tagged_sequence = expression.add_expression_tag(chimeric_tag_sequence, sequence)\n                else:\n                    tag_name = tag.get('name')\n                    if tag_name:\n                        tag_sequence = expression.tags[tag_name]\n                        if tag['termini'] == 'n':\n                            if sequence[0] == 'M':  # Remove existing n-term Met to append tag to n-term\n                                sequence = sequence[1:]\n                            tagged_sequence = tag_sequence + tag_linker + sequence\n                        else:  # termini == 'c'\n                            tagged_sequence = sequence + tag_linker + tag_sequence\n                    else:\n                        tagged_sequence = sequence\n\n                # If no MET start site, include one\n                if tagged_sequence[0] != 'M':\n                    tagged_sequence = f'M{tagged_sequence}'\n\n                # If there is an unrecognized amino acid, modify\n                unknown_char = 'X'\n                if unknown_char in tagged_sequence:\n                    logger.critical(f'An unrecognized amino acid was specified in the sequence {design_string}. '\n                                    'This requires manual intervention!')\n                    # idx = 0\n                    seq_length = len(tagged_sequence)\n                    while True:\n                        missing_idx = tagged_sequence.find(unknown_char)\n                        if missing_idx == -1:\n                            break\n                        low_idx = missing_idx - 6 if missing_idx - 6 &gt; 0 else 0\n                        high_idx = missing_idx + 6 if missing_idx + 6 &lt; seq_length else seq_length\n                        print(f'Which amino acid should be swapped for \"{unknown_char}\" in this sequence context?\\n'\n                              f'\\t{low_idx + 1}{\" \" * (missing_idx-low_idx-len(str(low_idx)))}|'\n                              f'{\" \" * (high_idx-missing_idx-2)}{high_idx + 1}'  # Subtract 2 for slicing and high_idx\n                              # f'{\" \" * (high_idx-low_idx - (len(str(low_idx))+1))}{high_idx + 1}'\n                              f'\\n\\t{tagged_sequence[low_idx:high_idx]}')\n                        new_amino_acid = validate_input(input_string, protein_letters_alph1)\n                        tagged_sequence = tagged_sequence[:missing_idx] \\\n                            + new_amino_acid + tagged_sequence[missing_idx + 1:]\n\n                # For a final manual check of sequence generation, find sequence additions compared to the design\n                # model and save to view where additions lie on sequence. Cross these additions with design\n                # structure to check if insertions are compatible\n                # all_insertions = {residue: {'to': aa} for residue, aa in enumerate(tagged_sequence)}\n                # all_insertions.update(generate_mutations(design_sequence, ''.join(designed_atom_sequences[idx]),\n                #                                          keep_gaps=True))\n                # generated_insertion_mutations = \\\n                #     generate_mutations(tagged_sequence, ''.join(designed_atom_sequences[idx]),\n                #                        keep_gaps=True, zero_index=True)\n                # logger.debug(f'generated_insertion_mutations: {generated_insertion_mutations}')\n                # all_insertions.update(generated_insertion_mutations)\n                # formatted_comparison = {}\n                # for mutation_index in sorted(all_insertions.keys()):\n                generated_insertion_mutations = \\\n                    generate_mutations(tagged_sequence, designed_atom_sequences[idx],\n                                       return_all=True, keep_gaps=True, zero_index=True)\n                # for mutations in generated_insertion_mutations.values():\n                #     reference = mutations['from']\n                #     query = mutations['to']\n\n                # Reduce to sequence only\n                inserted_sequences[design_string] = \\\n                    f'Expressed: {\"\".join([res[\"from\"] for res in generated_insertion_mutations.values()])}\\n' \\\n                    f'Designed : {\"\".join([res[\"to\"] for res in generated_insertion_mutations.values()])}'\n                # # Reduce to sequence only\n                # inserted_sequences[design_string] = \\\n                #     f'{\"\".join([res[\"to\"] for res in all_insertions.values()])}\\n{tagged_sequence}'\n                logger.info(f'Formatted sequence comparison:\\n{inserted_sequences[design_string]}')\n                final_sequences[design_string] = tagged_sequence\n                sequences_for_metrics.append(tagged_sequence)\n                if job.nucleotide:\n                    try:\n                        nucleotide_sequence = \\\n                            optimize_protein_sequence(tagged_sequence, species=job.optimize_species)\n                    except NoSolutionError:  # Add the protein sequence?\n                        logger.warning(f\"Optimization of {design_string} wasn't successful\")\n                        codon_optimization_errors[design_string] = tagged_sequence\n                        break\n\n                    if job.multicistronic:\n                        if idx &gt; 0:\n                            cistronic_sequence += intergenic_sequence\n                        cistronic_sequence += nucleotide_sequence\n                    else:\n                        nucleotide_sequences[design_string] = nucleotide_sequence\n            # Finish processing for the design\n            metrics_sequences[pose_job].append(sequences_for_metrics)\n            if job.multicistronic:\n                nucleotide_sequences[str(pose_job)] = cistronic_sequence\n        # Clear memory of the PoseJob\n        pose_job.clear_state()\n\n    # Format expression sequence metrics\n    sequence_metrics = {}\n    for pose_job, designs_sequences in metrics_sequences.items():\n        pose_job.load_pose()\n        pose_radius_of_gyration = pose_job.pose.assembly.radius_of_gyration\n        for design, design_sequences in zip(pose_job.current_designs, designs_sequences):\n            # Iterate over each Entity\n            pose_sequence = ''\n            for entity_idx, sequence in enumerate(design_sequences, 1):\n                entity_sequence_features = expression.get_sequence_features(sequence)\n                sequence_metrics[(pose_job.pose_identifier, design.name, entity_idx)] = entity_sequence_features\n                pose_sequence += sequence\n            pose_sequence_features = expression.get_sequence_features(pose_sequence)\n            pose_sequence_features['radius_of_gyration'] = pose_radius_of_gyration\n            sequence_metrics[(pose_job.pose_identifier, design.name, 'pose')] = pose_sequence_features\n    # Format DataFrame and save metrics\n    sequence_metrics_df = pd.DataFrame(sequence_metrics.values(),\n                                       index=pd.MultiIndex.from_tuples(sequence_metrics.keys()))\n    sequence_metrics_df = sequence_metrics_df.unstack(-1).swaplevel(axis=1)\n    sequence_metrics_filename = os.path.join(job.output_directory, 'SequenceExpressionMetrics.csv')\n    sequence_metrics_df.to_csv(sequence_metrics_filename)\n    logger.info(f'Biochemical protein sequence metrics written to: {sequence_metrics_filename}')\n\n    # Report Errors\n    if codon_optimization_errors:\n        # Todo utilize errors\n        error_file = \\\n            write_sequences(codon_optimization_errors, csv=job.csv,\n                            file_name=os.path.join(job.output_directory, 'OptimizationErrorProteinSequences'))\n    # Write output sequences to fasta file\n    seq_file = write_sequences(final_sequences, csv=job.csv,\n                               file_name=os.path.join(job.output_directory, 'SelectedSequences'))\n    logger.info(f'Protein designed sequences written to: {seq_file}')\n    seq_comparison_file = \\\n        write_sequences(inserted_sequences, csv=job.csv,\n                        file_name=os.path.join(job.output_directory, 'SelectedSequencesExpressionAdditions'))\n    logger.info(f'Protein expression sequence comparison to designed sequences written to: {seq_comparison_file}')\n    # check for protein or nucleotide output\n    if job.nucleotide:\n        nucleotide_sequence_file = \\\n            write_sequences(nucleotide_sequences, csv=job.csv,\n                            file_name=os.path.join(job.output_directory, 'SelectedSequencesNucleotide'))\n        logger.info(f'Nucleotide designed sequences written to: {nucleotide_sequence_file}')\n\n    return pose_jobs\n</code></pre>"},{"location":"reference/protocols/utils/","title":"utils","text":""},{"location":"reference/protocols/utils/#protocols.utils.close_logs","title":"close_logs","text":"<pre><code>close_logs(func: Callable)\n</code></pre> <p>Wrap a function/method to close the functions first arguments .log attribute FileHandlers after use</p> Source code in <code>symdesign/protocols/utils.py</code> <pre><code>def close_logs(func: Callable):\n    \"\"\"Wrap a function/method to close the functions first arguments .log attribute FileHandlers after use\"\"\"\n    @functools.wraps(func)\n    def wrapped(job, *args, **kwargs):\n        func_return = func(job, *args, **kwargs)\n        # Adapted from https://stackoverflow.com/questions/15435652/python-does-not-release-filehandles-to-logfile\n        for handler in job.log.handlers:\n            handler.close()\n        return func_return\n    return wrapped\n</code></pre>"},{"location":"reference/protocols/utils/#protocols.utils.remove_structure_memory","title":"remove_structure_memory","text":"<pre><code>remove_structure_memory(func)\n</code></pre> <p>Decorator to remove large memory attributes from the instance after processing is complete</p> Source code in <code>symdesign/protocols/utils.py</code> <pre><code>def remove_structure_memory(func):\n    \"\"\"Decorator to remove large memory attributes from the instance after processing is complete\"\"\"\n    @functools.wraps(func)\n    def wrapped(job, *args, **kwargs):\n        func_return = func(job, *args, **kwargs)\n        if job.job.reduce_memory:\n            job.clear_state()\n        return func_return\n    return wrapped\n</code></pre>"},{"location":"reference/protocols/utils/#protocols.utils.handle_design_errors","title":"handle_design_errors","text":"<pre><code>handle_design_errors(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable\n</code></pre> <p>Wrap a function/method with try: except errors: and log exceptions to the functions first argument .log attribute</p> <p>This argument is typically self and is in a class with .log attribute</p> <p>Parameters:</p> <ul> <li> <code>errors</code>             (<code>tuple[Type[Exception], ...]</code>, default:                 <code>catch_exceptions</code> )         \u2013          <p>A tuple of exceptions to monitor. Must be a tuple even if single exception</p> </li> </ul> <p>Returns:     Function return upon proper execution, else is error if exception raised, else None</p> Source code in <code>symdesign/protocols/utils.py</code> <pre><code>def handle_design_errors(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable:\n    \"\"\"Wrap a function/method with try: except errors: and log exceptions to the functions first argument .log attribute\n\n    This argument is typically self and is in a class with .log attribute\n\n    Args:\n        errors: A tuple of exceptions to monitor. Must be a tuple even if single exception\n    Returns:\n        Function return upon proper execution, else is error if exception raised, else None\n    \"\"\"\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        def wrapped(job, *args, **kwargs) -&gt; Any:\n            try:\n                return func(job, *args, **kwargs)\n            except errors as error:\n                # Perform exception reporting using self.log\n                job.report_exception(context=func.__name__)\n                return ReportException(str(error))\n        return wrapped\n    return wrapper\n</code></pre>"},{"location":"reference/protocols/utils/#protocols.utils.handle_job_errors","title":"handle_job_errors","text":"<pre><code>handle_job_errors(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable\n</code></pre> <p>Wrap a function/method with try/except <code>errors</code></p> <p>Parameters:</p> <ul> <li> <code>errors</code>             (<code>tuple[Type[Exception], ...]</code>, default:                 <code>catch_exceptions</code> )         \u2013          <p>A tuple of exceptions to monitor. Must be a tuple even if single exception</p> </li> </ul> <p>Returns:     Function return upon proper execution, else is error if exception raised, else None</p> Source code in <code>symdesign/protocols/utils.py</code> <pre><code>def handle_job_errors(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable:\n    \"\"\"Wrap a function/method with try/except `errors`\n\n    Args:\n        errors: A tuple of exceptions to monitor. Must be a tuple even if single exception\n    Returns:\n        Function return upon proper execution, else is error if exception raised, else None\n    \"\"\"\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        def wrapped(*args, **kwargs) -&gt; list[Any]:\n            try:\n                return func(*args, **kwargs)\n            except errors as error:\n                # Perform exception reporting\n                return [ReportException(str(error))]\n        return wrapped\n    return wrapper\n</code></pre>"},{"location":"reference/protocols/utils/#protocols.utils.protocol_decorator","title":"protocol_decorator","text":"<pre><code>protocol_decorator(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable\n</code></pre> <p>Wrap a function/method with try: except errors: and log exceptions to the functions first argument .log attribute</p> <p>This argument is typically self and is in a class with .log attribute</p> <p>Parameters:</p> <ul> <li> <code>errors</code>             (<code>tuple[Type[Exception], ...]</code>, default:                 <code>catch_exceptions</code> )         \u2013          <p>A tuple of exceptions to monitor. Must be a tuple even if single exception</p> </li> </ul> <p>Returns:     Function return upon proper execution, else is error if exception raised, else None</p> Source code in <code>symdesign/protocols/utils.py</code> <pre><code>def protocol_decorator(errors: tuple[Type[Exception], ...] = catch_exceptions) -&gt; Callable:\n    \"\"\"Wrap a function/method with try: except errors: and log exceptions to the functions first argument .log attribute\n\n    This argument is typically self and is in a class with .log attribute\n\n    Args:\n        errors: A tuple of exceptions to monitor. Must be a tuple even if single exception\n    Returns:\n        Function return upon proper execution, else is error if exception raised, else None\n    \"\"\"\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        def wrapped(job, *args, **kwargs) -&gt; Any:\n            # Todo\n            #  Ensure that the below setting doesn't conflict with PoseJob inherent setting\n            #  job.protocol = job.job.module\n            # distribute_protocol()\n            if job.job.distribute_work:\n                # Skip any execution, instead create the command and add as job.current_script attribute\n                base_cmd = list(putils.program_command_tuple) + job.job.get_parsed_arguments()\n                base_cmd += ['--single', job.pose_directory]\n                # cmd, *additional_cmds = getattr(job, f'get_cmd_{job.protocol}')()\n                os.makedirs(job.scripts_path, exist_ok=True)\n                job.current_script = distribute.write_script(\n                    list2cmdline(base_cmd), name=f'{starttime}_{job.job.module}.sh', out_path=job.scripts_path,\n                    # additional=[list2cmdline(_cmd) for _cmd in additional_cmds]\n                )\n                return None\n\n            logger.info(f'Processing {func.__name__}({repr(job)})')\n            # handle_design_errors()\n            try:\n                func_return = func(job, *args, **kwargs)\n            except errors as error:\n                # Perform exception reporting using job.log\n                job.log.error(error)\n                job.log.info(''.join(traceback.format_exc()))\n                func_return = ReportException(str(error))\n            # remove_structure_memory()\n            if job.job.reduce_memory:\n                job.clear_state()\n            job.protocol = None\n            # close_logs()\n            # Adapted from https://stackoverflow.com/questions/15435652/python-does-not-release-filehandles-to-logfile\n            for handler in job.log.handlers:\n                handler.close()\n\n            return func_return\n        return wrapped\n    return wrapper\n</code></pre>"},{"location":"reference/resources/","title":"resources","text":""},{"location":"reference/resources/#resources.OptimalTx","title":"OptimalTx","text":"<pre><code>OptimalTx(dof_ext: ndarray = None, zshift1: ndarray = None, zshift2: ndarray = None, max_z_value: float = 1.0, number_of_coordinates: int = 3)\n</code></pre> <p>Identifies the translation(s) that optimally satisfies two overlapping translational spaces</p> <p>Parameters:</p> <ul> <li> <code>dof_ext</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The degrees of freedom that are external to the two translational spaces</p> </li> <li> <code>zshift1</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The translational shift of the first space</p> </li> <li> <code>zshift2</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The translational shift of the second space</p> </li> <li> <code>max_z_value</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The largest z-value considered acceptable</p> </li> <li> <code>number_of_coordinates</code>             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>The dimension of the search</p> </li> </ul> Source code in <code>symdesign/resources/__init__.py</code> <pre><code>def __init__(self, dof_ext: np.ndarray = None, zshift1: np.ndarray = None, zshift2: np.ndarray = None,\n             max_z_value: float = 1., number_of_coordinates: int = 3):\n    \"\"\"Construct the instance\n\n    Args:\n        dof_ext: The degrees of freedom that are external to the two translational spaces\n        zshift1: The translational shift of the first space\n        zshift2: The translational shift of the second space\n        max_z_value: The largest z-value considered acceptable\n        number_of_coordinates: The dimension of the search\n    \"\"\"\n    self.max_z_value = max_z_value\n    self.number_of_coordinates = number_of_coordinates\n    if dof_ext is None:  # Todo include np.zeros((1, 3)) like in SymEntry\n        raise ValueError(f\"Can't initialize {type(self.__name__)} without passing dof_ext\")\n    else:\n        self.dof_ext = dof_ext  # External translational DOF with shape (number DOF external, 3)\n    self.dof = self.dof_ext.copy()\n    # logger.debug('self.dof', self.dof)\n    self.zshift1 = zshift1  # internal translational DOF1\n    self.zshift2 = zshift2  # internal translational DOF2\n    self.dof9 = None\n    self.dof9_t = None\n    self.dof9t_dof9 = None\n\n    # Add internal z-shift degrees of freedom to 9-dim arrays if they exist\n    self.n_dof_internal = 0\n    if self.zshift1 is not None:\n        # logger.debug('self.zshift1', self.zshift1)\n        self.dof = np.append(self.dof, -self.zshift1, axis=0)\n        self.n_dof_internal += 1\n    if self.zshift2 is not None:\n        self.dof = np.append(self.dof, self.zshift2, axis=0)\n        self.n_dof_internal += 1\n    # logger.debug('self.dof', self.dof)\n\n    # Get the length of the array as n_dof_external\n    self.n_dof_external = len(self.dof_ext)\n    self.n_dof = len(self.dof)\n    if self.n_dof &gt; 0:\n        self.dof_convert9()\n    else:\n        raise ValueError(f\"n_dof isn't set! Can't get the {self.__class__.__name__}\"\n                         \" without passing dof_ext, zshift1, or zshift2\")\n</code></pre>"},{"location":"reference/resources/#resources.OptimalTx.dof_convert9","title":"dof_convert9","text":"<pre><code>dof_convert9()\n</code></pre> <p>Convert input degrees of freedom to 9-dim arrays. Repeat DOF ext for each set of 3 coordinates (3 sets)</p> Source code in <code>symdesign/resources/__init__.py</code> <pre><code>def dof_convert9(self):\n    \"\"\"Convert input degrees of freedom to 9-dim arrays. Repeat DOF ext for each set of 3 coordinates (3 sets)\"\"\"\n    self.dof9_t = np.zeros((self.n_dof, 9))\n    for dof_idx in range(self.n_dof):\n        # self.dof9_t[dof_idx] = np.array(self.number_of_coordinates * [self.dof[dof_idx]]).flatten()\n        self.dof9_t[dof_idx] = np.tile(self.dof[dof_idx], self.number_of_coordinates)\n        # dof[dof_idx] = (np.array(3 * [self.dof_ext[dof_idx]])).flatten()\n    # logger.debug('self.dof9_t', self.dof9_t)\n    self.dof9 = np.transpose(self.dof9_t)\n    # logger.debug('self.dof9', self.dof9)\n    self.dof9t_dof9 = np.matmul(self.dof9_t, self.dof9)\n</code></pre>"},{"location":"reference/resources/#resources.OptimalTx.solve_optimal_shift","title":"solve_optimal_shift","text":"<pre><code>solve_optimal_shift(coords1: ndarray, coords2: ndarray, coords_rmsd_reference: float) -&gt; ndarray | None\n</code></pre> <p>This routine solves the optimal shift problem for overlapping a pair of coordinates and comparing to a reference RMSD to compute an error</p> <p>Parameters:</p> <ul> <li> <code>coords1</code>             (<code>ndarray</code>)         \u2013          <p>A 3 x 3 array with cartesian coordinates</p> </li> <li> <code>coords2</code>             (<code>ndarray</code>)         \u2013          <p>A 3 x 3 array with cartesian coordinates</p> </li> <li> <code>coords_rmsd_reference</code>             (<code>float</code>)         \u2013          <p>The reference deviation to compare to the coords1 and coords2 error</p> </li> </ul> <p>Returns:     Returns the optimal translation or None if error is too large.         Optimal translation has external dof first, followed by internal tx dof</p> Source code in <code>symdesign/resources/__init__.py</code> <pre><code>def solve_optimal_shift(self, coords1: np.ndarray, coords2: np.ndarray, coords_rmsd_reference: float) -&gt; \\\n        np.ndarray | None:\n    \"\"\"This routine solves the optimal shift problem for overlapping a pair of coordinates and comparing to a\n    reference RMSD to compute an error\n\n    Args:\n        coords1: A 3 x 3 array with cartesian coordinates\n        coords2: A 3 x 3 array with cartesian coordinates\n        coords_rmsd_reference: The reference deviation to compare to the coords1 and coords2 error\n    Returns:\n        Returns the optimal translation or None if error is too large.\n            Optimal translation has external dof first, followed by internal tx dof\n    \"\"\"\n    # form the guide coords into a matrix (column vectors)\n    # guide_target_10 = np.transpose(coords1)\n    # guide_query_10 = np.transpose(coords2)\n\n    # calculate the initial difference between query and target (9 dim vector)\n    # With the transpose and the flatten, it could be accomplished by normal flatten!\n    guide_delta = np.transpose([coords1.flatten() - coords2.flatten()])\n    # flatten column vector matrix above [[x, y, z], [x, y, z], [x, y, z]] -&gt; [x, y, z, x, y, z, x, y, z], then T\n\n    # # isotropic case based on simple rmsd\n    # | var_tot_inv = np.zeros([9, 9])\n    # | for i in range(9):\n    # |     # fill in var_tot_inv with 1/ 3x the mean squared deviation (deviation sum)\n    # |     var_tot_inv[i, i] = 1. / (float(self.number_of_coordinates) * coords_rmsd_reference ** 2)\n    # can be simplified to just use the scalar\n    var_tot = float(self.number_of_coordinates) * coords_rmsd_reference ** 2\n\n    # solve the problem using 9-dim degrees of freedom arrays\n    # self.dof9 is column major (9 x n_dof_ext) degree of freedom matrix\n    # self.dof9_t transpose (row major: n_dof_ext x 9)\n    # below is degrees_of_freedom / variance\n    # | dinvv = np.matmul(var_tot_inv, self.dof9)  # 1/variance (9 x 9) x degree of freedom (9 x n_dof) = (9 x n_dof)\n    # dinvv = self.dof9 / var_tot\n    # below, each i, i is the (individual_dof^2) * 3 / variance. i, j is the (covariance of i and jdof * 3) / variance\n    # | vtdinvv = np.matmul(self.dof9_t, dinvv)  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n    # above could be simplifed to vtdinvv = np.matmul(self.dof9_t, self.dof9) / var_tot_inv  # first part same for each guide coord\n    # now done below\n    # vtdinvv = np.matmul(self.dof9_t, self.dof9) / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n    vtdinvv = self.dof9t_dof9 / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n    vtdinvvinv = np.linalg.inv(vtdinvv)  # Inverse of above - (n_dof x n_dof)\n    # below is guide atom difference / variance\n    # | dinvdelta = np.matmul(var_tot_inv, guide_delta)  # 1/variance (9 x 9) x guide atom diff (9 x 1) = (9 x 1)\n    # dinvdelta = guide_delta / var_tot\n    # below is essentially (SUM(dof basis * guide atom basis difference) for each guide atom) /variance by each DOF\n    # | vtdinvdelta = np.matmul(self.dof9_t, dinvdelta)  # transpose of degrees of freedom (n_dof x 9) x (9 x 1) = (n_dof x 1)\n    vtdinvdelta = np.matmul(self.dof9_t, guide_delta) / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x 1) = (n_dof x 1)\n\n    # below is inverse dof covariance matrix/variance * dof guide_atom_delta sum / variance\n    # | shift = np.matmul(vtdinvvinv, vtdinvdelta)  # (n_dof x n_dof) x (n_dof x 1) = (n_dof x 1)\n    shift = np.matmul(vtdinvvinv, vtdinvdelta)  # (n_dof x n_dof) x (n_dof x 1) = (n_dof x 1)\n\n    # get error value from the ideal translation and the delta\n    resid = np.matmul(self.dof9, shift) - guide_delta  # (9 x n_dof) x (n_dof x 1) - (9 x 1) = (9 x 1)\n    error = \\\n        np.sqrt(np.matmul(np.transpose(resid), resid) / float(self.number_of_coordinates)) / coords_rmsd_reference\n    # NEW. Is float(3.0) a scale?\n    # OLD. sqrt(variance / 3) / cluster_rmsd\n\n    if error &lt;= self.max_z_value:\n        return shift[:, 0]  # .tolist()  # , error\n    else:\n        return None\n</code></pre>"},{"location":"reference/resources/#resources.OptimalTx.solve_optimal_shifts","title":"solve_optimal_shifts","text":"<pre><code>solve_optimal_shifts(coords1: ndarray, coords2: ndarray, coords_rmsd_reference: ndarray) -&gt; ndarray\n</code></pre> <p>This routine solves the optimal shift problem for overlapping a pair of coordinates and comparing to a reference RMSD to compute an error</p> <p>Parameters:</p> <ul> <li> <code>coords1</code>             (<code>ndarray</code>)         \u2013          <p>A N x 3 x 3 array with cartesian coordinates</p> </li> <li> <code>coords2</code>             (<code>ndarray</code>)         \u2013          <p>A N x 3 x 3 array with cartesian coordinates</p> </li> <li> <code>coords_rmsd_reference</code>             (<code>ndarray</code>)         \u2013          <p>Array with length N with reference deviation to compare to the coords1 and coords2 error</p> </li> </ul> <p>Returns:     Returns the optimal translations with shape (N, number_degrees_of_freedom) if the translation is less than         the calculated error. Axis 1 has degrees of freedom with external first, then internal dof</p> Source code in <code>symdesign/resources/__init__.py</code> <pre><code>def solve_optimal_shifts(self, coords1: np.ndarray, coords2: np.ndarray, coords_rmsd_reference: np.ndarray) -&gt; \\\n        np.ndarray:\n    \"\"\"This routine solves the optimal shift problem for overlapping a pair of coordinates and comparing to a\n    reference RMSD to compute an error\n\n    Args:\n        coords1: A N x 3 x 3 array with cartesian coordinates\n        coords2: A N x 3 x 3 array with cartesian coordinates\n        coords_rmsd_reference: Array with length N with reference deviation to compare to the coords1 and coords2\n            error\n    Returns:\n        Returns the optimal translations with shape (N, number_degrees_of_freedom) if the translation is less than\n            the calculated error. Axis 1 has degrees of freedom with external first, then internal dof\n    \"\"\"\n    # calculate the initial difference between each query and target (9 dim vector by coords.shape[0])\n    guide_delta = (coords1 - coords2).reshape(-1, 1, 9).swapaxes(-2, -1)\n    # flatten column vector matrix above [[x, y, z], [x, y, z], [x, y, z]] -&gt; [x, y, z, x, y, z, x, y, z], then T\n    # # isotropic case based on simple rmsd\n    # | var_tot_inv = np.zeros([9, 9])\n    # | for i in range(9):\n    # |     # fill in var_tot_inv with 1/ 3x the mean squared deviation (deviation sum)\n    # |     var_tot_inv[i, i] = 1. / (float(self.number_of_coordinates) * coords_rmsd_reference ** 2)\n    # can be simplified to just use the scalar\n    var_tot = (float(self.number_of_coordinates) * coords_rmsd_reference ** 2).reshape(-1, 1, 1)\n\n    # solve the problem using 9-dim degrees of freedom arrays\n    # self.dof9 is column major (9 x n_dof_ext) degree of freedom matrix\n    # self.dof9_t transpose (row major: n_dof_ext x 9)\n    # below is degrees_of_freedom / variance\n    # | dinvv = np.matmul(var_tot_inv, self.dof9)  # 1/variance (9 x 9) x degree of freedom (9 x n_dof) = (9 x n_dof)\n    # dinvv = self.dof9 / var_tot\n    # below, each i, i is the (individual_dof^2) * 3 / variance. i, j is the (covariance of i and jdof * 3) / variance\n    # | vtdinvv = np.matmul(self.dof9_t, dinvv)  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n    # above could be simplifed to vtdinvv = np.matmul(self.dof9_t, self.dof9) / var_tot_inv  # first part same for each guide coord\n    # now done below\n    # vtdinvv = np.matmul(self.dof9_t, self.dof9) / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n    # vtdinvv = np.tile(self.dof9t_dof9, (coords1.shape[0], 1, 1)) / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x n_dof) = (n_dof x n_dof)\n\n    # vtdinvvinv = np.linalg.inv(vtdinvv)  # Inverse of above - (n_dof x n_dof)\n    # below is guide atom difference / variance\n    # | dinvdelta = np.matmul(var_tot_inv, guide_delta)  # 1/variance (9 x 9) x guide atom diff (9 x 1) = (9 x 1)\n    # dinvdelta = guide_delta / var_tot\n    # below is essentially (SUM(dof basis * guide atom basis difference) for each guide atom) /variance by each DOF\n    # | vtdinvdelta = np.matmul(self.dof9_t, dinvdelta)  # transpose of degrees of freedom (n_dof x 9) x (9 x 1) = (n_dof x 1)\n    # vtdinvdelta = np.matmul(np.tile(self.dof9_t, (coords1.shape[0], 1, 1)), guide_delta) / var_tot  # transpose of degrees of freedom (n_dof x 9) x (9 x 1) = (n_dof x 1)\n\n    # below is inverse dof covariance matrix/variance * dof guide_atom_delta sum / variance\n    # shift = np.matmul(vtdinvvinv, vtdinvdelta)  # (n_dof x n_dof) x (n_dof x 1) = (n_dof x 1)\n    # print('self.dof9t_dof9', self.dof9t_dof9)\n    # print('tiled_array', np.tile(self.dof9t_dof9, (coords1.shape[0], 1, 1)))\n    coords1_len = len(coords1)\n    shift = np.matmul(np.linalg.inv(np.tile(self.dof9t_dof9, (coords1_len, 1, 1)) / var_tot),\n                      np.matmul(np.tile(self.dof9_t, (coords1_len, 1, 1)), guide_delta) / var_tot)  # (n_dof x n_dof) x (n_dof x 1) = (n_dof x 1)\n\n    # get error value from the ideal translation and the delta\n    resid = np.matmul(np.tile(self.dof9, (coords1_len, 1, 1)), shift) - guide_delta  # (9 x n_dof) x (n_dof x 1) - (9 x 1) = (9 x 1)\n    error = np.sqrt(np.matmul(resid.swapaxes(-2, -1), resid) / float(self.number_of_coordinates)).flatten() \\\n        / coords_rmsd_reference\n\n    return shift[np.nonzero(error &lt;= self.max_z_value)].reshape(-1, self.n_dof)\n</code></pre>"},{"location":"reference/resources/config/","title":"config","text":""},{"location":"reference/resources/database/","title":"database","text":""},{"location":"reference/resources/database/#resources.database.DataStore","title":"DataStore","text":"<pre><code>DataStore(location: str = None, extension: str = '.txt', glob_extension: str = None, load_file: Callable = None, save_file: Callable = None, sql=None, log: Logger = logger)\n</code></pre> <p>Stores data (currently in a directory) for a singular type of data with particular load and save behaviors</p> <p>Parameters:</p> <ul> <li> <code>location</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The location to store/retrieve data if directories are used</p> </li> <li> <code>extension</code>             (<code>str</code>, default:                 <code>'.txt'</code> )         \u2013          <p>The extension of files to use during file handling. If extension is other than .txt or .json, the arguments load_file/save_file must be provided to handle storage</p> </li> <li> <code>load_file</code>             (<code>Callable</code>, default:                 <code>None</code> )         \u2013          <p>Callable taking the file_name as first argument</p> </li> <li> <code>save_file</code>             (<code>Callable</code>, default:                 <code>None</code> )         \u2013          <p>Callable taking the object to save as first argument and file_name as second argument</p> </li> <li> <code>sql</code>         \u2013          <p>The database to use if the storage is based on a SQL database</p> </li> <li> <code>log</code>             (<code>Logger</code>, default:                 <code>logger</code> )         \u2013          <p>The Logger to handle operation reporting</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def __init__(self, location: str = None, extension: str = '.txt', glob_extension: str = None,\n             load_file: Callable = None, save_file: Callable = None, sql=None, log: Logger = logger):\n    \"\"\"Construct the instance\n\n    Args:\n        location: The location to store/retrieve data if directories are used\n        extension: The extension of files to use during file handling. If extension is other than .txt or .json, the\n            arguments load_file/save_file must be provided to handle storage\n        load_file: Callable taking the file_name as first argument\n        save_file: Callable taking the object to save as first argument and file_name as second argument\n        sql: The database to use if the storage is based on a SQL database\n        log: The Logger to handle operation reporting\n    \"\"\"\n    self.log = log\n    if sql is not None:\n        self.sql = sql\n    else:\n        self.sql = sql\n        self.location = location\n        self.extension = extension\n        if glob_extension:\n            self.glob_extension = glob_extension\n        else:\n            self.glob_extension = extension\n\n        if '.txt' in extension:  # '.txt' read the file and return the lines\n            load_file = read_file\n            save_file = write_list_to_file\n        elif '.json' in extension:\n            load_file = utils.read_json\n            save_file = utils.write_json\n        else:\n            if load_file is None:\n                load_file = not_implemented\n            if save_file is None:\n                save_file = not_implemented\n\n        self.load_file = load_file\n        self.save_file = save_file\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.make_path","title":"make_path","text":"<pre><code>make_path(condition: bool = True)\n</code></pre> <p>Make all required directories in specified path if it doesn't exist, and optional condition is True</p> <p>Parameters:</p> <ul> <li> <code>condition</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>A condition to check before the path production is executed</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def make_path(self, condition: bool = True):\n    \"\"\"Make all required directories in specified path if it doesn't exist, and optional condition is True\n\n    Args:\n        condition: A condition to check before the path production is executed\n    \"\"\"\n    if condition:\n        os.makedirs(self.location, exist_ok=True)\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.path_to","title":"path_to","text":"<pre><code>path_to(name: str = '*') -&gt; AnyStr\n</code></pre> <p>Return the path_to the storage location given an entity name</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def path_to(self, name: str = '*') -&gt; AnyStr:\n    \"\"\"Return the path_to the storage location given an entity name\"\"\"\n    return os.path.join(self.location, f'{name}{self.extension}')\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.glob_path","title":"glob_path","text":"<pre><code>glob_path(name: str = '*') -&gt; AnyStr\n</code></pre> <p>Return the glob_path of the storage location given an entity name</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def glob_path(self, name: str = '*') -&gt; AnyStr:\n    \"\"\"Return the glob_path of the storage location given an entity name\"\"\"\n    return os.path.join(self.location, f'{name}{self.glob_extension}')\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.retrieve_file","title":"retrieve_file","text":"<pre><code>retrieve_file(name: str) -&gt; AnyStr | None\n</code></pre> <p>Returns the actual location by combining the requested name with the stored .location</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_file(self, name: str) -&gt; AnyStr | None:\n    \"\"\"Returns the actual location by combining the requested name with the stored .location\"\"\"\n    path = self.glob_path(name)\n    files = sorted(glob(path))\n    if files:\n        file = files[0]\n        if len(files) &gt; 1:\n            self.log.warning(f'Found more than one file with glob \"{path}\". Grabbing the first one: {file}')\n        return file\n    else:\n        self.log.debug(f'No file found for \"{path}\"')\n        return None\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.retrieve_files","title":"retrieve_files","text":"<pre><code>retrieve_files() -&gt; list\n</code></pre> <p>Returns the actual location of all files in the stored .location</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_files(self) -&gt; list:\n    \"\"\"Returns the actual location of all files in the stored .location\"\"\"\n    path = self.glob_path()\n    files = sorted(glob(path))\n    if not files:\n        self.log.debug(f'No files found in \"{path}\"')\n    return files\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.retrieve_names","title":"retrieve_names","text":"<pre><code>retrieve_names() -&gt; list[str]\n</code></pre> <p>Returns the names of all objects in the stored .location</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_names(self) -&gt; list[str]:\n    \"\"\"Returns the names of all objects in the stored .location\"\"\"\n    path = self.glob_path()\n    names = list(map(os.path.basename, [os.path.splitext(file)[0] for file in sorted(glob(path))]))\n    if not names:\n        self.log.debug(f'No data found with names at \"{path}\"')\n    return names\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.store_data","title":"store_data","text":"<pre><code>store_data(data: Any, name: str, **kwargs)\n</code></pre> <p>Store the data specfied by data and name to the DataStore. Saves the data as well</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Any</code>)         \u2013          <p>The data object to be stored with name</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the data to be used</p> </li> </ul> Sets <p>self.name = data</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def store_data(self, data: Any, name: str, **kwargs):\n    \"\"\"Store the data specfied by data and name to the DataStore. Saves the data as well\n\n    Args:\n        data: The data object to be stored with name\n        name: The name of the data to be used\n\n    Sets:\n        self.name = data\n    \"\"\"\n    self.__setattr__(name, data)\n    self._save_data(data, name, **kwargs)\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.retrieve_data","title":"retrieve_data","text":"<pre><code>retrieve_data(name: str = None) -&gt; Any | None\n</code></pre> <p>Return the data requested by name. Otherwise, load into the Database from a specified location</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any | None</code>         \u2013          <p>If the data is available, the object requested will be returned, else None</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_data(self, name: str = None) -&gt; Any | None:\n    \"\"\"Return the data requested by name. Otherwise, load into the Database from a specified location\n\n    Args:\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n\n    Returns:\n        If the data is available, the object requested will be returned, else None\n    \"\"\"\n    if name is None:\n        return None\n    data = getattr(self, name, None)\n    if data:\n        self.log.debug(f'{name}{self.extension} was retrieved from {self.__class__.__name__}')\n    else:\n        data = self.load_data(name, log=None)  # Attempt to retrieve the new data\n        if data:\n            self.__setattr__(name, data)  # Attempt to store the new data as an attribute\n            self.log.debug(f'The file {name}{self.extension} was loaded into the {self.__class__.__name__}')\n        else:\n            self.log.debug(f'Failed to load file {name}{self.extension} into the {self.__class__.__name__}')\n\n    return data\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.load_data","title":"load_data","text":"<pre><code>load_data(name: str, **kwargs) -&gt; Any | None\n</code></pre> <p>Return the data located in a particular entry specified by name</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the data to be used</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any | None</code>         \u2013          <p>The data found at the requested name if any</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def load_data(self, name: str, **kwargs) -&gt; Any | None:\n    \"\"\"Return the data located in a particular entry specified by name\n\n    Args:\n        name: The name of the data to be used\n\n    Returns:\n        The data found at the requested name if any\n    \"\"\"\n    if self.sql:\n        dummy = True\n    else:\n        file = self.retrieve_file(name)\n        if file:\n            return self.load_file(file, **kwargs)\n    return None\n</code></pre>"},{"location":"reference/resources/database/#resources.database.DataStore.load_all_data","title":"load_all_data","text":"<pre><code>load_all_data(**kwargs)\n</code></pre> <p>Loads all data located in the particular DataStore storage location</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def load_all_data(self, **kwargs):\n    \"\"\"Loads all data located in the particular DataStore storage location\"\"\"\n    if self.sql:\n        dummy = True\n    else:\n        for file in sorted(glob(self.glob_path())):\n            # self.log.debug('Fetching %s' % file)\n            self.__setattr__(os.path.splitext(os.path.basename(file))[0], self.load_file(file))\n</code></pre>"},{"location":"reference/resources/database/#resources.database.Database","title":"Database","text":"<pre><code>Database(sql=None, log: Logger = logger)\n</code></pre> <p>A common interface to interact with DataStore instances</p> <p>Parameters:</p> <ul> <li> <code>sql</code>         \u2013          <p>The database to use if the storage is based on a SQL database</p> </li> <li> <code>log</code>             (<code>Logger</code>, default:                 <code>logger</code> )         \u2013          <p>The Logger to handle operation reporting</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def __init__(self, sql=None, log: Logger = logger):\n    \"\"\"Construct the instance\n\n    Args:\n        sql: The database to use if the storage is based on a SQL database\n        log: The Logger to handle operation reporting\n    \"\"\"\n    super().__init__()\n    if sql:\n        raise NotImplementedError('SQL set up has not been completed')\n\n    self.sql = sql\n    self.log = log\n    self.job: 'resources.job.JobResources' | None = None\n</code></pre>"},{"location":"reference/resources/database/#resources.database.Database.load_all_data","title":"load_all_data","text":"<pre><code>load_all_data()\n</code></pre> <p>For every resource, acquire all existing data in memory</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def load_all_data(self):\n    \"\"\"For every resource, acquire all existing data in memory\"\"\"\n    for source in self.sources:\n        try:\n            source.load_all_data()\n        except ValueError:\n            raise ValueError(\n                f'Issue loading data from source {source}')\n</code></pre>"},{"location":"reference/resources/database/#resources.database.Database.source","title":"source","text":"<pre><code>source(name: str) -&gt; DataStore\n</code></pre> <p>Return on of the various DataStores supported by the Database</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the data source to use</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def source(self, name: str) -&gt; DataStore:\n    \"\"\"Return on of the various DataStores supported by the Database\n\n    Args:\n        name: The name of the data source to use\n    \"\"\"\n    try:\n        return getattr(self, name)\n    except AttributeError:\n        raise AttributeError(\n            f\"There was no source named '{name}' found in the {self.__class__.__name__}. \"\n            f'Possible sources are: {\", \".join(self.__dict__)}')\n</code></pre>"},{"location":"reference/resources/database/#resources.database.Database.retrieve_data","title":"retrieve_data","text":"<pre><code>retrieve_data(source: str = None, name: str = None) -&gt; Any | None\n</code></pre> <p>Return the data requested by name from the specified source. Otherwise, load into the Database from a specified location</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data source to use</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any | None</code>         \u2013          <p>If the data is available, the object requested will be returned, else None</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_data(self, source: str = None, name: str = None) -&gt; Any | None:\n    \"\"\"Return the data requested by name from the specified source. Otherwise, load into the Database from a\n    specified location\n\n    Args:\n        source: The name of the data source to use\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n\n    Returns:\n        If the data is available, the object requested will be returned, else None\n    \"\"\"\n    return self.source(source).retrieve_data(name)\n</code></pre>"},{"location":"reference/resources/database/#resources.database.Database.retrieve_file","title":"retrieve_file","text":"<pre><code>retrieve_file(source: str = None, name: str = None) -&gt; AnyStr | None\n</code></pre> <p>Retrieve the file specified by the source and identifier name</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data source to use</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>If the file is available, it will be returned, else None</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def retrieve_file(self, source: str = None, name: str = None) -&gt; AnyStr | None:\n    \"\"\"Retrieve the file specified by the source and identifier name\n\n    Args:\n        source: The name of the data source to use\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n\n    Returns:\n        If the file is available, it will be returned, else None\n    \"\"\"\n    return self.source(source).retrieve_file(name)\n</code></pre>"},{"location":"reference/resources/database/#resources.database.read_file","title":"read_file","text":"<pre><code>read_file(file, **kwargs) -&gt; list[AnyStr]\n</code></pre> <p>The simplest form of parsing a file encoded in ASCII characters</p> Source code in <code>symdesign/resources/database.py</code> <pre><code>def read_file(file, **kwargs) -&gt; list[AnyStr]:\n    \"\"\"The simplest form of parsing a file encoded in ASCII characters\"\"\"\n    with open(file, 'r') as f:\n        return f.readlines()\n</code></pre>"},{"location":"reference/resources/database/#resources.database.write_str_to_file","title":"write_str_to_file","text":"<pre><code>write_str_to_file(string, file_name, **kwargs) -&gt; AnyStr\n</code></pre> <p>Use standard file IO to write a string to a file</p> <p>Parameters:</p> <ul> <li> <code>string</code>         \u2013          <p>The string to write</p> </li> <li> <code>file_name</code>         \u2013          <p>The location of the file to write</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The name of the written file</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def write_str_to_file(string, file_name, **kwargs) -&gt; AnyStr:\n    \"\"\"Use standard file IO to write a string to a file\n\n    Args:\n        string: The string to write\n        file_name: The location of the file to write\n\n    Returns:\n        The name of the written file\n    \"\"\"\n    with open(file_name, 'w') as f_save:\n        f_save.write(f'{string}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/resources/database/#resources.database.write_list_to_file","title":"write_list_to_file","text":"<pre><code>write_list_to_file(_list, file_name, **kwargs) -&gt; AnyStr\n</code></pre> <p>Use standard file IO to write a string to a file</p> <p>Parameters:</p> <ul> <li> <code>_list</code>         \u2013          <p>The string to write</p> </li> <li> <code>file_name</code>         \u2013          <p>The location of the file to write</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The name of the written file</p> </li> </ul> Source code in <code>symdesign/resources/database.py</code> <pre><code>def write_list_to_file(_list, file_name, **kwargs) -&gt; AnyStr:\n    \"\"\"Use standard file IO to write a string to a file\n\n    Args:\n        _list: The string to write\n        file_name: The location of the file to write\n\n    Returns:\n        The name of the written file\n    \"\"\"\n    with open(file_name, 'w') as f_save:\n        lines = '\\n'.join(map(str, _list))\n        f_save.write(f'{lines}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/resources/distribute/","title":"distribute","text":"<p>Module for Distribution of commands found for individual poses to SLURM/PBS computational cluster</p>"},{"location":"reference/resources/distribute/#resources.distribute.is_sbatch_available","title":"is_sbatch_available","text":"<pre><code>is_sbatch_available() -&gt; bool\n</code></pre> <p>Ensure the sbatch executable is available and executable</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def is_sbatch_available() -&gt; bool:\n    \"\"\"Ensure the sbatch executable is available and executable\"\"\"\n    global sbatch_exe\n    sbatch_exe = shutil.which(sbatch)  # get_sbatch_exe()\n    # if sbatch_exe is not None:\n    try:\n        return os.path.exists(sbatch_exe) and os.access(sbatch_exe, os.X_OK)\n    except TypeError:  # NoneType\n        return False\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.create_file","title":"create_file","text":"<pre><code>create_file(file: AnyStr = None)\n</code></pre> <p>If file doesn't exist, create a blank one</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def create_file(file: AnyStr = None):\n    \"\"\"If file doesn't exist, create a blank one\"\"\"\n    if file and not os.path.exists(file):\n        with open(file, 'w') as new_file:\n            dummy = True\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.run","title":"run","text":"<pre><code>run(cmd: list[str] | AnyStr, log_file_name: str, program: str = None, srun: str = None) -&gt; bool\n</code></pre> <p>Executes specified command and appends command results to log file</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>             (<code>list[str] | AnyStr</code>)         \u2013          <p>The name of a command file which should be executed by the system</p> </li> <li> <code>log_file_name</code>             (<code>str</code>)         \u2013          <p>Location on disk of log file</p> </li> <li> <code>program</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The interpreter for said command</p> </li> <li> <code>srun</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Whether to utilize a job step prefix during command issuance</p> </li> </ul> <p>Returns:     Whether the command executed successfully</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def run(cmd: list[str] | AnyStr, log_file_name: str, program: str = None, srun: str = None) -&gt; bool:\n    \"\"\"Executes specified command and appends command results to log file\n\n    Args:\n        cmd: The name of a command file which should be executed by the system\n        log_file_name: Location on disk of log file\n        program: The interpreter for said command\n        srun: Whether to utilize a job step prefix during command issuance\n    Returns:\n        Whether the command executed successfully\n    \"\"\"\n    cluster_prefix = srun if srun else []\n    program = [program] if program else []\n    command = [cmd] if isinstance(cmd, str) else cmd\n    if log_file_name:\n        with open(log_file_name, 'a') as log_f:\n            command = cluster_prefix + program + command\n            log_f.write(f'Command: {subprocess.list2cmdline(command)}\\n')\n            p = subprocess.Popen(command, stdout=log_f, stderr=log_f)\n            p.communicate()\n    else:\n        print(f'Command: {subprocess.list2cmdline(command)}\\n')\n        p = subprocess.Popen(command)\n        p.communicate()\n\n    return p.returncode == 0\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.check_scripts_exist","title":"check_scripts_exist","text":"<pre><code>check_scripts_exist(directives: Iterable[str] = None, file: AnyStr = None)\n</code></pre> <p>Check for the existence of scripts provided by an Iterable or present in a file</p> <p>Parameters:</p> <ul> <li> <code>directives</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>The locations of scripts which should be executed</p> </li> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The location of a file containing the location(s) of scripts/commands</p> </li> </ul> <p>Raises:     InputError: When the scripts/commands passed are malformed or do not exist Returns:     None</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def check_scripts_exist(directives: Iterable[str] = None, file: AnyStr = None):\n    \"\"\"Check for the existence of scripts provided by an Iterable or present in a file\n\n    Args:\n        directives: The locations of scripts which should be executed\n        file: The location of a file containing the location(s) of scripts/commands\n    Raises:\n        InputError: When the scripts/commands passed are malformed or do not exist\n    Returns:\n        None\n    \"\"\"\n    script_or_command = \\\n        '{} is malformed at line {}. All commands should match.\\n* * *\\n{}\\n* * *' \\\n        '\\nEither a file extension OR a command requried. Cannot mix'\n    # Automatically detect if the commands file has executable scripts or errors\n    if directives is None:\n        if file is None:\n            raise ValueError(f\"Must pass either 'directives' or 'file'. Neither were passed\")\n        else:\n            # Use collect_designs to get commands from the provided file\n            scripts, _ = collect_designs(files=[file])\n\n    # Check if the file lines (commands) contain a script or a command\n    first_directive, *remaining_directives = directives\n    contains_scripts = True if first_directive.endswith('.sh') else False\n    for idx, directive in enumerate(remaining_directives, 1):\n        # Check if the directive string is a shell script type file string. Ex: \"refine.sh\"\n        if directive[-3:] == '.sh':  # This is a file\n            if not os.path.exists(directive):  # Check if file is missing\n                raise InputError(\n                    f\"The command at location '{directive}' doesn't exist\")\n            if not contains_scripts:  # There was a change from non-script files\n                raise InputError(script_or_command.format(file, idx, directive))\n        else:  # directive is a command\n            # Check if there was a change from script files to non-script files\n            if contains_scripts:\n                raise InputError(script_or_command.format(file, idx, directive))\n            else:\n                contains_scripts = False\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.distribute","title":"distribute","text":"<pre><code>distribute(file: AnyStr, scale: protocols_literal, number_of_commands: int, out_path: AnyStr = os.getcwd(), success_file: AnyStr = None, failure_file: AnyStr = None, log_file: AnyStr = None, max_jobs: int = 80, mpi: int = None, finishing_commands: Iterable[str] = None, batch: bool = is_sbatch_available(), **kwargs) -&gt; str\n</code></pre> <p>Take a file of commands formatted for execution in the SLURM environment and process into a script</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file which contains your commands to distribute through a sbatch array</p> </li> <li> <code>scale</code>             (<code>protocols_literal</code>)         \u2013          <p>The stage of design to distribute. Works with CommandUtils and PathUtils to allocate jobs</p> </li> <li> <code>number_of_commands</code>             (<code>int</code>)         \u2013          <p>The size of the job array</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Where to write out the sbatch script</p> </li> <li> <code>success_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>What file to write the successful jobs to for job organization</p> </li> <li> <code>failure_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>What file to write the failed jobs to for job organization</p> </li> <li> <code>log_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The name of a log file to write command results to</p> </li> <li> <code>max_jobs</code>             (<code>int</code>, default:                 <code>80</code> )         \u2013          <p>The size of the job array limiter. This caps the number of commands executed at once</p> </li> <li> <code>mpi</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of processes to run concurrently with MPI</p> </li> <li> <code>finishing_commands</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>Commands to run once all sbatch processes are completed</p> </li> <li> <code>batch</code>             (<code>bool</code>, default:                 <code>is_sbatch_available()</code> )         \u2013          <p>Whether the distribution file should be formatted as a SLURM sbatch script</p> </li> </ul> <p>Returns:     The name of the script that was written</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def distribute(file: AnyStr, scale: protocols_literal, number_of_commands: int, out_path: AnyStr = os.getcwd(),\n               success_file: AnyStr = None, failure_file: AnyStr = None, log_file: AnyStr = None, max_jobs: int = 80,\n               mpi: int = None,\n               finishing_commands: Iterable[str] = None, batch: bool = is_sbatch_available(), **kwargs) -&gt; str:\n    \"\"\"Take a file of commands formatted for execution in the SLURM environment and process into a script\n\n    Args:\n        file: The location of the file which contains your commands to distribute through a sbatch array\n        scale: The stage of design to distribute. Works with CommandUtils and PathUtils to allocate jobs\n        number_of_commands: The size of the job array\n        out_path: Where to write out the sbatch script\n        success_file: What file to write the successful jobs to for job organization\n        failure_file: What file to write the failed jobs to for job organization\n        log_file: The name of a log file to write command results to\n        max_jobs: The size of the job array limiter. This caps the number of commands executed at once\n        mpi: The number of processes to run concurrently with MPI\n        finishing_commands: Commands to run once all sbatch processes are completed\n        batch: Whether the distribution file should be formatted as a SLURM sbatch script\n    Returns:\n        The name of the script that was written\n    \"\"\"\n    # Create success and failures files\n    name, ext = os.path.splitext(os.path.basename(file))\n    if success_file is None:\n        success_file = os.path.join(out_path, f'{name}.success')\n    if failure_file is None:\n        failure_file = os.path.join(out_path, f'{name}.failures')\n    output = os.path.join(out_path, 'output')\n    putils.make_path(output)\n\n    # Make sbatch file from template, array details, and command distribution script\n    if batch:\n        filename = os.path.join(out_path, f'{name}-{sbatch}.sh')\n    else:\n        filename = os.path.join(out_path, f'{name}.sh')\n\n    with open(filename, 'w') as new_f:\n        # Todo set up sbatch accordingly. Include a multiplier for the number of CPU's. Actually, might be passed\n        #  if mpi:\n        #      do_mpi_stuff = True\n        if batch:\n            # Write sbatch template\n            template = sbatch_templates.get(scale)\n            if not template:\n                template = default_sbatch_template\n                logger.warning(f\"Couldn't find an sbatch script template for '{scale}'\")\n            with open(template) as template_f:\n                new_f.write(''.join(template_f.readlines()))\n            out = f'output={output}/%A_%a.out'\n            new_f.write(f'{sb_flag}{out}\\n')\n            # Removing possibility to run multiple jobs simultaneously\n            # array = f'array=1-{int(number_of_commands/process_scale[scale] + 0.5)}%{max_jobs}'\n            array = f'array=1-{number_of_commands}%{max_jobs}'\n            new_f.write(f'{sb_flag}{array}\\n\\n')\n            distributer_command = f'python {putils.distributer_tool} {f\"--log-file {log_file} \" if log_file else \"\"}' \\\n                                  f'--success-file {success_file} --failure-file {failure_file} --command-file {file}'\n        else:\n            distributer_command = f'python {putils.distributer_tool} {f\"--log-file {log_file} \" if log_file else \"\"}' \\\n                                  f'--success-file {success_file} --failure-file {failure_file} --command-file {file}' \\\n                                  f' --number-of-processes {max_jobs}'\n        new_f.write(distributer_command)\n        if finishing_commands:\n            if batch:\n                new_f.write('\\n# Wait for all to complete\\n'\n                            'wait\\n'\n                            '\\n'\n                            '# Then execute\\n'\n                            '%s\\n' % '\\n'.join(finishing_commands))\n            else:\n                new_f.write(' &amp;&amp;\\n# Wait for all to complete, then execute\\n'\n                            '%s\\n' % '\\n'.join(finishing_commands))\n        else:\n            new_f.write('\\n')\n\n    return filename\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.commands","title":"commands","text":"<pre><code>commands(commands: Sequence[str], name: str, protocol: protocols_literal, out_path: AnyStr = os.getcwd(), commands_out_path: AnyStr = None, **kwargs) -&gt; str\n</code></pre> <p>Given a batch of commands, write them to a file and distribute that work for completion using specified computational resources</p> <p>Parameters:</p> <ul> <li> <code>commands</code>             (<code>Sequence[str]</code>)         \u2013          <p>The commands which should be written to a file and then formatted for distribution</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the collection of commands. Will be applied to commands file and distribution file(s)</p> </li> <li> <code>protocol</code>             (<code>protocols_literal</code>)         \u2013          <p>The type of protocol to distribute</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Where should the distributed script be written?</p> </li> <li> <code>commands_out_path</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>Where should the commands file be written? If not specified, is written to out_path</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>success_file</code>         \u2013          <p>AnyStr = None - What file to write the successful jobs to for job organization</p> </li> <li> <code>failure_file</code>         \u2013          <p>AnyStr = None - What file to write the failed jobs to for job organization</p> </li> <li> <code>log_file</code>         \u2013          <p>AnyStr = None - The name of a log file to write command results to</p> </li> <li> <code>max_jobs</code>         \u2013          <p>int = 80 - The size of the job array limiter. This caps the number of commands executed at once</p> </li> <li> <code>mpi</code>         \u2013          <p>bool = False - The number of processes to run concurrently with MPI</p> </li> <li> <code>finishing_commands</code>         \u2013          <p>Iterable[str] = None - Commands to run once all sbatch processes are completed</p> </li> <li> <code>batch</code>         \u2013          <p>bool = is_sbatch_available() - Whether the distribution file should be formatted as a SLURM sbatch script</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The name of the distribution script that was written</p> </li> </ul> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def commands(commands: Sequence[str], name: str, protocol: protocols_literal,\n             out_path: AnyStr = os.getcwd(), commands_out_path: AnyStr = None, **kwargs) -&gt; str:\n    \"\"\"Given a batch of commands, write them to a file and distribute that work for completion using specified\n    computational resources\n\n    Args:\n        commands: The commands which should be written to a file and then formatted for distribution\n        name: The name of the collection of commands. Will be applied to commands file and distribution file(s)\n        protocol: The type of protocol to distribute\n        out_path: Where should the distributed script be written?\n        commands_out_path: Where should the commands file be written? If not specified, is written to out_path\n\n    Keyword Args:\n        success_file: AnyStr = None - What file to write the successful jobs to for job organization\n        failure_file: AnyStr = None - What file to write the failed jobs to for job organization\n        log_file: AnyStr = None - The name of a log file to write command results to\n        max_jobs: int = 80 - The size of the job array limiter. This caps the number of commands executed at once\n        mpi: bool = False - The number of processes to run concurrently with MPI\n        finishing_commands: Iterable[str] = None - Commands to run once all sbatch processes are completed\n        batch: bool = is_sbatch_available() - Whether the distribution file should be formatted as a SLURM sbatch script\n\n    Returns:\n        The name of the distribution script that was written\n    \"\"\"\n    if is_sbatch_available():\n        shell = sbatch\n        logger.info(sbatch_warning)\n    else:\n        shell = default_shell\n        logger.info(script_warning)\n\n    putils.make_path(out_path)\n    if commands_out_path is None:\n        commands_out_path = out_path\n    else:\n        putils.make_path(commands_out_path)\n    command_file = write_commands(commands, name=name, out_path=commands_out_path)\n    script_file = distribute(command_file, protocol, len(commands), out_path=out_path, **kwargs)\n\n    logger.info(f'Once you are satisfied, enter the following to distribute:\\n\\t{shell} {script_file}')\n\n    return script_file\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.write_script","title":"write_script","text":"<pre><code>write_script(command: str, name: str = 'script', out_path: AnyStr = os.getcwd(), additional: list = None, shell: str = 'bash', status_wrap: str = None) -&gt; AnyStr\n</code></pre> <p>Take a command and write to a name.sh script. By default, bash is used as the shell interpreter</p> <p>Parameters:</p> <ul> <li> <code>command</code>             (<code>str</code>)         \u2013          <p>The command formatted using subprocess.list2cmdline(list())</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>'script'</code> )         \u2013          <p>The name of the output shell script</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the script will be written</p> </li> <li> <code>additional</code>             (<code>list</code>, default:                 <code>None</code> )         \u2013          <p>Additional commands also formatted using subprocess.list2cmdline()</p> </li> <li> <code>shell</code>             (<code>str</code>, default:                 <code>'bash'</code> )         \u2013          <p>The shell which should interpret the script</p> </li> <li> <code>status_wrap</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a file in which to check and set the status of the command in the shell</p> </li> </ul> <p>Returns:     The name of the file</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def write_script(command: str, name: str = 'script', out_path: AnyStr = os.getcwd(),\n                 additional: list = None, shell: str = 'bash', status_wrap: str = None) -&gt; AnyStr:\n    \"\"\"Take a command and write to a name.sh script. By default, bash is used as the shell interpreter\n\n    Args:\n        command: The command formatted using subprocess.list2cmdline(list())\n        name: The name of the output shell script\n        out_path: The location where the script will be written\n        additional: Additional commands also formatted using subprocess.list2cmdline()\n        shell: The shell which should interpret the script\n        status_wrap: The name of a file in which to check and set the status of the command in the shell\n    Returns:\n        The name of the file\n    \"\"\"\n    if status_wrap:\n        modifier = '&amp;&amp;'\n        _base_cmd = ['python', putils.distributer_tool, '--stage', name, 'status', '--info', status_wrap]\n        check = subprocess.list2cmdline(_base_cmd + ['--check', modifier, '\\n'])\n        _set = subprocess.list2cmdline(_base_cmd + ['--set'])\n    else:\n        check = _set = modifier = ''\n\n    file_name = os.path.join(out_path, name if name.endswith('.sh') else f'{name}.sh')\n    with open(file_name, 'w') as f:\n        f.write(f'#!/bin/{shell}\\n\\n{check}{command} {modifier}\\n\\n')\n        if additional:\n            f.write('%s\\n\\n' % ('\\n\\n'.join(f'{command} {modifier}' for command in additional)))\n        f.write(f'{_set}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/resources/distribute/#resources.distribute.write_commands","title":"write_commands","text":"<pre><code>write_commands(commands: Iterable[str], name: str = 'all_commands', out_path: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write a list of commands out to a file</p> <p>Parameters:</p> <ul> <li> <code>commands</code>             (<code>Iterable[str]</code>)         \u2013          <p>An iterable with the commands as values</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>'all_commands'</code> )         \u2013          <p>The name of the file. Will be appended with '.cmd(s)'</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The directory where the file will be written</p> </li> </ul> <p>Returns:     The filename of the new file</p> Source code in <code>symdesign/resources/distribute.py</code> <pre><code>def write_commands(commands: Iterable[str], name: str = 'all_commands', out_path: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write a list of commands out to a file\n\n    Args:\n        commands: An iterable with the commands as values\n        name: The name of the file. Will be appended with '.cmd(s)'\n        out_path: The directory where the file will be written\n    Returns:\n        The filename of the new file\n    \"\"\"\n    file = os.path.join(out_path, f'{name}.cmds' if len(commands) &gt; 1 else f'{name}.cmd')\n    with open(file, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(command for command in commands))\n\n    return file\n</code></pre>"},{"location":"reference/resources/job/","title":"job","text":""},{"location":"reference/resources/job/#resources.job.job_resources_factory","title":"job_resources_factory  <code>module-attribute</code>","text":"<pre><code>job_resources_factory: Annotated[JobResourcesFactory, 'Calling this factory method returns the single instance of the JobResources class'] = JobResourcesFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the JobResources class</p>"},{"location":"reference/resources/job/#resources.job.DBInfo","title":"DBInfo","text":"<pre><code>DBInfo(location: AnyStr, echo: bool = False)\n</code></pre> Source code in <code>symdesign/resources/job.py</code> <pre><code>def __init__(self, location: AnyStr, echo: bool = False):\n    self.location = location\n    self.engine: Engine = create_engine(self.location, echo=echo, future=True)\n    self.session: sessionmaker = sessionmaker(self.engine, future=True)\n\n    if 'sqlite' in self.location:\n        # The below functions are recommended to help overcome issues with SQLite transaction scope\n        # See: https://docs.sqlalchemy.org/en/20/dialects/sqlite.html#pysqlite-serializable\n        @event.listens_for(self.engine, 'connect')\n        def do_connect(dbapi_connection, connection_record):\n            \"\"\"Disable pysqlite's emitting of the BEGIN statement entirely.\n            Also stops it from emitting COMMIT before any DDL\n            \"\"\"\n            dbapi_connection.isolation_level = None\n\n        @event.listens_for(self.engine, 'begin')\n        def do_begin(conn):\n            \"\"\"Emit our own BEGIN\"\"\"\n            conn.exec_driver_sql('BEGIN')\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources","title":"JobResources","text":"<pre><code>JobResources(program_root: AnyStr = None, arguments: Namespace = None, initial: bool = False, **kwargs)\n</code></pre> <p>The intention of JobResources is to serve as a singular source of design info which is common across all jobs. This includes common paths, databases, and design flags which should only be set once in program operation, then shared across all member designs</p> <p>Parameters:</p> <ul> <li> <code>program_root</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The root location of program operation</p> </li> <li> <code>arguments</code>             (<code>Namespace</code>, default:                 <code>None</code> )         \u2013          <p>The argparse.Namespace object with associated program flags</p> </li> <li> <code>initial</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether this is the first instance of the particular program output</p> </li> </ul> Source code in <code>symdesign/resources/job.py</code> <pre><code>def __init__(self, program_root: AnyStr = None, arguments: argparse.Namespace = None, initial: bool = False,\n             **kwargs):\n    \"\"\"Parse the program operation location, ensure paths to these resources are available, and parse arguments\n\n    Args:\n        program_root: The root location of program operation\n        arguments: The argparse.Namespace object with associated program flags\n        initial: Whether this is the first instance of the particular program output\n    \"\"\"\n    try:\n        if os.path.exists(program_root):\n            self.program_root = program_root\n        else:\n            raise FileNotFoundError(\n                f\"Path doesn't exist\\n\\t{program_root}\")\n    except TypeError:\n        raise TypeError(\n            f\"Can't initialize {JobResources.__name__} without parameter 'program_root'\")\n\n    # Format argparse.Namespace arguments\n    if arguments is not None:\n        kwargs.update(deepcopy(vars(arguments)))\n\n    # Set the module for the current job. This will always be a '-' separated string when more than one name\n    self.module: str = kwargs.get(flags.module)\n    # Ensure that the protocol is viable\n    if self.module == flags.protocol:\n        self.protocol_module = True\n        self.modules = kwargs.get(flags.modules)\n    else:\n        self.protocol_module = False\n        # Instead of setting this, let self.module be used dynamically with property\n        # self.modules = [self.module]\n\n    # Computing environment and development Flags\n    # self.command_only: bool = kwargs.get('command_only', False)\n    # \"\"\"Whether to reissue commands, only if distribute_work=False\"\"\"\n    self.log_level: bool = kwargs.get(flags.log_level._)\n    self.debug: bool = True if self.log_level == logging.DEBUG else False\n    self.force: bool = kwargs.get(flags.force._)\n    self.development: bool = kwargs.get(flags.development._)\n    self.profile_memory: bool = kwargs.get(flags.profile_memory._)\n    if self.profile_memory and not self.development:\n        logger.warning(f\"{flags.profile_memory.long} was set but {flags.development.long} wasn't\")\n\n    self.mpi: int = kwargs.get(flags.mpi)\n    if self.mpi is None:\n        self.mpi = 0\n        self.distribute_work: bool = kwargs.get(flags.distribute_work._)\n        # # Todo implement, see symdesign.utils and CommandDistributor\n        # # extras = ' mpi {CommmandDistributer.mpi}'\n        # number_mpi_processes = CommmandDistributer.mpi - 1\n        # logger.info('Setting job up for submission to MPI capable computer. Pose trajectories run in parallel, '\n        #             f'{number_mpi_processes} at a time. This will speed up processing ~\n        #             f'{job.design.number / number_mpi_processes:2f}-fold.')\n    else:  # self.mpi &gt; 0\n        self.distribute_work = True\n        raise NotImplementedError(\n            f\"Can't compute the number of resources to allocate using {flags.mpi.long} yet...\")\n\n    self.multi_processing: int = kwargs.get(flags.multi_processing._)\n    if self.multi_processing:\n        # Calculate the number of cores to use depending on computer resources\n        self.cores = utils.calculate_mp_cores(cores=kwargs.get(flags.cores))  # Todo mpi=self.mpi\n    else:\n        self.cores: int = 1\n    self.threads = self.cores * 2\n    self.gpu_available = False\n\n    # Input parameters\n    self.project_name = kwargs.get(flags.project_name._)\n    # program_root subdirectories\n    self.data = os.path.join(self.program_root, putils.data.title())\n    self.projects = os.path.join(self.program_root, putils.projects)\n    self.job_paths = os.path.join(self.program_root, 'JobPaths')\n    self.sbatch_scripts = os.path.join(self.program_root, 'Scripts')\n    self.all_scores = os.path.join(self.program_root, 'AllScores')\n\n    self.api_db = wrapapi.api_database_factory.get(source=self.data)\n    self.sequences = self.api_db.sequences.location\n    self.profiles = self.api_db.hhblits_profiles.location\n    self.pdb_api = self.api_db.pdb.location\n    self.uniprot_api = self.api_db.uniprot.location\n    # data subdirectories\n    self.clustered_poses = os.path.join(self.data, 'ClusteredPoses')\n    # pdbs subdirectories\n    self.structure_db = structure_db.structure_database_factory.get(\n        source=os.path.join(self.data, 'StructureInfo'))\n    self.pdbs = self.structure_db.models.location  # Used to store downloaded PDB's\n    self.orient_dir = self.structure_db.oriented.location\n    self.orient_asu_dir = self.structure_db.oriented_asu.location\n    self.refine_dir = self.structure_db.refined.location\n    self.full_model_dir = self.structure_db.full_models.location\n    self.stride_dir = self.structure_db.stride.location\n\n    # Set the job instance on these db objects\n    self.api_db.job = self\n    self.structure_db.job = self\n    self.fragment_source = kwargs.get(flags.fragment_source._)\n    self.fragment_db: structure.fragment.db.FragmentDatabase | None = None\n\n    default_db = f'sqlite:///{os.path.join(self.data, f\"{putils.program_name}.db\")}'\n    self.db_config = os.path.join(self.data, 'db.cfg')\n    database_url = kwargs.get(flags.database_url._)\n    if initial:\n        if database_url is None:\n            database_url = default_db\n        db_cfg = {'url': database_url}\n        with open(self.db_config, 'w') as f:\n            json.dump(db_cfg, f)\n    else:\n        if os.path.exists(self.db_config):\n            with open(self.db_config, 'r') as f:\n                db_cfg = json.load(f)\n            if database_url is not None:\n                raise utils.InputError(\n                    f\"The {flags.database_url.long} '{database_url}' can't be used as this {putils.program_output} \"\n                    f\"was already initialized with the url='{db_cfg.get('url')}\")\n            else:\n                database_url = db_cfg.get('url')\n        else:  # This should always exist\n            database_url = default_db\n\n    self.database_url = database_url\n    self.debug_db = kwargs.get('debug_db')\n    self.db: DBInfo = DBInfo(self.database_url, echo=self.debug_db)\n    if initial:  # if not os.path.exists(self.internal_db):\n        # Emit CREATE TABLE DDL\n        sql.Base.metadata.create_all(self.db.engine)\n    self.load_to_db = kwargs.get(flags.load_to_db._)\n    self.reset_db = kwargs.get(flags.reset_db._)\n    if self.reset_db:\n        response = input(f\"All database information will be wiped if you proceed. Enter 'YES' to proceed\"\n                         f\"{utils.query.input_string}\")\n        if response == 'YES':\n            logger.warning(f'Dropping all tables and data from DB')\n            # All tables are deleted\n            sql.Base.metadata.drop_all(self.db.engine)\n            # Emit CREATE TABLE DDL\n            sql.Base.metadata.create_all(self.db.engine)\n        else:\n            logger.info(f'Skipping {flags.format_args(flags.reset_db)}')\n            pass\n    # else:  # When --no-database is provided as a flag\n    #     self.db = None\n\n    # PoseJob initialization flags\n    self.init = Init.from_flags(**kwargs)\n    self.specify_entities = kwargs.get(flags.specify_entities._)\n    # self.init.pre_refined\n    # self.init.pre_loop_modeled\n    # self.init.refine_input\n    # self.init.loop_model_input\n\n    # self.preprocessed = kwargs.get(flags.preprocessed)\n    # if self.init.pre_loop_modeled or self.init.pre_refined:\n    #     self.preprocessed = True\n    # else:\n    #     self.preprocessed = False\n    self.range = kwargs.get(flags.range_._)\n    if self.range is not None:\n        try:\n            self.low, self.high = map(float, self.range.split('-'))\n        except ValueError:  # Didn't unpack correctly\n            raise ValueError(\n                f'The {flags.format_args(flags.range_args)} flag must take the form \"LOWER-UPPER\"')\n    else:\n        self.low = self.high = None\n    # Program flags\n    # self.consensus: bool = kwargs.get(consensus, False)  # Whether to run consensus\n    self.background_profile: str = kwargs.get(flags.background_profile._)\n    \"\"\"The type of position specific profile (per-residue amino acid frequencies) to utilize as the design \n    background profile. \n    Choices include putils.design_profile, putils.evolutionary_profile, and putils.fragment_profile\n    \"\"\"\n    # Process design_selector\n    self.design_selector: PoseSpecification = parse_design_selector_flags(**kwargs)\n\n    self.update_metadata = kwargs.get(flags.update_metadata._)\n    self.component1 = kwargs.get(flags.component1._)\n    self.query_codes = kwargs.get(flags.query_codes._)\n    pdb_codes = kwargs.get(flags.pdb_code._, kwargs.get('target_pdb_code'))\n    if pdb_codes:\n        # Collect all provided codes required for component 1 processing\n        codes = []\n        for code_or_file in pdb_codes:\n            codes.extend(utils.to_iterable(code_or_file))\n        self.pdb_codes = utils.remove_duplicates(codes)\n    else:\n        self.pdb_codes = None\n\n    self.component2 = kwargs.get(flags.component2._)\n    self.query_codes2 = kwargs.get('query_codes2')\n    pdb_codes2 = kwargs.get('pdb_code2', kwargs.get('aligned_pdb_code'))\n    if pdb_codes2:\n        # Collect all provided codes required for component 1 processing\n        codes = []\n        for code_or_file in pdb_codes2:\n            codes.extend(utils.to_iterable(code_or_file))\n        self.pdb_codes2 = utils.remove_duplicates(codes)\n    else:\n        self.pdb_codes2 = None\n\n    # Docking flags\n    self.dock = Dock.from_flags(**kwargs)\n    if self.development:\n        self.dock.quick = True\n    if self.dock.perturb_dof or self.dock.perturb_dof_rot or self.dock.perturb_dof_tx:\n        # Check if no other values were set and set them if so\n        if not self.dock.perturb_dof_rot and not self.dock.perturb_dof_tx:\n            # Set all perturb_dof on and set to the provided default\n            self.dock.perturb_dof_rot = self.dock.perturb_dof_tx = True\n            if self.dock.perturb_dof_steps is None:\n                self.dock.perturb_dof_steps_rot = self.dock.perturb_dof_steps_tx = flags.default_perturbation_steps\n            else:\n                self.dock.perturb_dof_steps_rot = self.dock.perturb_dof_steps_tx = self.dock.perturb_dof_steps\n        else:  # Parse the provided values\n            self.dock.perturb_dof = True\n            if self.dock.perturb_dof_rot:\n                if self.dock.perturb_dof_steps_rot is None:\n                    self.dock.perturb_dof_steps_rot = flags.default_perturbation_steps\n            else:\n                self.dock.perturb_dof_steps_rot = 1\n\n            if self.dock.perturb_dof_tx:\n                if self.dock.perturb_dof_steps_tx is None:\n                    self.dock.perturb_dof_steps_tx = flags.default_perturbation_steps\n            else:\n                self.dock.perturb_dof_steps_tx = 1\n    else:  # None provided, set the unavailable dof to 1 step and warn if one was provided\n        if self.dock.perturb_dof_steps is not None:\n            logger.warning(f\"Couldn't use the flag {flags.perturb_dof_steps.long} as {flags.perturb_dof.long}\"\n                           f\" wasn't set\")\n        if self.dock.perturb_dof_steps_rot is not None:\n            logger.warning(f\"Couldn't use the flag {flags.perturb_dof_steps_rot.long} as \"\n                           f\"{flags.perturb_dof_rot.long} wasn't set\")\n        if self.dock.perturb_dof_steps_tx is not None:\n            logger.warning(f\"Couldn't use the flag {flags.perturb_dof_steps_tx.long} as {flags.perturb_dof_tx.long}\"\n                           f\" wasn't set\")\n        self.dock.perturb_dof_steps = self.dock.perturb_dof_steps_rot = self.dock.perturb_dof_steps_tx = 1\n\n    # dock_weight = kwargs.get('weight')\n    # dock_weight_file = kwargs.get('weight_file')\n    if self.dock.weight or self.dock.weight_file is not None:\n        self.dock.weight = flags.parse_weights(self.dock.weight, file=self.dock.weight_file)\n    # No option to get filters on the fly...\n    # elif self.dock.weight is not None:  # --dock-weight was provided, but as a boolean-esq. Query the user\n    #     self.dock.weight = []\n    else:\n        self.dock.weight = None\n    if self.dock.filter or self.dock.filter_file is not None:\n        self.dock.filter = flags.parse_filters(self.dock.filter, file=self.dock.filter_file)\n    # No option to get filters on the fly...\n    # elif self.dock.weight is not None:  # --dock-weight was provided, but as a boolean-esq. Query the user\n    #     self.dock.weight = []\n    else:\n        self.dock.filter = None\n    # self.proteinmpnn_score: bool = kwargs.get('proteinmpnn_score', False)\n    # self.contiguous_ghosts: bool = kwargs.get('contiguous_ghosts', False)\n\n    # self.rotation_step1: bool = kwargs.get('rotation_step1', False)\n    # self.rotation_step2: bool = kwargs.get('rotation_step2', False)\n    # self.min_matched: bool = kwargs.get('min_matched', False)\n    # self.match_value: bool = kwargs.get('match_value', False)\n    # self.initial_z_value: bool = kwargs.get('initial_z_value', False)\n\n    self.fuse_chains: list[tuple[str]] = [tuple(pair.split(':')) for pair in kwargs.get(flags.fuse_chains._, [])]\n\n    self.interface_distance = kwargs.get(flags.interface_distance._)\n    self.interface = kwargs.get(flags.interface._)\n    self.interface_only = kwargs.get(flags.interface_only._)\n    self.oligomeric_interfaces = kwargs.get(flags.oligomeric_interfaces._)\n    self.use_proteinmpnn = kwargs.get(flags.use_proteinmpnn._)\n    self.use_evolution = kwargs.get(flags.use_evolution._)\n    # Explicitly set to false if not designing or predicting\n    use_evolution_modules = [\n        flags.nanohedra, flags.initialize_building_blocks, flags.refine, flags.interface_metrics,\n        flags.process_rosetta_metrics, flags.analysis, flags.predict_structure, flags.design\n    ]\n    if self.use_evolution and not any([module in use_evolution_modules for module in self.modules]):\n        logger.info(f'Setting {flags.format_args(flags.use_evolution_args)} to False as no module '\n                    'requesting evolutionary information is utilized')\n        self.use_evolution = False\n\n    # Design flags\n    self.design = Design.from_flags(**kwargs)\n    if self.design.ignore_clashes:\n        self.design.ignore_pose_clashes = self.design.ignore_symmetric_clashes = True\n    # Handle protocol specific flags\n    if self.module == flags.interface_design:  # or self.design.neighbors:\n        # Handle interface-design module alias\n        self.module = flags.design\n        self.design.interface = True\n    if self.design.method == putils.consensus:\n        self.design.term_constraint = True\n    if self.design.term_constraint:\n        self.generate_fragments: bool = True\n    else:\n        self.generate_fragments = False\n\n    if self.design.structure_background:\n        self.design.evolution_constraint = False\n        self.design.hbnet = False\n        self.design.scout = False\n        self.design.term_constraint = False\n\n    # if self.design.evolution_constraint and flags.design not in self.modules:\n    #     logger.debug(f'Setting {flags.format_args(flags.evolution_constraint_args)} to False as the no module '\n    #                  f'requesting evolutionary information is utilized')\n    #     self.design.evolution_constraint = False\n\n    # self.dock_only: bool = kwargs.get('dock_only')\n    # if self.dock_only:\n    #     self.design.sequences = self.design.structures = False\n    self.only_write_frag_info: bool = kwargs.get(flags.only_write_frag_info._)\n    self.increment_chains: bool = kwargs.get(flags.increment_chains._)\n    self.interface_to_alanine: bool = kwargs.get(flags.interface_to_alanine._)\n    self.metrics: bool = kwargs.get(flags.metrics._)\n    self.measure_pose: str = kwargs.get(flags.measure_pose._)\n    self.specific_protocol: str = kwargs.get(flags.specific_protocol._)\n    # Process symmetry\n    sym_entry_number = kwargs.get(flags.sym_entry._)\n    symmetry = kwargs.get(flags.symmetry._)\n    if sym_entry_number is None and symmetry is None:\n        self.sym_entry: SymEntry.SymEntry | str | None = None\n    else:\n        if symmetry and utils.symmetry.CRYST in symmetry.upper():\n            # Later, symmetry information will be retrieved from the file header\n            self.sym_entry = SymEntry.CrystRecord  # Input was provided as 'cryst'\n        else:\n            self.sym_entry = SymEntry.parse_symmetry_to_sym_entry(\n                sym_entry_number=sym_entry_number, symmetry=symmetry)\n\n    # Selection flags\n    self.save_total = kwargs.get(flags.save_total._)\n    # self.total = kwargs.get('total')\n    self.protocol = kwargs.get(flags.protocol._)\n    _filter = kwargs.get(flags.filter_._)\n    _filter_file = kwargs.get(flags.filter_file._)\n    if _filter == list():\n        # --filter was provided, but as a boolean-esq. Query the user once there is a df\n        self.filter = True\n    elif _filter or _filter_file is not None:\n        self.filter = flags.parse_filters(_filter, file=_filter_file)\n    else:\n        self.filter = None\n    _weight = kwargs.get(flags.weight._)\n    _weight_file = kwargs.get(flags.weight_file._)\n    if _weight == list():\n        # --weight was provided, but as a boolean-esq. Query the user once there is a df\n        self.weight = True\n    elif _weight or _weight_file is not None:\n        self.weight = flags.parse_weights(_weight, file=_weight_file)\n    else:\n        self.weight = None\n    self.weight_function = kwargs.get(flags.weight_function._)\n    self.select_number = kwargs.get(flags.select_number._)\n    self.designs_per_pose = kwargs.get(flags.designs_per_pose._)\n    # self.allow_multiple_poses = kwargs.get('allow_multiple_poses')\n    self.tag_entities = kwargs.get(flags.tag_entities._)\n    self.specification_file = kwargs.get(flags.specification_file._)\n    # Don't need this at the moment...\n    # self.poses = kwargs.get(flags.poses)\n    \"\"\"Used to specify whether specific designs should be fetched for select_* modules\"\"\"\n    self.dataframe = kwargs.get(flags.dataframe._)\n\n    # Sequence flags\n    self.avoid_tagging_helices = kwargs.get(flags.avoid_tagging_helices._)\n    self.csv = kwargs.get(flags.csv._)\n    self.nucleotide = kwargs.get(flags.nucleotide)\n    self.optimize_species = kwargs.get(flags.optimize_species._)\n    self.preferred_tag = kwargs.get(flags.preferred_tag._)\n    self.tag_linker = kwargs.get(flags.tag_linker._)\n    self.multicistronic = kwargs.get(flags.multicistronic._)\n    self.multicistronic_intergenic_sequence = kwargs.get(flags.multicistronic_intergenic_sequence._)\n\n    # Output flags\n    self.overwrite: bool = kwargs.get(flags.overwrite._)\n    self.pose_format = kwargs.get(flags.pose_format._)\n    prefix = kwargs.get(flags.prefix._)\n    if prefix:\n        self.prefix = f'{prefix}_'\n    else:\n        self.prefix = ''\n\n    suffix = kwargs.get(flags.suffix._)\n    if suffix:\n        self.suffix = f'_{suffix}'\n    else:\n        self.suffix = ''\n\n    # Check if output already exists or --overwrite is provided\n    if self.module in flags.select_modules:\n        if self.prefix == '':\n            # self.location must not be None\n            self.prefix = f'{utils.starttime}_{os.path.basename(os.path.splitext(self.input_source)[0])}_'\n        output_directory = kwargs.get(flags.output_directory._)\n        # if not self.output_to_directory:\n        if not output_directory:\n            output_directory = os.path.join(os.path.dirname(self.program_root), f'SelectedDesigns')\n            #     os.path.join(os.path.dirname(self.program_root), f'{self.prefix}SelectedDesigns{self.suffix}')\n    else:  # if output_directory:\n        output_directory = kwargs.get(flags.output_directory._)\n\n    if output_directory:\n        self.output_directory = output_directory\n        if os.path.exists(self.output_directory) and not self.overwrite:\n            print(f\"The specified output directory '{self.output_directory}' already exists. Proceeding may \"\n                  f'overwrite your old data. Either specify a new one or use the flags {flags.prefix.long} or '\n                  f'{flags.suffix.long} to modify the name. To proceed, append {flags.overwrite.long} to your '\n                  f'command')\n            sys.exit(1)\n        putils.make_path(self.output_directory)\n\n    output_file = kwargs.get(flags.output_file._)\n    if output_file:\n        self.output_file = output_file\n        if os.path.exists(self.output_file) and not self.overwrite:\n            # if self.module in flags.analysis:  # Todo this was allowed, but it's outdated...\n            print(f\"The specified output file '{self.output_file}' already exists. Proceeding may \"\n                  f'overwrite your old data. Either specify a new one or use the flags {flags.prefix.long} or '\n                  f'{flags.suffix.long} to modify the name. To proceed, append {flags.overwrite.long} to your '\n                  f'command')\n            sys.exit(1)\n\n    # When we are performing expand-asu, make sure we set output_assembly to True\n    if self.module == flags.expand_asu:\n        self.output_assembly = True\n    else:\n        self.output_assembly: bool = kwargs.get(flags.output_assembly._)\n    self.output_surrounding_uc: bool = kwargs.get(flags.output_surrounding_uc._)\n    self.output_fragments: bool = kwargs.get(flags.output_fragments._)\n    self.output_interface: bool = kwargs.get(flags.output_interface._)\n    self.output_oligomers: bool = kwargs.get(flags.output_oligomers._)\n    self.output_entities: bool = kwargs.get(flags.output_entities._)\n    self.output_structures: bool = kwargs.get(flags.output_structures._)\n    self.output_trajectory: bool = kwargs.get(flags.output_trajectory._)\n\n    self.skip_logging: bool = kwargs.get(flags.skip_logging._)\n    # self.merge: bool = kwargs.get(flags.merge._)\n    # self.save: bool = kwargs.get(flags.save._)\n    # self.figures: bool = kwargs.get(flags.figures._)\n\n    if self.output_structures or self.output_assembly or self.output_surrounding_uc or self.output_fragments \\\n            or self.output_oligomers or self.output_entities or self.output_trajectory:\n        self.output: bool = True\n    else:\n        self.output: bool = False\n\n    # self.nanohedra_output: bool = kwargs.get(flags.nanohedra_output)\n    # self.nanohedra_root: str | None = None\n    # if self.nanohedra_output:\n    #     self.construct_pose: bool = kwargs.get('construct_pose', True)\n    # else:\n    # self.construct_pose = True\n\n    # Align helix flags\n    self.aligned_start = kwargs.get(flags.aligned_start._)\n    self.aligned_end = kwargs.get(flags.aligned_end._)\n    self.aligned_chain = kwargs.get(flags.aligned_chain._)\n    self.alignment_length = kwargs.get(flags.alignment_length._)\n    self.bend = kwargs.get(flags.bend._)\n    self.extension_length = kwargs.get(flags.extend._)\n    self.target_start = kwargs.get(flags.target_start._)\n    self.target_end = kwargs.get(flags.target_end._)\n    self.target_chain = kwargs.get(flags.target_chain._)\n    self.target_termini = kwargs.get(flags.target_termini._)\n    self.trim_termini = kwargs.get(flags.trim_termini._)\n    # Helix Bending flags\n    self.direction = kwargs.get(flags.direction._)\n    self.joint_residue = kwargs.get(flags.joint_residue._)\n    self.joint_chain = kwargs.get(flags.joint_chain._)\n    self.sample_number = kwargs.get(flags.sample_number._)\n    # Prediction flags\n    self.predict = Predict.from_flags(**kwargs)\n    # self.num_predictions_per_model = kwargs.get('num_predictions_per_model')\n    # if self.predict.num_predictions_per_model is None:\n    #     if 'monomer' in self.predict.mode:\n    #         self.num_predictions_per_model = 1\n    #     else:  # 'multimer\n    #         self.num_predictions_per_model = 5\n    if self.predict.models_to_relax == 'none':\n        self.predict.models_to_relax = None\n\n    # Clustering flags\n    # Todo\n    #  This is pretty sloppy. Modify this DataClass mechanism...\n    self.cluster_selection = kwargs.get(flags.cluster_selection._)\n    # self.cluster_map = kwargs.get('cluster_map')\n    # self.as_objects: bool = kwargs.get('as_objects')\n    # self.mode: bool = kwargs.get('mode')\n    if flags.cluster_poses in self.modules or flags.cluster_map._ in kwargs:\n        self.cluster = Cluster.from_flags(**kwargs)\n        # self.cluster.map: AnyStr\n        # \"\"\"The path to a file containing the currently loaded mapping from cluster representatives to members\"\"\"\n    else:\n        self.cluster = False\n\n    # Finally perform checks on desired work to see if viable\n    # if self.protocol_module:\n    self.check_protocol_module_arguments()\n    # Start with None and set this once a session is opened\n    self.job_protocol = None\n    # self.job_protocol = self.load_job_protocol()\n    self.parsed_arguments = None\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.background_profile","title":"background_profile  <code>instance-attribute</code>","text":"<pre><code>background_profile: str = get(_)\n</code></pre> <p>The type of position specific profile (per-residue amino acid frequencies) to utilize as the design  background profile.  Choices include putils.design_profile, putils.evolutionary_profile, and putils.fragment_profile</p>"},{"location":"reference/resources/job/#resources.job.JobResources.specification_file","title":"specification_file  <code>instance-attribute</code>","text":"<pre><code>specification_file = get(_)\n</code></pre> <p>Used to specify whether specific designs should be fetched for select_* modules</p>"},{"location":"reference/resources/job/#resources.job.JobResources.id","title":"id  <code>property</code>","text":"<pre><code>id: int\n</code></pre> <p>Get the JobProtocol.id for reference to the work performed</p>"},{"location":"reference/resources/job/#resources.job.JobResources.modules","title":"modules  <code>property</code> <code>writable</code>","text":"<pre><code>modules: list[str]\n</code></pre> <p>Return the modules slated to run during the job</p>"},{"location":"reference/resources/job/#resources.job.JobResources.output_to_directory","title":"output_to_directory  <code>property</code>","text":"<pre><code>output_to_directory: bool\n</code></pre> <p>If True, broadcasts that output is not typical putils.program_output directory structure</p>"},{"location":"reference/resources/job/#resources.job.JobResources.output_directory","title":"output_directory  <code>property</code> <code>writable</code>","text":"<pre><code>output_directory: AnyStr | None\n</code></pre> <p>Where to output the Job</p>"},{"location":"reference/resources/job/#resources.job.JobResources.output_file","title":"output_file  <code>property</code> <code>writable</code>","text":"<pre><code>output_file: AnyStr | None\n</code></pre> <p>Where to output info related to successful Job operation</p>"},{"location":"reference/resources/job/#resources.job.JobResources.location","title":"location  <code>property</code> <code>writable</code>","text":"<pre><code>location: str | None\n</code></pre> <p>The location where PoseJob instances are located</p>"},{"location":"reference/resources/job/#resources.job.JobResources.input_source","title":"input_source  <code>property</code>","text":"<pre><code>input_source: str\n</code></pre> <p>Provide the name of the specified PoseJob instances to perform work on</p>"},{"location":"reference/resources/job/#resources.job.JobResources.default_output_tuple","title":"default_output_tuple  <code>property</code>","text":"<pre><code>default_output_tuple: tuple[str, str, str]\n</code></pre> <p>Format fields for the output file depending on time, specified name and module type</p>"},{"location":"reference/resources/job/#resources.job.JobResources.construct_pose","title":"construct_pose  <code>property</code> <code>writable</code>","text":"<pre><code>construct_pose\n</code></pre> <p>Whether to construct the PoseJob</p>"},{"location":"reference/resources/job/#resources.job.JobResources.number_of_modules","title":"number_of_modules  <code>property</code>","text":"<pre><code>number_of_modules: int\n</code></pre> <p>The number of modules for the specified Job</p>"},{"location":"reference/resources/job/#resources.job.JobResources.get_parsed_arguments","title":"get_parsed_arguments","text":"<pre><code>get_parsed_arguments() -&gt; list[str]\n</code></pre> <p>Return the arguments submitted during application initialization</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>Each of the submitted flags, removed of input arguments, and formatted such as were parsed at runtime, i.e. --file, --poses, or -d are removed, and the remainder are left in the same order so as coule be formatted by subprocess.list2cmdline()</p> </li> </ul> Source code in <code>symdesign/resources/job.py</code> <pre><code>def get_parsed_arguments(self) -&gt; list[str]:\n    \"\"\"Return the arguments submitted during application initialization\n\n    Returns:\n        Each of the submitted flags, removed of input arguments, and formatted such as were parsed at runtime,\n            i.e. --file, --poses, or -d are removed, and the remainder are left in the same order so as coule be\n            formatted by subprocess.list2cmdline()\n    \"\"\"\n    if self.parsed_arguments:\n        return self.parsed_arguments\n\n    # Remove the program\n    parsed_arguments = sys.argv[1:]\n    logger.debug(f'Starting with arguments {parsed_arguments}')\n    # Todo\n    #  Should the module be removed?\n    #  sys.argv.remove(self.module)\n    # Remove the input\n    possible_input_args = [arg for args in flags.input_mutual_arguments.keys() for arg in args] \\\n        + [arg for args in flags.pose_inputs.keys() for arg in args] \\\n        + [arg for args in flags.component_mutual1_arguments.keys() for arg in args] \\\n        + [arg for args in flags.component_mutual2_arguments.keys() for arg in args]\n    for input_arg in possible_input_args:\n        try:\n            pop_index = parsed_arguments.index(input_arg)\n        except ValueError:  # Not in list\n            continue\n        else:\n            removed_flag = parsed_arguments.pop(pop_index)\n            while parsed_arguments[pop_index][0] != flags.flag_delimiter:\n                removed_arg = parsed_arguments.pop(pop_index)\n                logger.debug(f'From {removed_flag}, removed argument {removed_arg}')\n            # # If the flag requires an argument, pop the index a second time\n            # if input_arg not in single_input_flags:\n\n    # Remove distribution flags\n    for arg in flags.distribute_args:\n        try:\n            pop_index = parsed_arguments.index(arg)\n        except ValueError:  # Not in list\n            continue\n        else:\n            parsed_arguments.pop(pop_index)\n\n    # Set for the next time\n    self.parsed_arguments = parsed_arguments\n\n    return parsed_arguments\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.load_job_protocol","title":"load_job_protocol","text":"<pre><code>load_job_protocol()\n</code></pre> <p>Acquire the JobProtocol for the current set of input instructions</p> Sets <p>self.job_protocol (sql.JobProtocol)</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def load_job_protocol(self):\n    \"\"\"Acquire the JobProtocol for the current set of input instructions\n\n    Sets:\n        self.job_protocol (sql.JobProtocol)\n    \"\"\"\n    # Tabulate the protocol arguments that should be provided to the JobProtocol search/creation\n    if self.module == flags.design:\n        protocol_kwargs = dict(\n            ca_only=self.design.ca_only,\n            evolution_constraint=self.design.evolution_constraint,\n            interface=self.design.interface,\n            term_constraint=self.design.term_constraint,\n            neighbors=self.design.neighbors,\n            proteinmpnn_model_name=self.design.proteinmpnn_model,\n        )\n    elif self.module == flags.nanohedra:\n        protocol_kwargs = dict(\n            ca_only=self.design.ca_only,\n            contiguous_ghosts=self.dock.contiguous_ghosts,\n            initial_z_value=self.dock.initial_z_value,\n            match_value=self.dock.match_value,\n            minimum_matched=self.dock.minimum_matched,\n            proteinmpnn_model_name=self.design.proteinmpnn_model,\n        )\n    elif self.module == flags.predict_structure:\n        protocol_kwargs = dict(\n            number_predictions=self.predict.num_predictions_per_model,\n            prediction_model=self.predict.models_to_relax,\n            use_gpu_relax=self.predict.use_gpu_relax,\n        )\n    elif self.module == flags.analysis:\n        protocol_kwargs = dict(\n            ca_only=self.design.ca_only,\n            proteinmpnn_model_name=self.design.proteinmpnn_model,\n        )\n    # Todo\n    #  raise NotImplementedError()\n    # elif self.module == flags.interface_metrics:\n    # elif self.module == flags.generate_fragments:\n    else:\n        protocol_kwargs = {}\n\n    protocol_kwargs.update(dict(\n        module=self.module,\n        commit=putils.commit,\n    ))\n\n    job_protocol_stmt = select(sql.JobProtocol)\\\n        .where(*[getattr(sql.JobProtocol, table_column) == job_resources_attr\n                 for table_column, job_resources_attr in protocol_kwargs.items()])\n    # logger.debug(job_protocol_stmt.compile(compile_kwargs={\"literal_binds\": True}))\n    with self.db.session(expire_on_commit=False) as session:\n        job_protocol_result = session.scalars(job_protocol_stmt).all()\n        if not job_protocol_result:  # Create a new one\n            job_protocol = sql.JobProtocol(**protocol_kwargs)\n            session.add(job_protocol)\n            session.commit()\n        elif len(job_protocol_result) &gt; 1:\n            for result in job_protocol_result:\n                print(result)\n            raise utils.InputError(\n                f\"sqlalchemy.IntegrityError should've been raised. \"\n                f\"Can't have more than one matching {sql.JobProtocol.__name__}\")\n        else:\n            job_protocol = job_protocol_result[0]\n\n    self.job_protocol = job_protocol\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.get_range_slice","title":"get_range_slice","text":"<pre><code>get_range_slice(jobs: Sequence) -&gt; Sequence[Any]\n</code></pre> <p>Slice the input work by a set increment. This is parsed from the flags.range_args</p> <p>Parameters:</p> <ul> <li> <code>jobs</code>             (<code>Sequence</code>)         \u2013          <p>The work that should be sliced by the specified range</p> </li> </ul> <p>Returns:     The work, limited to the range provided by -r/--range input flag</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def get_range_slice(self, jobs: Sequence) -&gt; Sequence[Any]:\n    \"\"\"Slice the input work by a set increment. This is parsed from the flags.range_args\n\n    Args:\n        jobs: The work that should be sliced by the specified range\n    Returns:\n        The work, limited to the range provided by -r/--range input flag\n    \"\"\"\n    if self.range:\n        path_number = len(jobs)\n        # Adding 0.5 to ensure rounding occurs\n        low_range = int((self.low/100) * path_number + 0.5)\n        high_range = int((self.high/100) * path_number + 0.5)\n        if low_range &lt; 0 or high_range &gt; path_number:\n            raise ValueError(\n                f'The {flags.format_args(flags.range_args)} flag is outside of the acceptable bounds [0-100]')\n        logger.debug(f'Selecting input work ({path_number}) with range: {low_range}-{high_range}')\n        range_slice = slice(low_range, high_range)\n    else:\n        range_slice = slice(None)\n\n    return jobs[range_slice]\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.check_protocol_module_arguments","title":"check_protocol_module_arguments","text":"<pre><code>check_protocol_module_arguments()\n</code></pre> <p>Given provided modules for the 'protocol' module, check to ensure the work is adequate</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def check_protocol_module_arguments(self):\n    \"\"\"Given provided modules for the 'protocol' module, check to ensure the work is adequate\n\n    Raises:\n        InputError if the inputs are found to be incompatible\n    \"\"\"\n    protocol_module_allowed_modules = [\n        flags.align_helices,\n        flags.bend,\n        flags.expand_asu,\n        flags.rename_chains,\n        flags.check_clashes,\n        flags.generate_fragments,\n        flags.nanohedra,\n        flags.predict_structure,\n        flags.interface_metrics,\n        flags.optimize_designs,\n        flags.refine,\n        flags.design,\n        flags.interface_design,\n        flags.analysis,\n        flags.cluster_poses,\n        flags.select_poses,\n        flags.select_designs\n    ]\n    disallowed_modules = [\n        # 'custom_script',\n        # flags.select_sequences,\n        flags.initialize_building_blocks\n    ]\n\n    def check_gpu() -&gt; str | bool:\n        available_devices = jax.local_devices()\n        for idx, device in enumerate(available_devices):\n            if device.platform == 'gpu':\n                self.gpu_available = True\n                return device.device_kind\n                # device_id = idx\n                # return True\n        return False\n\n    problematic_modules = []\n    not_recognized_modules = []\n    nanohedra_prior = False\n    gpu_device_kind = None\n    for idx, module in enumerate(self.modules, 1):\n        if module == flags.nanohedra:\n            if idx &gt; 1:\n                raise utils.InputError(\n                    f\"For {flags.protocol} module, {module} can only be run in --modules position #1\")\n            nanohedra_prior = True\n            continue\n        elif module in flags.select_modules and self.protocol_module:\n            if idx != self.number_of_modules:\n                raise utils.InputError(\n                    f\"For {flags.protocol} module, {module} can only be run in --modules position N i.e. #1,2,...N\")\n\n        elif module == flags.predict_structure:\n            if gpu_device_kind is None:\n                # Check for GPU access\n                gpu_device_kind = check_gpu()\n\n            if gpu_device_kind:\n                logger.info(f'Running {module} on {gpu_device_kind} GPU')\n                # Disable GPU on tensorflow. I think that this is so tensorflow doesn't leak any calculations\n                tf.config.set_visible_devices([], 'GPU')\n            else:  # device.platform == 'cpu':\n                logger.warning(f'No GPU detected, will {module} using CPU')\n        elif module == flags.design:\n            if self.design.method == putils.proteinmpnn:\n                if gpu_device_kind is None:\n                    # Check for GPU access\n                    gpu_device_kind = check_gpu()\n\n                if gpu_device_kind:\n                    logger.info(f'Running {module} on {gpu_device_kind} GPU')\n                else:  # device.platform == 'cpu':\n                    logger.warning(f'No GPU detected, will {module} using CPU')\n\n        if nanohedra_prior:\n            if module in flags.select_modules:\n                # We only should allow select-poses after nanohedra\n                if module == flags.select_poses:\n                    logger.critical(f\"Running {module} after {flags.nanohedra} won't produce any Designs to \"\n                                    f\"operate on. In order to {module}, ensure you run a design protocol first\")\n                else:  # flags.select_designs, flags.select_sequences\n                    if not self.weight:  # not self.filter or\n                        logger.critical(f'Using {module} after {flags.nanohedra} without specifying the flag '\n                                        # f'{flags.format_args(flags.filter_args)} or '\n                                        f'{flags.format_args(flags.weight_args)} defaults to selection '\n                                        f'parameters {config.default_weight_parameter[flags.nanohedra]}')\n        nanohedra_prior = False\n        if self.protocol_module:\n            if module in protocol_module_allowed_modules:\n                continue\n            elif module in disallowed_modules:\n                problematic_modules.append(module)\n            else:\n                not_recognized_modules.append(module)\n\n    if not_recognized_modules:\n        raise utils.InputError(\n            f\"For {flags.protocol} module, the --{flags.modules} {', '.join(not_recognized_modules)} aren't \"\n            f'recognized modules. See\"{putils.program_help}\" for available module names')\n\n    if problematic_modules:\n        raise utils.InputError(\n            f\"For {flags.protocol} module, the --{flags.modules} {', '.join(problematic_modules)} aren't possible \"\n            f'modules\\n\\nAllowed modules are {\", \".join(protocol_module_allowed_modules)}')\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.report_specified_arguments","title":"report_specified_arguments","text":"<pre><code>report_specified_arguments(arguments: Namespace) -&gt; dict[str, Any]\n</code></pre> <p>Filter all flags for only those that were specified as different on the command line</p> <p>Parameters:</p> <ul> <li> <code>arguments</code>             (<code>Namespace</code>)         \u2013          <p>The arguments as parsed from the command-line argparse namespace</p> </li> </ul> <p>Returns:     Arguments specified during program execution</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def report_specified_arguments(self, arguments: argparse.Namespace) -&gt; dict[str, Any]:\n    \"\"\"Filter all flags for only those that were specified as different on the command line\n\n    Args:\n        arguments: The arguments as parsed from the command-line argparse namespace\n    Returns:\n        Arguments specified during program execution\n    \"\"\"\n    arguments = vars(arguments).copy()\n\n    reported_args = {}\n    # Start with JobResources flags that should be reported, or if the argument is not important, format it\n    if self.module:\n        reported_args['module'] = self.module\n    if self.sym_entry:\n        reported_args[flags.sym_entry._] = self.sym_entry.number\n    # if self.design_selector:\n    #     reported_args.pop('design_selector', None)\n\n    # # Custom removal/formatting for all remaining\n    # for custom_arg in list(arguments.keys()):\n    #     value = arguments.pop(custom_arg, None)\n    #     if value is not None:\n    #         reported_args[custom_arg] = value\n\n    if self.debug:\n        def report_arg(_dest, _default):\n            try:\n                value = arguments.pop(_dest)\n                if value is not None:\n                    reported_args[arg.dest] = value\n            except KeyError:\n                return\n    else:\n        def report_arg(_dest, _default):\n            try:\n                value = arguments.pop(_dest)\n                if value is not None and value != _default:\n                    reported_args[arg.dest] = value\n            except KeyError:\n                return\n\n    # Get all the default program args and compare them to the provided values\n    for group in flags.entire_parser._action_groups:\n        for arg in group._group_actions:\n            if isinstance(arg, argparse._SubParsersAction):  # This is a subparser, recurse\n                for name, sub_parser in arg.choices.items():\n                    for sub_group in sub_parser._action_groups:\n                        for arg in sub_group._group_actions:\n                            report_arg(arg.dest, arg.default)\n            else:\n                report_arg(arg.dest, arg.default)\n\n    return dict(sorted(reported_args.items()))  # , key=lambda arg: arg[0]\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.calculate_memory_requirements","title":"calculate_memory_requirements","text":"<pre><code>calculate_memory_requirements(number_jobs: int)\n</code></pre> <p>Format memory requirements with module dependencies and set self.reduce_memory</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def calculate_memory_requirements(self, number_jobs: int):\n    \"\"\"Format memory requirements with module dependencies and set self.reduce_memory\"\"\"\n    if self.module == flags.nanohedra:  # Todo\n        required_memory = putils.baseline_program_memory + putils.nanohedra_memory  # 30 GB ?\n    elif self.module == flags.analysis:\n        required_memory = (putils.baseline_program_memory +\n                           number_jobs * putils.approx_ave_design_directory_memory_w_assembly) * 1.2\n    else:\n        required_memory = (putils.baseline_program_memory +\n                           number_jobs * putils.approx_ave_design_directory_memory_w_pose) * 1.2\n\n    available_memory = psutil.virtual_memory().available\n    logger.debug(f'Available memory: {available_memory / gb_divisior:.2f} GB')\n    logger.debug(f'Required memory: {required_memory / gb_divisior:.2f} GB')\n    # If we are running a protocol, check for reducing memory requirements\n    if self.protocol_module and self.number_of_modules &gt; 2:\n        self.reduce_memory = True\n    elif available_memory &lt; required_memory:\n        self.reduce_memory = True\n    else:\n        # Todo when requirements are more accurate with database\n        #  self.reduce_memory = False\n        self.reduce_memory = True\n    logger.debug(f'Reduce job memory?: {self.reduce_memory}')\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.can_process_evolutionary_profiles","title":"can_process_evolutionary_profiles  <code>staticmethod</code>","text":"<pre><code>can_process_evolutionary_profiles() -&gt; bool\n</code></pre> <p>Return True if the current computer has the computational requirements to collect evolutionary profiles</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>@staticmethod\ndef can_process_evolutionary_profiles() -&gt; bool:\n    \"\"\"Return True if the current computer has the computational requirements to collect evolutionary profiles\"\"\"\n    # Run specific checks\n    if psutil.virtual_memory().available &lt;= distribute.hhblits_memory_threshold:\n        print('\\n')\n        logger.critical(f'The available RAM is probably insufficient to run {putils.hhblits}. '\n                        f'Required/Available memory: {distribute.hhblits_memory_threshold / gb_divisior:.2f} GB/'\n                        f'{psutil.virtual_memory().available / gb_divisior:.2f} GB')\n        logger.critical(f'Creating scripts that can be distributed to a capable computer instead')\n        return False\n    return True\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.evolutionary_profile_processes","title":"evolutionary_profile_processes  <code>staticmethod</code>","text":"<pre><code>evolutionary_profile_processes() -&gt; int\n</code></pre> <p>Return the number of evolutionary profile processes that can be run given the available memory</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>@staticmethod\ndef evolutionary_profile_processes() -&gt; int:\n    \"\"\"Return the number of evolutionary profile processes that can be run given the available memory\"\"\"\n    return int(psutil.virtual_memory().available &lt;= distribute.hhblits_memory_threshold)\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResources.process_evolutionary_info","title":"process_evolutionary_info","text":"<pre><code>process_evolutionary_info(uniprot_entities: Iterable[UniProtEntity] = None, entities: Iterable[GeneEntity] = None, batch_commands: bool = False) -&gt; list[str]\n</code></pre> <p>Format the job with evolutionary constraint options</p> <p>Parameters:</p> <ul> <li> <code>uniprot_entities</code>             (<code>Iterable[UniProtEntity]</code>, default:                 <code>None</code> )         \u2013          <p>A list of the UniProtIDs for the Job</p> </li> <li> <code>entities</code>             (<code>Iterable[GeneEntity]</code>, default:                 <code>None</code> )         \u2013          <p>A list of the Entity instances initialized for the Job</p> </li> <li> <code>batch_commands</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether commands should be made for batch submission</p> </li> </ul> <p>Returns:     A list evolutionary setup instructions</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def process_evolutionary_info(self, uniprot_entities: Iterable[wrapapi.UniProtEntity] = None,\n                              entities: Iterable[structure.sequence.GeneEntity] = None,\n                              batch_commands: bool = False) -&gt; list[str]:\n    \"\"\"Format the job with evolutionary constraint options\n\n    Args:\n        uniprot_entities: A list of the UniProtIDs for the Job\n        entities: A list of the Entity instances initialized for the Job\n        batch_commands: Whether commands should be made for batch submission\n    Returns:\n        A list evolutionary setup instructions\n    \"\"\"\n    info_messages = []\n    hhblits_cmds, bmdca_cmds, msa_cmds = [], [], []\n    # Set up sequence data using hhblits and profile bmDCA for each input entity\n    # all_entity_ids = []\n    putils.make_path(self.sequences)\n    if not putils.uniclust_db:\n        # No database has been set up\n        logger.warning(f\"Couldn't locate a compatible database to run hhblits. Skipping evolutionary acquisition\")\n    elif uniprot_entities is not None:\n        for uniprot_entity in uniprot_entities:\n            evolutionary_profile_file = self.api_db.hhblits_profiles.retrieve_file(name=uniprot_entity.id)\n            if not evolutionary_profile_file:\n                hhblits_cmds.append(hhblits(uniprot_entity.id,\n                                            sequence=uniprot_entity.reference_sequence,\n                                            out_dir=self.profiles, threads=self.threads,\n                                            return_command=True))\n                msa_file = None\n            else:\n                msa_file = self.api_db.alignments.retrieve_file(name=uniprot_entity.id)\n\n            if not msa_file:\n                sto_cmd = [\n                    putils.reformat_msa_exe_path, 'a3m', 'sto',\n                    f\"{os.path.join(self.profiles, f'{uniprot_entity.id}.a3m')}\", '.sto', '-num', '-uc']\n                fasta_cmd = [\n                    putils.reformat_msa_exe_path, 'a3m', 'fas',\n                    f\"{os.path.join(self.profiles, f'{uniprot_entity.id}.a3m')}\", '.fasta', '-M', 'first', '-r']\n                msa_cmds.extend([sto_cmd, fasta_cmd])\n\n    elif entities is not None:\n        raise NotImplementedError(\n            f'Currently must use {wrapapi.UniProtEntity.__class__.__name__} in '\n            f'{self.process_evolutionary_info.__name__}'\n        )\n        for entity in entities:\n            evolutionary_profile_file = self.api_db.hhblits_profiles.retrieve_file(name=entity.name)\n            if not evolutionary_profile_file:\n                sequence_file = self.api_db.sequences.retrieve_file(name=entity.name)\n                if not sequence_file:\n                    sequence_file = entity.write_sequence_to_fasta(out_dir=self.sequences)\n\n                hhblits_cmds.append(entity.hhblits(sequence_file=sequence_file, out_dir=self.profiles,\n                                                   return_command=True))\n                msa_file = None\n            else:\n                msa_file = self.api_db.alignments.retrieve_file(name=entity.name)\n\n            if not msa_file:\n                sto_cmd = [\n                    putils.reformat_msa_exe_path, 'a3m', 'sto',\n                    f\"{os.path.join(self.profiles, f'{entity.name}.a3m')}\", '.sto', '-num', '-uc']\n                fasta_cmd = [\n                    putils.reformat_msa_exe_path, 'a3m', 'fas',\n                    f\"{os.path.join(self.profiles, f'{entity.name}.a3m')}\", '.fasta', '-M', 'first', '-r']\n                msa_cmds.extend([sto_cmd, fasta_cmd])\n\n    if hhblits_cmds:\n        protocol = putils.hhblits\n        logger.info(f\"Starting Profile(dtype='{protocol}') generation\")\n\n        if protocol == putils.hhblits:\n            if not os.access(putils.hhblits_exe, os.X_OK):\n                raise RuntimeError(\n                    f\"Couldn't locate the {protocol} executable. Ensure the executable file referenced by \"\n                    f\"'{putils.hhblits_exe}' exists then try your job again. Otherwise, use the argument \"\n                    f'--no-{flags.use_evolution} OR set up hhblits to run.{utils.guide.hhblits_setup_instructions}')\n        else:\n            assert_never(protocol)\n\n        putils.make_path(self.profiles)\n        putils.make_path(self.sbatch_scripts)\n        protocol_log_file = os.path.join(self.profiles, 'generate_profiles.log')\n\n        # Run hhblits commands\n        if not batch_commands and self.can_process_evolutionary_profiles():\n            logger.info(f'Writing {protocol} results to file: {protocol_log_file}')\n            # Run commands in this process\n            if self.multi_processing:\n                zipped_args = zip(hhblits_cmds, repeat(hhblits_log_file))\n                utils.mp_starmap(distribute.run, zipped_args, processes=self.cores)\n            else:\n                for cmd in tqdm(hhblits_cmds):\n                    logger.debug(f'Starting command: {subprocess.list2cmdline(cmd)}')\n                    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                    stdout, stderr = p.communicate()\n                    if stderr:\n                        logger.warning(stderr.decode('utf-8'))\n\n            # Format .a3m multiple sequence alignments to .sto/.fasta\n            for cmd in msa_cmds:\n                p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n                stdout, stderr = p.communicate()\n                if stderr:\n                    logger.warning(stderr.decode('utf-8'))\n        else:  # Convert each command to a string and write to distribute\n            hhblits_cmds = [subprocess.list2cmdline(cmd) for cmd in hhblits_cmds]\n            msa_cmds = [subprocess.list2cmdline(cmd) for cmd in msa_cmds]\n            all_evolutionary_commands = hhblits_cmds + msa_cmds\n            evolutionary_cmds_file = distribute.write_commands(\n                all_evolutionary_commands, name=f'{utils.starttime}-{protocol}', out_path=self.profiles)\n\n            if distribute.is_sbatch_available():\n                shell = distribute.sbatch\n                max_jobs = len(hhblits_cmds)  # number_of_hhblits_cmds\n            else:\n                shell = distribute.default_shell\n                max_jobs = self.evolutionary_profile_processes()\n\n            # Todo\n            #  distribution.Attrs()\n            protocol_kwargs = dict(out_path=self.sbatch_scripts, scale=protocol,\n                                   max_jobs=max_jobs, number_of_commands=len(all_evolutionary_commands),\n                                   log_file=protocol_log_file)\n            # reformat_msa_cmds_script = distribute.distribute(file=reformat_msa_cmd_file, **hhblits_kwargs)\n            protocol_script = distribute.distribute(file=evolutionary_cmds_file, **protocol_kwargs)\n            # Format messages\n            info_messages.append(\n                'Please follow the instructions below to generate sequence profiles for input proteins')\n            protocol_job_info_message = f'Enter the following to distribute {protocol} jobs:\\n\\t'\n            protocol_job_info_message += f'{shell} {protocol_script}'\n            info_messages.append(protocol_job_info_message)\n    elif msa_cmds:  # These may still be missing\n        putils.make_path(self.profiles)\n\n        if not os.access(putils.reformat_msa_exe_path, os.X_OK):\n            logger.error(f\"Couldn't execute multiple sequence alignment reformatting script\")\n\n        # Format .a3m multiple sequence alignments to .sto/.fasta\n        for cmd in msa_cmds:\n            p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            stdout, stderr = p.communicate()\n            if stderr:\n                logger.warning(stderr.decode('utf-8'))\n\n    if bmdca_cmds:\n        putils.make_path(self.profiles)\n        putils.make_path(self.sbatch_scripts)\n        # bmdca_cmds = \\\n        #     [list2cmdline([putils.bmdca_exe_path, '-i', os.path.join(self.profiles, '%s.fasta' % entity.name),\n        #                   '-d', os.path.join(self.profiles, '%s_bmDCA' % entity.name)])\n        #      for entity in entities.values()]\n        bmdca_cmd_file = \\\n            distribute.write_commands(bmdca_cmds, name=f'{utils.starttime}-bmDCA', out_path=self.profiles)\n        bmdca_script = distribute.distribute(file=bmdca_cmd_file, out_path=self.sbatch_scripts,\n                                             scale='bmdca', max_jobs=len(bmdca_cmds),\n                                             number_of_commands=len(bmdca_cmds),\n                                             log_file=os.path.join(self.profiles, 'generate_couplings.log'))\n        # reformat_msa_cmd_file = \\\n        #     SDUtils.write_commands(reformat_msa_cmds, name='%s-reformat_msa' % SDUtils.starttime,\n        #                            out_path=self.profiles)\n        # reformat_sbatch = distribute(file=reformat_msa_cmd_file, out_path=self.program_root,\n        #                              scale='script', max_jobs=len(reformat_msa_cmds),\n        #                              log_file=os.path.join(self.profiles, 'generate_profiles.log'),\n        #                              number_of_commands=len(reformat_msa_cmds))\n        if distribute.is_sbatch_available():\n            shell = distribute.sbatch\n        else:\n            shell = distribute.default_shell\n\n        bmdca_script_message = \\\n            f'Once you are satisfied, enter the following to distribute jobs:\\n\\t{shell} %s' \\\n            % bmdca_script if not info_messages else 'ONCE this job is finished, to calculate evolutionary ' \\\n                                                     'couplings i,j for each amino acid in the multiple ' \\\n                                                     f'sequence alignment, enter:\\n\\t{shell} {bmdca_script}'\n        info_messages.append(bmdca_script_message)\n\n    return info_messages\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResourcesFactory","title":"JobResourcesFactory","text":"<pre><code>JobResourcesFactory(**kwargs)\n</code></pre> <p>Return a JobResource instance by calling the Factory instance</p> <p>Handles creation and allotment to other processes by making a shared pointer to the JobResource for the current Job</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def __init__(self, **kwargs):\n    self._resources = {}\n    self._warn = True\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResourcesFactory.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs) -&gt; JobResources\n</code></pre> <p>Return the specified JobResources object singleton</p> <p>Returns:</p> <ul> <li> <code>JobResources</code>         \u2013          <p>The instance of the specified JobResources</p> </li> </ul> Source code in <code>symdesign/resources/job.py</code> <pre><code>def __call__(self, **kwargs) -&gt; JobResources:\n    \"\"\"Return the specified JobResources object singleton\n\n    Returns:\n        The instance of the specified JobResources\n    \"\"\"\n    #         Args:\n    #             source: The JobResources source name\n    source = 'single'\n    job = self._resources.get(source)\n    if job:\n        if kwargs and self._warn:\n            # try:\n            #     fragment_db.update(kwargs)\n            # except RuntimeError:\n            self._warn = False\n            logger.warning(f\"Can't pass the new arguments {', '.join(kwargs.keys())} to JobResources \"\n                           f'since it was already initialized and is a singleton')\n        return job\n    else:\n        logger.info(f'Initializing {JobResources.__name__}({kwargs.get(\"program_root\", os.getcwd())})')\n        self._resources[source] = JobResources(**kwargs)\n\n    return self._resources[source]\n</code></pre>"},{"location":"reference/resources/job/#resources.job.JobResourcesFactory.get","title":"get","text":"<pre><code>get(**kwargs) -&gt; JobResources\n</code></pre> <p>Return the specified JobResources object singleton</p> <p>Returns:</p> <ul> <li> <code>JobResources</code>         \u2013          <p>The instance of the specified JobResources</p> </li> </ul> Source code in <code>symdesign/resources/job.py</code> <pre><code>def get(self, **kwargs) -&gt; JobResources:\n    \"\"\"Return the specified JobResources object singleton\n\n    Returns:\n        The instance of the specified JobResources\n    \"\"\"\n    # Keyword Args:\n    #     source: The JobResource source name\n    return self.__call__(**kwargs)\n</code></pre>"},{"location":"reference/resources/job/#resources.job.generate_sequence_mask","title":"generate_sequence_mask","text":"<pre><code>generate_sequence_mask(fasta_file: AnyStr) -&gt; list[int]\n</code></pre> <p>From a sequence with a design_selector, grab the residue indices that should be designed in the target structural calculation</p> <p>Parameters:</p> <ul> <li> <code>fasta_file</code>             (<code>AnyStr</code>)         \u2013          <p>The path to a file with fasta information</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The residue numbers (in pose format) that should be ignored in design</p> </li> </ul> Source code in <code>symdesign/resources/job.py</code> <pre><code>def generate_sequence_mask(fasta_file: AnyStr) -&gt; list[int]:\n    \"\"\"From a sequence with a design_selector, grab the residue indices that should be designed in the target\n    structural calculation\n\n    Args:\n        fasta_file: The path to a file with fasta information\n\n    Returns:\n        The residue numbers (in pose format) that should be ignored in design\n    \"\"\"\n    sequence_and_mask = list(sequence.read_fasta_file(fasta_file))\n    # sequences = sequence_and_mask\n    _sequence, mask, *_ = sequence_and_mask\n    if not len(_sequence) == len(mask):\n        raise ValueError(\n            'The sequence and design_selector are different lengths. Please correct the alignment before proceeding')\n\n    return [idx for idx, aa in enumerate(mask, 1) if aa != '-']\n</code></pre>"},{"location":"reference/resources/job/#resources.job.generate_chain_mask","title":"generate_chain_mask","text":"<pre><code>generate_chain_mask(chains: str) -&gt; set[str]\n</code></pre> <p>From a string with a design_selection, format the chains provided</p> <p>Parameters:</p> <ul> <li> <code>chains</code>             (<code>str</code>)         \u2013          <p>The specified chains separated by commas to split</p> </li> </ul> <p>Returns:     The provided chain ids in pose format</p> Source code in <code>symdesign/resources/job.py</code> <pre><code>def generate_chain_mask(chains: str) -&gt; set[str]:\n    \"\"\"From a string with a design_selection, format the chains provided\n\n    Args:\n        chains: The specified chains separated by commas to split\n    Returns:\n        The provided chain ids in pose format\n    \"\"\"\n    return set(utils.clean_comma_separated_string(chains))\n</code></pre>"},{"location":"reference/resources/ml/","title":"ml","text":""},{"location":"reference/resources/ml/#resources.ml.proteinmpnn_factory","title":"proteinmpnn_factory  <code>module-attribute</code>","text":"<pre><code>proteinmpnn_factory: Annotated[ProteinMPNNFactory, 'Calling this factory method returns the single instance of the ProteinMPNN class located at the \"source\" keyword argument'] = ProteinMPNNFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the Database class located at the \"source\" keyword  argument</p>"},{"location":"reference/resources/ml/#resources.ml.ProteinMPNNFactory","title":"ProteinMPNNFactory","text":"<pre><code>ProteinMPNNFactory(**kwargs)\n</code></pre> <p>Return a ProteinMPNN instance by calling the Factory instance with the ProteinMPNN model name</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer to the named ProteinMPNN model</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def __init__(self, **kwargs):\n    self._models = {}\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.ProteinMPNNFactory.__call__","title":"__call__","text":"<pre><code>__call__(model_name: str = 'v_48_020', backbone_noise: float = 0.0, ca_only: bool = False, **kwargs) -&gt; ProteinMPNN\n</code></pre> <p>Return the specified ProteinMPNN object singleton</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>             (<code>str</code>, default:                 <code>'v_48_020'</code> )         \u2013          <p>The name of the model to use from ProteinMPNN taking the format v_X_Y, where X is neighbor distance and Y is noise</p> </li> <li> <code>backbone_noise</code>             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>The amount of backbone noise to add to the pose during design</p> </li> <li> <code>ca_only</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a minimal CA variant of the protein should be used for design calculations</p> </li> </ul> <p>Returns:     The instance of the initialized ProteinMPNN model</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def __call__(self, model_name: str = 'v_48_020', backbone_noise: float = 0., ca_only: bool = False, **kwargs) \\\n        -&gt; ProteinMPNN:\n    \"\"\"Return the specified ProteinMPNN object singleton\n\n    Args:\n        model_name: The name of the model to use from ProteinMPNN taking the format v_X_Y,\n            where X is neighbor distance and Y is noise\n        backbone_noise: The amount of backbone noise to add to the pose during design\n        ca_only: Whether a minimal CA variant of the protein should be used for design calculations\n    Returns:\n        The instance of the initialized ProteinMPNN model\n    \"\"\"\n    if ca_only:\n        ca = '_ca'\n        if model_name == 'v_48_030':\n            logger.error(f\"No such ca_only model 'v_48_030'. Loading ca_only model 'v_48_020' (highest \"\n                         f\"backbone noise ca_only model) instead\")\n            model_name = 'v_48_020'\n        weights_dir = utils.path.protein_mpnn_ca_weights_dir\n        required_memory = ca_model_memory\n    else:\n        ca = ''\n        weights_dir = utils.path.protein_mpnn_weights_dir\n        required_memory = vanilla_model_memory\n\n    model_name_key = f'{model_name}{ca}_{backbone_noise}'\n    model = self._models.get(model_name_key)\n    if model:\n        return model\n    else:  # Create a new ProteinMPNN model instance\n        # if not self._models:  # Nothing initialized\n        # Acquire an adequate computing device\n        if torch.cuda.is_available():\n            max_memory = required_memory\n            for device_int in range(torch.cuda.device_count()):\n                available_memory = get_device_memory(torch.device(device_int), free=True)\n                if available_memory &gt; max_memory:\n                    max_memory = available_memory\n                    device_id = device_int\n            try:\n                device: torch.device = torch.device(device_id)\n            except UnboundLocalError:  # No device has memory greater than ProteinMPNN minimum required\n                device = torch.device('cpu')\n            else:\n                # Set the environment to use memory efficient cuda management\n                max_split = 1000\n                pytorch_conf = f'max_split_size_mb:{max_split},' \\\n                               f'roundup_power2_divisions:4,' \\\n                               f'garbage_collection_threshold:0.7'\n                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = pytorch_conf\n                logger.debug(f'Setting pytorch configuration:\\n{pytorch_conf}\\n'\n                             f'Result:{os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\")}')\n        else:\n            device = torch.device('cpu')\n\n        checkpoint = torch.load(os.path.join(weights_dir, f'{model_name}.pt'), map_location=device)\n        hidden_dim = 128\n        num_layers = 3\n        with torch.no_grad():\n            model = _ProteinMPNN(num_letters=mpnn_alphabet_length,\n                                 node_features=hidden_dim,\n                                 edge_features=hidden_dim,\n                                 hidden_dim=hidden_dim,\n                                 num_encoder_layers=num_layers,\n                                 num_decoder_layers=num_layers,\n                                 augment_eps=backbone_noise,\n                                 k_neighbors=checkpoint['num_edges'],\n                                 ca_only=ca_only)\n            model.to(device)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            model.eval()\n            model.device = device\n            model.model_name = model_name_key\n\n        model.log.info(f\"ProteinMPNN model '{model_name_key}' on device '{device}' has \"\n                       f'{checkpoint[\"num_edges\"]} edges and {checkpoint[\"noise_level\"]} Angstroms of training '\n                       'noise')\n        # number_of_mpnn_model_parameters = sum([math.prod(param.size()) for param in model.parameters()])\n        # logger.debug(f'The number of proteinmpnn model parameters is: {number_of_mpnn_model_parameters}')\n\n        self._models[model_name_key] = model\n\n    return model\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.ProteinMPNNFactory.get","title":"get","text":"<pre><code>get(**kwargs) -&gt; ProteinMPNN\n</code></pre> <p>Return the specified ProteinMPNN object singleton</p> <p>Returns:</p> <ul> <li> <code>ProteinMPNN</code>         \u2013          <p>The instance of the initialized ProteinMPNN model</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def get(self, **kwargs) -&gt; ProteinMPNN:\n    \"\"\"Return the specified ProteinMPNN object singleton\n\n    Keyword Args:\n        model_name - str = 'v_48_020' - The name of the model to use from ProteinMPNN.\n            v_X_Y where X is neighbor distance, and Y is noise\n        backbone_noise - float = 0.0 - The amount of backbone noise to add to the pose during design\n\n    Returns:\n        The instance of the initialized ProteinMPNN model\n    \"\"\"\n    return self.__call__(**kwargs)\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.RunModel","title":"RunModel","text":"<pre><code>RunModel(config: ConfigDict, params: Optional[Mapping[str, Mapping[str, ndarray]]] = None, device: Device = None)\n</code></pre> <p>             Bases: <code>RunModel</code></p> <p>Container for JAX model.</p> <pre><code>params:\ndevice: The device the model should be compiled on\n</code></pre> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def __init__(self,\n             config: ml_collections.ConfigDict,\n             params: Optional[Mapping[str, Mapping[str, jnp.ndarray]]] = None,\n             # SYMDESIGN\n             device: jax_xla.Device = None):\n  \"\"\"\n\n  Args:\n      config:\n      params:\n      device: The device the model should be compiled on\n  \"\"\"\n  # SYMDESIGN\n  self.config = config\n  self.params = params\n  self.multimer_mode = config.model.global_config.multimer_mode\n  # SYMDESIGN\n  self.parameter_map = {}\n  # SYMDESIGN\n\n  if self.multimer_mode:\n    def _forward_fn(batch):\n      # SYMDESIGN\n      model = multimer.AlphaFoldInitialGuess(self.config.model)\n      # SYMDESIGN\n      return model(\n          batch,\n          is_training=False)\n  else:\n    def _forward_fn(batch):\n      # SYMDESIGN\n      model = monomer.AlphaFoldInitialGuess(self.config.model)\n      # SYMDESIGN\n      return model(\n          batch,\n          is_training=False,\n          compute_loss=False,\n          ensemble_representations=True)\n\n  self.apply = jax.jit(hk.transform(_forward_fn).apply, device=device)\n  self.init = jax.jit(hk.transform(_forward_fn).init, device=device)\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.RunModel.predict","title":"predict","text":"<pre><code>predict(feat: FeatureDict, random_seed: int) -&gt; Mapping[str, Any]\n</code></pre> <p>Makes a prediction by inferencing the model on the provided features.</p> <p>Parameters:</p> <ul> <li> <code>feat</code>             (<code>FeatureDict</code>)         \u2013          <p>A dictionary of NumPy feature arrays as output by RunModel.process_features.</p> </li> <li> <code>random_seed</code>             (<code>int</code>)         \u2013          <p>The random seed to use when running the model. In the multimer model this controls the MSA sampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Mapping[str, Any]</code>         \u2013          <p>A dictionary of model outputs.</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def predict(self,\n            feat: features.FeatureDict,\n            random_seed: int,\n            ) -&gt; Mapping[str, Any]:\n  \"\"\"Makes a prediction by inferencing the model on the provided features.\n\n  Args:\n    feat: A dictionary of NumPy feature arrays as output by\n      RunModel.process_features.\n    random_seed: The random seed to use when running the model. In the\n      multimer model this controls the MSA sampling.\n\n  Returns:\n    A dictionary of model outputs.\n  \"\"\"\n  self.init_params(feat)\n  logging.info('Running predict with shape(feat) = %s',\n               tree.map_structure(lambda x: x.shape, feat))\n  result = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)\n\n  # This block is to ensure benchmark timings are accurate. Some blocking is\n  # already happening when computing get_confidence_metrics, and this ensures\n  # all outputs are blocked on.\n  jax.tree_map(lambda x: x.block_until_ready(), result)\n  # SYMDESIGN\n  result.update(\n      afmodel.get_confidence_metrics(result, multimer_mode=self.multimer_mode))\n  # SYMDESIGN\n  logging.info('Output shape was %s',\n               tree.map_structure(lambda x: x.shape, result))\n  return result\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.RunModel.set_params","title":"set_params","text":"<pre><code>set_params(model_params: dict[str, Mapping[str, Mapping[str, ndarray]]])\n</code></pre> <p>Set a collection of parameters that a single compiled model should run</p> <p>Parameters:</p> <ul> <li> <code>model_params</code>             (<code>dict[str, Mapping[str, Mapping[str, ndarray]]]</code>)         \u2013          <p>A dictionary of model parameters</p> </li> </ul> <p>Returns:     None</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def set_params(self, model_params: dict[str, Mapping[str, Mapping[str, jnp.ndarray]]]):\n    \"\"\"Set a collection of parameters that a single compiled model should run\n\n    Args:\n        model_params: A dictionary of model parameters\n    Returns:\n        None\n    \"\"\"\n    self.parameter_map = model_params\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.RunModel.predict_with_params","title":"predict_with_params","text":"<pre><code>predict_with_params(parameter_type: str, feat: FeatureDict, random_seed: int) -&gt; Mapping[str, Any]\n</code></pre> <p>Makes a prediction by inferencing the model on the provided features.</p> <p>Parameters:</p> <ul> <li> <code>parameter_type</code>             (<code>str</code>)         \u2013          <p>The name of the parameter set to fetch</p> </li> <li> <code>feat</code>             (<code>FeatureDict</code>)         \u2013          <p>A dictionary of NumPy feature arrays as output by RunModel.process_features.</p> </li> <li> <code>random_seed</code>             (<code>int</code>)         \u2013          <p>The random seed to use when running the model. In the multimer model this controls the MSA sampling.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Mapping[str, Any]</code>         \u2013          <p>A dictionary of model outputs.</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def predict_with_params(self, parameter_type: str,\n                        feat: features.FeatureDict,\n                        random_seed: int,\n                        ) -&gt; Mapping[str, Any]:\n    \"\"\"Makes a prediction by inferencing the model on the provided features.\n\n    Args:\n        parameter_type: The name of the parameter set to fetch\n        feat: A dictionary of NumPy feature arrays as output by\n            RunModel.process_features.\n        random_seed: The random seed to use when running the model. In the\n            multimer model this controls the MSA sampling.\n\n    Returns:\n        A dictionary of model outputs.\n    \"\"\"\n    logging.info('Running predict with shape(feat) = %s',\n                 tree.map_structure(lambda x: x.shape, feat))\n    try:\n        params = self.parameter_map[parameter_type]\n    except KeyError:\n        raise KeyError(f\"The parameter_type='{parameter_type}' isn't available from the viable parameter \"\n                       f\"sets\\nCurrently available types include: {', '.join(self.parameter_map.keys())}\")\n    result = self.apply(params, jax.random.PRNGKey(random_seed), feat)\n\n    # This block is to ensure benchmark timings are accurate. Some blocking is\n    # already happening when computing get_confidence_metrics, and this ensures\n    # all outputs are blocked on.\n    jax.tree_map(lambda x: x.block_until_ready(), result)\n    result.update(\n        afmodel.get_confidence_metrics(result, multimer_mode=self.multimer_mode))\n    logging.info('Output shape was %s',\n                 tree.map_structure(lambda x: x.shape, result))\n    return result\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.get_device_memory","title":"get_device_memory","text":"<pre><code>get_device_memory(device: device | int | str | None, free: bool = False) -&gt; int\n</code></pre> <p>Get the memory available for a requested device to calculate computational constraints</p> <p>Parameters:</p> <ul> <li> <code>device</code>             (<code>device | int | str | None</code>)         \u2013          <p>The current device of the pytorch model in question</p> </li> <li> <code>free</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return the free memory if the device is a cuda GPU, otherwise return all pytorch memory</p> </li> </ul> <p>Returns:     The bytes of memory available</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def get_device_memory(device: torch.device | int | str | None, free: bool = False) -&gt; int:\n    \"\"\"Get the memory available for a requested device to calculate computational constraints\n\n    Args:\n        device: The current device of the pytorch model in question\n        free: Whether to return the free memory if the device is a cuda GPU, otherwise return all pytorch memory\n    Returns:\n        The bytes of memory available\n    \"\"\"\n    if not isinstance(device, torch.device):\n        device = torch.device(device)\n\n    if device.type == 'cpu':  # device is None or\n        memory_constraint = utils.get_available_memory()\n        logger.debug(f'The available cpu memory is: {memory_constraint}')\n    else:\n        free_memory, gpu_memory_total = torch.cuda.mem_get_info(device)\n        logger.debug(f'The available gpu memory is: {free_memory}')\n        memory_reserved = torch.cuda.memory_reserved(device)\n        logger.debug(f'The reserved gpu memory is: {memory_reserved}')\n        if free:\n            memory_constraint = free_memory\n        else:\n            memory_constraint = free_memory + memory_reserved\n\n    return memory_constraint\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.calculate_proteinmpnn_batch_length","title":"calculate_proteinmpnn_batch_length","text":"<pre><code>calculate_proteinmpnn_batch_length(model: ProteinMPNN, number_of_residues: int, element_memory: int = 4) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>ProteinMPNN</code>)         \u2013          <p>The ProteinMPNN model</p> </li> <li> <code>number_of_residues</code>             (<code>int</code>)         \u2013          <p>The number of residues used in the ProteinMPNN model</p> </li> <li> <code>element_memory</code>             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>Where each element is np.int64, np.float32, etc.</p> </li> </ul> <p>Returns:     The size of the batch that can be completed for the ProteinMPNN model given it's device</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def calculate_proteinmpnn_batch_length(model: ProteinMPNN, number_of_residues: int, element_memory: int = 4) -&gt; int:\n    \"\"\"\n\n    Args:\n        model: The ProteinMPNN model\n        number_of_residues: The number of residues used in the ProteinMPNN model\n        element_memory: Where each element is np.int64, np.float32, etc.\n    Returns:\n        The size of the batch that can be completed for the ProteinMPNN model given it's device\n    \"\"\"\n    memory_constraint = get_device_memory(model.device)\n\n    number_of_elements_available = memory_constraint // element_memory\n    logger.debug(f'The number_of_elements_available is: {number_of_elements_available}')\n    number_of_model_parameter_elements = sum([math.prod(param.size()) for param in model.parameters()])\n    logger.debug(f'The number_of_model_parameter_elements is: {number_of_model_parameter_elements}')\n    model_elements = number_of_model_parameter_elements\n    # Todo use 5 as ideal CB is added by the model later with ca_only = False\n    num_model_residues = 5\n    model_elements += math.prod((number_of_residues, num_model_residues, 3))  # X,\n    model_elements += number_of_residues  # S.shape\n    model_elements += number_of_residues  # chain_mask.shape\n    model_elements += number_of_residues  # chain_encoding.shape\n    model_elements += number_of_residues  # residue_idx.shape\n    model_elements += number_of_residues  # mask.shape\n    model_elements += number_of_residues  # residue_mask.shape\n    model_elements += math.prod((number_of_residues, 21))  # omit_AA_mask.shape\n    model_elements += number_of_residues  # pssm_coef.shape\n    model_elements += math.prod((number_of_residues, 20))  # pssm_bias.shape\n    model_elements += math.prod((number_of_residues, 20))  # pssm_log_odds_mask.shape\n    model_elements += number_of_residues  # tied_beta.shape\n    model_elements += math.prod((number_of_residues, 21))  # bias_by_res.shape\n    logger.debug(f'The number of model_elements is: {model_elements}')\n\n    number_of_batches = number_of_elements_available // model_elements\n    batch_length = number_of_batches // proteinmpnn_batch_divisor\n    if batch_length == 0:\n        not_enough_proteinmpnn_memory = f\"Can't find a device for {model} with enough memory to complete a single \" \\\n                                        f\"batch of work with {number_of_residues} residues in the model\"\n        if model.device.type == 'cpu':\n            raise RuntimeError(not_enough_proteinmpnn_memory)\n\n        old_device = model.device\n        # This won't work. Try to put the model on a new device\n        max_memory = vanilla_model_memory\n        for device_int in range(torch.cuda.device_count()):\n            available_memory = get_device_memory(torch.device(device_int), free=True)\n            if available_memory &gt; max_memory:\n                max_memory = available_memory\n                device_id = device_int\n        try:\n            device: torch.device = torch.device(device_id)\n        except UnboundLocalError:  # No device has memory greater than ProteinMPNN minimum required\n            device = torch.device('cpu')\n\n        if device == old_device:\n            # Solve using gpu is stuck\n            if device.type == 'cpu':\n                # This hasn't been changed or device is cpu\n                raise RuntimeError(not_enough_proteinmpnn_memory)\n            else:\n                # Try one more time ensuring cpu. This will be caught above if still not enough memory\n                device = torch.device('cpu')\n\n        # Set the device parameters\n        model.to(device)\n        model.device = device\n        # Recurse\n        return calculate_proteinmpnn_batch_length(model, number_of_residues, element_memory)\n\n    return batch_length\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.batch_calculation","title":"batch_calculation","text":"<pre><code>batch_calculation(size: int, batch_length: int, setup: Callable = None, compute_failure_exceptions: tuple[Type[Exception], ...] = (Exception)) -&gt; Callable\n</code></pre> <p>Use as a decorator to execute a function in batches over an input that is too large for available computational resources, typically memory</p> <p>Produces the variables actual_batch_length and batch_slice that can be used inside the decorated function</p> <p>Parameters:</p> <ul> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>The total number of units of work to be done</p> </li> <li> <code>batch_length</code>             (<code>int</code>)         \u2013          <p>The starting length of a batch. This should be chosen empirically</p> </li> <li> <code>setup</code>             (<code>Callable</code>, default:                 <code>None</code> )         \u2013          <p>A Callable which should be called before the batches are executed to produce data that is passed to the function. The first argument of this Callable should be batch_length</p> </li> <li> <code>compute_failure_exceptions</code>             (<code>tuple[Type[Exception], ...]</code>, default:                 <code>(Exception)</code> )         \u2013          <p>A tuple of possible exceptions which upon raising should be allowed to restart</p> </li> </ul> <p>Decorated Callable Args:     args: The arguments to pass to the function     kwargs: Keyword Arguments to pass to the decorated Callable     setup_args: Arguments to pass to the setup Callable     setup_kwargs: Keyword Arguments to pass to the setup Callable     return_containers: dict - The key and SupportsIndex value to store decorated Callable returns inside Returns:     The populated function_return_containers</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def batch_calculation(size: int, batch_length: int, setup: Callable = None,\n                      compute_failure_exceptions: tuple[Type[Exception], ...] = (Exception,)) -&gt; Callable:\n    \"\"\"Use as a decorator to execute a function in batches over an input that is too large for available computational\n    resources, typically memory\n\n    Produces the variables actual_batch_length and batch_slice that can be used inside the decorated function\n\n    Args:\n        size: The total number of units of work to be done\n        batch_length: The starting length of a batch. This should be chosen empirically\n        setup: A Callable which should be called before the batches are executed to produce data that is passed to the\n            function. The first argument of this Callable should be batch_length\n        compute_failure_exceptions: A tuple of possible exceptions which upon raising should be allowed to restart\n    Decorated Callable Args:\n        args: The arguments to pass to the function\n        kwargs: Keyword Arguments to pass to the decorated Callable\n        setup_args: Arguments to pass to the setup Callable\n        setup_kwargs: Keyword Arguments to pass to the setup Callable\n        return_containers: dict - The key and SupportsIndex value to store decorated Callable returns inside\n    Returns:\n        The populated function_return_containers\n    \"\"\"\n    def wrapper(func: Callable) -&gt; Callable[[tuple[Any, ...], dict | None, tuple, dict | None, dict[str, Any]], dict]:\n        if setup is None:\n            def setup_(*_args, **_kwargs) -&gt; dict:\n                return {}\n        else:\n            setup_ = setup\n\n        @functools.wraps(func)\n        def wrapped(*args, return_containers: dict = None,\n                    setup_args: tuple = tuple(), setup_kwargs: dict = None, **kwargs) -&gt; dict:\n\n            if return_containers is None:\n                return_containers = {}\n\n            if setup_kwargs is None:\n                setup_kwargs = {}\n\n            _batch_length = batch_length\n            # finished = False\n            _error = last_error = None\n            while True:  # not finished:\n                logger.debug(f'The batch_length is: {_batch_length}')\n                try:  # The next batch_length\n                    # The number_of_batches indicates how many iterations are needed to exhaust all models\n                    try:\n                        number_of_batches = int(ceil(size/_batch_length) or 1)  # Select at least 1\n                    except ZeroDivisionError:  # We hit the minimal batch size. Report the previous error\n                        if last_error is not None:  # This exited from the compute_failure_exceptions except\n                            break  # break out and raise the _error\n                        else:\n                            raise ValueError(\n                                f'The batch_length ({batch_length}) must be greater than 0')\n                    # Perform any setup operations\n                    # logger.critical(f'Before SETUP\\nmemory_allocated: {torch.cuda.memory_allocated()}'\n                    #                 f'\\nmemory_reserved: {torch.cuda.memory_reserved()}')\n                    setup_start = time.time()\n                    setup_returns = setup_(_batch_length, *setup_args, **setup_kwargs)\n                    logger.debug(f'{batch_calculation.__name__} setup function took {time.time() - setup_start:8f}s')\n                    # logger.critical(f'After SETUP\\nmemory_allocated: {torch.cuda.memory_allocated()}'\n                    #                 f'\\nmemory_reserved: {torch.cuda.memory_reserved()}')\n                    batch_start = time.time()\n                    for batch in range(number_of_batches):\n                        # Find the upper slice limit\n                        batch_slice = slice(batch * _batch_length, min((batch+1) * _batch_length, size))\n                        # Perform the function, batch_slice must be used inside the func\n                        logger.debug(f'Calculating batch {batch + 1}')\n                        function_returns = func(batch_slice, *args, **kwargs, **setup_returns)\n                        # Set the returned values in the order they were received to the precalculated return_container\n                        for return_key, return_value in list(function_returns.items()):\n                            try:  # To access the return_container_key in the function\n                                return_containers[return_key][batch_slice] = return_value\n                            except KeyError:  # If it doesn't exist\n                                raise KeyError(\n                                    f\"Couldn't return the data specified by {return_key} to the return_container with \"\n                                    f\"keys:{', '.join(return_containers.keys())}\")\n                            except ValueError as error:  # Arrays are incorrectly sized\n                                raise ValueError(\n                                    f\"Couldn't return the data specified by {return_key} from {func.__name__} due to: \"\n                                    f\"{error}\")\n                        # for return_container_key, return_container in list(return_containers.items()):\n                        #     try:  # To access the return_container_key in the function\n                        #         return_container[batch_slice] = function_returns[return_container_key]\n                        #     except KeyError:  # If it doesn't exist\n                        #         # Remove the data from the return_containers\n                        #         return_containers.pop(return_container_key)\n\n                    # Report success\n                    logger.debug(f'Successful execution with batch_length of {_batch_length}. '\n                                 f'Took {time.time() - batch_start:8f}s')\n                    last_error = None\n                    break  # finished = True\n                except compute_failure_exceptions as error:\n                    # del setup_returns\n                    # logger.critical(f'After ERROR\\nmemory_allocated: {torch.cuda.memory_allocated()}'\n                    #                 f'\\nmemory_reserved: {torch.cuda.memory_reserved()}')\n                    # gc.collect()\n                    # logger.critical(f'After GC\\nmemory_allocated: {torch.cuda.memory_allocated()}'\n                    #                 f'\\nmemory_reserved: {torch.cuda.memory_reserved()}')\n                    if _error is None:  # Set the error the first time\n                        # _error = last_error = error\n                        _error = last_error = traceback.format_exc()  # .format_exception(error)\n                    else:\n                        # last_error = error\n                        last_error = traceback.format_exc()  # .format_exception(error)\n                    _batch_length -= 1\n\n            if last_error is not None:  # This exited from the ZeroDivisionError except\n                # try:\n                logger.critical(f'{batch_calculation.__name__} exited with the following exceptions:\\n\\nThe first '\n                                f'exception in the traceback was the result of the first iteration, while the '\n                                f'most recent exception in the traceback is last\\n')\n                # raise _error\n                print(''.join(_error))\n                # except compute_failure_exceptions:\n                #     raise last_error\n                print(''.join(last_error))\n                raise utils.SymDesignException(\n                    f\"{func.__name__}() wasn't able to be executed. See the above traceback\")\n\n            return return_containers\n        return wrapped\n    return wrapper\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.create_decoding_order","title":"create_decoding_order","text":"<pre><code>create_decoding_order(randn: Tensor, chain_mask: Tensor, tied_pos: Iterable[Container] = None, to_device: str = None, **kwargs) -&gt; Tensor\n</code></pre> <p>Parameters:</p> <ul> <li> <code>randn</code>             (<code>Tensor</code>)         \u2013          </li> <li> <code>chain_mask</code>             (<code>Tensor</code>)         \u2013          </li> <li> <code>tied_pos</code>             (<code>Iterable[Container]</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>to_device</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          </li> </ul> <p>Returns:</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def create_decoding_order(randn: torch.Tensor, chain_mask: torch.Tensor, tied_pos: Iterable[Container] = None,\n                          to_device: str = None, **kwargs) -&gt; torch.Tensor:\n    \"\"\"\n\n    Args:\n        randn:\n        chain_mask:\n        tied_pos:\n        to_device:\n\n    Returns:\n\n    \"\"\"\n    if to_device is None:\n        to_device = randn.device\n    # Numbers are smaller for places where chain_mask = 0.0 and higher for places where chain_mask = 1.0\n    decoding_order = torch.argsort((chain_mask+0.0001) * (torch.abs(randn)))\n\n    if tied_pos is not None:\n        # Calculate the tied decoding order according to ProteinMPNN.tied_sample()\n        new_decoding_order: list[list[int]] = []\n        found_decoding_indices = []\n        for t_dec in list(decoding_order[0].cpu().numpy()):\n            if t_dec not in found_decoding_indices:\n                for item in tied_pos:\n                    if t_dec in item:\n                        break\n                else:\n                    item = [t_dec]\n                # Keep list of lists format\n                new_decoding_order.append(item)\n                # Add all found decoding_indices\n                found_decoding_indices.extend(item)\n\n        decoding_order = torch.tensor(found_decoding_indices, device=to_device)[None].repeat(len(randn), 1)\n\n    return decoding_order\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.batch_proteinmpnn_input","title":"batch_proteinmpnn_input","text":"<pre><code>batch_proteinmpnn_input(size: int = None, **kwargs) -&gt; dict[str, ndarray]\n</code></pre> <p>Set up all data for batches of proteinmpnn design</p> <p>Parameters:</p> <ul> <li> <code>size</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of inputs to use. If left blank, the size will be inferred from axis=0 of the X array</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>X</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter X</p> </li> <li> <code>X_unbound</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter X_unbound</p> </li> <li> <code>S</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter S</p> </li> <li> <code>randn</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter randn</p> </li> <li> <code>chain_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_mask</p> </li> <li> <code>chain_encoding</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_encoding</p> </li> <li> <code>residue_idx</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter residue_idx</p> </li> <li> <code>mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter mask</p> </li> <li> <code>chain_M_pos</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_M_pos (residue_mask)</p> </li> <li> <code>omit_AA_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter omit_AA_mask</p> </li> <li> <code>pssm_coef</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_coef</p> </li> <li> <code>pssm_bias</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_bias</p> </li> <li> <code>pssm_log_odds_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_log_odds_mask</p> </li> <li> <code>bias_by_res</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter bias_by_res</p> </li> <li> <code>tied_beta</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter tied_beta</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>A dictionary with each of the ProteinMPNN parameters formatted in a batch</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def batch_proteinmpnn_input(size: int = None, **kwargs) -&gt; dict[str, np.ndarray]:\n    \"\"\"Set up all data for batches of proteinmpnn design\n\n    Args:\n        size: The number of inputs to use. If left blank, the size will be inferred from axis=0 of the X array\n\n    Keyword Args:\n        X: numpy.ndarray = None - The array specifying the parameter X\n        X_unbound: numpy.ndarray = None - The array specifying the parameter X_unbound\n        S: numpy.ndarray = None - The array specifying the parameter S\n        randn: numpy.ndarray = None - The array specifying the parameter randn\n        chain_mask: numpy.ndarray = None - The array specifying the parameter chain_mask\n        chain_encoding: numpy.ndarray = None - The array specifying the parameter chain_encoding\n        residue_idx: numpy.ndarray = None - The array specifying the parameter residue_idx\n        mask: numpy.ndarray = None - The array specifying the parameter mask\n        chain_M_pos: numpy.ndarray = None - The array specifying the parameter chain_M_pos (residue_mask)\n        omit_AA_mask: numpy.ndarray = None - The array specifying the parameter omit_AA_mask\n        pssm_coef: numpy.ndarray = None - The array specifying the parameter pssm_coef\n        pssm_bias: numpy.ndarray = None - The array specifying the parameter pssm_bias\n        pssm_log_odds_mask: numpy.ndarray = None - The array specifying the parameter pssm_log_odds_mask\n        bias_by_res: numpy.ndarray = None - The array specifying the parameter bias_by_res\n        tied_beta: numpy.ndarray = None - The array specifying the parameter tied_beta\n\n    Returns:\n        A dictionary with each of the ProteinMPNN parameters formatted in a batch\n    \"\"\"\n    # This is my preferred name for the chain_M_pos...\n    # residue_mask: (numpy.ndarray) = None - The array specifying the parameter residue_mask of ProteinMPNN\n    if size is None:  # Use X as is\n        X = kwargs.get('X')\n        if X is None:\n            raise ValueError(\n                f\"{batch_proteinmpnn_input.__name__} must pass keyword argument 'X' if argument 'size' is None\")\n        size = len(X)\n    # else:\n    #     X = np.tile(X, (size,) + (1,)*X.ndim)\n\n    # Stack ProteinMPNN sequence design task in \"batches\"\n    device_kwargs = {}\n    for key in batch_params:\n        param = kwargs.pop(key, None)\n        if param is not None:\n            device_kwargs[key] = np.tile(param, (size,) + (1,)*param.ndim)\n\n    # Add all kwargs that were not accessed back to the return dictionary\n    device_kwargs.update(**kwargs)\n    return device_kwargs\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.proteinmpnn_to_device","title":"proteinmpnn_to_device","text":"<pre><code>proteinmpnn_to_device(device: str = None, **kwargs) -&gt; dict[str, Tensor]\n</code></pre> <p>Set up all data to torch.Tensors for ProteinMPNN design</p> <p>Parameters:</p> <ul> <li> <code>device</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The device to load tensors to</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>X</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter X</p> </li> <li> <code>X_unbound</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter X_unbound</p> </li> <li> <code>S</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter S</p> </li> <li> <code>randn</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter randn</p> </li> <li> <code>chain_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_mask</p> </li> <li> <code>chain_encoding</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_encoding</p> </li> <li> <code>residue_idx</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter residue_idx</p> </li> <li> <code>mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter mask</p> </li> <li> <code>chain_M_pos</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter chain_M_pos (residue_mask)</p> </li> <li> <code>omit_AA_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter omit_AA_mask</p> </li> <li> <code>pssm_coef</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_coef</p> </li> <li> <code>pssm_bias</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_bias</p> </li> <li> <code>pssm_log_odds_mask</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter pssm_log_odds_mask</p> </li> <li> <code>bias_by_res</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter bias_by_res</p> </li> <li> <code>tied_beta</code>         \u2013          <p>numpy.ndarray = None - The array specifying the parameter tied_beta</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Tensor]</code>         \u2013          <p>The torch.Tensor ProteinMPNN parameters</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def proteinmpnn_to_device(device: str = None, **kwargs) -&gt; dict[str, torch.Tensor]:\n    \"\"\"Set up all data to torch.Tensors for ProteinMPNN design\n\n    Args:\n        device: The device to load tensors to\n\n    Keyword Args:\n        X: numpy.ndarray = None - The array specifying the parameter X\n        X_unbound: numpy.ndarray = None - The array specifying the parameter X_unbound\n        S: numpy.ndarray = None - The array specifying the parameter S\n        randn: numpy.ndarray = None - The array specifying the parameter randn\n        chain_mask: numpy.ndarray = None - The array specifying the parameter chain_mask\n        chain_encoding: numpy.ndarray = None - The array specifying the parameter chain_encoding\n        residue_idx: numpy.ndarray = None - The array specifying the parameter residue_idx\n        mask: numpy.ndarray = None - The array specifying the parameter mask\n        chain_M_pos: numpy.ndarray = None - The array specifying the parameter chain_M_pos (residue_mask)\n        omit_AA_mask: numpy.ndarray = None - The array specifying the parameter omit_AA_mask\n        pssm_coef: numpy.ndarray = None - The array specifying the parameter pssm_coef\n        pssm_bias: numpy.ndarray = None - The array specifying the parameter pssm_bias\n        pssm_log_odds_mask: numpy.ndarray = None - The array specifying the parameter pssm_log_odds_mask\n        bias_by_res: numpy.ndarray = None - The array specifying the parameter bias_by_res\n        tied_beta: numpy.ndarray = None - The array specifying the parameter tied_beta\n\n    Returns:\n        The torch.Tensor ProteinMPNN parameters\n    \"\"\"\n    if device is None:\n        raise ValueError('Must provide the desired device to load proteinmpnn')\n    logger.debug(f'Loading ProteinMPNN parameters to device: {device}')\n\n    # Convert all numpy arrays to pytorch\n    device_kwargs = {}\n    for key, dtype in dtype_map.items():\n        param = kwargs.pop(key, None)\n        if param is not None:\n            device_kwargs[key] = torch.from_numpy(param).to(dtype=dtype, device=device)\n\n    # Add all kwargs that were not accessed back to the return dictionary\n    device_kwargs.update(**kwargs)\n    return device_kwargs\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.setup_pose_batch_for_proteinmpnn","title":"setup_pose_batch_for_proteinmpnn","text":"<pre><code>setup_pose_batch_for_proteinmpnn(batch_length: int, device, **parameters) -&gt; dict[str, ndarray | Tensor]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>batch_length</code>             (<code>int</code>)         \u2013          <p>The length the batch to set up</p> </li> <li> <code>device</code>         \u2013          <p>The device used for batch calculations</p> </li> </ul> <p>Returns:     A mapping of necessary containers for ProteinMPNN inference in batches and loaded to the device</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>@torch.no_grad()  # Ensure no gradients are produced\ndef setup_pose_batch_for_proteinmpnn(batch_length: int, device, **parameters) -&gt; dict[str, np.ndarray | torch.Tensor]:\n    \"\"\"\n\n    Args:\n        batch_length: The length the batch to set up\n        device: The device used for batch calculations\n    Returns:\n        A mapping of necessary containers for ProteinMPNN inference in batches and loaded to the device\n    \"\"\"\n    # batch_length = batch_slice.stop - batch_slice.start\n    # Create batch_length fixed parameter data which are the same across poses\n    batch_parameters: dict[str, np.ndarray | torch.Tensor] = \\\n        batch_proteinmpnn_input(size=batch_length, **parameters)\n    # Move fixed data structures to the model device\n    # Update parameters as some are not transferred to the identified device\n    batch_parameters.update(proteinmpnn_to_device(device, **batch_parameters))\n\n    return batch_parameters\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.proteinmpnn_batch_design","title":"proteinmpnn_batch_design","text":"<pre><code>proteinmpnn_batch_design(batch_slice: slice, proteinmpnn: ProteinMPNN, X: Tensor = None, randn: Tensor = None, S: Tensor = None, chain_mask: Tensor = None, chain_encoding: Tensor = None, residue_idx: Tensor = None, mask: Tensor = None, temperatures: Sequence[float] = (0.1), pose_length: int = None, bias_by_res: Tensor = None, tied_pos: Iterable[Container] = None, X_unbound: Tensor = None, **batch_parameters) -&gt; dict[str, ndarray]\n</code></pre> <p>Perform ProteinMPNN design tasks on input that is split into batches</p> <p>Parameters:</p> <ul> <li> <code>batch_slice</code>             (<code>slice</code>)         \u2013          </li> <li> <code>proteinmpnn</code>             (<code>ProteinMPNN</code>)         \u2013          </li> <li> <code>X</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>randn</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>S</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain_mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain_encoding</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>residue_idx</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>temperatures</code>             (<code>Sequence[float]</code>, default:                 <code>(0.1)</code> )         \u2013          </li> <li> <code>pose_length</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>bias_by_res</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>tied_pos</code>             (<code>Iterable[Container]</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>X_unbound</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> </ul> <p>Returns:     A mapping of the key describing to the corresponding value, i.e. sequences, complex_sequence_loss, and         unbound_sequence_loss</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def proteinmpnn_batch_design(batch_slice: slice, proteinmpnn: ProteinMPNN,\n                             X: torch.Tensor = None,\n                             randn: torch.Tensor = None,\n                             S: torch.Tensor = None,\n                             chain_mask: torch.Tensor = None,\n                             chain_encoding: torch.Tensor = None,\n                             residue_idx: torch.Tensor = None,\n                             mask: torch.Tensor = None,\n                             temperatures: Sequence[float] = (0.1,),\n                             pose_length: int = None,\n                             bias_by_res: torch.Tensor = None,\n                             tied_pos: Iterable[Container] = None,\n                             X_unbound: torch.Tensor = None,\n                             **batch_parameters\n                             ) -&gt; dict[str, np.ndarray]:\n    \"\"\"Perform ProteinMPNN design tasks on input that is split into batches\n\n    Args:\n        batch_slice:\n        proteinmpnn:\n        X:\n        randn:\n        S:\n        chain_mask:\n        chain_encoding:\n        residue_idx:\n        mask:\n        temperatures:\n        pose_length:\n        bias_by_res:\n        tied_pos:\n        X_unbound:\n    Returns:\n        A mapping of the key describing to the corresponding value, i.e. sequences, complex_sequence_loss, and\n            unbound_sequence_loss\n    \"\"\"\n    # X = batch_parameters.pop('X', None)\n    # S = batch_parameters.pop('S', None)\n    # chain_mask = batch_parameters.pop('chain_mask', None)\n    # chain_encoding = batch_parameters.pop('chain_encoding', None)\n    # residue_idx = batch_parameters.pop('residue_idx', None)\n    # mask = batch_parameters.pop('mask', None)\n    # randn = batch_parameters.pop('randn', None)\n    # # omit_AAs_np = batch_parameters.get('omit_AAs_np', None)\n    # # bias_AAs_np = batch_parameters.get('bias_AAs_np', None)\n    residue_mask = batch_parameters.pop('chain_M_pos', None)  # name change makes more sense\n    # # omit_AA_mask = batch_parameters.get('omit_AA_mask', None)\n    # # pssm_coef = batch_parameters.get('pssm_coef', None)\n    # # pssm_bias = batch_parameters.get('pssm_bias', None)\n    # # pssm_multi = batch_parameters.get('pssm_multi', None)\n    # # pssm_log_odds_flag = batch_parameters.get('pssm_log_odds_flag', None)\n    # # pssm_log_odds_mask = batch_parameters.get('pssm_log_odds_mask', None)\n    # # pssm_bias_flag = batch_parameters.get('pssm_bias_flag', None)\n    # tied_pos = batch_parameters.pop('tied_pos', None)\n    # # tied_beta = batch_parameters.pop('tied_beta', None)\n    # # bias_by_res = batch_parameters.get('bias_by_res', None)\n\n    actual_batch_length = batch_slice.stop - batch_slice.start\n    # # Clone the data from the sequence tensor so that it can be set with the null token below\n    # S_design = S.detach().clone()\n    if pose_length is None:\n        batch_length, pose_length, *_ = S.shape\n    else:\n        batch_length, *_ = S.shape\n\n    if actual_batch_length != batch_length:\n        # Slice these for the last iteration\n        X = X[:actual_batch_length]  # , None)\n        chain_mask = chain_mask[:actual_batch_length]  # , None)\n        chain_encoding = chain_encoding[:actual_batch_length]  # , None)\n        residue_idx = residue_idx[:actual_batch_length]  # , None)\n        mask = mask[:actual_batch_length]  # , None)\n        bias_by_res = bias_by_res[:actual_batch_length]  # , None)\n        randn = randn[:actual_batch_length]\n        residue_mask = residue_mask[:actual_batch_length]\n        S = S[:actual_batch_length]  # , None)\n        # S_design = S_design[:actual_batch_length]  # , None)\n        # Unpack, unpacked keyword args\n        omit_AA_mask = batch_parameters.get('omit_AA_mask')\n        pssm_coef = batch_parameters.get('pssm_coef')\n        pssm_bias = batch_parameters.get('pssm_bias')\n        pssm_log_odds_mask = batch_parameters.get('pssm_log_odds_mask')\n        # Set keyword args\n        batch_parameters['omit_AA_mask'] = omit_AA_mask[:actual_batch_length]\n        batch_parameters['pssm_coef'] = pssm_coef[:actual_batch_length]\n        batch_parameters['pssm_bias'] = pssm_bias[:actual_batch_length]\n        batch_parameters['pssm_log_odds_mask'] = pssm_log_odds_mask[:actual_batch_length]\n        try:\n            X_unbound = X_unbound[:actual_batch_length]  # , None)\n        except TypeError:  # Can't slice NoneType\n            pass\n\n    # # Use the sequence as an unknown token then guess the probabilities given the remaining\n    # # information, i.e. the sequence and the backbone\n    # S_design_null[residue_mask.type(torch.bool)] = MPNN_NULL_IDX\n    chain_residue_mask = chain_mask * residue_mask\n\n    batch_sequences = []\n    _per_residue_complex_sequence_loss = []\n    _per_residue_unbound_sequence_loss = []\n    number_of_temps = len(temperatures)\n    for temp_idx, temperature in enumerate(temperatures):\n        sample_start_time = time.time()\n        if tied_pos is None:\n            sample_dict = proteinmpnn.sample(X, randn, S, chain_mask, chain_encoding, residue_idx, mask,\n                                             chain_M_pos=residue_mask, temperature=temperature, bias_by_res=bias_by_res,\n                                             **batch_parameters)\n        else:\n            sample_dict = proteinmpnn.tied_sample(X, randn, S, chain_mask, chain_encoding, residue_idx,\n                                                  mask, chain_M_pos=residue_mask, temperature=temperature,\n                                                  bias_by_res=bias_by_res, tied_pos=tied_pos, **batch_parameters)\n        proteinmpnn.log.info(f'Sample calculation took {time.time() - sample_start_time:8f}s')\n\n        # Format outputs - All have at lease shape (batch_length, model_length,)\n        S_sample = sample_dict['S']\n        _batch_sequences = S_sample[:, :pose_length]\n        # Check for null sequence output\n        null_seq = _batch_sequences == 20\n        # null_indices = np.argwhere(null_seq == 1)\n        # if null_indices.nelement():  # Not an empty tensor...\n        # Find the indices that are null on each axis\n        null_design_indices, null_sequence_indices = torch.nonzero(null_seq == 1, as_tuple=True)\n        if null_design_indices.nelement():  # Not an empty tensor...\n            proteinmpnn.log.warning(f'Found null sequence output... Resampling selected positions')\n            proteinmpnn.log.debug(f'At sequence position(s): {null_sequence_indices}')\n            null_seq = (False,)\n            sampled_probs = sample_dict['probs'].cpu()\n            while not all(null_seq):  # null_indices.nelement():  # Not an empty tensor...\n                # _decoding_order = decoding_order.cpu().numpy()[:, :pose_length] / 12  # Hard coded symmetry divisor\n                # # (batch_length, pose_length)\n                # print(f'Shape of null_seq: {null_seq.shape}')\n                # print(f'Shape of _decoding_order: {_decoding_order.shape}')\n                # print(f'Shape of _batch_sequences: {_batch_sequences.shape}')\n                # print(f'Found the decoding sites with a null output: {_decoding_order[null_seq]}')\n                # print(f'Found the probabilities with a null output: {_probabilities[null_seq]}')\n                # print(_batch_sequences.numpy()[_decoding_order])\n                # _probabilities = sample_dict['probs']  # .cpu().numpy()[:, :pose_length]\n                # _probabilities with shape (batch_length, model_length, mpnn_alphabet_length)\n                new_amino_acid_types = \\\n                    torch.multinomial(sampled_probs[null_design_indices, null_sequence_indices],\n                                      1).squeeze(-1)\n                # _batch_sequences[null_indices] = new_amino_acid_type\n                # new_amino_acid_type = torch.multinomial(sample_dict['probs'][null_seq], 1)\n                # _batch_sequences[null_seq] = new_amino_acid_type\n                null_seq = new_amino_acid_types != 20\n                # null_seq = _batch_sequences == 20\n                # null_indices = np.argwhere(null_seq == 1)\n            else:\n                # Set the\n                _batch_sequences[null_design_indices, null_sequence_indices] = new_amino_acid_types\n            # proteinmpnn.log.debug('Fixed null sequence elements')\n\n        decoding_order = sample_dict['decoding_order']\n        # decoding_order_out = decoding_order  # When using the same decoding order for all\n        log_probs_start_time = time.time()\n        if X_unbound is not None:\n            # unbound_log_prob_start_time = time.time()\n            # logger.critical(f'Starting unbound calc: '\n            #                 f'available memory={get_device_memory(proteinmpnn.device)/gb_divisor}')\n            unbound_log_probs = \\\n                proteinmpnn(X_unbound, S_sample, mask, chain_residue_mask, residue_idx, chain_encoding,\n                            None,  # This argument is provided but with below args, is not used\n                            use_input_decoding_order=True, decoding_order=decoding_order)\n            # logger.critical(f'After unbound calc: '\n            #                 f'available memory={get_device_memory(proteinmpnn.device)/gb_divisor}')\n            _per_residue_unbound_sequence_loss.append(\n                sequence_nllloss(_batch_sequences, unbound_log_probs[:, :pose_length]).cpu().numpy())\n            # logger.debug(f'Unbound log probabilities calculation took '\n            #              f'{time.time() - unbound_log_prob_start_time:8f}s')\n\n        # logger.critical(f'Starting bound calc: '\n        #                 f'available memory={get_device_memory(proteinmpnn.device) / gb_divisor}')\n        complex_log_probs = \\\n            proteinmpnn(X, S_sample, mask, chain_residue_mask, residue_idx, chain_encoding,\n                        None,  # This argument is provided but with below args, is not used\n                        use_input_decoding_order=True, decoding_order=decoding_order)\n        # logger.critical(f'After bound calc: '\n        #                 f'available memory={get_device_memory(proteinmpnn.device) / gb_divisor}')\n        # complex_log_probs is\n        # tensor([[[-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772],\n        #          [-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772],\n        #          [-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772],\n        #          ...,\n        #          [-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772],\n        #          [-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772],\n        #          [-2.7691, -3.5265, -2.9001,  ..., -3.3623, -3.0247, -4.2772]],\n        #         [[-2.6934, -4.0610, -2.6506, ..., -4.2404, -3.4620, -4.8641],\n        #          [-2.8753, -4.3959, -2.4042,  ..., -4.4922, -3.5962, -5.1403],\n        #          [-2.5235, -4.0181, -2.7738,  ..., -4.2454, -3.4768, -4.8088],\n        #          ...,\n        #          [-3.4500, -4.4373, -3.7814,  ..., -5.1637, -4.6107, -5.2295],\n        #          [-0.9690, -4.9492, -3.9373,  ..., -2.0154, -2.2262, -4.3334],\n        #          [-3.1118, -4.3809, -3.8763,  ..., -4.7145, -4.1524, -5.3076]]])\n        # Score the redesigned structure-sequence\n        # mask_for_loss = chain_mask_and_mask*residue_mask\n        # batch_scores = sequence_nllloss(S_sample, complex_log_probs, mask_for_loss, per_residue=False)\n        # batch_scores is\n        # tensor([2.1039, 2.0618, 2.0802, 2.0538, 2.0114, 2.0002], device='cuda:0')\n        _per_residue_complex_sequence_loss.append(\n            sequence_nllloss(_batch_sequences, complex_log_probs[:, :pose_length]).cpu().numpy())\n        proteinmpnn.log.info(f'Log probabilities score calculation took {time.time() - log_probs_start_time:8f}s')\n        batch_sequences.append(_batch_sequences.cpu())\n\n    # Reshape data structures to have shape (batch_length, number_of_temperatures, pose_length)\n    _residue_indices_of_interest = residue_mask[:, :pose_length].cpu().numpy().astype(bool)\n    sequences = np.concatenate(batch_sequences, axis=1).reshape(actual_batch_length, number_of_temps, pose_length)\n    complex_sequence_loss =\\\n        np.concatenate(_per_residue_complex_sequence_loss, axis=1) \\\n        .reshape(actual_batch_length, number_of_temps, pose_length)\n    if X_unbound is not None:\n        unbound_sequence_loss = \\\n            np.concatenate(_per_residue_unbound_sequence_loss, axis=1) \\\n            .reshape(actual_batch_length, number_of_temps, pose_length)\n    else:\n        unbound_sequence_loss = np.empty_like(complex_sequence_loss)\n\n    return {'sequences': sequences,\n            'proteinmpnn_loss_complex': complex_sequence_loss,\n            'proteinmpnn_loss_unbound': unbound_sequence_loss,\n            'design_indices': _residue_indices_of_interest}\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.proteinmpnn_batch_score","title":"proteinmpnn_batch_score","text":"<pre><code>proteinmpnn_batch_score(batch_slice: slice, proteinmpnn: ProteinMPNN, X: Tensor = None, S: Tensor = None, chain_mask: Tensor = None, chain_encoding: Tensor = None, residue_idx: Tensor = None, mask: Tensor = None, pose_length: int = None, X_unbound: Tensor = None, chain_M_pos: Tensor = None, residue_mask: Tensor = None, randn: Tensor = None, decoding_order: Tensor = None, **batch_parameters) -&gt; dict[str, ndarray]\n</code></pre> <p>Perform ProteinMPNN design tasks on input that is split into batches</p> <p>Parameters:</p> <ul> <li> <code>batch_slice</code>             (<code>slice</code>)         \u2013          </li> <li> <code>proteinmpnn</code>             (<code>ProteinMPNN</code>)         \u2013          </li> <li> <code>X</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>S</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain_mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain_encoding</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>residue_idx</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>pose_length</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>X_unbound</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>chain_M_pos</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>residue_mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>randn</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>decoding_order</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          </li> </ul> <p>Returns:     A mapping of the key describing to the corresponding value, i.e. sequences, complex_sequence_loss, and         unbound_sequence_loss</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def proteinmpnn_batch_score(batch_slice: slice, proteinmpnn: ProteinMPNN,\n                            X: torch.Tensor = None,\n                            S: torch.Tensor = None,\n                            chain_mask: torch.Tensor = None,\n                            chain_encoding: torch.Tensor = None,\n                            residue_idx: torch.Tensor = None,\n                            mask: torch.Tensor = None,\n                            pose_length: int = None,\n                            X_unbound: torch.Tensor = None,\n                            chain_M_pos: torch.Tensor = None,\n                            residue_mask: torch.Tensor = None,\n                            randn: torch.Tensor = None,\n                            decoding_order: torch.Tensor = None,\n                            **batch_parameters\n                            ) -&gt; dict[str, np.ndarray]:\n    \"\"\"Perform ProteinMPNN design tasks on input that is split into batches\n\n    Args:\n        batch_slice:\n        proteinmpnn:\n        X:\n        S:\n        chain_mask:\n        chain_encoding:\n        residue_idx:\n        mask:\n        pose_length:\n        X_unbound:\n        chain_M_pos:\n        residue_mask:\n        randn:\n        decoding_order:\n    Returns:\n        A mapping of the key describing to the corresponding value, i.e. sequences, complex_sequence_loss, and\n            unbound_sequence_loss\n    \"\"\"\n    if chain_M_pos is not None:\n        residue_mask = chain_M_pos  # Name change makes more sense\n    elif residue_mask is not None:\n        pass\n    else:\n        raise ValueError(\"Must pass either 'residue_mask' or 'chain_M_pos'\")\n\n    if pose_length is None:\n        batch_length, pose_length, *_ = X.shape\n    else:\n        batch_length, *_ = X.shape\n\n    actual_batch_length = batch_slice.stop - batch_slice.start\n\n    # Slice the sequence according to those that are currently batched for scoring\n    S = S[batch_slice]  # , None)\n    if actual_batch_length != batch_length:\n        # Slice these for the last iteration\n        X = X[:actual_batch_length]  # , None)\n        chain_mask = chain_mask[:actual_batch_length]  # , None)\n        chain_encoding = chain_encoding[:actual_batch_length]  # , None)\n        residue_idx = residue_idx[:actual_batch_length]  # , None)\n        mask = mask[:actual_batch_length]  # , None)\n        # randn = randn[:actual_batch_length]\n        residue_mask = residue_mask[:actual_batch_length]\n        try:\n            X_unbound = X_unbound[:actual_batch_length]  # , None)\n        except TypeError:  # Can't slice NoneType\n            pass\n\n    # logger.debug(f'S shape: {S.shape}')\n    # logger.debug(f'X shape: {X.shape}')\n    # # logger.debug(f'chain_mask shape: {chain_mask.shape}')\n    # logger.debug(f'chain_encoding shape: {chain_encoding.shape}')\n    # logger.debug(f'residue_idx shape: {residue_idx.shape}')\n    # logger.debug(f'mask shape: {mask.shape}')\n    # # logger.debug(f'residue_mask shape: {residue_mask.shape}')\n\n    chain_residue_mask = chain_mask * residue_mask\n    # logger.debug(f'chain_residue_mask shape: {chain_residue_mask.shape}')\n\n    # Score and format outputs - All have at lease shape (batch_length, model_length,)\n    if decoding_order is not None:\n        # logger.debug(f'decoding_order shape: {decoding_order.shape}, type: {decoding_order.dtype}')\n        decoding_order = decoding_order[:actual_batch_length]\n        provided_decoding_order = True\n        randn = None\n    elif randn is not None:\n        # logger.debug(f'decoding_order shape: {randn.shape}, type: {randn.dtype}')\n        randn = randn[:actual_batch_length]\n        decoding_order = None\n        provided_decoding_order = False\n    else:\n        # Todo generate a randn fresh?\n        raise ValueError(\"Missing required argument 'randn' or 'decoding_order'\")\n\n    # decoding_order_out = decoding_order  # When using the same decoding order for all\n    log_probs_start_time = time.time()\n\n    # Todo debug the input Tensor. Most likely the sequence must be (batch, pose, aa?)\n    # RuntimeError: Index tensor must have the same number of dimensions as input tensor\n    complex_log_probs = \\\n        proteinmpnn(X, S, mask, chain_residue_mask, residue_idx, chain_encoding, randn,\n                    use_input_decoding_order=provided_decoding_order, decoding_order=decoding_order)\n    per_residue_complex_sequence_loss = \\\n        sequence_nllloss(S[:, :pose_length], complex_log_probs[:, :pose_length]).cpu().numpy()\n\n    # Reshape data structures to have shape (batch_length, number_of_temperatures, pose_length)\n    # _residue_indices_of_interest = residue_mask[:, :pose_length].cpu().numpy().astype(bool)\n    # sequences = np.concatenate(batch_sequences, axis=1).reshape(actual_batch_length, number_of_temps, pose_length)\n    # complex_sequence_loss = \\\n    #     np.concatenate(per_residue_complex_sequence_loss, axis=1)\\\n    #     .reshape(actual_batch_length, number_of_temps, pose_length)\n    # if X_unbound is not None:\n    #     unbound_sequence_loss = \\\n    #         np.concatenate(per_residue_unbound_sequence_loss, axis=1)\\\n    #         .reshape(actual_batch_length, number_of_temps, pose_length)\n    # else:\n    #     unbound_sequence_loss = np.empty_like(complex_sequence_loss)\n    if X_unbound is not None:\n        # unbound_log_prob_start_time = time.time()\n        unbound_log_probs = \\\n            proteinmpnn(X_unbound, S, mask, chain_residue_mask, residue_idx, chain_encoding, randn,\n                        use_input_decoding_order=provided_decoding_order, decoding_order=decoding_order)\n        per_residue_unbound_sequence_loss = \\\n            sequence_nllloss(S[:, :pose_length], unbound_log_probs[:, :pose_length]).cpu().numpy()\n        # logger.debug(f'Unbound log probabilities calculation took '\n        #              f'{time.time() - unbound_log_prob_start_time:8f}s')\n    else:\n        per_residue_unbound_sequence_loss = np.empty_like(complex_log_probs)\n    proteinmpnn.log.info(f'Log probabilities score calculation took {time.time() - log_probs_start_time:8f}s')\n\n    return {'proteinmpnn_loss_complex': per_residue_complex_sequence_loss,\n            'proteinmpnn_loss_unbound': per_residue_unbound_sequence_loss}\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.sequence_nllloss","title":"sequence_nllloss","text":"<pre><code>sequence_nllloss(sequence: Tensor, log_probs: Tensor, mask: Tensor = None, per_residue: bool = True) -&gt; Tensor\n</code></pre> <p>Score designed sequences using the Negative log likelihood loss function</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Tensor</code>)         \u2013          <p>The sequence tensor</p> </li> <li> <code>log_probs</code>             (<code>Tensor</code>)         \u2013          <p>The logarithmic probabilities at each residue for every amino acid. This may be found by an evolutionary profile or a forward pass through ProteinMPNN</p> </li> <li> <code>mask</code>             (<code>Tensor</code>, default:                 <code>None</code> )         \u2013          <p>Any positions that are masked in the design task</p> </li> <li> <code>per_residue</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to return scores per residue</p> </li> </ul> <p>Returns:     The loss calculated over the log probabilities compared to the sequence tensor.         If per_residue=True, the returned Tensor is the same shape as sequence (i.e. (batch, length)),         otherwise, it is just the length of sequence as calculated by the average loss over every residue</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def sequence_nllloss(sequence: torch.Tensor, log_probs: torch.Tensor,\n                     mask: torch.Tensor = None, per_residue: bool = True) -&gt; torch.Tensor:\n    \"\"\"Score designed sequences using the Negative log likelihood loss function\n\n    Args:\n        sequence: The sequence tensor\n        log_probs: The logarithmic probabilities at each residue for every amino acid.\n            This may be found by an evolutionary profile or a forward pass through ProteinMPNN\n        mask: Any positions that are masked in the design task\n        per_residue: Whether to return scores per residue\n    Returns:\n        The loss calculated over the log probabilities compared to the sequence tensor.\n            If per_residue=True, the returned Tensor is the same shape as sequence (i.e. (batch, length)),\n            otherwise, it is just the length of sequence as calculated by the average loss over every residue\n    \"\"\"\n    criterion = torch.nn.NLLLoss(reduction='none')\n    # Measure log_probs loss with respect to the sequence. Make each sequence and log probs stacked along axis=0\n    loss = criterion(\n        log_probs.contiguous().view(-1, log_probs.size(-1)),\n        sequence.contiguous().view(-1)\n    ).view(sequence.size())  # Revert the shape to the original sequence shape\n    # Take the average over every designed position and return the single score\n    if per_residue:\n        return loss\n    elif mask is None:\n        return torch.sum(loss, dim=-1)\n    else:\n        return torch.sum(loss*mask, dim=-1) / torch.sum(mask, dim=-1)\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.jnp_to_np","title":"jnp_to_np","text":"<pre><code>jnp_to_np(jax_dict: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Recursively changes jax arrays to numpy arrays</p> <p>Parameters:</p> <ul> <li> <code>jax_dict</code>             (<code>dict[str, Any]</code>)         \u2013          <p>A dictionary with the keys mapped to jax.numpy.array types</p> </li> </ul> <p>Returns:     The input dictionary modified with the keys mapped to np.array type</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def jnp_to_np(jax_dict: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Recursively changes jax arrays to numpy arrays\n\n    Args:\n        jax_dict: A dictionary with the keys mapped to jax.numpy.array types\n    Returns:\n        The input dictionary modified with the keys mapped to np.array type\n    \"\"\"\n    for k, v in jax_dict.items():\n        if isinstance(v, dict):\n            jax_dict[k] = jnp_to_np(v)\n        elif isinstance(v, jnp.ndarray):\n            jax_dict[k] = np.array(v)\n    return jax_dict\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.calculate_alphafold_batch_length","title":"calculate_alphafold_batch_length","text":"<pre><code>calculate_alphafold_batch_length(device: Device, number_of_residues: int, element_memory: int = 4) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li> <code>device</code>             (<code>Device</code>)         \u2013          <p>The ProteinMPNN model</p> </li> <li> <code>number_of_residues</code>             (<code>int</code>)         \u2013          <p>The number of residues used in the ProteinMPNN model</p> </li> <li> <code>element_memory</code>             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>Where each element is np.int64, np.float32, etc.</p> </li> </ul> <p>Returns:     The size of the batch that can be completed for the ProteinMPNN model given it's device</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def calculate_alphafold_batch_length(device: jax_xla.Device, number_of_residues: int, element_memory: int = 4) -&gt; int:\n    \"\"\"\n\n    Args:\n        device: The ProteinMPNN model\n        number_of_residues: The number of residues used in the ProteinMPNN model\n        element_memory: Where each element is np.int64, np.float32, etc.\n    Returns:\n        The size of the batch that can be completed for the ProteinMPNN model given it's device\n    \"\"\"\n    memory_constraint = get_device_memory(device)\n\n    number_of_elements_available = memory_constraint // element_memory\n    logger.debug(f'The number_of_elements_available is: {number_of_elements_available}')\n    number_of_model_parameter_elements = sum([math.prod(param.size()) for param in model.parameters()])\n    logger.debug(f'The number_of_model_parameter_elements is: {number_of_model_parameter_elements}')\n    model_elements = number_of_model_parameter_elements\n    # Todo use 5 as ideal CB is added by the model later with ca_only = False\n    num_model_residues = 5\n    model_elements += math.prod((number_of_residues, num_model_residues, 3))  # X,\n    model_elements += number_of_residues  # S.shape\n    model_elements += number_of_residues  # chain_mask.shape\n    model_elements += number_of_residues  # chain_encoding.shape\n    model_elements += number_of_residues  # residue_idx.shape\n    model_elements += number_of_residues  # mask.shape\n    model_elements += number_of_residues  # residue_mask.shape\n    model_elements += math.prod((number_of_residues, 21))  # omit_AA_mask.shape\n    model_elements += number_of_residues  # pssm_coef.shape\n    model_elements += math.prod((number_of_residues, 20))  # pssm_bias.shape\n    model_elements += math.prod((number_of_residues, 20))  # pssm_log_odds_mask.shape\n    model_elements += number_of_residues  # tied_beta.shape\n    model_elements += math.prod((number_of_residues, 21))  # bias_by_res.shape\n    logger.debug(f'The number of model_elements is: {model_elements}')\n\n    number_of_batches = number_of_elements_available // model_elements\n    batch_length = number_of_batches // proteinmpnn_batch_divisor\n    if batch_length == 0:\n        not_enough_proteinmpnn_memory = f\"Can't find a device for {model} with enough memory to complete a single \" \\\n                                        f\"batch of work with {number_of_residues} residues in the model\"\n        if device.platform == 'cpu':\n            raise RuntimeError(not_enough_proteinmpnn_memory)\n\n        old_device = device\n        # This won't work. Try to put the model on a new device\n        max_memory = vanilla_model_memory\n        for device_int in range(torch.cuda.device_count()):\n            available_memory = get_device_memory(torch.device(device_int), free=True)\n            if available_memory &gt; max_memory:\n                max_memory = available_memory\n                device_id = device_int\n        try:\n            device: torch.device = torch.device(device_id)\n        except UnboundLocalError:  # No device has memory greater than ProteinMPNN minimum required\n            device = jax.devices('cpu')[0]\n\n        if device == old_device:\n            # Solve using gpu is stuck\n            if device.type == 'cpu':\n                # This hasn't been changed or device is cpu\n                raise RuntimeError(not_enough_proteinmpnn_memory)\n            else:\n                # Try one more time ensuring cpu. This will be caught above if still not enough memory\n                device = torch.device('cpu')\n\n        # Recurse\n        return calculate_proteinmpnn_batch_length(model, number_of_residues, element_memory)\n\n    return device\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.get_jax_device_memory","title":"get_jax_device_memory","text":"<pre><code>get_jax_device_memory(device_int: int) -&gt; int\n</code></pre> <p>Based on the device number, use torch to get the device memory</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def get_jax_device_memory(device_int: int) -&gt; int:  # jax_xla.Device\n    \"\"\"Based on the device number, use torch to get the device memory\"\"\"\n    return get_device_memory(device_int)\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.alphafold_required_memory","title":"alphafold_required_memory","text":"<pre><code>alphafold_required_memory(number_of_residues: int)\n</code></pre> <p>Get the bytes required for the number of residues in the model</p> <p>Parameters:</p> <ul> <li> <code>number_of_residues</code>             (<code>int</code>)         \u2013          <p>The number of residues in the model</p> </li> </ul> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def alphafold_required_memory(number_of_residues: int):\n    \"\"\"Get the bytes required for the number of residues in the model\n\n    Args:\n        number_of_residues: The number of residues in the model\n    \"\"\"\n    if number_of_residues &gt; 100:\n        return 629145600  # '600 M'\n    elif number_of_residues &gt; 200:\n        return 1258291200  # '1200 M'\n    elif number_of_residues &gt; 400:\n        return 2516582400  # '2400 M'\n    elif number_of_residues &gt; 800:\n        return 5033164800  # '4800 M'\n    elif number_of_residues &gt; 1000:\n        return 6291456000  # '6000 M'\n    else:  # Assume 2000+\n        return 12582912000  # '12000 M'\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.get_alphafold_model_device","title":"get_alphafold_model_device","text":"<pre><code>get_alphafold_model_device(number_of_residues: int) -&gt; Device\n</code></pre> <p>Get the GPU capable of performing the AlphaFold inference for the number of residues in the model</p> <p>Parameters:</p> <ul> <li> <code>number_of_residues</code>             (<code>int</code>)         \u2013          <p>The number of residues in the model</p> </li> </ul> <p>Returns:     The jax.Device to use with the number of residues present in the model</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def get_alphafold_model_device(number_of_residues: int) -&gt; jax_xla.Device:\n    \"\"\"Get the GPU capable of performing the AlphaFold inference for the number of residues in the model\n\n    Args:\n        number_of_residues: The number of residues in the model\n    Returns:\n        The jax.Device to use with the number of residues present in the model\n    \"\"\"\n    use_device = None\n    max_memory = alphafold_required_memory(number_of_residues)\n    for int, device in enumerate(jax.devices('gpu')):\n        available_memory = get_jax_device_memory(int)\n        if available_memory &gt; max_memory:\n            max_memory = available_memory\n            use_device = device\n\n    if use_device is None:\n        raise RuntimeError(\n            f\"Couldn't find a usable device with the memory requirement {max_memory}\")\n    else:\n        return use_device\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.set_up_model_runners","title":"set_up_model_runners","text":"<pre><code>set_up_model_runners(model_type: af_model_literal = 'monomer', number_of_residues: int = 1000, num_predictions_per_model: int = 1, num_ensemble: int = 1, development: bool = False) -&gt; dict[str, RunModel]\n</code></pre> <p>Produce Alphafold RunModel class loaded with their training parameters</p> <p>Parameters:</p> <ul> <li> <code>model_type</code>             (<code>af_model_literal</code>, default:                 <code>'monomer'</code> )         \u2013          <p>The type of model to load. Should be one of the viable Alphafold models including: 'monomer', 'monomer_casp14', 'monomer_ptm', 'multimer'</p> </li> <li> <code>number_of_residues</code>             (<code>int</code>, default:                 <code>1000</code> )         \u2013          <p>The number of residues in the model. Used only to calculate approximate memory needs during device allocation</p> </li> <li> <code>num_predictions_per_model</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of predictions to make for each Alphafold model. Essentially duplicates the original models 'num_predictions_per_model' times</p> </li> <li> <code>num_ensemble</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number of model ensembles to make. Typically, 1 is sufficient, but during CASP14, 8 were used</p> </li> <li> <code>development</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a smaller subset of models should be used for increased testing performance</p> </li> </ul> <p>Returns:     A dictionary of the model name to the RunModel instance for each 'model_type'/'num_predictions_per_model'         requested</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def set_up_model_runners(model_type: af_model_literal = 'monomer', number_of_residues: int = 1000, num_predictions_per_model: int = 1,\n                         num_ensemble: int = 1, development: bool = False) -&gt; dict[str, RunModel]:\n    \"\"\"Produce Alphafold RunModel class loaded with their training parameters\n\n    Args:\n        model_type: The type of model to load. Should be one of the viable Alphafold models including:\n            'monomer', 'monomer_casp14', 'monomer_ptm', 'multimer'\n        number_of_residues: The number of residues in the model. Used only to calculate approximate memory needs during\n            device allocation\n        num_predictions_per_model: The number of predictions to make for each Alphafold model. Essentially duplicates\n            the original models 'num_predictions_per_model' times\n        num_ensemble: The number of model ensembles to make. Typically, 1 is sufficient, but during CASP14, 8 were used\n        development: Whether a smaller subset of models should be used for increased testing performance\n    Returns:\n        A dictionary of the model name to the RunModel instance for each 'model_type'/'num_predictions_per_model'\n            requested\n    \"\"\"\n    # model_runners = {}\n    # model_names = afconfig.MODEL_PRESETS[model_type]\n    # for model_name in model_names:\n    #     if development and model_name != 'model_2_multimer_v3':\n    #         continue\n    #     model_config = afconfig.model_config(model_name)\n    #     if model_config.model.global_config.multimer_mode:\n    #         model_config.model.num_ensemble_eval = num_ensemble\n    #     else:\n    #         model_config.data.eval.num_ensemble = num_ensemble\n    #     model_params = afdata.get_model_haiku_params(model_name=model_name, data_dir=putils.alphafold_db_dir)\n    #     # This is using prev_pos init\n    #     model_runner = RunModel(model_config, model_params)\n    #     # This should be used if the prediction is not for a design and we have an msa\n    #     # model_runner = afmodel.RunModel(model_config, model_params)\n    #\n    #     for i in range(num_predictions_per_model):\n    #         model_runners[f'{model_name}_pred_{i}'] = model_runner\n    #\n    # num_models = len(model_runners)\n    # logger.info(f'Loaded {num_models} Alphafold models: {list(model_runners.keys())}')\n    #\n    # return model_runners\n\n    # This routine is used to store each separate model parameters on one RunModel\n    # Get model config\n    model_config = afconfig.model_config(model_type_to_config_name[model_type])\n    if model_config.model.global_config.multimer_mode:\n        model_config.model.num_ensemble_eval = num_ensemble\n    else:\n        model_config.data.eval.num_ensemble = num_ensemble\n    # Set up model params\n    model_params = {}\n    for model_name in afconfig.MODEL_PRESETS[model_type]:\n        model_param = afdata.get_model_haiku_params(model_name=model_name, data_dir=putils.alphafold_db_dir)\n        if 'model_1' in model_name:\n            # Using the config for model_1 as it is most similar to other models\n            #  model_1/2 includes template embeddings (monomer),\n            #  while multimer model_1 is fairly similar to 2-5\n            af_device = get_alphafold_model_device(number_of_residues)\n            logger.info(f'Using device {af_device} for model calculations')\n            # RunModel is using prev_pos init\n            model_runner = RunModel(model_config, model_param, device=af_device)\n            # # ?? Not sure why this would be the case -&gt; if the prediction is not for a design and there is an msa\n            # model_runner = afmodel.RunModel(model_config, model_params)\n            if development:\n                model_params[f'{model_name}_pred_{0}'] = model_param\n                break\n\n        for i in range(num_predictions_per_model):\n            model_params[f'{model_name}_pred_{i}'] = model_param\n\n    num_models = len(model_params)\n    logger.info(f'Loaded {num_models} Alphafold models: {\", \".join(model_params)}')\n\n    model_runner.set_params(model_params)\n    return {model_param_name: model_runner for model_param_name in model_params}\n</code></pre>"},{"location":"reference/resources/ml/#resources.ml.af_predict","title":"af_predict","text":"<pre><code>af_predict(features: FeatureDict, model_runners: dict[str, RunModel], gpu_relax: bool = False, models_to_relax: relax_options_literal = None, random_seed: int = None, confidence_stop_threshold: float = 0.85) -&gt; tuple[dict[str, dict[str, str]], dict[str, FeatureDict]]\n</code></pre> <p>Run Alphafold to predict a structure from sequence/msa/template features</p> <p>Parameters:</p> <ul> <li> <code>#</code>             (<code>length</code>)         \u2013          <p>The length of the desired output for prediction metrics</p> </li> <li> <code>features</code>             (<code>FeatureDict</code>)         \u2013          <p>The sequence/msa/template feature parameters to populate the jax model</p> </li> <li> <code>model_runners</code>             (<code>dict[str, RunModel]</code>)         \u2013          <p>The RunModel instances which should predict the structure</p> </li> <li> <code>gpu_relax</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether predictions should be relaxed using a GPU (if one is available)</p> </li> <li> <code>models_to_relax</code>             (<code>relax_options_literal</code>, default:                 <code>None</code> )         \u2013          <p>Specify which predictions should be relaxed</p> </li> <li> <code>random_seed</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>A random integer to seed the model. Could be provided to ensure consistency across runs</p> </li> <li> <code>confidence_stop_threshold</code>             (<code>float</code>, default:                 <code>0.85</code> )         \u2013          <p>The confidence threshold to stop prediction if a prediction scores higher than it. Value provided should be between [0-1]. Will use mean plddt if the model is monomer, if model is multimer, will use 0.8interface_predicted_template_modeling_score + 0.2predicted_template_modeling_score</p> </li> </ul> <p>Returns:     The tuple of structure and score dictionaries. Where structures contains the keys 'relaxed' and     'unrelaxed' mapped to the model name and the model PDB string and folding_scores contain the model name     mapped to each of the score types 'predicted_aligned_error' (length, length), 'plddt' (length),     'predicted_template_modeling_score' (1), and 'predicted_interface_template_modeling_score' (1)</p> Source code in <code>symdesign/resources/ml.py</code> <pre><code>def af_predict(features: FeatureDict, model_runners: dict[str, RunModel], gpu_relax: bool = False,\n               models_to_relax: relax_options_literal = None, random_seed: int = None,\n               confidence_stop_threshold: float = 0.85) -&gt; tuple[dict[str, dict[str, str]], dict[str, FeatureDict]]:\n    \"\"\"Run Alphafold to predict a structure from sequence/msa/template features\n\n    Args:\n        # length: The length of the desired output for prediction metrics\n        features: The sequence/msa/template feature parameters to populate the jax model\n        model_runners: The RunModel instances which should predict the structure\n        gpu_relax: Whether predictions should be relaxed using a GPU (if one is available)\n        models_to_relax: Specify which predictions should be relaxed\n        random_seed: A random integer to seed the model. Could be provided to ensure consistency across runs\n        confidence_stop_threshold: The confidence threshold to stop prediction if a prediction scores higher than it.\n            Value provided should be between [0-1]. Will use mean plddt if the model is monomer, if model is multimer,\n            will use 0.8*interface_predicted_template_modeling_score + 0.2*predicted_template_modeling_score\n    Returns:\n        The tuple of structure and score dictionaries. Where structures contains the keys 'relaxed' and\n        'unrelaxed' mapped to the model name and the model PDB string and folding_scores contain the model name\n        mapped to each of the score types 'predicted_aligned_error' (length, length), 'plddt' (length),\n        'predicted_template_modeling_score' (1), and 'predicted_interface_template_modeling_score' (1)\n    \"\"\"\n    num_models = len(model_runners)\n    if random_seed is None:  # Make one\n        random_seed = random.randrange(sys.maxsize // num_models)\n\n    if confidence_stop_threshold &gt; 1:\n        raise ValueError(f\"confidence_stop_threshold must be between 0 and 1. If using monomer models with plddt, \"\n                         f\"will take the confidence metric as the percent plddt (out of a maximum of 100)\")\n\n    # # Set up folding_scores dictionary\n    # scores = {\n    #     'predicted_aligned_error': np.zeros((num_models, length, length), dtype=np.float32),\n    #     'plddt': np.zeros((num_models, length), dtype=np.float32),\n    #     'predicted_template_modeling_score': np.zeros(num_models, dtype=np.float32),\n    #     'predicted_interface_template_modeling_score': np.zeros(num_models, dtype=np.float32)\n    # }\n    for model_name, model_runner in model_runners.items():\n        if model_runner.multimer_mode:\n            change_scores = [('iptm', 'predicted_interface_template_modeling_score'),\n                             ('ptm', 'predicted_template_modeling_score')]\n            # scores_ = {'predicted_template_modeling_score': [],\n            #            'predicted_interface_template_modeling_score': []}\n        elif 'ptm' in model_name:\n            change_scores = [('ptm', 'predicted_template_modeling_score')]\n            # scores_ = {'predicted_template_modeling_score': []}\n        else:\n            change_scores = []\n            # raise NotImplementedError()\n        break\n    else:  # Can't run without model_runners...\n        change_scores = []\n    #     scores_ = {}\n    # scores = {model_name: copy.deepcopy(scores_) for model_name in model_runners}\n\n    unneeded_scores = [\n        'distogram', 'experimentally_resolved', 'masked_msa', 'predicted_lddt', 'structure_module',\n        'final_atom_positions', 'num_recycles', 'aligned_confidence_probs',\n        'max_predicted_aligned_error',\n        # 'ranking_confidence',\n        # 'ptm', 'iptm', 'predicted_aligned_error', 'plddt',\n    ]\n    scores = {}\n    ranking_confidences = {}\n    unrelaxed_proteins = {}\n    unrelaxed_pdbs_ = {}\n    # Run the models.\n    for model_index, (model_name, model_runner) in enumerate(model_runners.items()):\n        logger.info(f'Running JAX {model_name}')\n        model_random_seed = model_index + random_seed*num_models\n        processed_feature_dict = \\\n            model_runner.process_features(features, random_seed=model_random_seed)\n\n        t_0 = time.time()\n        # prediction_result = model_runner.predict(processed_feature_dict, random_seed=model_random_seed)\n        prediction_result = \\\n            model_runner.predict_with_params(model_name, processed_feature_dict, random_seed=model_random_seed)\n        logger.info(f'Prediction took {time.time() - t_0:.1f}s')\n        # if this is the first go in the model_runner, then f'(includes compilation time)' would be accurate\n        # Monomer?\n        #  Should take about 96 secs on a 1000 residue protein using 3 recycles...\n\n        # Remove jax dependency from results.\n        np_prediction_result = jnp_to_np(dict(prediction_result))\n        # logger.debug(f'Found prediction_results: {np_prediction_result}')\n        # monomer\n        # ['distogram', 'experimentally_resolved', 'masked_msa', 'predicted_lddt', 'structure_module', 'plddt',\n        #  'ranking_confidence']\n        # multimer\n        # ['distogram', 'experimentally_resolved', 'masked_msa', 'predicted_lddt', 'structure_module', 'plddt',\n        #  'ranking_confidence'\n        #  'num_recycles', 'predicted_aligned_error', 'aligned_confidence_probs', 'max_predicted_aligned_error',\n        #  'ptm', 'iptm']\n        # logger.debug(f'Found the prediction_result shapes: {model_runner.eval_shape(np_prediction_result)}')\n        # {'distogram': {'bin_edges': (63,), 'logits': (n_residues, n_residues, 64)},\n        #  'experimentally_resolved': {'logits': (n_residues, atom_types)},\n        #  'masked_msa': {'logits': (n_sequences, n_residues, n_amino_acid_types_gapped_unknown)},\n        #  'predicted_aligned_error': (n_residues, n_residues),\n        #  'predicted_lddt': {'logits': (n_residues, 50)},\n        #  'structure_module': {'final_atom_mask': (n_residues, atom_types),\n        #  'final_atom_positions': (n_residues, atom_types, 3)},\n        #  'plddt': (n_residues,), 'aligned_confidence_probs': (n_residues, n_residues, 64),\n        #  'max_predicted_aligned_error': (), 'ptm': (), 'iptm': (), 'ranking_confidence': (), 'num_recycles': (),\n        #  }\n        # Where ['predicted_lddt'] has the key ['logits'] which probably ?contains the raw logit values produced by\n        # model heads? for the binned distogram rankings?\n\n        # plddt = np_prediction_result['plddt']\n        # scores[model_name]['plddt'] = plddt  # [:length]\n        # if model_runner.multimer_mode:\n        #     # This is a 2d array. Clean up to ASU at some point\n        #     scores[model_name]['predicted_aligned_error'] = np_prediction_result['predicted_aligned_error']\n        #     # scores['predicted_interface_template_modeling_score'][model_index] = np_prediction_result['iptm']\n        #     scores[model_name]['predicted_interface_template_modeling_score'].append(np_prediction_result['iptm'])\n        #     scores[model_name]['predicted_template_modeling_score'].append(np_prediction_result['ptm'])\n        # elif 'ptm' in model_name:\n        #     scores[model_name]['predicted_aligned_error'] = \\\n        #         np_prediction_result['predicted_aligned_error']  # [:length, :length]\n        #     scores[model_name]['predicted_template_modeling_score'].append(np_prediction_result['ptm'])\n\n        # Add the predicted LDDT in the b-factor column.\n        plddt = np_prediction_result['plddt']\n        # Note that higher predicted LDDT value means higher model confidence.\n        plddt_b_factors = np.repeat(plddt[:, None], residue_constants.atom_type_num, axis=-1)\n        unrelaxed_protein = afprotein.from_prediction(\n            features=processed_feature_dict,\n            result=prediction_result,\n            b_factors=plddt_b_factors,\n            remove_leading_feature_dimension=not model_runner.multimer_mode)\n        unrelaxed_proteins[model_name] = unrelaxed_protein\n        unrelaxed_pdbs_[model_name] = afprotein.to_pdb(unrelaxed_protein)\n\n        ranking_confidences[model_name] = confidence_metric = np_prediction_result.pop('ranking_confidence')\n        # Process incoming scores to be returned\n        for old_score, new_score in change_scores:\n            np_prediction_result[new_score] = np_prediction_result.pop(old_score)\n        # Remove unnecessary scores\n        for score in unneeded_scores:\n            np_prediction_result.pop(score, None)\n        scores[model_name] = np_prediction_result\n\n        if model_runner.multimer_mode:\n            pass\n        else:  # monomer mode. Divide by 100 to get a percentage\n            confidence_metric /= 100\n        if confidence_metric &gt; confidence_stop_threshold:\n            # The prediction quality is already satisfactory\n            break\n\n    # Rank model names by model confidence.\n    ranked_order = [design_model_name for design_model_name, confidence in\n                    sorted(ranking_confidences.items(), key=lambda x: x[1], reverse=True)]\n    # Sort the unrelaxed_pdbs accordingly\n    unrelaxed_pdbs = {name: unrelaxed_pdbs_.pop(name) for name in ranked_order}\n\n    # Relax predictions.\n    relaxed_pdbs = {}\n    # relax_metrics = {}\n    if models_to_relax is None:\n        pass\n    else:\n        if models_to_relax == 'best':\n            to_relax = [ranked_order[0]]\n        else:  # if models_to_relax == 'all':\n            to_relax = ranked_order\n\n        logger.info(f'Starting Amber relaxation')\n        t_0 = time.time()\n        for model_name in to_relax:\n            logger.info(f'Relaxing {model_name}')\n            # relaxed_pdb_str, _, violations = amber_relaxer.process(prot=unrelaxed_proteins[model_name])\n            try:\n                relaxed_pdb_str, violations = amber_relax(prot=unrelaxed_proteins[model_name], gpu=gpu_relax)\n            except ValueError as error:  # Minimization failed after {max_iterations} attempts.\n                logger.error(f'Ran into problem during Amber relax: {error}\\nSkipping {model_name}')\n                continue\n            else:\n                # relax_metrics[model_name] = {\n                #     'remaining_violations': violations,\n                #     'remaining_violations_count': sum(violations)\n                # }\n                relaxed_pdbs[model_name] = relaxed_pdb_str\n\n        logger.info(f'Relaxation took {time.time() - t_0:.1f}s')\n\n    return {'relaxed': relaxed_pdbs, 'unrelaxed': unrelaxed_pdbs}, scores\n</code></pre>"},{"location":"reference/resources/monomer/","title":"monomer","text":""},{"location":"reference/resources/monomer/#resources.monomer.AlphaFoldInitialGuess","title":"AlphaFoldInitialGuess","text":"<pre><code>AlphaFoldInitialGuess(config, name='alphafold')\n</code></pre> <p>             Bases: <code>Module</code></p> <p>AlphaFold model with recycling.</p> <p>Jumper et al. (2021) Suppl. Alg. 2 \"Inference\"</p> Source code in <code>symdesign/resources/monomer.py</code> <pre><code>def __init__(self, config, name='alphafold'):\n  super().__init__(name=name)\n  self.config = config\n  self.global_config = config.global_config\n</code></pre>"},{"location":"reference/resources/monomer/#resources.monomer.AlphaFoldInitialGuess.__call__","title":"__call__","text":"<pre><code>__call__(batch, is_training, compute_loss=False, ensemble_representations=False, return_representations=False)\n</code></pre> <p>Run the AlphaFold model.</p> <p>Parameters:</p> <ul> <li> <code>batch</code>         \u2013          <p>Dictionary with inputs to the AlphaFold model.</p> </li> <li> <code>is_training</code>         \u2013          <p>Whether the system is in training or inference mode.</p> </li> <li> <code>compute_loss</code>         \u2013          <p>Whether to compute losses (requires extra features to be present in the batch and knowing the true structure).</p> </li> <li> <code>ensemble_representations</code>         \u2013          <p>Whether to use ensembling of representations.</p> </li> <li> <code>return_representations</code>         \u2013          <p>Whether to also return the intermediate representations.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>When compute_loss is True: a tuple of loss and output of AlphaFoldIteration.</p> </li> <li>         \u2013          <p>When compute_loss is False: just output of AlphaFoldIteration.</p> </li> <li>         \u2013          <p>The output of AlphaFoldIteration is a nested dictionary containing</p> </li> <li>         \u2013          <p>predictions from the various heads.</p> </li> </ul> Source code in <code>symdesign/resources/monomer.py</code> <pre><code>def __call__(\n    self,\n    batch,\n    is_training,\n    compute_loss=False,\n    ensemble_representations=False,\n    return_representations=False):\n  \"\"\"Run the AlphaFold model.\n\n  Arguments:\n    batch: Dictionary with inputs to the AlphaFold model.\n    is_training: Whether the system is in training or inference mode.\n    compute_loss: Whether to compute losses (requires extra features\n      to be present in the batch and knowing the true structure).\n    ensemble_representations: Whether to use ensembling of representations.\n    return_representations: Whether to also return the intermediate\n      representations.\n\n  Returns:\n    When compute_loss is True:\n      a tuple of loss and output of AlphaFoldIteration.\n    When compute_loss is False:\n      just output of AlphaFoldIteration.\n\n    The output of AlphaFoldIteration is a nested dictionary containing\n    predictions from the various heads.\n  \"\"\"\n  # SYMDESIGN - Attempt to extract a previous position passed as initialization\n  prev_pos = batch.pop('prev_pos', None)\n\n  impl = AlphaFoldIteration(self.config, self.global_config)\n  batch_size, num_residues = batch['aatype'].shape\n\n  def get_prev(ret):\n    new_prev = {\n        'prev_pos':\n            ret['structure_module']['final_atom_positions'],\n        'prev_msa_first_row': ret['representations']['msa_first_row'],\n        'prev_pair': ret['representations']['pair'],\n    }\n    return jax.tree_map(jax.lax.stop_gradient, new_prev)\n\n  def do_call(prev,\n              recycle_idx,\n              compute_loss=compute_loss):\n    if self.config.resample_msa_in_recycling:\n      num_ensemble = batch_size // (self.config.num_recycle + 1)\n      def slice_recycle_idx(x):\n        start = recycle_idx * num_ensemble\n        size = num_ensemble\n        return jax.lax.dynamic_slice_in_dim(x, start, size, axis=0)\n      ensembled_batch = jax.tree_map(slice_recycle_idx, batch)\n    else:\n      num_ensemble = batch_size\n      ensembled_batch = batch\n\n    non_ensembled_batch = jax.tree_map(lambda x: x, prev)\n\n    return impl(\n        ensembled_batch=ensembled_batch,\n        non_ensembled_batch=non_ensembled_batch,\n        is_training=is_training,\n        compute_loss=compute_loss,\n        ensemble_representations=ensemble_representations)\n\n  prev = {}\n  emb_config = self.config.embeddings_and_evoformer\n  if emb_config.recycle_pos:\n    # SYMDESIGN\n    if prev_pos is None:\n      prev['prev_pos'] = jnp.zeros(\n        [num_residues, residue_constants.atom_type_num, 3])\n    else:\n      prev['prev_pos'] = prev_pos\n    # SYMDESIGN\n  if emb_config.recycle_features:\n    prev['prev_msa_first_row'] = jnp.zeros(\n        [num_residues, emb_config.msa_channel])\n    prev['prev_pair'] = jnp.zeros(\n        [num_residues, num_residues, emb_config.pair_channel])\n\n  if self.config.num_recycle:\n    if 'num_iter_recycling' in batch:\n      # Training time: num_iter_recycling is in batch.\n      # The value for each ensemble batch is the same, so arbitrarily taking\n      # 0-th.\n      num_iter = batch['num_iter_recycling'][0]\n\n      # Add insurance that we will not run more\n      # recyclings than the model is configured to run.\n      num_iter = jnp.minimum(num_iter, self.config.num_recycle)\n    else:\n      # Eval mode or tests: use the maximum number of iterations.\n      num_iter = self.config.num_recycle\n\n    body = lambda x: (x[0] + 1,  # pylint: disable=g-long-lambda\n                      get_prev(do_call(x[1], recycle_idx=x[0],\n                                       compute_loss=False)))\n    if hk.running_init():\n      # When initializing the Haiku module, run one iteration of the\n      # while_loop to initialize the Haiku modules used in `body`.\n      _, prev = body((0, prev))\n    else:\n      _, prev = hk.while_loop(\n          lambda x: x[0] &lt; num_iter,\n          body,\n          (0, prev))\n  else:\n    num_iter = 0\n\n  ret = do_call(prev=prev, recycle_idx=num_iter)\n  if compute_loss:\n    ret = ret[0], [ret[1]]\n\n  if not return_representations:\n    del (ret[0] if compute_loss else ret)['representations']  # pytype: disable=unsupported-operands\n  return ret\n</code></pre>"},{"location":"reference/resources/multimer/","title":"multimer","text":""},{"location":"reference/resources/multimer/#resources.multimer.AlphaFoldInitialGuess","title":"AlphaFoldInitialGuess","text":"<pre><code>AlphaFoldInitialGuess(config, name='alphafold')\n</code></pre> <p>             Bases: <code>Module</code></p> <p>AlphaFold-Multimer model with recycling.</p> Source code in <code>symdesign/resources/multimer.py</code> <pre><code>def __init__(self, config, name='alphafold'):\n  super().__init__(name=name)\n  self.config = config\n  self.global_config = config.global_config\n</code></pre>"},{"location":"reference/resources/sql/","title":"sql","text":""},{"location":"reference/resources/sql/#resources.sql.PoseMetadata","title":"PoseMetadata","text":"<p>             Bases: <code>Base</code></p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.number_of_entities","title":"number_of_entities  <code>property</code>","text":"<pre><code>number_of_entities: int\n</code></pre> <p>Return the number of distinct entities (Gene/Protein products) found in the PoseMetadata</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.entity_names","title":"entity_names  <code>property</code>","text":"<pre><code>entity_names: list[str]\n</code></pre> <p>Return the names of each entity (Gene/Protein products) found in the PoseMetadata</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.design_ids","title":"design_ids  <code>property</code>","text":"<pre><code>design_ids: list[str]\n</code></pre> <p>Get the names of each DesignData in the PoseJob</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.design_names","title":"design_names  <code>property</code>","text":"<pre><code>design_names: list[str]\n</code></pre> <p>Get the names of each DesignData in the PoseJob</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.pose_source","title":"pose_source  <code>property</code>","text":"<pre><code>pose_source\n</code></pre> <p>Provide the DesignData for the Pose itself</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetadata.symmetry","title":"symmetry  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>symmetry = Column(String(8))\n</code></pre> <p>The result of the SymEntry</p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetrics","title":"PoseMetrics","text":"<p>             Bases: <code>Base</code></p>"},{"location":"reference/resources/sql/#resources.sql.PoseMetrics.symmetric_interface","title":"symmetric_interface  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>symmetric_interface = Column(Boolean)\n</code></pre> <p>Thermophilicity implies this is a spectrum, while thermophilic implies binary</p>"},{"location":"reference/resources/sql/#resources.sql.ProteinMetadata","title":"ProteinMetadata","text":"<p>             Bases: <code>Base</code></p> <p>Used for hold fixed metadata of protein structures, typically pulled from PDB API</p>"},{"location":"reference/resources/sql/#resources.sql.ProteinMetadata.entity_id","title":"entity_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entity_id = Column(String(20), nullable=False, index=True, unique=True)\n</code></pre> <p>This could be described as the PDB API EntityID</p>"},{"location":"reference/resources/sql/#resources.sql.ProteinMetadata.thermophilicity","title":"thermophilicity  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>thermophilicity = Column(Float)\n</code></pre> <p>Thermophilicity implies this is a spectrum, while thermophilic implies binary</p>"},{"location":"reference/resources/sql/#resources.sql.ProteinMetadata.uniprot_ids","title":"uniprot_ids  <code>property</code>","text":"<pre><code>uniprot_ids: tuple[str, ...]\n</code></pre> <p>Access the UniProtID's associated with this instance</p>"},{"location":"reference/resources/sql/#resources.sql.ProteinMetadata.entity_info","title":"entity_info  <code>property</code>","text":"<pre><code>entity_info: dict[str, dict[str, Any]]\n</code></pre> <p>Format the instance for population of metadata via the entity_info kwargs</p>"},{"location":"reference/resources/sql/#resources.sql.EntityData","title":"EntityData","text":"<p>             Bases: <code>Base</code></p> <p>Used for unique Pose instances to connect multiple sources of information</p>"},{"location":"reference/resources/sql/#resources.sql.EntityData.entity_info","title":"entity_info  <code>property</code>","text":"<pre><code>entity_info: dict[str, dict[str, Any]]\n</code></pre> <p>Format the instance for population of metadata via the entity_info kwargs</p>"},{"location":"reference/resources/sql/#resources.sql.EntityTransform","title":"EntityTransform","text":"<p>             Bases: <code>Base</code></p>"},{"location":"reference/resources/sql/#resources.sql.EntityTransform.transformation","title":"transformation  <code>property</code> <code>writable</code>","text":"<pre><code>transformation: TransformationMapping | dict\n</code></pre> <p>Provide the names of all Entity instances mapped to the Pose</p>"},{"location":"reference/resources/sql/#resources.sql.DesignData","title":"DesignData","text":"<p>             Bases: <code>Base</code></p> <p>Account for design metadata created from pose metadata</p>"},{"location":"reference/resources/sql/#resources.sql.initialize_metadata","title":"initialize_metadata","text":"<pre><code>initialize_metadata(session: Session, possibly_new_uniprot_to_prot_data: dict[tuple[str, ...], Iterable[ProteinMetadata]] = None, existing_uniprot_entities: Iterable[UniProtEntity] = None, existing_protein_metadata: Iterable[ProteinMetadata] = None) -&gt; dict[tuple[str, ...], list[ProteinMetadata]] | dict\n</code></pre> <p>Compare newly described work to the existing database and set up metadata for all described entities</p> <p>Doesn't commit new instances to the database in case they are attached to existing objects</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>possibly_new_uniprot_to_prot_data</code>             (<code>dict[tuple[str, ...], Iterable[ProteinMetadata]]</code>, default:                 <code>None</code> )         \u2013          <p>A mapping of the possibly required UniProtID entries and their associated ProteinMetadata. These could already exist in database, but were indicated they are needed</p> </li> <li> <code>existing_uniprot_entities</code>             (<code>Iterable[UniProtEntity]</code>, default:                 <code>None</code> )         \u2013          <p>If any UniProtEntity instances are already loaded, pass them to expedite setup</p> </li> <li> <code>existing_protein_metadata</code>             (<code>Iterable[ProteinMetadata]</code>, default:                 <code>None</code> )         \u2013          <p>If any ProteinMetadata instances are already loaded, pass them to expedite setup</p> </li> </ul> Source code in <code>symdesign/resources/sql.py</code> <pre><code>def initialize_metadata(session: Session,\n                        possibly_new_uniprot_to_prot_data: dict[tuple[str, ...], Iterable[ProteinMetadata]] = None,\n                        existing_uniprot_entities: Iterable[wrapapi.UniProtEntity] = None,\n                        existing_protein_metadata: Iterable[ProteinMetadata] = None) -&gt; \\\n        dict[tuple[str, ...], list[ProteinMetadata]] | dict:\n    \"\"\"Compare newly described work to the existing database and set up metadata for all described entities\n\n    Doesn't commit new instances to the database in case they are attached to existing objects\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        possibly_new_uniprot_to_prot_data: A mapping of the possibly required UniProtID entries and their associated\n            ProteinMetadata. These could already exist in database, but were indicated they are needed\n        existing_uniprot_entities: If any UniProtEntity instances are already loaded, pass them to expedite setup\n        existing_protein_metadata: If any ProteinMetadata instances are already loaded, pass them to expedite setup\n    \"\"\"\n    if not possibly_new_uniprot_to_prot_data:\n        if existing_protein_metadata:\n            pass\n            # uniprot_id_to_metadata = {protein_data.uniprot_ids:\n            #                           protein_data for protein_data in existing_protein_metadata}\n        elif existing_uniprot_entities:\n            existing_protein_metadata = {unp_entity.protein_metadata for unp_entity in existing_uniprot_entities}\n        else:\n            existing_protein_metadata = {}\n\n        return {protein_data.uniprot_ids: [protein_data] for protein_data in existing_protein_metadata}\n\n    # Todo\n    #  If I ever adopt the UniqueObjectValidatedOnPending recipe, that could perform the work of getting the\n    #  correct objects attached to the database\n\n    # Get the set of all UniProtIDs\n    possibly_new_uniprot_ids = set()\n    for uniprot_ids in possibly_new_uniprot_to_prot_data.keys():\n        possibly_new_uniprot_ids.update(uniprot_ids)\n    # Find existing UniProtEntity instances from database\n    if existing_uniprot_entities is None:\n        existing_uniprot_entities = []\n    else:\n        existing_uniprot_entities = list(existing_uniprot_entities)\n\n    existing_uniprot_ids = {unp_ent.id for unp_ent in existing_uniprot_entities}\n    # Remove the certainly existing from possibly new and query for any new that already exist\n    query_additional_existing_uniprot_entities_stmt = \\\n        select(wrapapi.UniProtEntity).where(wrapapi.UniProtEntity.id.in_(\n            possibly_new_uniprot_ids.difference(existing_uniprot_ids)))\n    # Add all requested to those known about\n    existing_uniprot_entities += session.scalars(query_additional_existing_uniprot_entities_stmt).all()\n\n    # Todo Maybe needed?\n    #  Emit this select when there is a stronger association between the multiple\n    #  UniProtEntity.uniprot_ids and referencing a unique ProteinMetadata\n    #  The below were never tested\n    # existing_uniprot_entities_stmt = \\\n    #     select(UniProtProteinAssociation.protein)\\\n    #     .where(UniProtProteinAssociation.uniprot_id.in_(possibly_new_uniprot_ids))\n    #     # NEED TO GROUP THESE BY ProteinMetadata.uniprot_entities\n    # OR\n    # existing_uniprot_entities_stmt = \\\n    #     select(wrapapi.UniProtEntity).join(ProteinMetadata)\\\n    #     .where(wrapapi.UniProtEntity.uniprot_id.in_(possibly_new_uniprot_ids))\n    #     # NEED TO GROUP THESE BY ProteinMetadata.uniprot_entities\n\n    # Map the existing uniprot_id to UniProtEntity\n    uniprot_id_to_unp_entity = {unp_entity.id: unp_entity for unp_entity in existing_uniprot_entities}\n    insert_uniprot_ids = possibly_new_uniprot_ids.difference(uniprot_id_to_unp_entity.keys())\n\n    # Get the remaining UniProtIDs as UniProtEntity entries\n    new_uniprot_id_to_unp_entity = {uniprot_id: wrapapi.UniProtEntity(id=uniprot_id)\n                                    for uniprot_id in insert_uniprot_ids}\n    # Update entire dictionary for ProteinMetadata ops below\n    uniprot_id_to_unp_entity.update(new_uniprot_id_to_unp_entity)\n    # Insert new\n    new_uniprot_entities = list(new_uniprot_id_to_unp_entity.values())\n    session.add_all(new_uniprot_entities)\n\n    # Repeat the process for ProteinMetadata\n    # Map entity_id to uniprot_id for later cleaning of UniProtEntity\n    possibly_new_entity_id_to_uniprot_ids = \\\n        {protein_data.entity_id: uniprot_ids\n         for uniprot_ids, protein_datas in possibly_new_uniprot_to_prot_data.items()\n         for protein_data in protein_datas}\n    # Map entity_id to ProteinMetadata\n    possibly_new_entity_id_to_protein_data = \\\n        {protein_data.entity_id: protein_data\n         for protein_datas in possibly_new_uniprot_to_prot_data.values()\n         for protein_data in protein_datas}\n    possibly_new_entity_ids = set(possibly_new_entity_id_to_protein_data.keys())\n\n    if existing_protein_metadata is None:\n        existing_protein_metadata = []\n    else:\n        existing_protein_metadata = list(existing_protein_metadata)\n\n    existing_entity_ids = {protein_data.entity_id for protein_data in existing_protein_metadata}\n    # Remove the certainly existing from possibly new and query the new\n    existing_protein_metadata_stmt = \\\n        select(ProteinMetadata) \\\n        .where(ProteinMetadata.entity_id.in_(possibly_new_entity_ids.difference(existing_entity_ids)))\n    # Add all requested to those known about\n    existing_protein_metadata += session.scalars(existing_protein_metadata_stmt).all()\n\n    # Get all the existing ProteinMetadata.entity_ids to handle the certainly new ones\n    existing_entity_ids = {protein_data.entity_id for protein_data in existing_protein_metadata}\n    # Any remaining entity_ids are new and must be added\n    new_entity_ids = possibly_new_entity_ids.difference(existing_entity_ids)\n    # uniprot_ids_to_new_metadata = {\n    #     possibly_new_entity_id_to_uniprot_ids[entity_id]: possibly_new_entity_id_to_protein_data[entity_id]\n    #     for entity_id in new_entity_ids}\n    uniprot_ids_to_new_metadata = defaultdict(list)\n    for entity_id in new_entity_ids:\n        uniprot_ids_to_new_metadata[possibly_new_entity_id_to_uniprot_ids[entity_id]].append(\n            possibly_new_entity_id_to_protein_data[entity_id])\n\n    # Add all existing to UniProtIDs to ProteinMetadata mapping\n    all_uniprot_id_to_prot_data = defaultdict(list)\n    for protein_data in existing_protein_metadata:\n        all_uniprot_id_to_prot_data[protein_data.uniprot_ids].append(protein_data)\n\n    # Collect all new ProteinMetadata which remain\n    all_protein_metadata = []\n    for uniprot_ids, metadatas in uniprot_ids_to_new_metadata.items():\n        all_protein_metadata.extend(metadatas)\n        # Add to UniProtIDs to ProteinMetadata map\n        all_uniprot_id_to_prot_data[uniprot_ids].extend(metadatas)\n        # Attach UniProtEntity to new ProteinMetadata by UniProtID\n        for protein_metadata in metadatas:\n            # Create the ordered_list of UniProtIDs (UniProtEntity) on ProteinMetadata entry\n            try:\n                # protein_metadata.uniprot_entities.extend(\n                #     uniprot_id_to_unp_entity[uniprot_id] for uniprot_id in uniprot_ids)\n                protein_metadata.uniprot_entities = \\\n                    [uniprot_id_to_unp_entity[uniprot_id] for uniprot_id in uniprot_ids]\n            except KeyError:\n                # uniprot_id_to_unp_entity is missing a key. Not sure why it wouldn't be here...\n                raise SymDesignException(putils.report_issue)\n\n    # Insert remaining ProteinMetadata\n    session.add_all(all_protein_metadata)\n    # # Finalize additions to the database\n    # session.commit()\n\n    return all_uniprot_id_to_prot_data\n</code></pre>"},{"location":"reference/resources/sql/#resources.sql.insert_dataframe","title":"insert_dataframe","text":"<pre><code>insert_dataframe(session: Session, table: Base, df: DataFrame, mysql: bool = False, **kwargs)\n</code></pre> <p>Take a formatted pandas DataFrame and insert values into a sqlalchemy session, then commit the transaction</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>table</code>             (<code>Base</code>)         \u2013          <p>A Class mapped to SQL table with sqlalchemy</p> </li> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>The DataFrame with records to insert</p> </li> <li> <code>mysql</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the database is a MySQL dialect</p> </li> </ul> Source code in <code>symdesign/resources/sql.py</code> <pre><code>def insert_dataframe(session: Session, table: Base, df: pd.DataFrame, mysql: bool = False, **kwargs):\n    \"\"\"Take a formatted pandas DataFrame and insert values into a sqlalchemy session, then commit the transaction\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        table: A Class mapped to SQL table with sqlalchemy\n        df: The DataFrame with records to insert\n        mysql: Whether the database is a MySQL dialect\n    \"\"\"\n    if mysql:\n        insert = mysql_insert\n    else:\n        insert = sqlite_insert\n\n    insert_stmt = insert(table)\n    # # Get the columns that should be updated\n    # new_columns = df.columns.tolist()\n    # # logger.debug(f'Provided columns: {new_columns}')\n    # excluded_columns = insert_stmt.excluded\n    # update_columns = [c for c in excluded_columns if c.name in new_columns]\n    # update_dict = {getattr(c, 'name'): c for c in update_columns if not c.primary_key}\n    # table_ = table.__table__\n    # # Find relevant column indicators to parse the non-primary key non-nullable columns\n    # primary_keys = [key for key in table_.primary_key]\n    # non_null_keys = [col for col in table_.columns if not col.nullable]\n    # index_keys = [key for key in non_null_keys if key not in primary_keys]\n\n    # do_update_stmt = insert_stmt.on_conflict_do_update(\n    #     index_elements=index_keys,  # primary_keys,\n    #     set_=update_dict\n    # )\n    # # Can't insert with .returning() until version 2.0...\n    # # try:\n    # #     result = session.execute(do_update_stmt.returning(table_.id), df.reset_index().to_dict('records'))\n    # # except exc.CompileError as error:\n    # #     logger.error(error)\n    # #     try:\n    # #         result = session.execute(insert_stmt.returning(table_.id), df.reset_index().to_dict('records'))\n    # #     except exc.CompileError as _error:\n    # #         logger.error(_error)\n    # # try:\n    # This works using insert with conflict, however, doesn't return the auto-incremented ids\n    # result = session.execute(do_update_stmt, df.to_dict('records'))\n    # result = session.execute(insert_stmt, df.to_dict('records'))\n    start_time = time()\n    session.execute(insert_stmt, df.to_dict('records'))\n    logger.debug(f'Transaction with table \"{table.__tablename__}\" took {time() - start_time:8f}s')\n</code></pre>"},{"location":"reference/resources/sql/#resources.sql.upsert_dataframe","title":"upsert_dataframe","text":"<pre><code>upsert_dataframe(session: Session, table: Base, df: DataFrame, mysql: bool = False, **kwargs)\n</code></pre> <p>Take a formatted pandas DataFrame and insert/update values into a sqlalchemy session, then commit the transaction</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>table</code>             (<code>Base</code>)         \u2013          <p>A Class mapped to SQL table with sqlalchemy</p> </li> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>The DataFrame with records to insert</p> </li> <li> <code>mysql</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the database is a MySQL dialect</p> </li> </ul> Source code in <code>symdesign/resources/sql.py</code> <pre><code>def upsert_dataframe(session: Session, table: Base, df: pd.DataFrame, mysql: bool = False, **kwargs):\n    \"\"\"Take a formatted pandas DataFrame and insert/update values into a sqlalchemy session, then commit the transaction\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        table: A Class mapped to SQL table with sqlalchemy\n        df: The DataFrame with records to insert\n        mysql: Whether the database is a MySQL dialect\n    \"\"\"\n    if mysql:\n        insert_stmt = mysql_insert(table)\n        excluded_columns = insert_stmt.inserted\n    else:\n        insert_stmt = sqlite_insert(table)\n        excluded_columns = insert_stmt.excluded\n\n    # Get the columns that should be updated\n    new_columns = df.columns.tolist()\n    # logger.debug(f'Provided columns: {new_columns}')\n    update_columns = [c for c in excluded_columns if c.name in new_columns]\n    update_dict = {c.name: c for c in update_columns if not c.primary_key}\n    tablename = table.__tablename__\n    if mysql:\n        do_update_stmt = insert_stmt.on_duplicate_key_update(\n            update_dict\n        )\n    else:  # SQLite and postgresql are the same\n        # Find relevant column indicators to parse the non-primary key non-nullable columns\n        unique_constraints = inspect(session.connection()).get_unique_constraints(tablename)\n        # Returns\n        #  [{'name': '_pose_design_uc', 'column_names': ['pose_id', 'design_id']}]\n        table_unique_constraint_keys = set()\n        for constraint in unique_constraints:\n            table_unique_constraint_keys.update(constraint['column_names'])\n\n        table_ = table.__table__\n        unique_constraint_keys = {col.name for col in table_.columns if col.unique}\n        index_keys = unique_constraint_keys.union(table_unique_constraint_keys)\n        # primary_keys = [key for key in table_.primary_key]\n        # non_null_keys = [col for col in table_.columns if not col.nullable]\n        # index_keys = [key for key in non_null_keys if key not in primary_keys] \\\n        #     + unique_constraint_keys\n        do_update_stmt = insert_stmt.on_conflict_do_update(\n            index_elements=index_keys,  # primary_keys,\n            set_=update_dict\n        )\n    # Todo Error\n    #  sqlalchemy.exc.OperationalError:\n    #  MySQLdb._exceptions.OperationalError:\n    #    (1213, 'Deadlock found when trying to get lock; try restarting transaction')\n    start_time = time()\n    session.execute(do_update_stmt, df.to_dict('records'))\n    logger.debug(f'Transaction with table \"{tablename}\" took {time() - start_time:8f}s')\n</code></pre>"},{"location":"reference/resources/sql/#resources.sql.format_residues_df_for_write","title":"format_residues_df_for_write","text":"<pre><code>format_residues_df_for_write(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Take a typical per-residue DataFrame and orient the top column level (level=0) containing the residue numbers on the index innermost level</p> <p>Parameters:</p> <ul> <li> <code>df</code>             (<code>DataFrame</code>)         \u2013          <p>A per-residue DataFrame to transform</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>The transformed DataFrame</p> </li> </ul> Source code in <code>symdesign/resources/sql.py</code> <pre><code>def format_residues_df_for_write(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Take a typical per-residue DataFrame and orient the top column level (level=0) containing the residue numbers on\n    the index innermost level\n\n    Args:\n        df: A per-residue DataFrame to transform\n\n    Returns:\n        The transformed DataFrame\n    \"\"\"\n    # df.sort_index(level=0, axis=1, inplace=True, sort_remaining=False)\n    # # residue_metric_columns = residues.columns.levels[-1].tolist()\n    # # self.log.debug(f'Residues metrics present: {residue_metric_columns}')\n\n    # Place the residue indices from the column names into the index at position -1\n    df = df.stack(0)\n    df.index.set_names('index', level=-1, inplace=True)\n\n    return df\n</code></pre>"},{"location":"reference/resources/sql/#resources.sql.write_dataframe","title":"write_dataframe","text":"<pre><code>write_dataframe(session: Session, designs: DataFrame = None, design_residues: DataFrame = None, entity_designs: DataFrame = None, poses: DataFrame = None, pose_residues: DataFrame = None, residues: DataFrame = None, update: bool = True, transaction_kwargs: dict = dict())\n</code></pre> <p>Format each possible DataFrame type for output via csv or SQL database</p> <p>Parameters:</p> <ul> <li> <code>session</code>             (<code>Session</code>)         \u2013          <p>A currently open transaction within sqlalchemy</p> </li> <li> <code>designs</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-design metric DataFrame where each index is the design id and the columns are design metrics</p> </li> <li> <code>design_residues</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, Boolean for design utilization)</p> </li> <li> <code>entity_designs</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-design metric DataFrame for Entity instances where each index is the design id and the columns are design metrics</p> </li> <li> <code>poses</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-pose metric DataFrame where each index is the pose id and the columns are pose metrics</p> </li> <li> <code>pose_residues</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> <li> <code>residues</code>             (<code>DataFrame</code>, default:                 <code>None</code> )         \u2013          <p>The typical per-residue metric DataFrame where each index is the design id and the columns are (residue index, residue metric)</p> </li> <li> <code>update</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the output identifiers are already present in the metrics</p> </li> <li> <code>transaction_kwargs</code>             (<code>dict</code>, default:                 <code>dict()</code> )         \u2013          <p>Any keyword arguments that should be passed for the transaction. Automatically populated with the database backend as located from the session</p> </li> </ul> Source code in <code>symdesign/resources/sql.py</code> <pre><code>def write_dataframe(session: Session, designs: pd.DataFrame = None,\n                    design_residues: pd.DataFrame = None, entity_designs: pd.DataFrame = None,\n                    poses: pd.DataFrame = None, pose_residues: pd.DataFrame = None, residues: pd.DataFrame = None,\n                    update: bool = True, transaction_kwargs: dict = dict()):\n    \"\"\"Format each possible DataFrame type for output via csv or SQL database\n\n    Args:\n        session: A currently open transaction within sqlalchemy\n        designs: The typical per-design metric DataFrame where each index is the design id and the columns are\n            design metrics\n        design_residues: The typical per-residue metric DataFrame where each index is the design id and the columns\n            are (residue index, Boolean for design utilization)\n        entity_designs: The typical per-design metric DataFrame for Entity instances where each index is the design id\n            and the columns are design metrics\n        poses: The typical per-pose metric DataFrame where each index is the pose id and the columns are\n            pose metrics\n        pose_residues: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n        residues: The typical per-residue metric DataFrame where each index is the design id and the columns are\n            (residue index, residue metric)\n        update: Whether the output identifiers are already present in the metrics\n        transaction_kwargs: Any keyword arguments that should be passed for the transaction. Automatically populated\n            with the database backend as located from the session\n    \"\"\"\n    #     job: The resources for the current job\n    if update:\n        dataframe_function = upsert_dataframe\n    else:\n        dataframe_function = insert_dataframe\n\n    # If this is the first call, update the dictionary to specify the database dialect\n    if transaction_kwargs == dict():\n        transaction_kwargs.update({session.bind.dialect.name: True})\n        # transaction_kwargs.update(which_dialect(session))\n    # else:\n    #     input(transaction_kwargs)\n    # warn = warned = False\n    #\n    # def warn_multiple_update_results():\n    #     nonlocal warned\n    #     if warn and not warned:\n    #         logger.warning(\n    #             \"Performing multiple metrics SQL transactions will only return results for the last transaction\")\n    #         warned = True\n    replace_values = {np.nan: None, float('inf'): 1e6, float('-inf'): -1e6}\n\n    if poses is not None and not poses.empty:\n        # warn = True\n        df = poses.replace(replace_values).reset_index()\n        table = PoseMetrics\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n\n    if designs is not None and not designs.empty:\n        # warn_multiple_update_results()\n        # warn = True\n        df = designs.replace(replace_values).reset_index()\n        table = DesignMetrics\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n\n    if entity_designs is not None and not entity_designs.empty:\n        # warn_multiple_update_results()\n        # warn = True\n        df = entity_designs.replace(replace_values).reset_index()\n        table = DesignEntityMetrics\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n\n    if design_residues is not None and not design_residues.empty:\n        # warn_multiple_update_results()\n        # warn = True\n        df = format_residues_df_for_write(design_residues).replace(replace_values).reset_index()\n        table = DesignResidues\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n\n    if residues is not None and not residues.empty:\n        # warn_multiple_update_results()\n        # warn = True\n        df = format_residues_df_for_write(residues).replace(replace_values).reset_index()\n        table = ResidueMetrics\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n\n    if pose_residues is not None and not pose_residues.empty:\n        # warn_multiple_update_results()\n        # warn = True\n        df = format_residues_df_for_write(pose_residues).replace(replace_values).reset_index()\n        table = PoseResidueMetrics\n        dataframe_function(session, table=table, df=df, **transaction_kwargs)\n        logger.info(f'Wrote {table.__tablename__} to Database')\n</code></pre>"},{"location":"reference/resources/structure_db/","title":"structure_db","text":""},{"location":"reference/resources/structure_db/#resources.structure_db.structure_database_factory","title":"structure_database_factory  <code>module-attribute</code>","text":"<pre><code>structure_database_factory: Annotated[StructureDatabaseFactory, 'Calling this factory method returns the single instance of the Database class located at the \"source\" keyword argument'] = StructureDatabaseFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the Database class located at the \"source\" keyword  argument</p>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabase","title":"StructureDatabase","text":"<pre><code>StructureDatabase(models: AnyStr | Path = None, full_models: AnyStr | Path = None, oriented: AnyStr | Path = None, oriented_asu: AnyStr | Path = None, refined: AnyStr | Path = None, stride: AnyStr | Path = None, **kwargs)\n</code></pre> <p>             Bases: <code>Database</code></p> <p>A Database which holds structural data in particular</p> <p>Parameters:</p> <ul> <li> <code>models</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>full_models</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>oriented</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>oriented_asu</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>refined</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>stride</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path to the specified directory with stores these particular files</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def __init__(self, models: AnyStr | Path = None, full_models: AnyStr | Path = None, oriented: AnyStr | Path = None,\n             oriented_asu: AnyStr | Path = None, refined: AnyStr | Path = None, stride: AnyStr | Path = None,\n             **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        models: The path to the specified directory with stores these particular files\n        full_models: The path to the specified directory with stores these particular files\n        oriented: The path to the specified directory with stores these particular files\n        oriented_asu: The path to the specified directory with stores these particular files\n        refined: The path to the specified directory with stores these particular files\n        stride: The path to the specified directory with stores these particular files\n        **kwargs:\n    \"\"\"\n    # passed to Database\n    # sql: sqlite = None, log: Logger = logger\n    super().__init__(**kwargs)  # Database\n\n    self.models = DataStore(location=models, extension='.pdb', glob_extension='.pdb*',\n                            sql=self.sql, log=self.log, load_file=Pose.from_pdb)\n    # Old version when loop model came from an ensemble\n    # self.full_models = DataStore(location=full_models, extension='_ensemble.pdb', glob_extension='_ensemble.pdb*',\n    self.full_models = DataStore(location=full_models, extension='.pdb', glob_extension='.pdb*',\n                                 sql=self.sql, log=self.log, load_file=Pose.from_pdb)\n    self.oriented = DataStore(location=oriented, extension='.pdb', glob_extension='.pdb*',\n                              sql=self.sql, log=self.log, load_file=Pose.from_pdb)\n    self.oriented_asu = DataStore(location=oriented_asu, extension='.pdb', glob_extension='.pdb*',\n                                  sql=self.sql, log=self.log, load_file=Pose.from_pdb)\n    self.refined = DataStore(location=refined, extension='.pdb', glob_extension='.pdb*',\n                             sql=self.sql, log=self.log, load_file=Pose.from_pdb)\n    self.stride = DataStore(location=stride, extension='.stride', sql=self.sql, log=self.log,\n                            load_file=structure.utils.parse_stride)\n\n    self.sources = [self.oriented_asu, self.refined, self.stride]  # self.full_models\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabase.orient_structures","title":"orient_structures","text":"<pre><code>orient_structures(structure_identifiers: Iterable[str], sym_entry: SymEntry = None, by_file: bool = False) -&gt; tuple[dict[str, tuple[str, ...]], dict[tuple[str, ...], list[ProteinMetadata]]]\n</code></pre> <p>Given structure identifiers and their corresponding symmetry, retrieve, orient, and save oriented files to the Database, then return metadata for each</p> <p>Parameters:</p> <ul> <li> <code>structure_identifiers</code>             (<code>Iterable[str]</code>)         \u2013          <p>The names of all entity_ids requiring orientation</p> </li> <li> <code>sym_entry</code>             (<code>SymEntry</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry used to treat each passed Entity as symmetric. Default assumes no symmetry</p> </li> <li> <code>by_file</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to parse the structure_identifiers as file paths. Default treats as PDB EntryID/EntityID</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, tuple[str, ...]]</code>         \u2013          <p>The tuple consisting of ( A map of the entire Pose name to each contained Entity name, A mapping of the UniprotID's to their ProteinMetadata instance for every Entity loaded</p> </li> <li> <code>dict[tuple[str, ...], list[ProteinMetadata]]</code>         \u2013          <p>)</p> </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def orient_structures(self, structure_identifiers: Iterable[str], sym_entry: SymEntry = None,\n                      by_file: bool = False) \\\n        -&gt; tuple[dict[str, tuple[str, ...]], dict[tuple[str, ...], list[sql.ProteinMetadata]]]:\n    \"\"\"Given structure identifiers and their corresponding symmetry, retrieve, orient, and save oriented files to\n    the Database, then return metadata for each\n\n    Args:\n        structure_identifiers: The names of all entity_ids requiring orientation\n        sym_entry: The SymEntry used to treat each passed Entity as symmetric. Default assumes no symmetry\n        by_file: Whether to parse the structure_identifiers as file paths. Default treats as PDB EntryID/EntityID\n\n    Returns:\n        The tuple consisting of (\n            A map of the entire Pose name to each contained Entity name,\n            A mapping of the UniprotID's to their ProteinMetadata instance for every Entity loaded\n        )\n    \"\"\"\n    if not structure_identifiers:\n        return {}, {}\n\n    self.oriented.make_path()\n    self.oriented_asu.make_path()\n    models_dir = self.models.location\n\n    if isinstance(sym_entry, utils.SymEntry.SymEntry):\n        if sym_entry.number:\n            resulting_symmetry = sym_entry.resulting_symmetry\n            if resulting_symmetry in utils.symmetry.space_group_cryst1_fmt_dict:\n                # This is a crystalline symmetry, so use a TOKEN to specify use of the CRYST record\n                resulting_symmetry = CRYST\n            else:\n                logger.info(f'The requested {\"files\" if by_file else \"IDs\"} are being checked for proper '\n                            f'orientation with symmetry {resulting_symmetry}: {\", \".join(structure_identifiers)}')\n        else:  # This is entry_number 0, which is a TOKEN to use the CRYST record\n            resulting_symmetry = CRYST\n    else:  # Treat as asymmetric - i.e. C1\n        if sym_entry:\n            logger.warning(f\"The passed 'sym_entry' isn't of the required type {utils.SymEntry.SymEntry.__name__}. \"\n                           \"Treating as asymmetric\")\n        sym_entry = None  # Ensure not something else\n        resulting_symmetry = 'C1'\n        logger.info(f'The requested {\"files\" if by_file else \"IDs\"} are being set up into the DataBase: '\n                    f'{\", \".join(structure_identifiers)}')\n\n    orient_logger = logging.getLogger(putils.orient)\n    structure_identifier_tuples: dict[str, tuple[str, ...]] = {}\n    uniprot_id_to_protein_metadata: dict[tuple[str, ...], list[sql.ProteinMetadata]] = defaultdict(list)\n    non_viable_structures = []\n\n    def create_protein_metadata(model: ContainsEntities):\n        \"\"\"From a ContainsEntities instance, extract the unique metadata to identify the entities involved\n\n        Args:\n            model: The Entity instances to initialize to ProteinMetadata\n        \"\"\"\n        for entity in model.entities:\n            protein_metadata = sql.ProteinMetadata(\n                entity_id=entity.name,\n                reference_sequence=entity.reference_sequence,\n                thermophilicity=entity.thermophilicity,\n                symmetry_group=entity.symmetry,\n                model_source=entity.tmp_file_path\n            )\n            entity.calculate_secondary_structure(to_file=self.stride.path_to(name=entity.name))\n            protein_metadata.n_terminal_helix = entity.is_termini_helical()\n            protein_metadata.c_terminal_helix = entity.is_termini_helical('c')\n\n            try:\n                ''.join(entity.uniprot_ids)\n            except TypeError:  # Uniprot_ids is (None,)\n                entity.uniprot_ids = (entity.name,)\n            except AttributeError:  # Unable to retrieve .uniprot_ids\n                entity.uniprot_ids = (entity.name,)\n            # else:  # .uniprot_ids work. Use as parsed\n            uniprot_ids = entity.uniprot_ids\n\n            uniprot_id_to_protein_metadata[uniprot_ids].append(protein_metadata)\n\n        if resulting_symmetry == CRYST:\n            structure_identifier_tuples[model.name] = tuple()\n        else:\n            structure_identifier_tuples[model.name] = tuple(entity.name for entity in model.entities)\n\n    def report_non_viable_structures():\n        if non_viable_structures:\n            if len(non_viable_structures) &gt; 1:\n                non_str = ', '.join(non_viable_structures[:-1]) + f' and {non_viable_structures[-1]}'\n                plural_str = f\"s {non_str} weren't\"\n            else:\n                plural_str = f\" {non_viable_structures} wasn't\"\n            orient_logger.error(\n                f'The structure{plural_str} able to be oriented properly')\n\n    def write_entities_and_asu(model: ContainsEntities, assembly_integer: str):\n        \"\"\"Write the overall ASU, each Entity as an ASU and oligomer, and set the model.tmp_file_path attribute\n\n        Args:\n            model: The ContainsEntities instance being oriented\n            assembly_integer: The integer representing the assembly number (provided from \".pdb1\" type extensions)\n        \"\"\"\n        # Save .file_path attribute\n        model.tmp_file_path = os.path.join(self.oriented_asu.location, f'{model.name}.pdb{assembly_integer}')\n        with open(model.tmp_file_path, 'w') as f:\n            f.write(model.format_header())\n            # Write out each Entity in model to form the ASU\n            for entity in model.entities:\n                # Write each Entity to combined asu\n                entity.write(file_handle=f)\n                # Write each Entity to own file\n                oligomer_path = os.path.join(self.oriented.location, f'{entity.name}.pdb{assembly_integer}')\n                entity.write(assembly=True, out_path=oligomer_path)\n                # And asu\n                asu_path = os.path.join(self.oriented_asu.location, f'{entity.name}.pdb{assembly_integer}')\n                # Set the Entity.tmp_file_path for ProteinMetadata\n                entity.tmp_file_path = entity.write(out_path=asu_path)\n\n    def _orient_existing_files(files: Iterable[str], resulting_symmetry: str, sym_entry: SymEntry = None) -&gt; None:\n        \"\"\"Return the structure identifier for a file that is loaded and oriented\n\n        Args:\n            files: The files to orient in the canonical symmetry\n            resulting_symmetry: The symmetry to use during orient\n            sym_entry: The symmetry to use during orient protocol\n\n        Returns:\n            None\n        \"\"\"\n        for file in files:\n            # Load entities to solve multi-component orient problem\n            pose = Pose.from_file(file)\n            if resulting_symmetry == CRYST:\n                pose.set_symmetry(sym_entry=sym_entry)\n                pose.tmp_file_path = pose.write(out_path=self.models.path_to(name=pose.name))\n                # Set each Entity.tmp_file_path\n                for entity in pose.entities:\n                    entity.tmp_file_path = entity.write(out_path=self.models.path_to(name=entity.name))\n            else:\n                try:\n                    pose.orient(symmetry=resulting_symmetry)\n                except (ValueError, RuntimeError, structure.utils.SymmetryError) as error:\n                    orient_logger.error(str(error))\n                    non_viable_structures.append(file)\n                    continue\n                pose.set_symmetry(sym_entry=sym_entry)\n                assembly_integer = '' if pose.biological_assembly is None else pose.biological_assembly\n                orient_file = os.path.join(self.oriented.location, f'{pose.name}.pdb{assembly_integer}')\n                pose.write(out_path=orient_file)\n                orient_logger.info(f'Oriented: {orient_file}')  # &lt;- This isn't ASU\n                write_entities_and_asu(pose, assembly_integer)\n\n            create_protein_metadata(pose)\n\n    if by_file:\n        _orient_existing_files(structure_identifiers, resulting_symmetry, sym_entry)\n    else:  # Orienting the selected files and save\n        # First, check if using crystalline symmetry and prevent loading of existing files\n        if resulting_symmetry == CRYST:\n            orient_asu_names = orient_names = model_names = []\n        else:\n            orient_names = self.oriented.retrieve_names()\n            orient_asu_names = self.oriented_asu.retrieve_names()\n            model_names = self.models.retrieve_names()\n        # Using Pose simplifies ASU writing, however if the Pose isn't oriented correctly SymEntry won't work\n        # Todo\n        #  Should clashes be warned?\n        # , ignore_clashes=True)\n        pose_kwargs = dict(sym_entry=sym_entry)\n        for structure_identifier in structure_identifiers:\n            # First, check if the structure_identifier ASU has been processed\n            if structure_identifier in orient_asu_names:  # orient_asu file exists, just load\n                orient_asu_file = self.oriented_asu.retrieve_file(name=structure_identifier)\n                pose = Pose.from_file(orient_asu_file, name=structure_identifier, **pose_kwargs)\n                if pose.symmetric_assembly_is_clash(measure=self.job.design.clash_criteria,\n                                                    distance=self.job.design.clash_distance, warn=True):\n                    if not self.job.design.ignore_symmetric_clashes:\n                        logger.critical(f\"The structure '{structure_identifier}' isn't a viable symmetric assembly \"\n                                        f\"in the symmetry {resulting_symmetry}. Couldn't initialize\")\n                        continue\n\n                pose.tmp_file_path = pose.file_path\n                # Write each Entity in addition\n                for entity in pose.entities:\n                    entity_asu_file = self.oriented_asu.retrieve_file(name=entity.name)\n                    if entity_asu_file is None:\n                        entity.write(out_path=self.oriented_asu.path_to(name=entity.name))\n                    # Set the Entity.tmp_file_path for ProteinMetadata\n                    entity.tmp_file_path = entity_asu_file\n            elif structure_identifier in orient_names:  # ASU files don't exist. Load oriented and save asu\n                orient_file = self.oriented.retrieve_file(name=structure_identifier)\n                # These name=structure_identifier should be the default parsing method anyway...\n                pose = Pose.from_file(orient_file, name=structure_identifier, **pose_kwargs)\n\n                if pose.symmetric_assembly_is_clash(measure=self.job.design.clash_criteria,\n                                                    distance=self.job.design.clash_distance, warn=True):\n                    if not self.job.design.ignore_symmetric_clashes:\n                        logger.critical(f\"The structure '{structure_identifier}' isn't a viable symmetric assembly \"\n                                        f\"in the symmetry {resulting_symmetry}. Couldn't initialize\")\n                        continue\n\n                # Write out the Pose ASU\n                assembly_integer = '' if pose.biological_assembly is None else pose.biological_assembly\n                write_entities_and_asu(pose, assembly_integer)\n            else:  # orient is missing, retrieve the proper files using PDB ID's\n                if structure_identifier in model_names:\n                    model_file = self.models.retrieve_file(name=structure_identifier)\n                    pose = Pose.from_file(model_file, name=structure_identifier)\n                else:\n                    pose_models = download_structures([structure_identifier], out_dir=models_dir)\n                    if pose_models:\n                        # Get the first model and throw away the rest\n                        pose, *_ = pose_models\n                    else:  # Empty list\n                        non_viable_structures.append(structure_identifier)\n                        continue\n\n                if resulting_symmetry == CRYST:\n                    pose.set_symmetry(sym_entry=sym_entry)\n                    pose.tmp_file_path = pose.file_path\n                    # Set each Entity.tmp_file_path\n                    for entity in pose.entities:\n                        entity.tmp_file_path = entity.write(out_path=self.models.path_to(name=entity.name))\n                else:\n                    try:  # Orient the Pose\n                        pose.orient(symmetry=resulting_symmetry)\n                    except (ValueError, RuntimeError, structure.utils.SymmetryError) as error:\n                        orient_logger.error(str(error))\n                        non_viable_structures.append(structure_identifier)\n                        continue\n\n                    pose.set_symmetry(sym_entry=sym_entry)\n                    assembly_integer = '' if pose.biological_assembly is None else pose.biological_assembly\n                    # Write out files for the orient database\n                    base_file_name = f'{structure_identifier}.pdb{assembly_integer}'\n                    orient_file = os.path.join(self.oriented.location, base_file_name)\n\n                    if isinstance(pose, Entity):\n                        # The symmetry attribute should be set from parsing, so assembly=True will work\n                        # and create_protein_metadata has access to .symmetry\n                        pose.write(assembly=True, out_path=orient_file)\n                        # Write out ASU file\n                        asu_path = os.path.join(self.oriented_asu.location, base_file_name)\n                        # Set the Entity.tmp_file_path for ProteinMetadata\n                        pose.tmp_file_path = pose.write(out_path=asu_path)\n                    else:\n                        pose.write(out_path=orient_file)\n                        write_entities_and_asu(pose, assembly_integer)\n\n                    orient_logger.info(f'Oriented: {orient_file}')\n\n            create_protein_metadata(pose)\n\n    report_non_viable_structures()\n    return structure_identifier_tuples, uniprot_id_to_protein_metadata\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabase.preprocess_metadata_for_design","title":"preprocess_metadata_for_design","text":"<pre><code>preprocess_metadata_for_design(metadata: list[ProteinMetadata], script_out_path: AnyStr = os.getcwd(), batch_commands: bool = False) -&gt; list[str] | list\n</code></pre> <p>Assess whether structural data requires any processing prior to design calculations. Processing includes relaxation \"refine\" into the energy function and/or modeling missing segments \"loop model\"</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>             (<code>list[ProteinMetadata]</code>)         \u2013          <p>An iterable of ProteinMetadata objects of interest with the following attributes: model_source, symmetry_group, and entity_id</p> </li> <li> <code>script_out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Where should Entity processing commands be written?</p> </li> <li> <code>batch_commands</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether commands should be made for batch submission</p> </li> </ul> <p>Returns:     Any instructions if processing is needed, otherwise an empty list</p> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def preprocess_metadata_for_design(self, metadata: list[sql.ProteinMetadata], script_out_path: AnyStr = os.getcwd(),\n                                   batch_commands: bool = False) -&gt; list[str] | list:\n    \"\"\"Assess whether structural data requires any processing prior to design calculations.\n    Processing includes relaxation \"refine\" into the energy function and/or modeling missing segments \"loop model\"\n\n    Args:\n        metadata: An iterable of ProteinMetadata objects of interest with the following attributes:\n            model_source, symmetry_group, and entity_id\n        script_out_path: Where should Entity processing commands be written?\n        batch_commands: Whether commands should be made for batch submission\n    Returns:\n        Any instructions if processing is needed, otherwise an empty list\n    \"\"\"\n    if batch_commands:\n        putils.make_path(script_out_path)\n        if distribute.is_sbatch_available():\n            shell = distribute.sbatch\n        else:\n            shell = distribute.default_shell\n\n    api_db = self.job.api_db  # resources.wrapapi.api_database_factory()\n    self.full_models.make_path()\n    self.refined.make_path()\n    full_model_names = self.full_models.retrieve_names()\n    full_model_dir = self.full_models.location\n    # Identify the entities to refine and to model loops before proceeding\n    protein_data_to_loop_model = []\n    for data in metadata:\n        if not data.model_source:\n            logger.debug(f\"{self.preprocess_metadata_for_design.__name__}: Couldn't find the \"\n                         f\"ProteinMetadata.model_source for {data.entity_id}. Skipping loop model preprocessing\")\n            continue\n        # If data is here, it's model_source file should've been oriented...\n        if data.entity_id not in full_model_names:  # Assumes oriented_asu structure name is the same\n            protein_data_to_loop_model.append(data)\n\n    info_messages = []\n    if protein_data_to_loop_model:\n        logger.info(\"The following structures haven't been modeled for disorder: \"\n                    f'{\", \".join(sorted(set(protein.entity_id for protein in protein_data_to_loop_model)))}')\n        # Files found unloop_modeled, check to see if work should be done\n        if self.job.init.loop_model_input:  # is not None:\n        #     loop_model_input = self.job.init.loop_model_input\n        # else:  # Query user and set up commands to perform loop modeling on missing entities\n        #     print(f'If you plan on performing {flags.design}/{flags.predict_structure} with them, it is strongly '\n        #           f'encouraged that you build missing loops to avoid disordered region clashing/misalignment')\n        #     print('Would you like to model loops for these structures now?')\n        #     if boolean_choice():\n        #         loop_model_input = True\n        #     else:\n        #         print('To confirm, asymmetric units are going to be generated without modeling disordered loops. '\n        #               'Confirm with \"y\" to ensure this is what you want')\n        #         if boolean_choice():\n        #             loop_model_input = False\n        #         else:\n        #             loop_model_input = True\n        #\n        # if loop_model_input:\n            # Generate loop model commands\n            use_alphafold = True\n            if use_alphafold and self.job.gpu_available:\n                if batch_commands:\n                    # Write all commands to a file to perform in batches\n                    cmd = [*putils.program_command_tuple, flags.initialize_building_blocks,\n                           f'--{flags.loop_model_input}', flags.pdb_codes_args[-1]]\n                    commands = [cmd + [protein.entity_id, f'--symmetry', protein.symmetry_group]\n                                for idx, protein in enumerate(protein_data_to_loop_model)]\n\n                    loop_cmds_file = resources.distribute.write_commands(\n                        [subprocess.list2cmdline(cmd) for cmd in commands], out_path=script_out_path,\n                        name=f'{utils.starttime}-loop_model_entities',)\n                    loop_model_script = distribute.distribute(\n                        loop_cmds_file, flags.predict_structure, out_path=script_out_path,\n                        log_file=os.path.join(full_model_dir, 'loop_model.log'),\n                        max_jobs=int(len(commands)/2 + .5), number_of_commands=len(commands))\n                    loop_model_script_message = 'Once you are satisfied, run the following to distribute ' \\\n                                                f'loop-modeling jobs:\\n\\t{shell} {loop_model_script}'\n                    info_messages.append(loop_model_script_message)\n                    # This prevents refinement from trying as it will be called upon distribution of the script\n                    return info_messages\n                else:\n                    # # Hard code in parameters\n                    # model_type = 'monomer'\n                    relaxed = self.job.predict.models_to_relax is not None\n                    # Set up the various model_runners to supervise the prediction task for each sequence\n                    monomer_runners = \\\n                        resources.ml.set_up_model_runners(model_type='monomer', development=self.job.development)\n                    multimer_runners = \\\n                        resources.ml.set_up_model_runners(model_type='multimer', development=self.job.development)\n                    # I don't suppose I need to reinitialize these for different length inputs, but I'm sure I will\n\n                    # Predict each\n                    for idx, protein in enumerate(protein_data_to_loop_model):\n                        entity_name = protein.entity_id\n                        # .model_source should be a file containing an oriented, asymmetric version of the structure\n                        entity = Entity.from_file(protein.model_source, metadata=protein)\n\n                        # Using the protein.uniprot_entity.reference_sequence would be preferred, however, it should\n                        # be realigned to the structure.reference_sequence or .sequence in order to not have large\n                        # insertions well beyond the indicated structural domain\n                        # In a similar mechanism to load_evolutionary_profile(), these need to be combined...\n                        # Example:\n                        # for entity in protein.uniprot_entities:\n                        #     entity.reference_sequence\n\n                        # Remove tags from reference_sequence\n                        clean_reference_sequence = expression.remove_terminal_tags(entity.reference_sequence)\n                        logger.debug(f'Found the .reference_sequence:\\n{entity.reference_sequence}')\n                        logger.debug(f'Found the clean_reference_sequence:\\n{clean_reference_sequence}')\n                        source_gap_mutations = generate_mutations(clean_reference_sequence, entity.sequence,\n                                                                  zero_index=True, only_gaps=True)\n                        # Format the Pose to have the proper sequence to predict loops/disorder\n                        logger.debug(f'Found the source_gap_mutations: {source_gap_mutations}')\n                        for residue_index, mutation in source_gap_mutations.items():\n                            # residue_index is zero indexed\n                            new_aa_type = mutation['from']\n                            # What happens if Entity has resolved tag density?\n                            #  mutation_index: {'from': '-', 'to: LETTER}}\n                            if new_aa_type == '-':\n                                # This could be removed from the structure but that seems implicitly bad\n                                continue\n                            entity.insert_residue_type(residue_index, new_aa_type, chain_id=entity.chain_id)\n\n                        # If the msa features are present, the prediction should succeed with high probability...\n                        # Attach evolutionary info to the entity\n                        evolution_loaded, alignment_loaded = load_evolutionary_profile(api_db, entity)\n\n                        # After all sequence modifications, create the entity.assembly\n                        entity.make_oligomer(symmetry=protein.symmetry_group)\n                        if entity.number_of_symmetry_mates &gt; 1:\n                            af_symmetric = True\n                            model_runners = multimer_runners\n                            previous_position_coords = jnp.asarray(entity.assembly.alphafold_coords)\n                        else:\n                            af_symmetric = False\n                            model_runners = monomer_runners\n                            previous_position_coords = jnp.asarray(entity.alphafold_coords)\n                        # Don't get the msa (no_msa=True) if the alignment_loaded is missing (False)\n                        features = entity.get_alphafold_features(\n                            symmetric=af_symmetric, no_msa=not alignment_loaded, templates=True)\n                        # Put the entity oligomeric coordinates in as a prior to bias the prediction\n                        features['prev_pos'] = previous_position_coords\n                        # Run the prediction\n                        entity_structures, entity_scores = \\\n                            resources.ml.af_predict(features, model_runners,  # {**features, **template_features},\n                                                    gpu_relax=self.job.predict.use_gpu_relax,\n                                                    models_to_relax='best')  # self.job.predict.models_to_relax)\n                        if relaxed:\n                            structures_to_load = entity_structures.get('relaxed', [])\n                        else:\n                            structures_to_load = entity_structures.get('unrelaxed', [])\n\n                        pose_kwargs = dict(name=entity_name, entity_info=protein.entity_info,\n                                           symmetry=protein.symmetry_group)\n                        folded_entities = {\n                            model_name: Pose.from_pdb_lines(structure_.splitlines(), **pose_kwargs)\n                            for model_name, structure_ in structures_to_load.items()}\n                        if relaxed:  # Set b-factor data as relaxed get overwritten\n                            model_plddts = {model_name: scores['plddt'][:entity.number_of_residues]\n                                            for model_name, scores in entity_scores.items()}\n                            for model_name, entity_ in folded_entities.items():\n                                entity_.set_b_factor_data(model_plddts[model_name])\n                        # Check for the rmsd between the backbone of the provided Entity and\n                        # the Alphafold prediction\n                        # If the model were to be multimeric, then use this...\n                        # if multimer:\n                        #     entity_cb_coords = np.concatenate([mate.cb_coords for mate in entity.chains])\n                        #     Tod0 entity_backbone_and_cb_coords = entity.assembly.cb_coords\n\n                        # Only use the original indices to align\n                        new_indices = list(source_gap_mutations.keys())\n                        align_indices = [idx for idx in entity.residue_indices if idx not in new_indices]\n                        template_cb_coords = entity.cb_coords[align_indices]\n                        min_rmsd = float('inf')\n                        min_entity = None\n                        for af_model_name, entity_ in folded_entities.items():\n                            rmsd, rot, tx = structure.coords.superposition3d(\n                                template_cb_coords, entity_.cb_coords[align_indices])\n                            if rmsd &lt; min_rmsd:\n                                min_rmsd = rmsd\n                                # Move the Alphafold model into the Pose reference frame\n                                entity_.transform(rotation=rot, translation=tx)\n                                min_entity = entity_\n\n                        # Indicate that this ProteinMetadata has been processed for loop modeling\n                        protein.loop_modeled = True\n                        protein.refined = relaxed\n                        # Save the min_model asu (now aligned with entity, which was oriented prior)\n                        full_model_file = self.full_models.path_to(name=entity_name)\n                        min_entity.write(out_path=full_model_file)\n                        if relaxed:\n                            refined_path = self.refined.path_to(name=entity_name)\n                            shutil.copy(full_model_file, refined_path)\n            else:  # rosetta_loop_model\n                raise NotImplementedError(f\"Rosetta loop model hasn't been updated to use ProteinMetadata\")\n                flags_file = os.path.join(full_model_dir, 'loop_model_flags')\n                # if not os.path.exists(flags_file):\n                loop_model_flags = ['-remodel::save_top 0', '-run:chain A', '-remodel:num_trajectory 1']\n                #                   '-remodel:run_confirmation true', '-remodel:quick_and_dirty',\n                _flags = utils.rosetta.flags.copy() + loop_model_flags\n                # flags.extend(['-out:path:pdb %s' % full_model_dir, '-no_scorefile true'])\n                _flags.extend(['-no_scorefile true', '-no_nstruct_label true'])\n                # Generate 100 trial loops, 500 is typically sufficient\n                variables = [('script_nstruct', '100')]\n                _flags.append(f'-parser:script_vars {\" \".join(f\"{var}={val}\" for var, val in variables)}')\n                with open(flags_file, 'w') as f:\n                    f.write('%s\\n' % '\\n'.join(_flags))\n                loop_model_cmd = [f'@{flags_file}', '-parser:protocol',\n                                  os.path.join(putils.rosetta_scripts_dir, 'loop_model_ensemble.xml'),\n                                  '-parser:script_vars']\n                # Make all output paths and files for each loop ensemble\n                # logger.info('Preparing blueprint and loop files for structure:')\n                loop_model_cmds = []\n                sym_def_files = {}\n                for idx, protein_data in enumerate(protein_data_to_loop_model):\n                    if data.symmetry_group not in sym_def_files:\n                        sym_def_files[data.symmetry_group] = utils.SymEntry.sdf_lookup(data.symmetry_group)\n                    # Make a new directory for each structure\n                    structure_out_path = os.path.join(full_model_dir, protein_data.name)\n                    putils.make_path(structure_out_path)\n                    structure_ = Pose.from_file(protein_data.model_source)\n                    structure_.renumber_residues()\n                    structure_loop_file = structure_.make_loop_file(out_path=full_model_dir)\n                    if not structure_loop_file:  # No loops found, copy input file to the full model\n                        copy_cmd = ['scp', self.refined.path_to(protein_data.name),\n                                    self.full_models.path_to(protein_data.name)]\n                        loop_model_cmds.append(\n                            resources.distribute.write_script(\n                                subprocess.list2cmdline(copy_cmd), name=protein_data.name, out_path=full_model_dir))\n                        # Can't do this v as refined path doesn't exist yet\n                        # shutil.copy(self.refined.path_to(protein_data.name),\n                        #             self.full_models.path_to(protein_data.name))\n                        continue\n                    structure_blueprint = structure_.make_blueprint_file(out_path=full_model_dir)\n                    structure_cmd = utils.rosetta.script_cmd + loop_model_cmd \\\n                        + [f'blueprint={structure_blueprint}', f'loop_file={structure_loop_file}',\n                           '-in:file:s', self.refined.path_to(protein_data.name),\n                           '-out:path:pdb', structure_out_path] \\\n                        + [f'sdf={sym_def_files[protein_data.symmetry_group]}',\n                           f'symmetry={\"asymmetric\" if protein_data.symmetry_group == \"C1\" else \"make_point_group\"}']\n                    #     + (['-symmetry:symmetry_definition', sym_def_files[protein_data.symmetry]]\n                    #        if protein_data.symmetry != 'C1' else [])\n                    # Create a multimodel from all output loop models\n                    multimodel_cmd = ['python', putils.models_to_multimodel_exe, '-d', structure_loop_file,\n                                      '-o', os.path.join(full_model_dir, f'{protein_data.name}_ensemble.pdb')]\n                    # Copy the first model from output loop models to be the full model\n                    copy_cmd = ['scp', os.path.join(structure_out_path, f'{protein_data.name}_0001.pdb'),\n                                self.full_models.path_to(protein_data.name)]\n                    loop_model_cmds.append(\n                        resources.distribute.write_script(\n                            subprocess.list2cmdline(structure_cmd), name=protein_data.name, out_path=full_model_dir,\n                            additional=[subprocess.list2cmdline(multimodel_cmd),\n                                        subprocess.list2cmdline(copy_cmd)]))\n                if batch_commands:\n                    loop_cmds_file = \\\n                        resources.distribute.write_commands(\n                            loop_model_cmds, name=f'{utils.starttime}-loop_model_entities', out_path=full_model_dir)\n                    loop_model_script = \\\n                        distribute.distribute(loop_cmds_file, flags.refine, out_path=script_out_path,\n                                              log_file=os.path.join(full_model_dir, 'loop_model.log'),\n                                              max_jobs=int(len(loop_model_cmds)/2 + .5),\n                                              number_of_commands=len(loop_model_cmds))\n                    loop_model_script_message = 'Once you are satisfied, run the following to distribute ' \\\n                                                f'loop_modeling jobs:\\n\\t{shell} {loop_model_script}'\n                    info_messages.append(loop_model_script_message)\n                else:\n                    raise NotImplementedError(\"Currently, loop modeling can't be run in the shell. \"\n                                              'Implement this if you would like this feature')\n                # Todo this is sloppy as this doesn't necessarily indicate that work will be done (batch_command)\n                # Indicate that this ProteinMetadata has been processed\n                for protein in protein_data_to_refine:\n                    protein.loop_modeled = True\n        else:  # Indicate that this ProteinMetadata hasn't been processed\n            for protein in protein_data_to_loop_model:\n                protein.loop_modeled = False\n\n    refine_names = self.refined.retrieve_names()\n    refine_dir = self.refined.location\n    # Identify the entities to refine before proceeding\n    protein_data_to_refine = []\n    for data in metadata:\n        if not data.model_source:\n            logger.debug(f\"{self.preprocess_metadata_for_design.__name__}: Couldn't find the \"\n                         f\"ProteinMetadata.model_source for {data.entity_id}. Skipping refine preprocessing\")\n            continue\n        # If data is here, it's model_source file should've been oriented...\n        if data.entity_id not in refine_names:  # Assumes oriented_asu structure name is the same\n            protein_data_to_refine.append(data)\n\n    if protein_data_to_refine:\n        # Files found unrefined, check to see if work should be done\n        logger.info(\"The following structures haven't been refined: \"\n                    f'{\", \".join(sorted(set(protein.entity_id for protein in protein_data_to_refine)))}')\n        if self.job.init.refine_input:  # is not None:\n        #     refine_input = self.job.init.refine_input\n        # else:  # Query user and set up commands to perform refinement on missing entities\n        #     print(f'If you plan on performing {flags.design} using Rosetta, it is strongly encouraged that you '\n        #           f'perform initial refinement. You can also refine them later using the {flags.refine} module')\n        #     print('Would you like to refine them now?')\n        #     if boolean_choice():\n        #         refine_input = True\n        #     else:\n        #         print('To confirm, asymmetric units are going to be generated with input coordinates. Confirm '\n        #               'with \"y\" to ensure this is what you want')\n        #         if boolean_choice():\n        #             refine_input = False\n        #         else:\n        #             refine_input = True\n        #\n        # if refine_input:\n            if not sym_def_files:\n                sym_def_files = {}\n                for data in protein_data_to_refine:\n                    if data.symmetry_group not in sym_def_files:\n                        sym_def_files[data.symmetry_group] = utils.SymEntry.sdf_lookup(data.symmetry_group)\n            # Generate sbatch refine command\n            flags_file = os.path.join(refine_dir, 'refine_flags')\n            # if not os.path.exists(flags_file):\n            _flags = utils.rosetta.flags.copy() + utils.rosetta.relax_flags\n            _flags.extend([f'-out:path:pdb {refine_dir}', '-no_scorefile true'])\n            _flags.remove('-output_only_asymmetric_unit true')  # want full oligomers\n            variables = utils.rosetta.variables.copy()\n            variables.append(('dist', 0))  # Todo modify if not point groups used\n            _flags.append(f'-parser:script_vars {\" \".join(f\"{var}={val}\" for var, val in variables)}')\n\n            with open(flags_file, 'w') as f:\n                f.write('%s\\n' % '\\n'.join(_flags))\n\n            refine_cmd = [f'@{flags_file}', '-parser:protocol',\n                          os.path.join(putils.rosetta_scripts_dir, f'refine.xml')]\n            refine_cmds = [utils.rosetta.script_cmd + refine_cmd\n                           + ['-in:file:s', protein.model_source, '-parser:script_vars']\n                           + [f'sdf={sym_def_files[protein.symmetry_group]}',\n                              f'symmetry={\"asymmetric\" if protein.symmetry_group == \"C1\" else \"make_point_group\"}']\n                           for protein in protein_data_to_refine]\n            if batch_commands:\n                commands_file = \\\n                    resources.distribute.write_commands(\n                        [subprocess.list2cmdline(cmd) for cmd in refine_cmds], out_path=refine_dir,\n                        name=f'{utils.starttime}-refine_entities')\n                refine_script = \\\n                    distribute.distribute(commands_file, flags.refine, out_path=script_out_path,\n                                          log_file=os.path.join(refine_dir, f'{putils.refine}.log'),\n                                          max_jobs=int(len(refine_cmds)/2 + .5),\n                                          number_of_commands=len(refine_cmds))\n                multi_script_warning = \"\\n***Run this script AFTER completion of the loop modeling script***\\n\" \\\n                    if info_messages else \"\"\n                refine_script_message = f'Once you are satisfied, run the following to distribute refine jobs:' \\\n                                        f'{multi_script_warning}\\n\\t{shell} {refine_script}'\n                info_messages.append(refine_script_message)\n            else:\n                raise NotImplementedError(\"Currently, refinement can't be run in the shell. \"\n                                          'Implement this if you would like this feature')\n            # Todo this is sloppy as this doesn't necessarily indicate that this work will be done (batch_command)\n            # Indicate that this ProteinMetadata has been processed\n            for protein in protein_data_to_refine:\n                protein.refined = True\n        else:  # Indicate that this ProteinMetadata hasn't been processed\n            for protein in protein_data_to_refine:\n                protein.refined = False\n\n    return info_messages\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabaseFactory","title":"StructureDatabaseFactory","text":"<pre><code>StructureDatabaseFactory(**kwargs)\n</code></pre> <p>Return a StructureDatabase instance by calling the Factory instance with the StructureDatabase source name</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer to the named StructureDatabase</p> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def __init__(self, **kwargs):\n    self._database = None\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabaseFactory.__call__","title":"__call__","text":"<pre><code>__call__(source: str = None, sql: bool = False, **kwargs) -&gt; StructureDatabase\n</code></pre> <p>Return the specified StructureDatabase object singleton</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The StructureDatabase source path, or name if SQL database</p> </li> <li> <code>sql</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the StructureDatabase is a SQL database</p> </li> </ul> <p>Returns:     The instance of the specified StructureDatabase</p> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def __call__(self, source: str = None, sql: bool = False, **kwargs) -&gt; StructureDatabase:\n    \"\"\"Return the specified StructureDatabase object singleton\n\n    Args:\n        source: The StructureDatabase source path, or name if SQL database\n        sql: Whether the StructureDatabase is a SQL database\n    Returns:\n        The instance of the specified StructureDatabase\n    \"\"\"\n    if self._database:\n        return self._database\n    elif sql:\n        raise NotImplementedError('SQL set up has not been completed')\n    else:\n        pdbs = os.path.join(source, 'PDBs')  # Used to store downloaded PDB's\n        # stride directory\n        stride_dir = os.path.join(source, 'stride')\n        putils.make_path(stride_dir)\n        # pdbs subdirectories\n        orient_dir = os.path.join(pdbs, 'oriented')\n        orient_asu_dir = os.path.join(pdbs, 'oriented_asu')\n        refine_dir = os.path.join(pdbs, 'refined')\n        full_model_dir = os.path.join(pdbs, 'full_models')\n        putils.make_path(orient_dir)\n        putils.make_path(orient_asu_dir)\n        putils.make_path(refine_dir)\n        putils.make_path(full_model_dir)\n        logger.info(f'Initializing {StructureDatabase.__name__}({source})')\n\n        self._database = \\\n            StructureDatabase(pdbs, full_model_dir, orient_dir, orient_asu_dir, refine_dir, stride_dir, sql=None)\n\n    return self._database\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.StructureDatabaseFactory.get","title":"get","text":"<pre><code>get(source: str = None, **kwargs) -&gt; StructureDatabase\n</code></pre> <p>Return the specified Database object singleton</p> <p>Other Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>)         \u2013          <p>str = None - The StructureDatabase source path, or name if SQL database</p> </li> <li> <code>sql</code>         \u2013          <p>bool = False - Whether the StructureDatabase is a SQL database</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StructureDatabase</code>         \u2013          <p>The instance of the specified StructureDatabase</p> </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def get(self, source: str = None, **kwargs) -&gt; StructureDatabase:\n    \"\"\"Return the specified Database object singleton\n\n    Keyword Args:\n        source: str = None - The StructureDatabase source path, or name if SQL database\n        sql: bool = False - Whether the StructureDatabase is a SQL database\n\n    Returns:\n        The instance of the specified StructureDatabase\n    \"\"\"\n    return self.__call__(source, **kwargs)\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.fetch_pdb_file","title":"fetch_pdb_file","text":"<pre><code>fetch_pdb_file(pdb_code: str, asu: bool = True, location: AnyStr = putils.pdb_db, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Fetch PDB object from PDBdb or download from PDB server</p> <p>Parameters:</p> <ul> <li> <code>pdb_code</code>             (<code>str</code>)         \u2013          <p>The PDB ID/code. If the biological assembly is desired, supply 1ABC_1 where '_1' is assembly ID</p> </li> <li> <code>asu</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to fetch the ASU</p> </li> <li> <code>location</code>             (<code>AnyStr</code>, default:                 <code>pdb_db</code> )         \u2013          <p>Location of a local PDB mirror if one is linked on disk</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>assembly</code>         \u2013          <p>int = None - Location of a local PDB mirror if one is linked on disk</p> </li> <li> <code>out_dir</code>         \u2013          <p>AnyStr = os.getcwd() - The location to save retrieved files if fetched from PDB</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The path to the file if located successfully</p> </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def fetch_pdb_file(pdb_code: str, asu: bool = True, location: AnyStr = putils.pdb_db, **kwargs) -&gt; AnyStr | None:\n    #                assembly: int = 1, out_dir: AnyStr = os.getcwd()\n    \"\"\"Fetch PDB object from PDBdb or download from PDB server\n\n    Args:\n        pdb_code: The PDB ID/code. If the biological assembly is desired, supply 1ABC_1 where '_1' is assembly ID\n        asu: Whether to fetch the ASU\n        location: Location of a local PDB mirror if one is linked on disk\n\n    Keyword Args:\n        assembly: int = None - Location of a local PDB mirror if one is linked on disk\n        out_dir: AnyStr = os.getcwd() - The location to save retrieved files if fetched from PDB\n\n    Returns:\n        The path to the file if located successfully\n    \"\"\"\n    # if location == putils.pdb_db and asu:\n    if os.path.exists(location) and asu:\n        file_path = os.path.join(location, f'pdb{pdb_code.lower()}.ent')\n        # ^ Cassini format\n        def get_pdb(*args, **_kwargs): return sorted(glob(file_path))\n        # v KM local pdb and escher PDB mirror\n        # file_path = os.path.join(location, pdb_code[1:3], f'{pdb_code.lower()}.pdb')\n        logger.debug(f'Searching for PDB file at \"{file_path}\"')\n    else:\n        get_pdb = _fetch_pdb_from_api\n\n    # The matching file is the first (should only be one)\n    pdb_file: list[str] = get_pdb(pdb_code, asu=asu, location=location, **kwargs)\n    if not pdb_file:  # Empty list\n        logger.warning(f'No matching file found for PDB: {pdb_code}')\n        return None\n    else:  # Should only find one file, therefore, return the first\n        # if len(pdb_file) != 1:\n        #     logger.info(f'Found multiple file for EntryID {pdb_code}. Choosing the first: '\n        #                 f'{\", \".join(pdb_file)}')\n        return pdb_file[0]\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.download_structures","title":"download_structures","text":"<pre><code>download_structures(structure_identifiers: Iterable[str], out_dir: str = os.getcwd(), asu: bool = False) -&gt; list[Structure]\n</code></pre> <p>Retrieves/saves structure model files, given EntryIDs/EntityIDs, then returns a Structure for each identifier</p> <p>Defaults to fetching the biological assembly file, prioritizing the assemblies as predicted very high/high from QSBio, then using the first assembly if QSBio is missing</p> <p>Parameters:</p> <ul> <li> <code>structure_identifiers</code>             (<code>Iterable[str]</code>)         \u2013          <p>The names of all entity_ids requiring orientation</p> </li> <li> <code>out_dir</code>             (<code>str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The directory to write downloaded files to</p> </li> <li> <code>asu</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to get the asymmetric unit from the PDB instead of the biological assembly</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Structure]</code>         \u2013          <p>The requested Pose/Entity instances</p> </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def download_structures(\n    structure_identifiers: Iterable[str], out_dir: str = os.getcwd(), asu: bool = False\n) -&gt; list[Structure]:\n    \"\"\"Retrieves/saves structure model files, given EntryIDs/EntityIDs, then returns a Structure for each identifier\n\n    Defaults to fetching the biological assembly file, prioritizing the assemblies as predicted very high/high from\n    QSBio, then using the first assembly if QSBio is missing\n\n    Args:\n        structure_identifiers: The names of all entity_ids requiring orientation\n        out_dir: The directory to write downloaded files to\n        asu: Whether to get the asymmetric unit from the PDB instead of the biological assembly\n\n    Returns:\n        The requested Pose/Entity instances\n    \"\"\"\n    all_structures = []\n    for structure_identifier in structure_identifiers:\n        # Retrieve the proper files using PDB ID's\n        structure_identifier_ = structure_identifier\n        assembly_integer = entity_integer = None\n        splitter_iter = iter('_-')  # (entity, assembly))\n        idx = count(-1)\n        extra = None\n        while len(structure_identifier_) != 4:\n            try:  # To parse the name using standard PDB API entry ID's\n                structure_identifier_, *extra = structure_identifier_.split(next(splitter_iter))\n            except StopIteration:\n                # We didn't find an EntryID in structure_identifier_ from splitting typical PDB formatted strings\n                logger.debug(f\"The name '{structure_identifier}' can't be coerced to PDB API format\")\n                continue\n            else:\n                next(idx)\n        # Set the index to the index that was stopped at\n        idx = next(idx)\n\n        if extra:  # Extra not None or []\n            # Todo, use of elif means can't have 1ABC_1.pdb2\n            # Try to parse any found extra to an integer denoting entity or assembly ID\n            integer, *non_sense = extra\n            if integer.isdigit() and not non_sense:\n                integer = int(integer)\n                entry = structure_identifier_\n                if idx == 0:  # Entity integer, such as 1ABC_1.pdb\n                    entity_integer = integer\n                    # structure_identifier_ = f'{structure_identifier_}_{integer}'\n                    logger.debug(f'Fetching EntityID {entry}_{entity_integer} from PDB')\n                else:  # This is an assembly or unknown conjugation\n                    if idx == 1:  # This is an assembly integer, such as 1ABC-1.pdb\n                        entry = structure_identifier_\n                        assembly_integer = integer\n                        logger.debug(f'Fetching AssemblyID {entry}-{assembly_integer} from PDB')\n                    else:\n                        logger.critical(\"This logic happened and wasn't expected\")\n            else:  # This isn't an integer or there are extra characters\n                logger.info(f\"The name '{structure_identifier}' can't be coerced to PDB format\")\n                continue\n        elif extra is None:  # Nothing extra as it was correct length to begin with, just query entry\n            entry = structure_identifier_\n        else:\n            raise RuntimeError(\n                f\"This logic wasn't expected and shouldn't be allowed to persist: \"\n                f'structure_identifier={structure_identifier}, structure_identifier_={structure_identifier_}, '\n                f'extra={extra}, idx={idx}')\n\n        if assembly_integer is None:\n            assembly_integer = query_qs_bio(entry)\n        # Get the specified file_path for the assembly state of interest\n        file_path = fetch_pdb_file(entry, assembly=assembly_integer, asu=asu, out_dir=out_dir)\n\n        if file_path is None:\n            logger.error(f\"Couldn't locate the identifier '{structure_identifier}'. There may have been an issue \"\n                         'retrieving it from the PDB')\n            continue\n\n        # Remove any PDB Database mirror specific naming from fetch_pdb_file such as pdb1ABC.ent\n        file_name = os.path.splitext(os.path.basename(file_path))[0].replace('pdb', '')\n        pose = Pose.from_file(file_path, name=file_name)\n        if entity_integer is not None:\n            # Replace the Pose from fetched file with the Entity\n            # structure_identifier is formatted the exact same as the desired EntityID\n            entity = pose.get_entity(structure_identifier)\n            if entity:\n                # Set the file_path attribute on the Entity\n                file_path = pose.file_path\n                pose = entity\n                pose.tmp_file_path = file_path\n            else:  # Couldn't find the specified EntityID\n                logger.warning(f\"For {structure_identifier}, couldn't locate the specified {Entity.__name__} \"\n                               f\"'{structure_identifier}'. The available {Entity.__name__} instances are \"\n                               f'{\", \".join(entity.name for entity in pose.entities)}')\n                continue\n\n        all_structures.append(pose)\n\n    return all_structures\n</code></pre>"},{"location":"reference/resources/structure_db/#resources.structure_db.query_qs_bio","title":"query_qs_bio","text":"<pre><code>query_qs_bio(entry_id: str) -&gt; int\n</code></pre> <p>Retrieve the first matching Very High/High confidence QSBio assembly from a PDB EntryID</p> <p>Parameters:</p> <ul> <li> <code>entry_id</code>             (<code>str</code>)         \u2013          <p>The 4 character PDB EntryID (code) to query</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The integer of the corresponding PDB Assembly ID according to QSBio</p> </li> </ul> Source code in <code>symdesign/resources/structure_db.py</code> <pre><code>def query_qs_bio(entry_id: str) -&gt; int:\n    \"\"\"Retrieve the first matching Very High/High confidence QSBio assembly from a PDB EntryID\n\n    Args:\n        entry_id: The 4 character PDB EntryID (code) to query\n\n    Returns:\n        The integer of the corresponding PDB Assembly ID according to QSBio\n    \"\"\"\n    biological_assemblies = resources.query.pdb.qsbio_confirmed.get(entry_id.lower())\n    if biological_assemblies:\n        # Get the first assembly in matching oligomers\n        if len(biological_assemblies) != 1:\n            logger.info(f'Found multiple biological assemblies for EntryID {entry_id}. Choosing the first: '\n                        f'{\", \".join(map(str, biological_assemblies))}')\n        assembly = biological_assemblies[0]\n    else:\n        assembly = 1\n        logger.warning(f'No QSBio confirmed biological assembly for EntryID {entry_id}. Using the default assembly 1')\n    return assembly\n</code></pre>"},{"location":"reference/resources/wrapapi/","title":"wrapapi","text":""},{"location":"reference/resources/wrapapi/#resources.wrapapi.api_database_factory","title":"api_database_factory  <code>module-attribute</code>","text":"<pre><code>api_database_factory: Annotated[APIDatabaseFactory, 'Calling this factory method returns the single instance of the Database class located at the \"source\" keyword argument'] = APIDatabaseFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the Database class located at the \"source\" keyword  argument</p>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.APIDatabase","title":"APIDatabase","text":"<pre><code>APIDatabase(sequences: AnyStr | Path = None, hhblits_profiles: AnyStr | Path = None, pdb: AnyStr | Path = None, uniprot: AnyStr | Path = None, **kwargs)\n</code></pre> <p>             Bases: <code>Database</code></p> <p>A Database which stores general API queries</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path the data stored for these particular queries</p> </li> <li> <code>hhblits_profiles</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path the data stored for these particular queries</p> </li> <li> <code>pdb</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path the data stored for these particular queries</p> </li> <li> <code>uniprot</code>             (<code>AnyStr | Path</code>, default:                 <code>None</code> )         \u2013          <p>The path the data stored for these particular queries</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def __init__(self, sequences: AnyStr | Path = None,\n             hhblits_profiles: AnyStr | Path = None, pdb: AnyStr | Path = None,\n             uniprot: AnyStr | Path = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        sequences: The path the data stored for these particular queries\n        hhblits_profiles: The path the data stored for these particular queries\n        pdb: The path the data stored for these particular queries\n        uniprot: The path the data stored for these particular queries\n        **kwargs:\n    \"\"\"\n    # passed to Database\n    # sql: sqlite = None, log: Logger = logger\n    super().__init__(**kwargs)  # Database\n\n    self.sequences = DataStore(location=sequences, extension='.fasta', sql=self.sql, log=self.log,\n                               load_file=read_fasta_file, save_file=write_sequence_to_fasta)\n    # elif extension == '.fasta' and msa:  # Todo if msa is in fasta format\n    #  load_file = MultipleSequenceAlignment.from_fasta\n    self.alignments = DataStore(location=hhblits_profiles, extension='.sto', sql=self.sql, log=self.log,\n                                load_file=MultipleSequenceAlignment.from_stockholm)\n    # if extension == '.pssm':  # Todo for psiblast\n    #  load_file = parse_pssm\n    self.hhblits_profiles = DataStore(location=hhblits_profiles, extension='.hmm', sql=self.sql, log=self.log,\n                                      load_file=parse_hhblits_pssm)\n    self.pdb = PDBDataStore(location=pdb, extension='.json', sql=self.sql, log=self.log)\n    self.uniprot = UniProtDataStore(location=uniprot, extension='.json', sql=self.sql, log=self.log)\n    # self.bmdca_fields = \\\n    #     DataStore(location=hhblits_profiles, extension='_bmDCA%sparameters_h_final.bin' % os.sep,\n    #     sql=self.sql, log=self.log)\n    #  elif extension == f'_bmDCA{os.sep}parameters_h_final.bin':\n    #      self.load_file = bmdca.load_fields\n    #      self.save_file = not_implemented\n    # self.bmdca_couplings = \\\n    #     DataStore(location=hhblits_profiles, extension='_bmDCA%sparameters_J_final.bin' % os.sep,\n    #     sql=self.sql, log=self.log)\n    #  elif extension == f'_bmDCA{os.sep}sparameters_J_final.bin':\n    #      self.load_file = bmdca.load_couplings\n    #      self.save_file = not_implemented\n\n    self.sources = [self.sequences, self.alignments, self.hhblits_profiles, self.pdb, self.uniprot]\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.APIDatabaseFactory","title":"APIDatabaseFactory","text":"<pre><code>APIDatabaseFactory(**kwargs)\n</code></pre> <p>Return a APIDatabase instance by calling the Factory instance with the APIDatabase source name</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer to the named APIDatabase</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def __init__(self, **kwargs):\n    self._database = None\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.APIDatabaseFactory.__call__","title":"__call__","text":"<pre><code>__call__(source: str = os.path.join(os.getcwd(), f'{putils.program_name}{putils.data.title()}'), sql: bool = False, **kwargs) -&gt; APIDatabase\n</code></pre> <p>Return the specified APIDatabase object singleton</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>join(getcwd(), f'{program_name}{title()}')</code> )         \u2013          <p>The APIDatabase source path, or name if SQL database</p> </li> <li> <code>sql</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the APIDatabase is a SQL database</p> </li> </ul> <p>Returns:     The instance of the specified Database</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def __call__(self, source: str = os.path.join(os.getcwd(), f'{putils.program_name}{putils.data.title()}'),\n             sql: bool = False, **kwargs) -&gt; APIDatabase:\n    \"\"\"Return the specified APIDatabase object singleton\n\n    Args:\n        source: The APIDatabase source path, or name if SQL database\n        sql: Whether the APIDatabase is a SQL database\n    Returns:\n        The instance of the specified Database\n    \"\"\"\n    # Todo potentially configure, however, we really only want a single database\n    # database = self._databases.get(source)\n    # if database:\n    #     return database\n    if self._database:\n        return self._database\n    elif sql:\n        raise NotImplementedError('SQL set up has not been completed!')\n    else:\n        sequence_info_dir = os.path.join(source, 'SequenceInfo')\n        external_db = os.path.join(source, 'ExternalDatabases')\n        # sequence_info subdirectories\n        sequences = os.path.join(sequence_info_dir, 'sequences')\n        profiles = os.path.join(sequence_info_dir, 'profiles')\n        putils.make_path(sequences)\n        putils.make_path(profiles)\n        # external database subdirectories\n        pdb = os.path.join(external_db, 'pdb')\n        putils.make_path(pdb)\n        uniprot = os.path.join(external_db, 'uniprot')\n        putils.make_path(uniprot)\n        # self._databases[source] = APIDatabase(sequences, profiles, pdb, uniprot, sql=None)\n        self._database = APIDatabase(sequences, profiles, pdb, uniprot, sql=None)\n\n    # return self._databases[source]\n    return self._database\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.APIDatabaseFactory.get","title":"get","text":"<pre><code>get(**kwargs) -&gt; APIDatabase\n</code></pre> <p>Return the specified APIDatabase object singleton</p> <p>Other Parameters:</p> <ul> <li> <code>source</code>         \u2013          <p>str = 'current_working_directory/Data' - The APIDatabase source path, or name if SQL database</p> </li> <li> <code>sql</code>         \u2013          <p>bool = False - Whether the Database is a SQL database</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>APIDatabase</code>         \u2013          <p>The instance of the specified Database</p> </li> </ul> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def get(self, **kwargs) -&gt; APIDatabase:\n    \"\"\"Return the specified APIDatabase object singleton\n\n    Keyword Args:\n        source: str = 'current_working_directory/Data' - The APIDatabase source path, or name if SQL database\n        sql: bool = False - Whether the Database is a SQL database\n\n    Returns:\n        The instance of the specified Database\n    \"\"\"\n    return self.__call__(**kwargs)\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.PDBDataStore","title":"PDBDataStore","text":"<pre><code>PDBDataStore(location: str = None, extension: str = '.json', sql=None, log: Logger = logger)\n</code></pre> <p>             Bases: <code>DataStore</code></p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def __init__(self, location: str = None, extension: str = '.json', sql=None, log: logging.Logger = logger):\n    super().__init__(location=location, extension=extension, sql=sql, log=log)\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.PDBDataStore.entity_thermophilicity","title":"entity_thermophilicity","text":"<pre><code>entity_thermophilicity(name: str = None, **kwargs) -&gt; float\n</code></pre> <p>Return the extent to which the EntityID in question is thermophilic</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The EntityID</p> </li> </ul> <p>Returns:     Value ranging from 0-1 where 1 is completely thermophilic</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def entity_thermophilicity(self, name: str = None, **kwargs) -&gt; float:  # bool:\n    \"\"\"Return the extent to which the EntityID in question is thermophilic\n\n    Args:\n        name: The EntityID\n    Returns:\n        Value ranging from 0-1 where 1 is completely thermophilic\n    \"\"\"\n    # Todo make possible for retrieve_entry_data(name=name)\n    data = self.retrieve_entity_data(name=name)\n    if data is None:\n        return False\n    return thermophilicity_from_entity_json(data)\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.PDBDataStore.retrieve_entity_data","title":"retrieve_entity_data","text":"<pre><code>retrieve_entity_data(name: str = None, **kwargs) -&gt; dict | None\n</code></pre> <p>Return data requested by PDB EntityID. If in the Database, load, otherwise, query the PDB API and store</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:     If data is available, the JSON object from PDB API will be returned, else None</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def retrieve_entity_data(self, name: str = None, **kwargs) -&gt; dict | None:\n    \"\"\"Return data requested by PDB EntityID. If in the Database, load, otherwise, query the PDB API and store\n\n    Args:\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n    Returns:\n        If data is available, the JSON object from PDB API will be returned, else None\n    \"\"\"\n    data = super().retrieve_data(name=name)\n    if not data:\n        request = query_entity_id(entity_id=name)\n        if not request:\n            logger.warning(f'PDB API found no matching results for {name}')\n        else:\n            data = request.json()\n            self.store_data(data, name=name)\n\n    return data\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.PDBDataStore.retrieve_assembly_data","title":"retrieve_assembly_data","text":"<pre><code>retrieve_assembly_data(name: str = None, **kwargs) -&gt; dict | None\n</code></pre> <p>Return data requested by PDB AssemblyID. If in the Database, load, otherwise, query the PDB API and store</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:     If data is available, the JSON object from PDB API will be returned, else None</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def retrieve_assembly_data(self, name: str = None, **kwargs) -&gt; dict | None:\n    \"\"\"Return data requested by PDB AssemblyID. If in the Database, load, otherwise, query the PDB API and store\n\n    Args:\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n    Returns:\n        If data is available, the JSON object from PDB API will be returned, else None\n    \"\"\"\n    data = super().retrieve_data(name=name)\n    if not data:\n        request = query_assembly_id(assembly_id=name)\n        if not request:\n            logger.warning(f'PDB API found no matching results for {name}')\n        else:\n            data = request.json()\n            self.store_data(data, name=name)\n\n    return data\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.PDBDataStore.retrieve_data","title":"retrieve_data","text":"<pre><code>retrieve_data(entry: str = None, assembly_id: str = None, assembly_integer: int | str = None, entity_id: str = None, entity_integer: int | str = None, chain: str = None, **kwargs) -&gt; dict | list[list[str]] | None\n</code></pre> <p>Return data requested by PDB identifier. Loads into the Database or queries the PDB API</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>assembly_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The AssemblyID to query with format (1ABC-1)</p> </li> <li> <code>assembly_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The particular assembly integer to query. Must include entry as well</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> <li> <code>entity_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>chain</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest</p> </li> </ul> <p>Returns:     If the data is available, the object requested will be returned, else None     The possible return formats include:     If entry     {'entity':         {'EntityID':             {'chains': ['A', 'B', ...],              'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},              'reference_sequence': 'MSLEHHHHHH...',              'thermophilicity': 1.0},          ...}      'method': xray,      'res': resolution,      'struct': {'space': space_group, 'a_b_c': (a, b, c), 'ang_a_b_c': (ang_a, ang_b, ang_c)}      }     If entity_id OR entry AND entity_integer     {'EntityID':         {'chains': ['A', 'B', ...],          'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},          'reference_sequence': 'MSLEHHHHHH...',          'thermophilicity': 1.0},      ...}     If assembly_id OR entry AND assembly_integer     [['A', 'A', 'A', ...], ...]</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def retrieve_data(self, entry: str = None, assembly_id: str = None, assembly_integer: int | str = None,\n                  entity_id: str = None, entity_integer: int | str = None, chain: str = None, **kwargs) -&gt; \\\n        dict | list[list[str]] | None:\n    \"\"\"Return data requested by PDB identifier. Loads into the Database or queries the PDB API\n\n    Args:\n        entry: The 4 character PDB EntryID of interest\n        assembly_id: The AssemblyID to query with format (1ABC-1)\n        assembly_integer: The particular assembly integer to query. Must include entry as well\n        entity_id: The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n        entity_integer: The entity integer from the EntryID of interest\n        chain: The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest\n    Returns:\n        If the data is available, the object requested will be returned, else None\n        The possible return formats include:\n        If entry\n        {'entity':\n            {'EntityID':\n                {'chains': ['A', 'B', ...],\n                 'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},\n                 'reference_sequence': 'MSLEHHHHHH...',\n                 'thermophilicity': 1.0},\n             ...}\n         'method': xray,\n         'res': resolution,\n         'struct': {'space': space_group, 'a_b_c': (a, b, c), 'ang_a_b_c': (ang_a, ang_b, ang_c)}\n         }\n        If entity_id OR entry AND entity_integer\n        {'EntityID':\n            {'chains': ['A', 'B', ...],\n             'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},\n             'reference_sequence': 'MSLEHHHHHH...',\n             'thermophilicity': 1.0},\n         ...}\n        If assembly_id OR entry AND assembly_integer\n        [['A', 'A', 'A', ...], ...]\n    \"\"\"\n    if entry is not None:\n        if len(entry) == 4:\n            if entity_integer is not None:\n                # logger.debug(f'Querying PDB API with {entry}_{entity_integer}')\n                # data = self.entity_api.retrieve_data(name=f'{entry}_{entity_integer}')\n                # return parse_entities_json([self.entity_api.retrieve_data(name=f'{entry}_{entity_integer}')])\n                return parse_entities_json([self.retrieve_entity_data(name=f'{entry}_{entity_integer}')])\n            elif assembly_integer is not None:\n                # logger.debug(f'Querying PDB API with {entry}-{assembly_integer}')\n                # data = self.assembly_api.retrieve_data(name=f'{entry}_{assembly_integer}')\n                # return parse_assembly_json(self.assembly_api.retrieve_data(name=f'{entry}-{assembly_integer}'))\n                return parse_assembly_json(self.retrieve_assembly_data(name=f'{entry}-{assembly_integer}'))\n            else:\n                # logger.debug(f'Querying PDB API with {entry}')\n                # Perform the normal DataStore routine with super(), however, finish with API call if no data found\n                data = super().retrieve_data(name=entry)\n                if not data:\n                    entry_request = query_entry_id(entry)\n                    if not entry_request:\n                        logger.warning(f'PDB API found no matching results for {entry}')\n                        return None\n                    else:\n                        data = entry_request.json()\n                        # setattr(self, entry, data)\n                        self.store_data(data, name=entry)\n\n                data = dict(entity=parse_entities_json([self.retrieve_entity_data(name=f'{entry}_{integer}')\n                                                        for integer in range(1, int(data['rcsb_entry_info']\n                                                                                    ['polymer_entity_count']) + 1)\n                                                        ]),\n                            **parse_entry_json(data))\n                if chain is not None:\n                    integer = None\n                    for entity_idx, chains in data.get('entity').items():\n                        if chain in chains:\n                            integer = entity_idx\n                            break\n                    if integer:\n                        # logger.debug(f'Querying PDB API with {entry}_{integer}')\n                        return self.retrieve_entity_data(name=f'{entry}_{integer}')\n                    else:\n                        raise KeyError(f'No chain \"{chain}\" found in PDB ID {entry}. Possible chains '\n                                       f'{\", \".join(ch for chns in data.get(\"entity\", {}).items() for ch in chns)}')\n                else:  # Provide the formatted PDB API Entry ID information\n                    return data\n        else:\n            logger.debug(f\"EntryID '{entry}' isn't the required format and will not be found with the PDB API\")\n    elif assembly_id is not None:\n        try:\n            entry, assembly_integer, *extra = assembly_id.split('-')\n        except ValueError:  # Not enough values to unpack\n            pass\n        else:\n            if not extra and len(entry) == 4:\n                # logger.debug(f'Querying PDB API with {entry}-{assembly_integer}')\n                return parse_assembly_json(self.retrieve_assembly_data(name=f'{entry}-{assembly_integer}'))\n\n        logger.debug(f\"AssemblyID '{assembly_id}' isn't the required format and will not be found with the PDB API\")\n    elif entity_id is not None:\n        try:\n            entry, entity_integer, *extra = entity_id.split('_')\n        except ValueError:  # Not enough values to unpack\n            pass\n        else:\n            if not extra and len(entry) == 4:\n                # logger.debug(f'Querying PDB API with {entry}_{entity_integer}')\n                return parse_entities_json([self.retrieve_entity_data(name=f'{entry}_{entity_integer}')])\n\n        logger.debug(f\"EntityID '{entity_id}' isn't the required format and will not be found with the PDB API\")\n    else:  # This could've been passed as name=. This case would need to be solved with some parsing of the splitter\n        raise RuntimeError(f'No valid arguments passed to {self.retrieve_data.__name__}. Valid arguments include: '\n                           f'entry, assembly_id, assembly_integer, entity_id, entity_integer, chain')\n\n    return None\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtDataStore","title":"UniProtDataStore","text":"<pre><code>UniProtDataStore(location: str = None, extension: str = '.json', sql=None, log: Logger = logger)\n</code></pre> <p>             Bases: <code>DataStore</code></p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def __init__(self, location: str = None, extension: str = '.json', sql=None, log: logging.Logger = logger):\n    super().__init__(location=location, extension=extension, sql=sql, log=log)\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtDataStore.retrieve_data","title":"retrieve_data","text":"<pre><code>retrieve_data(name: str = None, **kwargs) -&gt; dict | None\n</code></pre> <p>Return data requested by UniProtID. Loads into the Database or queries the UniProt API</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the data to be retrieved. Will be found with location and extension attributes</p> </li> </ul> <p>Returns:     If the data is available, the object requested will be returned, else None</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def retrieve_data(self, name: str = None, **kwargs) -&gt; dict | None:\n    \"\"\"Return data requested by UniProtID. Loads into the Database or queries the UniProt API\n\n    Args:\n        name: The name of the data to be retrieved. Will be found with location and extension attributes\n    Returns:\n        If the data is available, the object requested will be returned, else None\n    \"\"\"\n    data = super().retrieve_data(name=name)\n    if not data:\n        response = query_uniprot(uniprot_id=name)\n        if not response:\n            logger.warning(f'UniprotKB API found no matching results for {name}')\n        else:\n            data = response.json()\n            self.store_data(data, name=name)\n\n    return data\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtDataStore.thermophilicity","title":"thermophilicity","text":"<pre><code>thermophilicity(uniprot_id: str) -&gt; float\n</code></pre> <p>Query if a UniProtID is thermophilic</p> <p>Parameters:</p> <ul> <li> <code>uniprot_id</code>             (<code>str</code>)         \u2013          <p>The formatted UniProtID which consists of either a 6 or 10 character code</p> </li> </ul> <p>Returns:     1 if the UniProtID of interest is a thermophilic organism according to taxonomic classification, else 0</p> Source code in <code>symdesign/resources/wrapapi.py</code> <pre><code>def thermophilicity(self, uniprot_id: str) -&gt; float:\n    \"\"\"Query if a UniProtID is thermophilic\n\n    Args:\n        uniprot_id: The formatted UniProtID which consists of either a 6 or 10 character code\n    Returns:\n        1 if the UniProtID of interest is a thermophilic organism according to taxonomic classification, else 0\n    \"\"\"\n    data = self.retrieve_data(name=uniprot_id)\n\n    # Exact - parsing the taxonomic ID and cross-reference\n    taxonomic_id = int(data.get('organism', {}).get('taxonId', -1))\n    if taxonomic_id in thermophilic_taxonomy_ids:\n        return 1.0\n\n    # # Coarse - parsing the taxonomy for 'thermo'\n    # for element in data.get('organism', {}).get('lineage', []):\n    #     if 'thermo' in element.lower():\n    #         return 1  # True\n\n    return 0.0  # False\n</code></pre>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtEntity","title":"UniProtEntity","text":"<p>             Bases: <code>Base</code></p>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtEntity.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id = Column(String(uniprot_accession_length), primary_key=True, autoincrement=False)\n</code></pre> <p>The UniProtID</p>"},{"location":"reference/resources/wrapapi/#resources.wrapapi.UniProtEntity.reference_sequence","title":"reference_sequence  <code>property</code>","text":"<pre><code>reference_sequence: str\n</code></pre> <p>Get the sequence from the UniProtID</p>"},{"location":"reference/resources/query/","title":"query","text":""},{"location":"reference/resources/query/pdb/","title":"pdb","text":""},{"location":"reference/resources/query/pdb/#resources.query.pdb.GB","title":"GB  <code>module-attribute</code>","text":"<pre><code>GB = 'GenBank'\n</code></pre> <p>The module level identifier for a GenBankID</p>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.NOR","title":"NOR  <code>module-attribute</code>","text":"<pre><code>NOR = 'Norine'\n</code></pre> <p>The module level identifier for a NorineID</p>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.UKB","title":"UKB  <code>module-attribute</code>","text":"<pre><code>UKB = 'UniProt'\n</code></pre> <p>The module level identifier for a UniProtID</p>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.qsbio_confirmed","title":"qsbio_confirmed  <code>module-attribute</code>","text":"<pre><code>qsbio_confirmed: Annotated[dict[str, list[int]], \"PDB EntryID (lowercase) mapped to biological assembly numbers for ID's with QSBio confidence as high or very high\"] = unpickle(qs_bio)\n</code></pre> <p>PDB EntryID (lowercase) mapped to biological assembly numbers for ID's with QSBio confidence as high or very high</p>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.retrieve_entity_id_by_sequence","title":"retrieve_entity_id_by_sequence","text":"<pre><code>retrieve_entity_id_by_sequence(sequence: str) -&gt; str | None\n</code></pre> <p>From a given sequence, retrieve the top matching Entity ID from the PDB API</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence used to query for the EntityID</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | None</code>         \u2013          <p>'1ABC_1'</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def retrieve_entity_id_by_sequence(sequence: str) -&gt; str | None:\n    \"\"\"From a given sequence, retrieve the top matching Entity ID from the PDB API\n\n    Args:\n        sequence: The sequence used to query for the EntityID\n\n    Returns:\n        '1ABC_1'\n    \"\"\"\n    matching_entities = find_matching_entities_by_sequence(sequence, all_matching=False)\n    if matching_entities:\n        logger.debug(f'Sequence search found the matching EntityIDs: {\", \".join(matching_entities)}')\n        return matching_entities[0]\n    else:\n        return None\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.find_matching_entities_by_sequence","title":"find_matching_entities_by_sequence","text":"<pre><code>find_matching_entities_by_sequence(sequence: str = None, return_id: return_types_literal = 'polymer_entity', **kwargs) -&gt; list[str] | None\n</code></pre> <p>Search the PDB for matching IDs given a sequence and a return_type. Pass all_matching=False to retrieve the top 10 IDs, otherwise return all IDs</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The sequence used to query for EntityID's</p> </li> <li> <code>return_id</code>             (<code>return_types_literal</code>, default:                 <code>'polymer_entity'</code> )         \u2013          <p>The type of value to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str] | None</code>         \u2013          <p>The EntityID's matching the sequence</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def find_matching_entities_by_sequence(sequence: str = None, return_id: return_types_literal = 'polymer_entity',\n                                       **kwargs) -&gt; list[str] | None:\n    \"\"\"Search the PDB for matching IDs given a sequence and a return_type. Pass all_matching=False to retrieve the top\n    10 IDs, otherwise return all IDs\n\n    Args:\n        sequence: The sequence used to query for EntityID's\n        return_id: The type of value to return\n\n    Returns:\n        The EntityID's matching the sequence\n    \"\"\"\n    if return_id not in return_type_args:\n        raise KeyError(\n            f\"The specified return_id '{return_id}' isn't supported. Allowed values: {', '.join(return_type_args)}\")\n    logger.debug(f'Using the default sequence similarity parameters: '\n                 f'{\", \".join(f\"{k}: {v}\" for k, v in default_sequence_values.items())}')\n    sequence_query = format_terminal_group(service='sequence', sequence=sequence)\n    sequence_query_results = query_pdb(\n        generate_query(sequence_query, return_id=return_id, cluster_uniprot=True, **kwargs))\n    if sequence_query_results:\n        return parse_pdb_response_for_ids(sequence_query_results)\n    else:\n        logger.warning(f\"Sequence wasn't found by the PDB API:\\n{sequence}\")\n        return None  # [None]\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.parse_pdb_response_for_ids","title":"parse_pdb_response_for_ids","text":"<pre><code>parse_pdb_response_for_ids(response: dict[str, dict[str, str]], groups: bool = False) -&gt; list[str]\n</code></pre> <p>Parse JSON PDB API returns for identifiers</p> <p>Parameters:</p> <ul> <li> <code>response</code>             (<code>dict[str, dict[str, str]]</code>)         \u2013          </li> <li> <code>groups</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the identifiers are clustered by group</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>The list of identifiers from the response</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def parse_pdb_response_for_ids(response: dict[str, dict[str, str]], groups: bool = False) -&gt; list[str]:\n    \"\"\"Parse JSON PDB API returns for identifiers\n\n    Args:\n        response:\n        groups: Whether the identifiers are clustered by group\n\n    Returns:\n        The list of identifiers from the response\n    \"\"\"\n    # logger.debug(f'Response contains the results: {response[\"result_set\"]}')\n    if groups:\n        return [result['identifier'] for result in response.get('group_set', [])]\n    else:\n        return [result['identifier'] for result in response.get('result_set', [])]\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.query_pdb","title":"query_pdb","text":"<pre><code>query_pdb(query_: dict[Any] | str, json_formatted: bool = False) -&gt; dict[str, Any] | None\n</code></pre> <p>Take a JSON formatted PDB API query and return the results</p> <p>PDB response can look like: {'query_id': 'ecc736b3-f19c-4a54-a5d6-3db58ce6520b',  'result_type': 'entry', 'total_count': 104, 'result_set': [{'identifier': '4A73', 'score': 1.0,                 'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,                                                                  'original_score': 222.23667907714844,                                                                  'norm_score': 1.0}]}]},                {'identifier': '5UCQ', 'score': 1.0,                 'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,                                                                  'original_score': 222.23667907714844,                                                                  'norm_score': 1.0}]}]},                {'identifier': '6P3L', 'score': 1.0,                 'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,                                                                  'original_score': 222.23667907714844,                                                                  'norm_score': 1.0}]}]},                 ...               ] }</p> <p>Parameters:</p> <ul> <li> <code>query_</code>             (<code>dict[Any] | str</code>)         \u2013          <p>The query formatted as a dictionary or a JSON string</p> </li> <li> <code>json_formatted</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the query is already formatted as a JSON string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any] | None</code>         \u2013          <p>The response formatted as a dictionary from the JSON format or None if the query failed</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def query_pdb(query_: dict[Any] | str, json_formatted: bool = False) -&gt; dict[str, Any] | None:\n    \"\"\"Take a JSON formatted PDB API query and return the results\n\n    PDB response can look like:\n    {'query_id': 'ecc736b3-f19c-4a54-a5d6-3db58ce6520b',\n     'result_type': 'entry',\n    'total_count': 104,\n    'result_set': [{'identifier': '4A73', 'score': 1.0,\n                    'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,\n                                                                     'original_score': 222.23667907714844,\n                                                                     'norm_score': 1.0}]}]},\n                   {'identifier': '5UCQ', 'score': 1.0,\n                    'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,\n                                                                     'original_score': 222.23667907714844,\n                                                                     'norm_score': 1.0}]}]},\n                   {'identifier': '6P3L', 'score': 1.0,\n                    'services': [{'service_type': 'text', 'nodes': [{'node_id': 11198,\n                                                                     'original_score': 222.23667907714844,\n                                                                     'norm_score': 1.0}]}]},\n                    ...\n                  ]\n    }\n\n    Args:\n        query_: The query formatted as a dictionary or a JSON string\n        json_formatted: Whether the query is already formatted as a JSON string\n\n    Returns:\n        The response formatted as a dictionary from the JSON format or None if the query failed\n    \"\"\"\n    if json_formatted:\n        formatted_query_ = query_\n    else:\n        formatted_query_ = dumps(query_)\n\n    query_response = None\n    iteration = 0\n    while True:\n        try:\n            query_response = requests.get(pdb_query_url, params={'json': formatted_query_})\n            # logger.debug(f'Found the PDB query with url: {query_response.url}')\n            if query_response.status_code == 200:\n                return query_response.json()\n            elif query_response.status_code == 204:\n                logger.warning('No response was returned. Your query likely found no matches!')\n                break\n            elif query_response.status_code == 429:\n                logger.debug('Too many requests, pausing momentarily')\n                time.sleep(2)\n            else:\n                logger.debug(f'Your query returned an unrecognized status code ({query_response.status_code})')\n                time.sleep(1)\n                iteration += 1\n        except requests.exceptions.ConnectionError:\n            logger.debug('Requests ran into a connection error')\n            time.sleep(1)\n            iteration += 1\n\n        if iteration &gt; 5:\n            logger.error('The maximum number of resource fetch attempts was made with no resolution. '\n                         f'Offending request {getattr(query_response, \"url\", pdb_query_url)}')  # Todo format url\n            break\n            # raise DesignError('The maximum number of resource fetch attempts was made with no resolution. '\n            #                   'Offending request %s' % getattr(query_response, 'url', pdb_query_url))\n    return None\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.pdb_id_matching_uniprot_id","title":"pdb_id_matching_uniprot_id","text":"<pre><code>pdb_id_matching_uniprot_id(uniprot_id, return_id: return_types_literal = 'polymer_entity') -&gt; list[str]\n</code></pre> <p>Find all matching PDB entries from a specified UniProt ID and specific return ID</p> <p>Parameters:</p> <ul> <li> <code>uniprot_id</code>         \u2013          <p>The UniProt ID of interest</p> </li> <li> <code>return_id</code>             (<code>return_types_literal</code>, default:                 <code>'polymer_entity'</code> )         \u2013          <p>The type of value to return</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>The list of matching IDs</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def pdb_id_matching_uniprot_id(uniprot_id, return_id: return_types_literal = 'polymer_entity') -&gt; list[str]:\n    \"\"\"Find all matching PDB entries from a specified UniProt ID and specific return ID\n\n    Args:\n        uniprot_id: The UniProt ID of interest\n        return_id: The type of value to return\n\n    Returns:\n        The list of matching IDs\n    \"\"\"\n    if return_id not in return_type_args:\n        raise KeyError(\n            f\"The specified return_id '{return_id}' isn't supported. Allowed values: {', '.join(return_type_args)}\")\n    database = {'attribute': 'rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_name',\n                'negation': False, 'operator': 'exact_match', 'value': 'UniProt'}\n    accession = \\\n        {'attribute': 'rcsb_polymer_entity_container_identifiers.reference_sequence_identifiers.database_accession',\n         'negation': False, 'operator': 'in', 'value': [uniprot_id]}\n\n    uniprot_query = [format_terminal_group(service='text', **database),\n                     format_terminal_group(service='text', **accession)]\n    final_query = generate_group('and', uniprot_query)\n    search_query = generate_query(final_query, return_id=return_id)\n    response_d = query_pdb(search_query)\n    if response_d:\n        return parse_pdb_response_for_ids(response_d)\n    else:\n        return []\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.generate_query","title":"generate_query","text":"<pre><code>generate_query(search: dict, return_id: return_types_literal = 'entry', cluster_uniprot: bool = False, cluster_sequence: bool = False, return_groups: bool = False, all_matching: bool = True) -&gt; dict[str, dict | str]\n</code></pre> <p>Format a PDB query with the specific return type and parameters affecting search results</p> <p>Parameters:</p> <ul> <li> <code>search</code>             (<code>dict</code>)         \u2013          <p>Contains the key, value pairs in accordance with groups and terminal groups</p> </li> <li> <code>return_id</code>             (<code>return_types_literal</code>, default:                 <code>'entry'</code> )         \u2013          <p>The type of ID that should be returned</p> </li> <li> <code>cluster_uniprot</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the query generated is a sequence type query</p> </li> <li> <code>cluster_sequence</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the query generated is clustered by sequence similarity</p> </li> <li> <code>return_groups</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return results as group IDs</p> </li> <li> <code>all_matching</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to get all matching IDs</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict | str]</code>         \u2013          <p>The formatted query to be sent via HTTP GET</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def generate_query(search: dict, return_id: return_types_literal = 'entry', cluster_uniprot: bool = False,\n                   cluster_sequence: bool = False, return_groups: bool = False, all_matching: bool = True) \\\n        -&gt; dict[str, dict | str]:\n    \"\"\"Format a PDB query with the specific return type and parameters affecting search results\n\n    Args:\n        search: Contains the key, value pairs in accordance with groups and terminal groups\n        return_id: The type of ID that should be returned\n        cluster_uniprot: Whether the query generated is a sequence type query\n        cluster_sequence: Whether the query generated is clustered by sequence similarity\n        return_groups: Whether to return results as group IDs\n        all_matching: Whether to get all matching IDs\n\n    Returns:\n        The formatted query to be sent via HTTP GET\n    \"\"\"\n    if return_id not in return_type_args:\n        raise KeyError(\n            f\"The specified return type '{return_id}' isn't supported. Viable types include \"\n            f\"{', '.join(return_type_args)}\")\n\n    query_d = {'query': search, 'return_type': return_id}\n    request_options = {'results_content_type': ['experimental'],  # \"computational\" for Alphafold\n                       'sort': [{\n                           'sort_by': 'score',\n                           'direction': 'desc'}],\n                       'scoring_strategy': 'combined'\n                       }\n    if cluster_uniprot or cluster_sequence:\n        if cluster_uniprot:\n            cluster_options = sequence_request_options.copy()\n        elif cluster_sequence:\n            cluster_options = sequence_cluster_request_options.copy()\n        else:\n            raise NotImplementedError()\n\n        if return_groups:\n            cluster_options.update({'group_by_return_type': 'groups'})\n\n        request_options.update(cluster_options)\n    elif return_groups:\n        logger.warning(\n            \"The argument 'return_groups' wasn't used as neither 'cluster_uniprot' or 'cluster_sequence' were provided\")\n\n    if all_matching:\n        request_options.update({'return_all_hits': True})\n\n    query_d.update({'request_options': request_options})\n\n    return query_d\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.retrieve_pdb_entries_by_advanced_query","title":"retrieve_pdb_entries_by_advanced_query","text":"<pre><code>retrieve_pdb_entries_by_advanced_query(save: bool = True, return_results: bool = True, force_schema_update: bool = False, entity: bool = False, assembly: bool = False, chain: bool = False, entry: bool = False, **kwargs) -&gt; str | list | None\n</code></pre> <p>Parameters:</p> <ul> <li> <code>save</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          </li> <li> <code>return_results</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          </li> <li> <code>force_schema_update</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          </li> <li> <code>entity</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          </li> <li> <code>assembly</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          </li> <li> <code>chain</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          </li> <li> <code>entry</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          </li> </ul> <p>Returns:</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def retrieve_pdb_entries_by_advanced_query(save: bool = True, return_results: bool = True,\n                                           force_schema_update: bool = False, entity: bool = False,\n                                           assembly: bool = False, chain: bool = False, entry: bool = False, **kwargs) \\\n        -&gt; str | list | None:\n    \"\"\"\n\n    Args:\n        save:\n        return_results:\n        force_schema_update:\n        entity:\n        assembly:\n        chain:\n        entry:\n\n    Returns:\n\n    \"\"\"\n    # {attribute: {'dtype': 'string', 'description': 'XYZ', 'operators': {'equals',}, 'choices': []}, ...}\n\n    def search_schema(term):\n        return [(key, schema[key]['description']) for key in schema if schema[key]['description'] and\n                term.lower() in schema[key]['description'].lower()]\n\n    def make_groups(*args, recursive_depth=0):\n        # Todo remove ^ * expression?\n        # on initialization have [{}, {}, ...]\n        #  was [(), (), ...]\n        # on recursion get (terminal_queries, grouping,\n        terminal_queries = args[0]\n        work_on_group = args[recursive_depth]\n        all_grouping_indices = {i for i in range(1, 1 + len(work_on_group))}\n\n        group_introduction = f'\\n{header_string % \"Grouping Instructions\"}\\n' \\\n                             f'Because you have {len(terminal_queries)} search queries, you need to combine these to ' \\\n                             'a total search strategy. This is accomplished by grouping your search queries together ' \\\n                             f'using the operations {group_operators}. You must eventually group all queries into a ' \\\n                             'single logical operation.\\nIf you have multiple groups, you will need to group those ' \\\n                             'groups, so on and so forth.\\nIndicate your group selections with a space separated list!'\\\n                             ' You will choose the group operation to combine this list afterwards.\\nFollow prior ' \\\n                             \"prompts if you need a reminder of how group#'s relate to query#'s\"\n        group_grouping_intro = '\\nGroups remain, you must group groups as before.'\n        group_inquiry_string = '\\nWhich of these (identified by #) would you like to combine into a group?%s' % \\\n                               input_string\n        group_specification_string = 'You specified \"%s\" as a single group.'\n        group_logic_string = f'\\nWhat group operator {group_operators} would you like for this group?{input_string}'\n\n        available_query_string = '\\nYour available queries are:\\n%s\\n' % \\\n                                 '\\n'.join(query_display_string % (query_num, service.upper(), attribute,\n                                                                   'NOT ' if negate else '', operator.upper(), value)\n                                           for query_num, (service, attribute, operator, negate, value)\n                                           in enumerate(list(terminal_queries.values()), 1))\n\n        if recursive_depth == 0:\n            intro_string = group_introduction\n            available_entity_string = available_query_string\n        else:\n            intro_string = group_grouping_intro\n            available_entity_string = '\\nYour available groups are:\\n%s\\n' % \\\n                                      '\\n'.join(f'\\tGroup Group #{i}{format_string.format(*group)}'\n                                                for i, group in enumerate(list(work_on_group.values()), 1))\n\n        print(intro_string)  # provide an introduction\n        print(available_entity_string)  # display available entities which switch between guery and group...\n\n        selected_grouping_indices = deepcopy(all_grouping_indices)\n        groupings = []\n        while len(selected_grouping_indices) &gt; 1:  # check if more work needs to be done\n            while True:  # ensure grouping input is viable\n                while True:\n                    grouping = set(map(int, input(group_inquiry_string).split()))  # get new grouping\n                    # error on isdigit() ^\n                    if len(grouping) &gt; 1:\n                        break\n                    else:\n                        print('More than one group is required. Your group \"%s\" is invalid' % grouping)\n                while True:\n                    confirm = input('%s\\n%s' % (group_specification_string % grouping, confirmation_string))\n                    if confirm.lower() in bool_d:\n                        break\n                    else:\n                        print('%s %s is not a valid choice!' % (invalid_string, confirm))\n\n                if bool_d[confirmation.lower()] or confirmation.isspace():  # confirm that grouping is as specified\n                    while True:  # check if logic input is viable\n                        group_logic = input(group_logic_string).lower()\n                        if group_logic in group_operators:\n                            break\n                        else:\n                            print(invalid_string)\n                    groupings.append((grouping, group_logic))\n                    break\n\n            # remove specified from the pool of available until all are gone\n            selected_grouping_indices = selected_grouping_indices.difference(grouping)\n\n        if len(selected_grouping_indices) &gt; 0:\n            groupings.append((selected_grouping_indices, 'and'))  # When only 1 remains, automatically add 'and'\n            # Todo test logic of and with one group?\n\n        args.extend((groupings,))  # now [{} {}, ..., ([(grouping, group_logic), (), ...])\n        # once all groupings are grouped, recurse\n        if len(groupings) &gt; 1:\n            # todo without the return call, the stack never comes back to update args?\n            make_groups(*args, recursive_depth=recursive_depth + 1)\n\n        return list(args)  # list() may be unnecessary\n\n    # Start the user input routine -------------------------------------------------------------------------------------\n    schema = get_rcsb_metadata_schema(force_update=force_schema_update)\n    print(f'\\n{header_string % \"PDB API Advanced Query\"}\\n'\n          f'This prompt will walk you through generating an advanced search query and retrieving the matching '\n          \"set of entry ID's from the PDB. This automatically parses the ID's of interest for downstream use, which \"\n          'can save you some headache. If you want to take advantage of the PDB webpage GUI to perform the advanced '\n          f'search, visit:\\n\\t{pdb_advanced_search_url}\\nThen enter \"json\" in the prompt below and follow those '\n          'instructions.\\n\\n'\n          'Otherwise, this command line prompt takes advantage of the same GUI functionality. If you have a '\n          'search specified from a prior query that you want to process again, using \"json\" will be useful as well. '\n          'To proceed with the command line search just hit \"Enter\"')\n    program_start = input(input_string)\n    if program_start.lower() == 'json':\n        if entity:\n            return_type = 'Polymer Entities'  # 'polymer_entity'\n        elif assembly:\n            return_type = 'Assemblies'  # 'assembly'\n        elif chain:\n            return_type = 'Polymer Entities'  # This isn't available on web GUI -&gt; 'polymer_instance'\n        elif entry:\n            return_type = 'Structures'  # 'entry'\n        else:\n            return_type = 'Structures'  # 'entry'\n\n        return_type_prompt = f'At the bottom left of the dialog, there is a drop down menu next to \"Return\". ' \\\n                             f'Choose {return_type}'\n        print('DETAILS: To save time formatting and immediately move to your design pipeline, build your Query with the'\n              ' PDB webpage GUI, then save the resulting JSON text to a file. To do this, first build your full query '\n              f'on the advanced search page, {return_type_prompt} then click the Search button (magnifying glass icon).'\n              ' After the page loads, a new section of the search page should appear above the Advanced Search Query '\n              'Builder dialog. There, click the JSON|-&gt;| button to open a new page with an automatically built JSON '\n              'representation of your query. Save the entirety of this JSON formatted query to a file to return your '\n              \"chosen ID's\\n\")\n        # ('Paste your JSON object below. IMPORTANT select from the opening \\'{\\' to '\n        #  '\\'\"return_type\": \"entry\"\\' and paste. Before hitting enter, add a closing \\'}\\'. This hack '\n        #  'ensures ALL results are retrieved with no sorting or pagination applied\\n\\n%s' %\n        #  input_string)\n        prior_query = input(f'Please specify the path where the JSON query file is located{input_string}')\n        while not os.path.exists(prior_query):\n            prior_query = input(f\"The specified path '{prior_query}' doesn't exist! Please try again{input_string}\")\n\n        with open(prior_query, 'r') as f:\n            json_input = load(f)\n\n        # remove any paginate instructions from the json_input\n        json_input['request_options'].pop('paginate', None)\n        # if all_matching:\n        # Ensure we get all matching\n        json_input['request_options'].update({'return_all_hits': True})\n        response_d = query_pdb(json_input)\n    # elif program_start.lower() == 'previous':\n    #     while True:\n    #         prior_query = input('Please specify the path where the search file is located%s' % input_string)\n    #         if os.path.exists(prior_query):\n    #             with open(prior_query, 'r') as f:\n    #                 search_query = loads(f.readlines())\n    #         else:\n    #             print('The specified path \\'%s\\' doesn\\'t exist! Please try again.' % prior_query)\n    else:\n        if entity:\n            return_type = 'polymer_entity'\n        elif assembly:\n            return_type = 'assembly'\n        elif chain:\n            return_type = 'polymer_instance'\n        elif entry:\n            return_type = 'entry'\n        else:\n            return_identifier_string = '\\nFor each set of options, choose the option from the first column for the ' \\\n                                       'description in the second.\\nWhat type of identifier do you want to search the '\\\n                                       f'PDB for?%s{input_string}' % user_input_format % \\\n                                       '\\n'.join(format_string.format(*item) for item in return_types.items())\n            return_type = validate_input(return_identifier_string, return_type_args)\n\n        terminal_group_queries = []\n        # terminal_group_queries = {}\n        increment = 1\n        while True:\n            # Todo only text search is available now\n            # query_builder_service_string = '\\nWhat type of search method would you like to use?%s%s' % \\\n            #                                (user_input_format % '\\n'.join(format_string % item\n            #                                                               for item in services.items()), input_string)\n            query_builder_attribute_string = \\\n                '\\nWhat type of attribute would you like to use? Examples include:\\n\\t%s\\n\\n' \\\n                f'For a more thorough list indicate \"s\" for search.\\nAlternatively, you can browse {attribute_url}\\n' \\\n                f'Ensure that your spelling is exact if you want your query to succeed!{input_string}' % \\\n                '\\n\\t'.join(utils.pretty_format_table(attributes.items(), header=('Option', 'Description')))\n            query_builder_operator_string = '\\nWhat operator would you like to use?\\nPossible operators include:' \\\n                                            '\\n\\t%s\\nIf you would like to negate the operator, on input type \"not\" ' \\\n                                            f'after your selection. Ex: equals not{input_string}'\n            query_builder_value_string = '\\nWhat value should be %s? Required type is: %s.%s%s'\n            query_display_string = 'Query #%d: Search the PDB by \"%s\" for \"%s\" attributes \"%s%s\" \"%s\".'\n\n            while True:  # start the query builder routine\n                while True:\n                    # service = input(query_builder_service_string)\n                    service = 'text'  # Todo\n                    if service in services:\n                        break\n                    else:\n                        print(invalid_string)\n\n                # {attribute: {'dtype': 'string', 'description': 'XYZ', 'operators': {'equals',}, 'choices': []}, ...}\n                while True:\n                    attribute = input(query_builder_attribute_string)\n                    while attribute.lower() == 's':  # If the user would like to search all possible\n                        search_term = input('What term would you like to search?%s' % input_string)\n                        attribute = input(f'Found the following instances of \"{search_term.upper()}\":\\n%s\\nWhich option'\n                                          f' are you interested in? Enter \"s\" to repeat search.{input_string}' %\n                                          user_input_format %\n                                          '\\n'.join(format_string.format(*key_description_pair) for key_description_pair\n                                                    in search_schema(search_term)))\n                        if attribute != 's':\n                            break\n                    if attribute in schema:  # Confirm the user wants to go forward with this\n                        break\n                    else:\n                        print(f'***ERROR: {attribute} was not found in PDB schema***')\n                        # while True:  # confirm that the confirmation input is valid\n                        #     confirmation = input('ERROR: %s was not found in PDB schema! If you proceed, your search is'\n                        #                          ' almost certain to fail.\\nProceed anyway? [y/n]%s' %\n                        #                          (attribute, input_string))\n                        #     if confirmation.lower() in bool_d:\n                        #         break\n                        #     else:\n                        #         print('%s %s is not a valid choice!' % invalid_string, confirmation)\n                        # if bool_d[confirmation.lower()] or confirmation.isspace():  # break the attribute routine on y or ''\n                        #     break\n\n                while True:  # Retrieve the operator for the search\n                    while True:  # Check if the operator should be negated\n                        operator = input(query_builder_operator_string % ', '.join(schema[attribute]['operators']))\n                        if len(operator.split()) &gt; 1:\n                            negation = operator.split()[1]\n                            operator = operator.split()[0]\n                            if negation.lower() == 'not':  # Can negate any search\n                                negate = True\n                                break\n                            else:\n                                print(f\"{invalid_string} {negation} is not a recognized negation!\\n \"\n                                      f\"Try '{operator} not' instead or remove extra input\")\n                        else:\n                            negate = False\n                            break\n                    if operator in schema[attribute]['operators']:\n                        break\n                    else:\n                        print(f\"{invalid_string} {operator} isn't a valid operator\")\n\n                op_in = True\n                while op_in:  # Check if operator is 'in'\n                    if operator == 'in':\n                        print(\"\\nThe 'in' operator can take multiple values. If you want multiple values, specify \"\n                              'each as a separate input')\n                    else:\n                        op_in = False\n\n                    while True:  # Retrieve the value for the search\n                        value = input(query_builder_value_string % (operator.upper(), instance_d[schema[attribute]['dtype']]\n                                                                    , ('\\nPossible choices:\\n\\t%s' %\n                                                                       ', '.join(schema[attribute]['choices'])\n                                                                       if schema[attribute]['choices'] else ''),\n                                                                    input_string))\n                        if isinstance(value, instance_d[schema[attribute]['dtype']]):  # check if the right data type\n                            break\n                        else:\n                            try:  # try to convert the input value to the specified type\n                                value = instance_d[schema[attribute]['dtype']](value)\n                                if schema[attribute]['choices']:  # if there is a choice\n                                    if value in schema[attribute]['choices']:  # check if the value is in the possible choices\n                                        break\n                                    else:  # if not, confirm the users desire to do this\n                                        while True:  # confirm that the confirmation input is valid\n                                            confirmation = input('%s was not found in the possible choices: %s\\nProceed'\n                                                                 ' anyway? [y/n]%s' %\n                                                                 (value, ', '.join(schema[attribute]['choices']),\n                                                                  input_string))\n                                            if confirmation.lower() in bool_d:\n                                                break\n                                            else:\n                                                print(f\"{invalid_string} {confirmation} isn't a valid choice\")\n                                        if bool_d[confirmation.lower()] or confirmation.isspace():  # break the value routine on y or ''\n                                            break\n\n                                else:\n                                    break\n                            except ValueError:  # catch any conversion issue like float('A')\n                                print(f\"{invalid_string} {value} isn't a valid {instance_d[schema[attribute]['dtype']]}\"\n                                      \" value!\")\n\n                    while op_in:\n                        # TODO ensure that the in parameters are spit out as a list\n                        additional = input(additional_input_string % \" value to your 'in' operator\")\n                        if additional.lower() in bool_d:\n                            if bool_d[additional.lower()] or additional.isspace():\n                                break  # Stop the inner 'in' check loop\n                            else:\n                                op_in = False  # Stop the inner and outer 'in' while loops\n                        else:\n                            print(f\"{invalid_string} {additional} isn't a valid choice\")\n\n                while True:\n                    confirmation = input('\\n%s\\n%s' % (query_display_string %\n                                                       (increment, service.upper(), attribute,\n                                                        'NOT ' if negate else '', operator.upper(), value),\n                                                       confirmation_string))\n                    if confirmation.lower() in bool_d:\n                        break\n                    else:\n                        print(f\"{invalid_string} {confirmation} isn't a valid choice\")\n                if bool_d[confirmation.lower()] or confirmation.isspace():\n                    break\n\n            # terminal_group_queries[increment] = (service, attribute, operator, negate, value)\n            terminal_group_queries.append(dict(service=service, attribute=attribute, operator=operator, negate=negate,\n                                               value=value))\n            increment += 1\n            while True:\n                additional = input(additional_input_string % ' query')\n                if additional.lower() in bool_d:\n                    break\n                else:\n                    print(f\"{invalid_string} {confirmation} isn't a valid choice\")\n            if not bool_d[additional.lower()]:  # or confirmation.isspace():\n                break\n\n        # Group terminal queries into groups if there are more than 1\n        if len(terminal_group_queries) &gt; 1:\n            recursive_query_tree = make_groups(terminal_group_queries)\n            # expecting return of [terminal_group_queries, bottom group hierarchy, second group hierarchy, ..., top]\n        else:\n            recursive_query_tree = [terminal_group_queries]\n            # recursive_query_tree = (terminal_group_queries, )\n        # recursive_query_tree = (queries, grouping1, grouping2, etc.)\n        for i, node in enumerate(recursive_query_tree):\n            if i == 0:\n                recursive_query_tree[i] = {j: format_terminal_group(**leaf) for j, leaf in enumerate(node, 1)}\n                # recursive_query_tree[i] = {j: format_terminal_group(*node[leaf]) for j, leaf in enumerate(node, 1)}\n\n                # terminal_group_queries = {j: format_terminal_group(*leaf) for j, leaf in enumerate(node)}\n                # format_terminal_group(parameter_args, service=service)\n                # terminal_group_queries[increment] = \\\n                #     format_terminal_group(attribute, operator, value, service=service)\n            else:\n                # if i == 1:\n                #     child_groups = terminal_group_queries\n                #     # child_groups = [terminal_group_queries[j] for j in child_nodes]\n                # else:\n                #     child_groups = recursive_query_tree[i]\n                # operation, child_nodes = node\n                # groups = {j: generate_group(operation, child_groups) for j, leaf in enumerate(node)}\n\n                # NOPE Subtract the k indices to ensure that the user input numbers match with python zero indexing\n                # i - 1 gives the index of the previous index of the recursive_query_tree to operate on\n                # k pulls the groups specified in the input out to make a list with the corresponding terminai groups\n                recursive_query_tree[i] = {j: generate_group(operation, [recursive_query_tree[i - 1][k]\n                                                                         for k in child_group_nums])\n                                           for j, (child_group_nums, operation) in enumerate(node, 1)}\n                # for k in child_group_nums}\n        final_query = recursive_query_tree[-1][1]  #\n\n        search_query = generate_query(final_query, return_id=return_type)\n        response_d = query_pdb(search_query)\n    logger.debug(f'The server returned:\\n{response_d}')\n\n    if response_d:\n        retrieved_ids = parse_pdb_response_for_ids(response_d)\n    else:\n        return []\n\n    if save:\n        utils.io_save(retrieved_ids)\n\n    if return_results:\n        return retrieved_ids\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.query_pdb_by","title":"query_pdb_by","text":"<pre><code>query_pdb_by(entry: str = None, assembly_id: str = None, assembly_integer: int | str = None, entity_id: str = None, entity_integer: int | str = None, chain: str = None, **kwargs) -&gt; dict | list[list[str]]\n</code></pre> <p>Retrieve information from the PDB API by EntryID, AssemblyID, or EntityID</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>assembly_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The AssemblyID to query with format (1ABC-1)</p> </li> <li> <code>assembly_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The particular assembly integer to query. Must include entry as well</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> <li> <code>entity_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>chain</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest</p> </li> </ul> <p>Returns:     The query result</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def query_pdb_by(entry: str = None, assembly_id: str = None, assembly_integer: int | str = None, entity_id: str = None,\n                 entity_integer: int | str = None, chain: str = None, **kwargs) -&gt; dict | list[list[str]]:\n    \"\"\"Retrieve information from the PDB API by EntryID, AssemblyID, or EntityID\n\n    Args:\n        entry: The 4 character PDB EntryID of interest\n        assembly_id: The AssemblyID to query with format (1ABC-1)\n        assembly_integer: The particular assembly integer to query. Must include entry as well\n        entity_id: The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n        entity_integer: The entity integer from the EntryID of interest\n        chain: The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest\n    Returns:\n        The query result\n    \"\"\"\n    if entry is not None:\n        if len(entry) == 4:\n            if entity_integer is not None:\n                logger.debug(f'Querying PDB API with {entry}_{entity_integer}')\n                return _get_entity_info(entry=entry, entity_integer=entity_integer)\n            elif assembly_integer is not None:\n                logger.debug(f'Querying PDB API with {entry}-{assembly_integer}')\n                return _get_assembly_info(entry=entry, assembly_integer=assembly_integer)\n            else:\n                logger.debug(f'Querying PDB API with {entry}')\n                data = _get_entry_info(entry)\n                if chain:\n                    integer = None\n                    for entity_idx, chains in data.get('entity').items():\n                        if chain in chains:\n                            integer = entity_idx\n                            break\n                    if integer:\n                        logger.debug(f'Querying PDB API with {entry}_{integer}')\n                        return _get_entity_info(entry=entry, entity_integer=integer)\n                    else:\n                        raise KeyError(\n                            f\"No chainID '{chain}' found in PDB ID {entry}. Possible chains \"\n                            f'{\", \".join(ch for chns in data.get(\"entity\", {}).items() for ch in chns)}')\n                else:\n                    return data\n        else:\n            logger.debug(f\"EntryID '{entry}' isn't the required format and will not be found with the PDB API\")\n    elif assembly_id is not None:\n        entry, assembly_integer, *extra = assembly_id.split('-')\n        if not extra and len(entry) == 4:\n            logger.debug(f'Querying PDB API with {entry}-{assembly_integer}')\n            return _get_assembly_info(entry=entry, assembly_integer=assembly_integer)\n\n        logger.debug(f\"AssemblyID '{assembly_id}' isn't the required format and will not be found with the PDB API\")\n\n    elif entity_id is not None:\n        entry, entity_integer, *extra = entity_id.split('_')\n        if not extra and len(entry) == 4:\n            logger.debug(f'Querying PDB API with {entry}_{entity_integer}')\n            return _get_entity_info(entry=entry, entity_integer=entity_integer)\n\n        logger.debug(f\"EntityID '{entity_id}' isn't the required format and will not be found with the PDB API\")\n    else:\n        raise RuntimeError(\n            f'No valid arguments passed to {query_pdb_by.__name__}. Valid arguments include: '\n            f'entry, assembly_id, assembly_integer, entity_id, entity_integer, chain')\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.query_assembly_id","title":"query_assembly_id","text":"<pre><code>query_assembly_id(assembly_id: str = None, entry: str = None, assembly_integer: str | int = None) -&gt; Response | None\n</code></pre> <p>Retrieve PDB AssemblyID information from the PDB API. More info at http://data.rcsb.org/#data-api</p> <p>For all method types the following keys are available: {'rcsb_polymer_entity_annotation', 'entity_poly', 'rcsb_polymer_entity', 'entity_src_gen',  'rcsb_polymer_entity_feature_summary', 'rcsb_polymer_entity_align', 'rcsb_id', 'rcsb_cluster_membership',  'rcsb_polymer_entity_container_identifiers', 'rcsb_entity_host_organism', 'rcsb_latest_revision',  'rcsb_entity_source_organism'} NMR only - {'rcsb_polymer_entity_feature'} EM only - set() X-ray_only_keys - {'rcsb_cluster_flexibility'}</p> <p>Parameters:</p> <ul> <li> <code>assembly_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The AssemblyID to query with format (1ABC-1)</p> </li> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>assembly_integer</code>             (<code>str | int</code>, default:                 <code>None</code> )         \u2013          <p>The particular assembly integer to query. Must include entry as well</p> </li> </ul> <p>Returns:     The assembly information according to the PDB</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def query_assembly_id(assembly_id: str = None, entry: str = None, assembly_integer: str | int = None) -&gt; \\\n        requests.Response | None:\n    \"\"\"Retrieve PDB AssemblyID information from the PDB API. More info at http://data.rcsb.org/#data-api\n\n    For all method types the following keys are available:\n    {'rcsb_polymer_entity_annotation', 'entity_poly', 'rcsb_polymer_entity', 'entity_src_gen',\n     'rcsb_polymer_entity_feature_summary', 'rcsb_polymer_entity_align', 'rcsb_id', 'rcsb_cluster_membership',\n     'rcsb_polymer_entity_container_identifiers', 'rcsb_entity_host_organism', 'rcsb_latest_revision',\n     'rcsb_entity_source_organism'}\n    NMR only - {'rcsb_polymer_entity_feature'}\n    EM only - set()\n    X-ray_only_keys - {'rcsb_cluster_flexibility'}\n\n    Args:\n        assembly_id: The AssemblyID to query with format (1ABC-1)\n        entry: The 4 character PDB EntryID of interest\n        assembly_integer: The particular assembly integer to query. Must include entry as well\n    Returns:\n        The assembly information according to the PDB\n    \"\"\"\n    if assembly_id:\n        entry, assembly_integer, *_ = assembly_id.split('-')  # assume that this was passed correctly\n\n    if entry and assembly_integer:\n        return connection_exception_handler(f'{pdb_rest_url}/assembly/{entry}/{assembly_integer}')\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.parse_assembly_json","title":"parse_assembly_json","text":"<pre><code>parse_assembly_json(assembly_json: dict[str, Any]) -&gt; list[list[str]]\n</code></pre> <p>For a PDB API AssemblyID, parse the associated 'clustered' chains</p> <p>Parameters:</p> <ul> <li> <code>assembly_json</code>             (<code>dict[str, Any]</code>)         \u2013          <p>The json type dictionary returned from requests.Response.json()</p> </li> </ul> <p>Returns:     The chain ID's which cluster in the assembly -     Ex: [['A', 'A', 'A', ...], ...]</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def parse_assembly_json(assembly_json: dict[str, Any]) -&gt; list[list[str]]:\n    \"\"\"For a PDB API AssemblyID, parse the associated 'clustered' chains\n\n    Args:\n        assembly_json: The json type dictionary returned from requests.Response.json()\n    Returns:\n        The chain ID's which cluster in the assembly -\n        Ex: [['A', 'A', 'A', ...], ...]\n    \"\"\"\n    entity_clustered_chains = []\n    if not assembly_json:\n        return entity_clustered_chains\n\n    for symmetry in assembly_json['rcsb_struct_symmetry']:\n        # symmetry contains:\n        # {symbol: \"O\", type: 'Octahedral, stoichiometry: [], oligomeric_state: \"Homo 24-mer\", clusters: [],\n        #  rotation_axes: [], kind: \"Global Symmetry\"}\n        for cluster in symmetry['clusters']:  # [{}, ...]\n            # CLUSTER_IDX is not a mapping to entity index...\n            # cluster contains:\n            # {members: [], avg_rmsd: 5.219512137974998e-14} which indicates how similar each member in the cluster is\n            entity_clustered_chains.append([member.get('asym_id') for member in cluster['members']])\n\n    return entity_clustered_chains\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.query_entry_id","title":"query_entry_id","text":"<pre><code>query_entry_id(entry: str = None) -&gt; Response | None\n</code></pre> <p>Fetches the JSON object for the EntryID from the PDB API</p> <p>The following information is returned: All methods (SOLUTION NMR, ELECTRON MICROSCOPY, X-RAY DIFFRACTION) have the following keys: {'rcsb_primary_citation', 'pdbx_vrpt_summary', 'pdbx_audit_revision_history', 'audit_author',  'pdbx_database_status', 'rcsb_id', 'pdbx_audit_revision_details', 'struct_keywords',  'rcsb_entry_container_identifiers', 'entry', 'rcsb_entry_info', 'struct', 'citation', 'exptl',  'rcsb_accession_info'} EM only keys: {'em3d_fitting', 'em3d_fitting_list', 'em_image_recording', 'em_specimen', 'em_software', 'em_entity_assembly',  'em_vitrification', 'em_single_particle_entity', 'em3d_reconstruction', 'em_experiment', 'pdbx_audit_support',  'em_imaging', 'em_ctf_correction'} Xray only keys: {'diffrn_radiation', 'cell', 'reflns', 'diffrn', 'software', 'refine_hist', 'diffrn_source', 'exptl_crystal',  'symmetry', 'diffrn_detector', 'refine', 'reflns_shell', 'exptl_crystal_grow'} NMR only keys: {'pdbx_nmr_exptl', 'pdbx_audit_revision_item', 'pdbx_audit_revision_category', 'pdbx_nmr_spectrometer',  'pdbx_nmr_refine', 'pdbx_nmr_representative', 'pdbx_nmr_software', 'pdbx_nmr_exptl_sample_conditions',  'pdbx_nmr_ensemble'}</p> <p>entry_json['rcsb_entry_info'] =         {'assembly_count': 1, 'branched_entity_count': 0, 'cis_peptide_count': 3, 'deposited_atom_count': 8492,     'deposited_model_count': 1, 'deposited_modeled_polymer_monomer_count': 989,     'deposited_nonpolymer_entity_instance_count': 0, 'deposited_polymer_entity_instance_count': 6,     'deposited_polymer_monomer_count': 1065, 'deposited_solvent_atom_count': 735,     'deposited_unmodeled_polymer_monomer_count': 76, 'diffrn_radiation_wavelength_maximum': 0.9797,     'diffrn_radiation_wavelength_minimum': 0.9797, 'disulfide_bond_count': 0, 'entity_count': 3,     'experimental_method': 'X-ray', 'experimental_method_count': 1, 'inter_mol_covalent_bond_count': 0,     'inter_mol_metalic_bond_count': 0, 'molecular_weight': 115.09, 'na_polymer_entity_types': 'Other',     'nonpolymer_entity_count': 0, 'polymer_composition': 'heteromeric protein', 'polymer_entity_count': 2,     'polymer_entity_count_dna': 0, 'polymer_entity_count_rna': 0, 'polymer_entity_count_nucleic_acid': 0,     'polymer_entity_count_nucleic_acid_hybrid': 0, 'polymer_entity_count_protein': 2,     'polymer_entity_taxonomy_count': 2, 'polymer_molecular_weight_maximum': 21.89,     'polymer_molecular_weight_minimum': 16.47, 'polymer_monomer_count_maximum': 201,     'polymer_monomer_count_minimum': 154, 'resolution_combined': [1.95],     'selected_polymer_entity_types': 'Protein (only)',     'software_programs_combined': ['PHASER', 'REFMAC', 'XDS', 'XSCALE'], 'solvent_entity_count': 1,     'diffrn_resolution_high': {'provenance_source': 'Depositor assigned', 'value': 1.95}}</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB code to search for</p> </li> </ul> <p>Returns:     The entry information according to the PDB</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def query_entry_id(entry: str = None) -&gt; requests.Response | None:\n    \"\"\"Fetches the JSON object for the EntryID from the PDB API\n\n    The following information is returned:\n    All methods (SOLUTION NMR, ELECTRON MICROSCOPY, X-RAY DIFFRACTION) have the following keys:\n    {'rcsb_primary_citation', 'pdbx_vrpt_summary', 'pdbx_audit_revision_history', 'audit_author',\n     'pdbx_database_status', 'rcsb_id', 'pdbx_audit_revision_details', 'struct_keywords',\n     'rcsb_entry_container_identifiers', 'entry', 'rcsb_entry_info', 'struct', 'citation', 'exptl',\n     'rcsb_accession_info'}\n    EM only keys:\n    {'em3d_fitting', 'em3d_fitting_list', 'em_image_recording', 'em_specimen', 'em_software', 'em_entity_assembly',\n     'em_vitrification', 'em_single_particle_entity', 'em3d_reconstruction', 'em_experiment', 'pdbx_audit_support',\n     'em_imaging', 'em_ctf_correction'}\n    Xray only keys:\n    {'diffrn_radiation', 'cell', 'reflns', 'diffrn', 'software', 'refine_hist', 'diffrn_source', 'exptl_crystal',\n     'symmetry', 'diffrn_detector', 'refine', 'reflns_shell', 'exptl_crystal_grow'}\n    NMR only keys:\n    {'pdbx_nmr_exptl', 'pdbx_audit_revision_item', 'pdbx_audit_revision_category', 'pdbx_nmr_spectrometer',\n     'pdbx_nmr_refine', 'pdbx_nmr_representative', 'pdbx_nmr_software', 'pdbx_nmr_exptl_sample_conditions',\n     'pdbx_nmr_ensemble'}\n\n    entry_json['rcsb_entry_info'] = \\\n        {'assembly_count': 1, 'branched_entity_count': 0, 'cis_peptide_count': 3, 'deposited_atom_count': 8492,\n        'deposited_model_count': 1, 'deposited_modeled_polymer_monomer_count': 989,\n        'deposited_nonpolymer_entity_instance_count': 0, 'deposited_polymer_entity_instance_count': 6,\n        'deposited_polymer_monomer_count': 1065, 'deposited_solvent_atom_count': 735,\n        'deposited_unmodeled_polymer_monomer_count': 76, 'diffrn_radiation_wavelength_maximum': 0.9797,\n        'diffrn_radiation_wavelength_minimum': 0.9797, 'disulfide_bond_count': 0, 'entity_count': 3,\n        'experimental_method': 'X-ray', 'experimental_method_count': 1, 'inter_mol_covalent_bond_count': 0,\n        'inter_mol_metalic_bond_count': 0, 'molecular_weight': 115.09, 'na_polymer_entity_types': 'Other',\n        'nonpolymer_entity_count': 0, 'polymer_composition': 'heteromeric protein', 'polymer_entity_count': 2,\n        'polymer_entity_count_dna': 0, 'polymer_entity_count_rna': 0, 'polymer_entity_count_nucleic_acid': 0,\n        'polymer_entity_count_nucleic_acid_hybrid': 0, 'polymer_entity_count_protein': 2,\n        'polymer_entity_taxonomy_count': 2, 'polymer_molecular_weight_maximum': 21.89,\n        'polymer_molecular_weight_minimum': 16.47, 'polymer_monomer_count_maximum': 201,\n        'polymer_monomer_count_minimum': 154, 'resolution_combined': [1.95],\n        'selected_polymer_entity_types': 'Protein (only)',\n        'software_programs_combined': ['PHASER', 'REFMAC', 'XDS', 'XSCALE'], 'solvent_entity_count': 1,\n        'diffrn_resolution_high': {'provenance_source': 'Depositor assigned', 'value': 1.95}}\n\n    Args:\n        entry: The PDB code to search for\n    Returns:\n        The entry information according to the PDB\n    \"\"\"\n    if entry:\n        return connection_exception_handler(f'{pdb_rest_url}/entry/{entry}')\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.parse_entry_json","title":"parse_entry_json","text":"<pre><code>parse_entry_json(entry_json: dict[str, Any]) -&gt; dict[str, dict]\n</code></pre> <p>For a PDB API EntryID, parse the associated entity ID's and chains</p> <p>Parameters:</p> <ul> <li> <code>entry_json</code>             (<code>dict[str, Any]</code>)         \u2013          <p>The json type dictionary returned from requests.Response.json()</p> </li> </ul> <p>Returns:     The structural information present in the PDB EntryID with format -     {'method': xray,      'res': resolution,      'struct': {'space': space_group, 'a_b_c': (a, b, c), 'ang_a_b_c': (ang_a, ang_b, ang_c)}      }</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def parse_entry_json(entry_json: dict[str, Any]) -&gt; dict[str, dict]:\n    \"\"\"For a PDB API EntryID, parse the associated entity ID's and chains\n\n    Args:\n        entry_json: The json type dictionary returned from requests.Response.json()\n    Returns:\n        The structural information present in the PDB EntryID with format -\n        {'method': xray,\n         'res': resolution,\n         'struct': {'space': space_group, 'a_b_c': (a, b, c), 'ang_a_b_c': (ang_a, ang_b, ang_c)}\n         }\n    \"\"\"\n    experimental_method = entry_json['rcsb_entry_info'].get('experimental_method')\n    if experimental_method:\n        # Todo make ray, diffraction\n        if 'ray' in experimental_method.lower() and 'cell' in entry_json and 'symmetry' in entry_json:\n            cell = entry_json['cell']\n            ang_a, ang_b, ang_c = cell['angle_alpha'], cell['angle_beta'], cell['angle_gamma']\n            a, b, c = cell['length_a'], cell['length_b'], cell['length_c']\n            space_group = entry_json['symmetry']['space_group_name_hm']\n            struct_d = {'space': space_group, 'a_b_c': (a, b, c), 'ang_a_b_c': (ang_a, ang_b, ang_c)}\n            resolution = entry_json['rcsb_entry_info']['resolution_combined'][0]\n        elif experimental_method == 'EM':\n            # em_keys = {\n            #     'em3d_fitting': [{'id': '1', 'ref_protocol': 'RIGID BODY FIT', 'ref_space': 'REAL'}]\n            #     'em3d_fitting_list': [{'id': '1', 'pdb_entry_id': '4ZK7', '3d_fitting_id': '1'}]\n            #     'em3d_reconstruction': [{'algorithm': 'BACK PROJECTION', 'id': '1', 'image_processing_id': '1', 'num_class_averages': 1, 'num_particles': 110369, 'resolution': 2.9, 'resolution_method': 'FSC 0.143 CUT-OFF', 'symmetry_type': 'POINT'}]\n            #     'em_ctf_correction': [{'em_image_processing_id': '1', 'id': '1', 'type': 'PHASE FLIPPING AND AMPLITUDE CORRECTION'}]\n            #     'em_entity_assembly': [{'details': 'The map was generated by focused refinement of BG505 SOSIP-T33-31 nanoparticle dataset using a mask around the T33-31 nanoparticle core (masking out the flexibly linked antigens).', 'entity_id_list': ['1', '2'], 'id': '1', 'name': 'Designed tetrahedral nanoparticle BG505 SOSIP-T33-31', 'parent_id': 0, 'source': 'RECOMBINANT', 'type': 'COMPLEX'}]\n            #     'em_experiment': {'aggregation_state': 'PARTICLE', 'entity_assembly_id': '1', 'id': '1', 'reconstruction_method': 'SINGLE PARTICLE'}\n            #     'em_image_recording': [{'average_exposure_time': 10.25, 'avg_electron_dose_per_image': 50.0, 'detector_mode': 'COUNTING', 'film_or_detector_model': 'GATAN K2 SUMMIT (4k x 4k)', 'id': '1', 'imaging_id': '1', 'num_grids_imaged': 2, 'num_real_images': 1751}]\n            #     'em_imaging': [{'accelerating_voltage': 200, 'alignment_procedure': 'BASIC', 'c2_aperture_diameter': 70.0, 'cryogen': 'NITROGEN', 'electron_source': 'FIELD EMISSION GUN', 'id': '1', 'illumination_mode': 'FLOOD BEAM', 'microscope_model': 'FEI TALOS ARCTICA', 'mode': 'BRIGHT FIELD', 'nominal_cs': 2.7, 'nominal_defocus_max': 2000.0, 'nominal_defocus_min': 800.0, 'nominal_magnification': 36000, 'specimen_holder_model': 'OTHER', 'specimen_id': '1'}]\n            #     'em_particle_selection': [{'details': 'Gaussian picker in Relion/3.0', 'id': '1', 'image_processing_id': '1', 'num_particles_selected': 223099}]\n            #     'em_single_particle_entity': [{'id': 1, 'image_processing_id': '1', 'point_symmetry': 'T'}]\n            #     'em_software': [{'category': 'PARTICLE SELECTION', 'id': '1', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'IMAGE ACQUISITION', 'id': '2', 'imaging_id': '1', 'name': 'Leginon'}, {'category': 'MASKING', 'id': '3'}, {'category': 'CTF CORRECTION', 'id': '4', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'LAYERLINE INDEXING', 'id': '5'}, {'category': 'DIFFRACTION INDEXING', 'id': '6'}, {'category': 'MODEL FITTING', 'fitting_id': '1', 'id': '7', 'name': 'UCSF Chimera'}, {'category': 'OTHER', 'id': '8'}, {'category': 'INITIAL EULER ASSIGNMENT', 'id': '9', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'FINAL EULER ASSIGNMENT', 'id': '10', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'CLASSIFICATION', 'id': '11', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'RECONSTRUCTION', 'id': '12', 'image_processing_id': '1', 'name': 'RELION', 'version': '3.0'}, {'category': 'MODEL REFINEMENT', 'fitting_id': '1', 'id': '13', 'name': 'Coot'}, {'category': 'MODEL REFINEMENT', 'fitting_id': '1', 'id': '14', 'name': 'RosettaEM'}]\n            #     'em_specimen': [{'concentration': 4.1, 'details': 'BG505 SOSIP-T33-31 nanoparticle was prepared by combining equimolar amounts of BG505 SOSIP-T33-31A and BG505 SOSIP-T33-31B components that were expressed separately.', 'embedding_applied': 'NO', 'experiment_id': '1', 'id': '1', 'shadowing_applied': 'NO', 'staining_applied': 'NO', 'vitrification_applied': 'YES'}]\n            #     'em_vitrification': [{'chamber_temperature': 283.0, 'cryogen_name': 'ETHANE', 'details': 'Blotting time varied between 3 and 7 seconds.', 'humidity': 100.0, 'id': '1', 'instrument': 'FEI VITROBOT MARK IV', 'specimen_id': '1'}]\n            # }\n            # for key in em_keys:\n            #     print(entry_json.get(key))\n            struct_d = {}\n            # Access the first entry in the list with [0] v\n            resolution = entry_json['em3d_reconstruction'][0]['resolution']\n        else:  # Todo NMR\n            logger.warning(f\"No useful information added with the experimental method {experimental_method} as \"\n                           \"this method hasn't been explored yet\")\n            struct_d = {}\n            resolution = None\n    else:\n        logger.warning('Entry has no \"experimental_method\" keyword')\n        struct_d = {}\n        resolution = None\n\n    return {'res': resolution, 'struct': struct_d, 'method': experimental_method.lower()}\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.format_symmetry_group","title":"format_symmetry_group","text":"<pre><code>format_symmetry_group(symmetry: str, homomeric_number: int = 1, heteromeric_number: int = None) -&gt; str\n</code></pre> <p>Return a PDB API length limitation query</p> <p>Parameters:</p> <ul> <li> <code>symmetry</code>             (<code>str</code>)         \u2013          <p>The symmetry to query for</p> </li> <li> <code>homomeric_number</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>If the symmetry desired is homomeric, how many copies of the entity are desired</p> </li> <li> <code>heteromeric_number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>If the symmetry desired is heteromeric, how many entities are present</p> </li> </ul> <p>Returns:     The symmetry formatted query limiting entity searches to the described symmetry</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def format_symmetry_group(symmetry: str, homomeric_number: int = 1, heteromeric_number: int = None) -&gt; str:\n    \"\"\"Return a PDB API length limitation query\n\n    Args:\n        symmetry: The symmetry to query for\n        homomeric_number: If the symmetry desired is homomeric, how many copies of the entity are desired\n        heteromeric_number: If the symmetry desired is heteromeric, how many entities are present\n    Returns:\n        The symmetry formatted query limiting entity searches to the described symmetry\n    \"\"\"\n    if 'c' in symmetry.lower():\n        symmetry_query = cyclic_symmetry_limiting_group % symmetry\n    elif 'd' in symmetry.lower():\n        symmetry_query = dihedral_symmetry_limiting_group % symmetry\n    else:  # point group symmetry\n        symmetry_query = point_symmetry_limiting_group % symmetry\n\n    symmetry_number = utils.symmetry.valid_subunit_number.get(symmetry)\n    if heteromeric_number:\n        symmetry_query += ',' + heteromer_termini % symmetry_number * heteromeric_number\n    else:  # if homomer:\n        symmetry_query += ',' + homomer_termini % symmetry_number * homomeric_number\n    # else:\n    #     raise ValueError(\"Must provide either 'homomeric_number' or 'heteromeric_number'\")\n\n    return symmetry_query\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.format_length_group","title":"format_length_group","text":"<pre><code>format_length_group(lower: int, upper: int) -&gt; str\n</code></pre> <p>Return a PDB API length limitation query</p> <p>Parameters:</p> <ul> <li> <code>lower</code>             (<code>int</code>)         \u2013          <p>The low end to limit entity length</p> </li> <li> <code>upper</code>             (<code>int</code>)         \u2013          <p>The upper limit on entity length</p> </li> </ul> <p>Returns:     The length formatted query limiting entity searches to between the values lower and upper (non-inclusive)</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def format_length_group(lower: int, upper: int) -&gt; str:\n    \"\"\"Return a PDB API length limitation query\n\n    Args:\n        lower: The low end to limit entity length\n        upper: The upper limit on entity length\n    Returns:\n        The length formatted query limiting entity searches to between the values lower and upper (non-inclusive)\n    \"\"\"\n    return length_group % (lower, upper)\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.nanohedra_building_blocks_query","title":"nanohedra_building_blocks_query","text":"<pre><code>nanohedra_building_blocks_query(symmetry: str, lower: int = None, upper: int = None, thermophile: bool = False, return_groups: bool = False, limit_by_groups: Iterable[str] = None, search_by_groups: Iterable[str] = None) -&gt; dict[Any] | None\n</code></pre> <p>Retrieve symmetric oligomers from the PDB to act as building blocks for nanohedra docking</p> <p>Parameters:</p> <ul> <li> <code>symmetry</code>             (<code>str</code>)         \u2013          <p>The symmetry to query for</p> </li> <li> <code>lower</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The low end to limit entity length</p> </li> <li> <code>upper</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The upper limit on entity length</p> </li> <li> <code>thermophile</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to limit search to entries from thermophilic species</p> </li> <li> <code>return_groups</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return results as groupID's</p> </li> <li> <code>limit_by_groups</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>Whether to limit the query, i.e. not return groupID's that are provided in this argument</p> </li> <li> <code>search_by_groups</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>Search only for groupID's that are provided to this argument</p> </li> </ul> <p>Returns:     Matching EntityID's formatted as a dictionary from the JSON formatted response or None if the query failed</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def nanohedra_building_blocks_query(\n        symmetry: str, lower: int = None, upper: int = None, thermophile: bool = False, return_groups: bool = False,\n        limit_by_groups: Iterable[str] = None, search_by_groups: Iterable[str] = None) -&gt; dict[Any] | None:\n    \"\"\"Retrieve symmetric oligomers from the PDB to act as building blocks for nanohedra docking\n\n    Args:\n        symmetry: The symmetry to query for\n        lower: The low end to limit entity length\n        upper: The upper limit on entity length\n        thermophile: Whether to limit search to entries from thermophilic species\n        return_groups: Whether to return results as groupID's\n        limit_by_groups: Whether to limit the query, i.e. not return groupID's that are provided in this argument\n        search_by_groups: Search only for groupID's that are provided to this argument\n    Returns:\n        Matching EntityID's formatted as a dictionary from the JSON formatted response or None if the query failed\n    \"\"\"\n    groups_and_terminal = common_quality_filters \\\n        + ',' + format_symmetry_group(symmetry) \\\n        + ',' + format_length_group(lower, upper)\n\n    if thermophile:\n        groups_and_terminal += ',' + thermophilic_json_terminal_operator\n    if limit_by_groups:\n        groups_and_terminal += ',' + not_in_entity_group_id_search_block \\\n                               % ','.join([f'\"{id_}\"' for id_ in limit_by_groups])\n    if search_by_groups:\n        groups_and_terminal += ',' + in_entity_group_id_search_block \\\n                               % ','.join([f'\"{id_}\"' for id_ in search_by_groups])\n\n    building_block_query = and_group_query % groups_and_terminal\n    logger.debug(f'Found building_block_query: {building_block_query}')\n    formatted_query = json.loads(building_block_query)\n\n    return query_pdb(generate_query(formatted_query, return_id='polymer_entity',\n                                    cluster_sequence=True, return_groups=return_groups, all_matching=True))\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.find_author_confirmed_assembly_from_entity_group","title":"find_author_confirmed_assembly_from_entity_group","text":"<pre><code>find_author_confirmed_assembly_from_entity_group(group_ids: Iterable[str], symmetry: str, lower: int = None, upper: int = None) -&gt; dict[Any] | None\n</code></pre> <p>For specific groupID's, request all EntityID's that have an assembly confirmed by depositing authors from PDB API</p> <p>Parameters:</p> <ul> <li> <code>group_ids</code>             (<code>Iterable[str]</code>)         \u2013          <p>The groupID's to limit search to</p> </li> <li> <code>symmetry</code>             (<code>str</code>)         \u2013          <p>The symmetry to query for</p> </li> <li> <code>lower</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The low end to limit entity length</p> </li> <li> <code>upper</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The upper limit on entity length</p> </li> </ul> <p>Returns:     Matching AssemblyID's formatted as a dictionary from the JSON formatted response or None if the query failed</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def find_author_confirmed_assembly_from_entity_group(\n        group_ids: Iterable[str], symmetry: str, lower: int = None, upper: int = None) -&gt; dict[Any] | None:\n    \"\"\"For specific groupID's, request all EntityID's that have an assembly confirmed by depositing authors from PDB API\n\n    Args:\n        group_ids: The groupID's to limit search to\n        symmetry: The symmetry to query for\n        lower: The low end to limit entity length\n        upper: The upper limit on entity length\n    Returns:\n        Matching AssemblyID's formatted as a dictionary from the JSON formatted response or None if the query failed\n    \"\"\"\n    groups_and_terminal = common_quality_filters \\\n        + ',' + format_symmetry_group(symmetry) \\\n        + ',' + format_length_group(lower, upper) \\\n        + ',' + assembly_author_defined \\\n        + ',' + in_entity_group_id_search_block % ','.join([f'\"{id_}\"' for id_ in group_ids])\n    author_confirmed_query = and_group_query % groups_and_terminal\n    logger.debug(f'Found author_confirmed_query: {author_confirmed_query}')\n    formatted_query = json.loads(author_confirmed_query)\n\n    return query_pdb(generate_query(formatted_query, return_id='assembly', all_matching=True))\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.solve_author_confirmed_assemblies","title":"solve_author_confirmed_assemblies","text":"<pre><code>solve_author_confirmed_assemblies(params: QueryParams, grouped_entity_ids: dict[str, list[str]]) -&gt; tuple[list[str], list[str]]\n</code></pre> <p>From a map of Entity group ID's to resolution sorted EntityIDs, solve for those EntityIDs that have an assembly</p> <p>First search for QSbio confirmed assemblies, then search the PDB API for 'author_defined_assembly' and 'author_and_software_defined_assembly'</p> <p>Parameters:</p> <ul> <li> <code>params</code>             (<code>QueryParams</code>)         \u2013          <p>The parameter profile specified for the search procedure</p> </li> <li> <code>grouped_entity_ids</code>             (<code>dict[str, list[str]]</code>)         \u2013          <p>A dictionary mapping groupID to EntryID's</p> </li> </ul> <p>Returns:     A tuple of the objects (         The best EntityIDs according to incoming sorting and that pass the assembly test         The Entity group ID of those groups that couldn't be solved     )</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def solve_author_confirmed_assemblies(params: QueryParams, grouped_entity_ids: dict[str, list[str]]) \\\n        -&gt; tuple[list[str], list[str]]:\n    \"\"\"From a map of Entity group ID's to resolution sorted EntityIDs, solve for those EntityIDs that have an assembly\n\n    First search for QSbio confirmed assemblies, then search the PDB API for 'author_defined_assembly' and\n    'author_and_software_defined_assembly'\n\n    Args:\n        params: The parameter profile specified for the search procedure\n        grouped_entity_ids: A dictionary mapping groupID to EntryID's\n    Returns:\n        A tuple of the objects (\n            The best EntityIDs according to incoming sorting and that pass the assembly test\n            The Entity group ID of those groups that couldn't be solved\n        )\n    \"\"\"\n    # Check if the top thermophilic ids are actually bona-fide assemblies\n    top_entity_ids: list[str | None] = []\n    # If they aren't, then solve by PDB API query\n    solve_group_by_pdb = []\n    for group_id, entity_ids in grouped_entity_ids.items():\n        group_assemblies = [qsbio_confirmed.get(id_[:4].lower()) for id_ in entity_ids]\n        for id_, assembly in zip(entity_ids, group_assemblies):\n            if assembly is None:\n                continue\n            else:\n                top_entity_ids.append(id_)\n                break\n        else:  # No assemblies are qsbio_confirmed\n            # Solve by PDB assembly inference\n            solve_group_by_pdb.append(group_id)\n            top_entity_ids.append(None)\n\n    author_confirmed_assembly_result = \\\n        find_author_confirmed_assembly_from_entity_group(\n            solve_group_by_pdb, params.symmetry, params.lower_length, params.upper_length)\n    if author_confirmed_assembly_result:\n        author_confirmed_assembly_ids = parse_pdb_response_for_ids(author_confirmed_assembly_result)\n    else:\n        author_confirmed_assembly_ids = []\n\n    # Limit AssemblyID's to EntryID's\n    author_confirmed_entry_ids = [id_[:4] for id_ in author_confirmed_assembly_ids]\n    remove_group_ids = []\n    remove_group_indices = []\n\n    # For orphaned groups, find and fit author confirmed assemblies in their corresponding groups\n    for group_idx, (top_id, group_id) in enumerate(zip(top_entity_ids, grouped_entity_ids.keys())):\n        if top_id is None:\n            for entity_id in grouped_entity_ids[group_id]:\n                if entity_id[:4] in author_confirmed_entry_ids:\n                    top_entity_ids[group_idx] = entity_id\n                    break\n            else:  # This still isn't solved. Remove from the pool\n                remove_group_ids.append(group_id)\n                remove_group_indices.append(group_idx)\n\n    for group_idx in reversed(remove_group_indices):\n        top_entity_ids.pop(group_idx)\n\n    return top_entity_ids, remove_group_ids\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.entity_thermophilicity","title":"entity_thermophilicity","text":"<pre><code>entity_thermophilicity(entry: str = None, entity_integer: int | str = None, entity_id: str = None) -&gt; float | None\n</code></pre> <p>Query the PDB API for an EntityID and return the associated chains and reference dictionary</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>entity_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> </ul> <p>Returns:     Value ranging from 0-1 where 1 is completely thermophilic according to taxonomic classification</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def entity_thermophilicity(entry: str = None, entity_integer: int | str = None, entity_id: str = None) -&gt; float | None:\n    \"\"\"Query the PDB API for an EntityID and return the associated chains and reference dictionary\n\n    Args:\n        entry: The 4 character PDB EntryID of interest\n        entity_integer: The entity integer from the EntryID of interest\n        entity_id: The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n    Returns:\n        Value ranging from 0-1 where 1 is completely thermophilic according to taxonomic classification\n    \"\"\"\n    entity_request = query_entity_id(entry=entry, entity_integer=entity_integer, entity_id=entity_id)\n    if not entity_request:\n        return None\n\n    return thermophilicity_from_entity_json(entity_request.json())\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.thermophilicity_from_entity_json","title":"thermophilicity_from_entity_json","text":"<pre><code>thermophilicity_from_entity_json(entity_json: dict[str, Any]) -&gt; float\n</code></pre> <p>Return the extent to which the entity json entry in question is thermophilic</p> <p>Parameters:</p> <ul> <li> <code>entity_json</code>             (<code>dict[str, Any]</code>)         \u2013          <p>The return json from PDB API query</p> </li> </ul> <p>Returns:     Value ranging from 0-1 where 1 is completely thermophilic</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def thermophilicity_from_entity_json(entity_json: dict[str, Any]) -&gt; float:\n    \"\"\"Return the extent to which the entity json entry in question is thermophilic\n\n    Args:\n        entity_json: The return json from PDB API query\n    Returns:\n        Value ranging from 0-1 where 1 is completely thermophilic\n    \"\"\"\n    thermophilic_source = []\n    for organism in entity_json.get('rcsb_entity_source_organism', {}):\n        taxonomy_id = int(organism.get('ncbi_taxonomy_id', -1))\n        if taxonomy_id in thermophilic_taxonomy_ids:\n            thermophilic_source.append(1)\n        else:\n            thermophilic_source.append(0)\n\n    if thermophilic_source:\n        return sum(thermophilic_source) / len(thermophilic_source)\n    else:\n        return 0.\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.parse_entities_json","title":"parse_entities_json","text":"<pre><code>parse_entities_json(entity_jsons: Iterable[dict[str, Any]]) -&gt; dict[str, dict]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>entity_jsons</code>             (<code>Iterable[dict[str, Any]]</code>)         \u2013          <p>An Iterable of json like objects containing EntityID information as retrieved from the PDB API</p> </li> </ul> <p>Returns:     The entity dictionary with format -     {'EntityID':         {'chains': ['A', 'B', ...],          'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},          'reference_sequence': 'MSLEHHHHHH...',          'thermophilicity': 1.0},      ...}</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def parse_entities_json(entity_jsons: Iterable[dict[str, Any]]) -&gt; dict[str, dict]:\n    \"\"\"\n\n    Args:\n        entity_jsons: An Iterable of json like objects containing EntityID information as retrieved from the PDB API\n    Returns:\n        The entity dictionary with format -\n        {'EntityID':\n            {'chains': ['A', 'B', ...],\n             'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},\n             'reference_sequence': 'MSLEHHHHHH...',\n             'thermophilicity': 1.0},\n         ...}\n    \"\"\"\n    def extract_dbref(entity_ids_json: dict[str, Any]) -&gt; dict[str, dict]:\n        \"\"\"For a PDB API EntityID, parse the associated chains and database reference identifiers\n\n        Args:\n            entity_ids_json: The json type dictionary returned from requests.Response.json()\n        Returns:\n            Ex: {'db': DATABASE, 'accession': 'Q96DC8'} where DATABASE can be one of 'GenBank', 'Norine', 'UniProt'\n        \"\"\"\n        database_keys = ['db', 'accession']\n        try:\n            uniprot_ids = entity_ids_json['uniprot_ids']\n            # Todo choose the most accurate if more than 2...\n            #  'rcsb_polymer_entity_align' indicates how the model from the PDB aligns to UniprotKB through SIFTS\n            #  [{provenance_source: \"SIFTS\",\n            #    reference_database_accession: \"P12528\",\n            #    reference_database_name: \"UniProt\",\n            #    aligned_regions: [{entity_beg_seq_id: 1,\n            #                       length: 124,\n            #                       ref_beg_seq_id: 2}]\n            #   },\n            #   {}, ...\n            #  ]\n            if len(uniprot_ids) &gt; 1:\n                logger.warning(f'For Entity {entity_ids_json[\"rcsb_id\"]}, found multiple UniProt Entries: '\n                               f'{\", \".join(uniprot_ids)}')\n            db_d = dict(zip(database_keys, (UKB, tuple(uniprot_ids))))\n        except KeyError:  # No 'uniprot_ids'\n            # GenBank = GB, which is mostly RNA or DNA structures or antibody complexes\n            # Norine = NOR, which is small peptide structures, sometimes bound to proteins...\n            try:\n                identifiers = [dict(db=ident['database_name'], accession=(ident['database_accession'],))\n                               for ident in entity_ids_json.get('reference_sequence_identifiers', [])]\n            except KeyError:  # There are really no identifiers of use\n                return {}\n            if identifiers:\n                if len(identifiers) == 1:  # Only one solution\n                    db_d = identifiers[0]\n                else:  # Find the most ideal accession_database UniProt &gt; GenBank &gt; Norine &gt; ???\n                    whatever_else = 0\n                    priority_l = [[] for _ in range(len(identifiers))]\n                    for idx, (database, accession) in enumerate(identifiers):\n                        if database == UKB:\n                            priority_l[0].append(idx)\n                        elif database == GB:\n                            # Two elements are required from above len check, never have IndexError\n                            priority_l[1].append(idx)\n                        # elif database == NOR:\n                        #     priority_l[2].append(idx)\n                        elif not whatever_else:\n                            # Only set the first time an unknown identifier is seen\n                            whatever_else = idx\n\n                    # Loop through the list of prioritized identifiers\n                    for identifier_idx in priority_l:\n                        if identifier_idx:  # we have found a priority database, choose the corresponding identifier idx\n                            # Make the db_d with the db name as first arg and all the identifiers as the second arg\n                            db_d = dict(zip(database_keys,\n                                            (identifiers[identifier_idx[0]]['db'], [identifiers[idx]['accession']\n                                                                                    for idx in identifier_idx])))\n                            break\n                    else:  # if no solution from priority but something else, choose the other\n                        db_d = identifiers[whatever_else]\n            else:\n                db_d = {}\n\n        return db_d\n\n    entity_info = {}\n    for entity_idx, entity_json in enumerate(entity_jsons, 1):\n        if entity_json is None:\n            continue\n        entity_json_ids = entity_json.get('rcsb_polymer_entity_container_identifiers')\n        if entity_json_ids:\n            entity_info[entity_json_ids['rcsb_id'].lower()] = dict(\n                chains=entity_json_ids['asym_ids'],\n                dbref=extract_dbref(entity_json_ids),\n                reference_sequence=entity_json['entity_poly']['pdbx_seq_one_letter_code_can'],\n                thermophilicity=thermophilicity_from_entity_json(entity_json),\n            )\n\n    return entity_info\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.query_entity_id","title":"query_entity_id","text":"<pre><code>query_entity_id(entry: str = None, entity_integer: str | int = None, entity_id: str = None) -&gt; Response | None\n</code></pre> <p>Retrieve PDB EntityID information from the PDB API. More info at http://data.rcsb.org/#data-api</p> <p>For all method types the following keys are available: {'rcsb_polymer_entity_annotation', 'entity_poly', 'rcsb_polymer_entity', 'entity_src_gen',  'rcsb_polymer_entity_feature_summary', 'rcsb_polymer_entity_align', 'rcsb_id', 'rcsb_cluster_membership',  'rcsb_polymer_entity_container_identifiers', 'rcsb_entity_host_organism', 'rcsb_latest_revision',  'rcsb_entity_source_organism'} NMR only - {'rcsb_polymer_entity_feature'} EM only - set() X-ray_only_keys - {'rcsb_cluster_flexibility'}</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>entity_integer</code>             (<code>str | int</code>, default:                 <code>None</code> )         \u2013          <p>The integer of the entity_id</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> </ul> <p>Returns:     The entity information according to the PDB</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def query_entity_id(entry: str = None, entity_integer: str | int = None, entity_id: str = None) -&gt; \\\n        requests.Response | None:\n    \"\"\"Retrieve PDB EntityID information from the PDB API. More info at http://data.rcsb.org/#data-api\n\n    For all method types the following keys are available:\n    {'rcsb_polymer_entity_annotation', 'entity_poly', 'rcsb_polymer_entity', 'entity_src_gen',\n     'rcsb_polymer_entity_feature_summary', 'rcsb_polymer_entity_align', 'rcsb_id', 'rcsb_cluster_membership',\n     'rcsb_polymer_entity_container_identifiers', 'rcsb_entity_host_organism', 'rcsb_latest_revision',\n     'rcsb_entity_source_organism'}\n    NMR only - {'rcsb_polymer_entity_feature'}\n    EM only - set()\n    X-ray_only_keys - {'rcsb_cluster_flexibility'}\n\n    Args:\n        entry: The 4 character PDB EntryID of interest\n        entity_integer: The integer of the entity_id\n        entity_id: The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n    Returns:\n        The entity information according to the PDB\n    \"\"\"\n    if entity_id:\n        entry, entity_integer, *_ = entity_id.split('_')  # Assume that this was passed correctly\n\n    if entry and entity_integer:\n        return connection_exception_handler(f'{pdb_rest_url}/polymer_entity/{entry}/{entity_integer}')\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.get_entity_id","title":"get_entity_id","text":"<pre><code>get_entity_id(entry: str = None, entity_integer: int | str = None, entity_id: str = None, chain: str = None) -&gt; tuple[str, str] | tuple[None]\n</code></pre> <p>Retrieve a UniProtID from the PDB API by passing various PDB identifiers or combinations thereof</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>entity_integer</code>             (<code>int | str</code>, default:                 <code>None</code> )         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> <li> <code>chain</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest</p> </li> </ul> <p>Returns:     The Entity_ID</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def get_entity_id(entry: str = None, entity_integer: int | str = None, entity_id: str = None, chain: str = None) -&gt; \\\n        tuple[str, str] | tuple[None]:\n    \"\"\"Retrieve a UniProtID from the PDB API by passing various PDB identifiers or combinations thereof\n\n    Args:\n        entry: The 4 character PDB EntryID of interest\n        entity_integer: The entity integer from the EntryID of interest\n        entity_id: The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n        chain: The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest\n    Returns:\n        The Entity_ID\n    \"\"\"\n    if entry is not None:\n        if len(entry) != 4:\n            logger.warning(f'EntryID \"{entry}\" is not of the required format and will not be found with the PDB API')\n        elif entity_integer is not None:\n            return entry, entity_integer\n            # entity_id = f'{entry}_{entity_integer}'\n        else:\n            info = _get_entry_info(entry)\n            chain_entity = {chain: entity_idx for entity_idx, chains in info.get('entity', {}).items() for chain in chains}\n            if chain is not None:\n                try:\n                    return entry, chain_entity[chain]\n                    # entity_id = f'{entry}_{chain_entity[chain]}'\n                except KeyError:\n                    raise KeyError(f'No chain \"{chain}\" found in PDB ID {entry}. '\n                                   f'Possible chains {\", \".join(chain_entity)}')\n            else:\n                entity_integer = next(iter(chain_entity.values()))\n                logger.warning('Using the argument \"entry\" without either \"entity_integer\" or \"chain\" is not '\n                               f'recommended. Choosing the first EntityID \"{entry}_{entity_integer}\"')\n                return entry, entity_integer\n                # entity_id = f'{entry}_{entity_integer}'\n\n    elif entity_id is not None:\n        entry, entity_integer, *extra = entity_id.split('_')\n        if not extra and len(entry) == 4:\n            return entry, entity_integer\n\n        logger.debug(f\"EntityID '{entity_id}' isn't the required format and will not be found with the PDB API\")\n\n    return None,\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.get_entity_uniprot_id","title":"get_entity_uniprot_id","text":"<pre><code>get_entity_uniprot_id(**kwargs) -&gt; str | None\n</code></pre> <p>Retrieve a UniProtID from the PDB API by passing various PDB identifiers or combinations thereof</p> <p>Other Parameters:</p> <ul> <li> <code>entry=None</code>             (<code>str</code>)         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>entity_integer=None</code>             (<code>str</code>)         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>entity_id=None</code>             (<code>str</code>)         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> <li> <code>chain=None</code>             (<code>str</code>)         \u2013          <p>The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | None</code>         \u2013          <p>The UniProt ID</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def get_entity_uniprot_id(**kwargs) -&gt; str | None:\n    \"\"\"Retrieve a UniProtID from the PDB API by passing various PDB identifiers or combinations thereof\n\n    Keyword Args:\n        entry=None (str): The 4 character PDB EntryID of interest\n        entity_integer=None (str): The entity integer from the EntryID of interest\n        entity_id=None (str): The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n        chain=None (str): The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of\n            interest\n\n    Returns:\n        The UniProt ID\n    \"\"\"\n    entity_request = query_entity_id(*get_entity_id(**kwargs))\n    if entity_request:\n        # return the first uniprot entry\n        return entity_request.json().get('rcsb_polymer_entity_container_identifiers')['uniprot_id'][0]\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.get_entity_reference_sequence","title":"get_entity_reference_sequence","text":"<pre><code>get_entity_reference_sequence(**kwargs) -&gt; str | None\n</code></pre> <p>Query the PDB API for the reference amino acid sequence for a specified entity ID (PDB EntryID_Entity_ID)</p> <p>Other Parameters:</p> <ul> <li> <code>entry=None</code>             (<code>str</code>)         \u2013          <p>The 4 character PDB EntryID of interest</p> </li> <li> <code>entity_integer=None</code>             (<code>str</code>)         \u2013          <p>The entity integer from the EntryID of interest</p> </li> <li> <code>entity_id=None</code>             (<code>str</code>)         \u2013          <p>The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)</p> </li> <li> <code>chain=None</code>             (<code>str</code>)         \u2013          <p>The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | None</code>         \u2013          <p>One letter amino acid sequence</p> </li> </ul> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def get_entity_reference_sequence(**kwargs) -&gt; str | None:\n    \"\"\"Query the PDB API for the reference amino acid sequence for a specified entity ID (PDB EntryID_Entity_ID)\n\n    Keyword Args:\n        entry=None (str): The 4 character PDB EntryID of interest\n        entity_integer=None (str): The entity integer from the EntryID of interest\n        entity_id=None (str): The PDB formatted EntityID. Has the format EntryID_Integer (1ABC_1)\n        chain=None (str): The polymer \"chain\" identifier otherwise known as the \"asym_id\" from the PDB EntryID of\n            interest\n\n    Returns:\n        One letter amino acid sequence\n    \"\"\"\n    entity_request = query_entity_id(*get_entity_id(**kwargs))\n    if entity_request:\n        return entity_request.json().get('entity_poly')['pdbx_seq_one_letter_code_can']  # returns non-cannonical as 'X'\n</code></pre>"},{"location":"reference/resources/query/pdb/#resources.query.pdb.get_rcsb_metadata_schema","title":"get_rcsb_metadata_schema","text":"<pre><code>get_rcsb_metadata_schema(file=os.path.join(current_dir, 'rcsb_schema.pkl'), search_only=True, force_update=False)\n</code></pre> <p>Parse the rcsb metadata schema for useful information from the format      {\"properties\" : {\"assignment_version\" : {\"type\" : \"string\", \"examples\" : [ \"V4_0_2\" ],                                          \"description\" : \"Identifies the version of the feature assignment.\",                                          \"rcsb_description\" : [                                           {\"text\" : \"Identifies the version of the feature assignment.\",                                            \"context\" : \"dictionary\"},                                           {\"text\" : \"Feature Version\", \"context\" : \"brief\"} ]                                         },                       ...                       \"symmetry_type\" : {\"type\" : \"string\",     &lt;-- provide data type            provide options     --&gt;       \"enum\" : [ \"2D CRYSTAL\", \"3D CRYSTAL\", \"HELICAL\", \"POINT\" ],            provide description --&gt;       \"description\" : \"The type of symmetry applied to the reconstruction\",            provide operators   --&gt;       \"rcsb_search_context\" : [ \"exact-match\" ],                                          \"rcsb_full_text_priority\" : 10,                                          \"rcsb_description\" : [                                             {\"text\" : \"The type of symmetry applied to the reconstruction\",                                              \"context\" : \"dictionary\"},                                             {\"text\" : \"Symmetry Type (Em 3d Reconstruction)\", \"context\" : \"brief\"} ]                                         },                       ... },       \"title\" : \"Core Metadata\", \"additionalProperties\" : false, \"$comment\" : \"Schema version: 1.14.0\"       \"required\" : [\"rcsb_id\", \"rcsb_entry_container_identifiers\", \"rcsb_entry_info\",                     \"rcsb_pubmed_container_identifiers\", \"rcsb_polymer_entity_container_identifiers\",                     \"rcsb_assembly_container_identifiers\", \"rcsb_uniprot_container_identifiers\" ],       \"$schema\" : \"http://json-schema.org/draft-07/schema#\",       \"description\" : \"Collective JSON schema that includes definitions for all indexed cores with RCSB metadata extensions.\",      } Returns:     (dict): {attribute: {'dtype': 'string', 'description': 'XYZ', 'operators': {'equals'}, 'choices': []}, ...}</p> Source code in <code>symdesign/resources/query/pdb.py</code> <pre><code>def get_rcsb_metadata_schema(file=os.path.join(current_dir, 'rcsb_schema.pkl'), search_only=True, force_update=False):\n    \"\"\"Parse the rcsb metadata schema for useful information from the format\n         {\"properties\" : {\"assignment_version\" : {\"type\" : \"string\", \"examples\" : [ \"V4_0_2\" ],\n                                             \"description\" : \"Identifies the version of the feature assignment.\",\n                                             \"rcsb_description\" : [\n                                              {\"text\" : \"Identifies the version of the feature assignment.\",\n                                               \"context\" : \"dictionary\"},\n                                              {\"text\" : \"Feature Version\", \"context\" : \"brief\"} ]\n                                            },\n                          ...\n                          \"symmetry_type\" : {\"type\" : \"string\",     &lt;-- provide data type\n               provide options     --&gt;       \"enum\" : [ \"2D CRYSTAL\", \"3D CRYSTAL\", \"HELICAL\", \"POINT\" ],\n               provide description --&gt;       \"description\" : \"The type of symmetry applied to the reconstruction\",\n               provide operators   --&gt;       \"rcsb_search_context\" : [ \"exact-match\" ],\n                                             \"rcsb_full_text_priority\" : 10,\n                                             \"rcsb_description\" : [\n                                                {\"text\" : \"The type of symmetry applied to the reconstruction\",\n                                                 \"context\" : \"dictionary\"},\n                                                {\"text\" : \"Symmetry Type (Em 3d Reconstruction)\", \"context\" : \"brief\"} ]\n                                            },\n                          ... },\n          \"title\" : \"Core Metadata\", \"additionalProperties\" : false, \"$comment\" : \"Schema version: 1.14.0\"\n          \"required\" : [\"rcsb_id\", \"rcsb_entry_container_identifiers\", \"rcsb_entry_info\",\n                        \"rcsb_pubmed_container_identifiers\", \"rcsb_polymer_entity_container_identifiers\",\n                        \"rcsb_assembly_container_identifiers\", \"rcsb_uniprot_container_identifiers\" ],\n          \"$schema\" : \"http://json-schema.org/draft-07/schema#\",\n          \"description\" : \"Collective JSON schema that includes definitions for all indexed cores with RCSB metadata extensions.\",\n         }\n    Returns:\n        (dict): {attribute: {'dtype': 'string', 'description': 'XYZ', 'operators': {'equals'}, 'choices': []}, ...}\n    \"\"\"\n    schema_pairs = {'dtype': 'type', 'description': 'description', 'operators': 'rcsb_search_context',\n                    'choices': 'enum'}\n    operator_d = {'full-text': 'contains_words, contains_phrase, exists', 'exact-match': 'in, exact_match, exists',\n                  'default-match': 'equals, greater, less, greater_or_equal, less_or_equal, range, range_closed, '\n                                   'exists', 'suggest': None}\n    # Types of rcsb_search_context: (can be multiple)\n    # full-text - contains_words, contains_phrase, exists\n    # exact-match - in, exact-match, exists\n    # default-match - equals, greater, less, greater_or_equal, less_or_equal, range, range_closed, exists\n    # suggests - provides an example to the user in the GUI\n    data_types = ['string', 'integer', 'number']\n\n    def recurse_metadata(metadata_d, stack=tuple()):  # this puts the yield inside a local iter so we don't return\n        for attribute in metadata_d:\n            if metadata_d[attribute]['type'] == 'array':  # 'items' must be a keyword in dictionary\n                # stack += (attribute, 'a')\n                if metadata_d[attribute]['items']['type'] in data_types:  # array is the final attribute of the branch\n                    yield stack + (attribute, 'a')\n                elif metadata_d[attribute]['items']['type'] == 'object':  # type must be object, therefore contain 'properties' key and then have more attributes as leafs\n                    yield from recurse_metadata(metadata_d[attribute]['items']['properties'], stack=stack + ((attribute, 'a', 'o',)))\n                else:\n                    logger.debug('Array with type %s found in %s' % (metadata_d[attribute], stack))\n            elif metadata_d[attribute]['type'] == 'object':  # This should never be reachable?\n                # print('%s object found %s' % (attribute, stack))\n                if 'properties' in metadata_d[attribute]:  # check may be unnecessary\n                    yield from recurse_metadata(metadata_d[attribute]['properties'], stack=stack + (attribute, 'o',))\n                else:\n                    logger.debug('Object with no properties found %s in %s' % (metadata_d[attribute], stack))\n                    # yield stack + ('o', attribute,)\n            elif metadata_d[attribute]['type'] in data_types:\n                yield stack + (attribute,)  # + ('o', attribute,) add 'o' as the parent had properties from the object type\n            else:\n                logger.debug('other type = %s' % metadata_d[attribute]['type'])\n\n    if not os.path.exists(file) or force_update:  # Todo and date.datetime - date.current is not greater than a month...\n        logger.info('Gathering the most current PDB metadata. This may take a couple minutes...')\n        metadata_json = requests.get(attribute_metadata_schema_json).json()\n        metadata_properties_d = metadata_json['properties']\n        gen_schema = recurse_metadata(metadata_properties_d)\n        schema_header_tuples = [yield_schema for yield_schema in gen_schema]\n\n        schema_dictionary_strings_d = {'a': \"['items']\", 'o': \"['properties']\"}  # 'a': \"['items']['properties']\"\n        schema_d = {}\n        for i, attribute_tuple in enumerate(schema_header_tuples):\n            attribute_full = '.'.join(attribute for attribute in attribute_tuple\n                                      if attribute not in schema_dictionary_strings_d)\n            if i &lt; 5:\n                logger.debug(attribute_full)\n            schema_d[attribute_full] = {}\n            d_search_string = ''.join(f\"['{attribute}']\" if attribute not in schema_dictionary_strings_d\n                                      else schema_dictionary_strings_d[attribute] for attribute in attribute_tuple)\n            evaluation_d = eval(f'{metadata_properties_d}{d_search_string}')\n            for key, value in schema_pairs.items():\n                if value in evaluation_d:\n                    schema_d[attribute_full][key] = evaluation_d[value]\n                else:\n                    schema_d[attribute_full][key] = None\n\n            if 'format' in evaluation_d:\n                schema_d[attribute_full]['dtype'] = 'date'\n\n            if schema_d[attribute_full]['description']:  # convert the description to a simplified descriptor\n                schema_d[attribute_full]['description'] = schema_d[attribute_full]['description'].split('\\n')[0]\n\n            if schema_d[attribute_full]['operators']:  # convert the rcsb_search_context to valid operator(s)\n                schema_d[attribute_full]['operators'] = set(', '.join(\n                    operator_d[search_context] for search_context in schema_d[attribute_full]['operators']\n                    if operator_d[search_context]).split(', '))\n            else:\n                if search_only:  # remove entries that don't have a corresponding operator as these aren't searchable\n                    schema_d.pop(attribute_full)\n\n        pickled_schema_file = utils.pickle_object(schema_d, file, out_path='')\n    else:\n        return utils.unpickle(file)\n\n    return schema_d\n</code></pre>"},{"location":"reference/resources/query/uniprot/","title":"uniprot","text":""},{"location":"reference/resources/query/uniprot/#resources.query.uniprot.query_uniprot","title":"query_uniprot","text":"<pre><code>query_uniprot(uniprot_id: str) -&gt; Response | None\n</code></pre> <p>Fetch the data from a specified UniProtID from the UniProt REST API</p> <p>Parameters:</p> <ul> <li> <code>uniprot_id</code>             (<code>str</code>)         \u2013          <p>The formatted UniProtID which consists of either a 6 or 10 character code</p> </li> </ul> <p>Returns:     The UniProtID entry JSON as a dictionary if query was successful</p> Source code in <code>symdesign/resources/query/uniprot.py</code> <pre><code>def query_uniprot(uniprot_id: str) -&gt; Response | None:\n    \"\"\"Fetch the data from a specified UniProtID from the UniProt REST API\n\n    Args:\n        uniprot_id: The formatted UniProtID which consists of either a 6 or 10 character code\n    Returns:\n        The UniProtID entry JSON as a dictionary if query was successful\n    \"\"\"\n    # Typical response fields from Q9HIA7 (PDB ID: 1NOG)\n    # {'entryType': 'UniProtKB unreviewed (TrEMBL)', 'primaryAccession': 'Q9HIA7', 'uniProtkbId': 'Q9HIA7_THEAC',\n    #  'entryAudit': {'firstPublicDate': '2001-03-01', 'lastAnnotationUpdateDate':\n    #                 '2021-06-02', 'lastSequenceUpdateDate': '2001-03-01', 'entryVersion': 97, 'sequenceVersion': 1},\n    #  'annotationScore': 15.100000000000001,\n    #  'organism': {'scientificName':\n    #               'Thermoplasma acidophilum (strain ATCC 25905 / DSM 1728 / JCM 9062 / NBRC 15155 / AMRC-C165)',\n    #               'taxonId': 273075,\n    #               'evidences': [{'evidenceCode': 'ECO:0000313', 'source': 'Proteomes', 'id': 'UP000001024'}],\n    #               'lineage': ['Archaea', 'Candidatus Thermoplasmatota', 'Thermoplasmata', 'Thermoplasmatales',\n    #                           'Thermoplasmataceae', 'Thermoplasma']},\n    #  'proteinExistence': '1: Evidence at protein level',\n    #  'proteinDescription': {'recommendedName': {'fullName': {'evidences': [{'evidenceCode': 'ECO:0000259', 'source':\n    #                                                                         'Pfam', 'id': 'PF01923'}],\n    #                                                          'value': 'Cob_adeno_trans domain-containing protein'}}},\n    #  'genes': [{'orderedLocusNames': [{'evidences': [{'evidenceCode': 'ECO:0000313', 'source': 'EMBL',\n    #                                                   'id': 'CAC12554.1'}],\n    #                                    'value': 'Ta1434'}]}],\n    #  'comments': [{'texts': [{'evidences': [{'evidenceCode': 'ECO:0000256', 'source': 'ARBA', 'id': 'ARBA00007487'}],\n    #                           'value': 'Belongs to the Cob(I)alamin adenosyltransferase family'}],\n    #                           'commentType': 'SIMILARITY'}],\n    #  'features': [\n    #   {'type': 'Domain',\n    #    'location': {'start': {'value': 2, 'modifier': 'EXACT'},\n    #                 'end': {'value': 158, 'modifier': 'EXACT'}},\n    #    'description': 'Cob_adeno_trans',\n    #    'evidences': [{'evidenceCode': 'ECO:0000259', 'source': 'Pfam', 'id': 'PF01923'}]\n    #    }],\n    #  'keywords': [{'evidences': [{'evidenceCode': 'ECO:0007829', 'source': 'PDB', 'id': '1NOG'}],\n    #                'id': 'KW-0002', 'category': 'Technical term', 'name': '3D-structure'},\n    #               {'evidences': [{'evidenceCode': 'ECO:0000256', 'source': 'ARBA',\n    #                               'id': 'ARBA00022840'}],\n    #                'id': 'KW-0067', 'category': 'Ligand', 'name': 'ATP-binding'},\n    #               {'evidences': [{'evidenceCode': 'ECO:0000256', 'source': 'ARBA',\n    #                               'id': 'ARBA00022741'}],\n    #                'id': 'KW-0547', 'category': 'Ligand', 'name': 'Nucleotide-binding'},\n    #               {'evidences': [{'evidenceCode': 'ECO:0000313', 'source': 'Proteomes',\n    #                               'id': 'UP000001024'}],\n    #                'id': 'KW-1185', 'category': 'Technical term', 'name': 'Reference proteome'},\n    #               {'evidences': [{'evidenceCode': 'ECO:0000256', 'source': 'ARBA',\n    #                               'id': 'ARBA00022679'}],\n    #                'id': 'KW-0808', 'category': 'Molecular function', 'name': 'Transferase'}],\n    #  'references': [\n    #   {'citation': {'id': '11029001', 'citationType': 'journal article',\n    #                 'authors': ['Ruepp A.', 'Graml W.', 'Santos-Martinez M.L.',\n    #                             'Koretke K.K.', 'Volker C.', 'Mewes H.W.', 'Frishman D.',\n    #                             'Stocker S.', 'Lupas A.N.', 'Baumeister W.'],\n    #                 'citationCrossReferences': [{'database': 'PubMed', 'id': '11029001'},\n    #                                             {'database': 'DOI', 'id': '10.1038/35035069'}],\n    #                 'title': 'The genome sequence of the thermoacidophilic scavenger '\n    #                          'Thermoplasma acidophilum.',\n    #                 'publicationDate': '2000',\n    #                 'journal': 'Nature',\n    #                 'firstPage': '508', 'lastPage': '513', 'volume': '407'},\n    #    'referencePositions': ['NUCLEOTIDE SEQUENCE [LARGE SCALE GENOMIC DNA]'],\n    #    'referenceComments': [{'evidences': [{'evidenceCode': 'ECO:0000313',\n    #                                         'source': 'Proteomes', 'id': 'UP000001024'}],\n    #                          'value': 'ATCC 25905 / DSM 1728 / JCM 9062 / NBRC 15155 / AM'\n    #                                   'RC-C165',\n    #                          'type': 'STRAIN'}],\n    #    'evidences': [{'evidenceCode': 'ECO:0000313', 'source': 'EMBL', 'id': 'CAC12554.1'},\n    #                  {'evidenceCode': 'ECO:0000313', 'source': 'Proteomes', 'id': 'UP000001024'}]},\n    #   {'citation': {'id': '15044458', 'citationType': 'journal article',\n    #                 'authors': ['Saridakis V.', 'Yakunin A.', 'Xu X.', 'Anandakumar P.',\n    #                             'Pennycooke M.', 'Gu J.', 'Cheung F.', 'Lew J.M.',\n    #                             'Sanishvili R.', 'Joachimiak A.', 'Arrowsmith C.H.',\n    #                             'Christendat D.', 'Edwards A.M.'],\n    #                 'citationCrossReferences': [{'database': 'PubMed', 'id': '15044458'},\n    #                                             {'database': 'DOI',\n    #                                              'id': '10.1074/jbc.M401395200'}],\n    #                 'title': 'The structural basis for methylmalonic aciduria. The '\n    #                          'crystal structure of archaeal ATP:cobalamin '\n    #                          'adenosyltransferase.',\n    #                 'publicationDate': '2004', 'journal': 'J. Biol. Chem.',\n    #                 'firstPage': '23646', 'lastPage': '23653', 'volume': '279'},\n    #    'referencePositions': ['X-RAY CRYSTALLOGRAPHY (1.55 ANGSTROMS)'],\n    #    'evidences': [{'evidenceCode': 'ECO:0007829', 'source': 'PDB', 'id': '1NOG'}]}\n    #    ],\n    #  'uniProtKBCrossReferences': [{'database': 'EMBL', 'id': 'AL445067',\n    #                                'properties': [{'key': 'ProteinId', 'value': 'CAC12554.1'},\n    #                                               {'key': 'Status', 'value': '-'},\n    #                                               {'key': 'MoleculeType', 'value': 'Genomic_DNA'}]},\n    #                               {'database': 'RefSeq', 'id': 'WP_010901837.1',\n    #                                'properties': [{'key': 'NucleotideSequenceId', 'value': 'NC_002578.1'}]},\n    #                               {'database': 'PDB', 'id': '1NOG',\n    #                                'properties': [{'key': 'Method', 'value': 'X-ray'},\n    #                                               {'key': 'Resolution', 'value': '1.55 A'},\n    #                                               {'key': 'Chains', 'value': 'A=1-177'}]},\n    #                               {'database': 'PDBsum', 'id': '1NOG',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'SMR', 'id': 'Q9HIA7',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'STRING', 'id': '273075.Ta1434',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'DNASU', 'id': '1456890',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'EnsemblBacteria', 'id': 'CAC12554',\n    #                                'properties': [{'key': 'ProteinId', 'value': 'CAC12554'},\n    #                                               {'key': 'GeneId', 'value': 'CAC12554'}]},\n    #                               {'database': 'GeneID', 'id': '1456890',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'KEGG', 'id': 'tac:Ta1434',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'eggNOG', 'id': 'arCOG00489',\n    #                                'properties': [{'key': 'ToxonomicScope', 'value': 'Archaea'}]},\n    #                               {'database': 'HOGENOM', 'id': 'CLU_083486_0_1_2',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'OMA', 'id': 'HQACTVV',\n    #                                'properties': [{'key': 'Fingerprint', 'value': '-'}]},\n    #                               {'database': 'OrthoDB', 'id': '98914at2157',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'BRENDA', 'id': '2.5.1.17',\n    #                                'properties': [{'key': 'OrganismId', 'value': '6324'}]},\n    #                               {'database': 'EvolutionaryTrace', 'id': 'Q9HIA7',\n    #                                'properties': [{'key': 'Description', 'value': '-'}]},\n    #                               {'database': 'Proteomes', 'id': 'UP000001024',\n    #                                'properties': [{'key': 'Component', 'value': 'Chromosome'}]},\n    #                               {'database': 'GO', 'id': 'GO:0005524',\n    #                                'properties': [{'key': 'GoTerm', 'value': 'F:ATP binding'},\n    #                                               {'key': 'GoEvidenceType', 'value': 'IEA:UniProtKB-KW'}]},\n    #                               {'database': 'GO', 'id': 'GO:0016740',\n    #                                'properties': [{'key': 'GoTerm', 'value': 'F:transferase activity'},\n    #                                               {'key': 'GoEvidenceType', 'value': 'IEA:UniProtKB-KW'}]},\n    #                               {'database': 'Gene3D', 'id': '1.20.1200.10',\n    #                                'properties': [{'key': 'EntryName', 'value': '-'},\n    #                                               {'key': 'MatchStatus', 'value': '1'}]},\n    #                               {'database': 'InterPro', 'id': 'IPR016030',\n    #                                'properties': [{'key': 'EntryName', 'value': 'CblAdoTrfase-like'}]},\n    #                               {'database': 'InterPro', 'id': 'IPR036451',\n    #                                'properties': [{'key': 'EntryName', 'value': 'CblAdoTrfase-like_sf'}]},\n    #                               {'database': 'InterPro', 'id': 'IPR029499',\n    #                                'properties': [{'key': 'EntryName', 'value': 'PduO-typ'}]},\n    #                               {'database': 'PANTHER', 'id': 'PTHR12213',\n    #                                'properties': [{'key': 'EntryName', 'value': 'PTHR12213'},\n    #                                               {'key': 'MatchStatus', 'value': '1'}]},\n    #                               {'database': 'Pfam', 'id': 'PF01923',\n    #                                'properties': [{'key': 'EntryName', 'value': 'Cob_adeno_trans'},\n    #                                               {'key': 'MatchStatus', 'value': '1'}]},\n    #                               {'database': 'SUPFAM', 'id': 'SSF89028',\n    #                                'properties': [{'key': 'EntryName', 'value': 'SSF89028'},\n    #                                               {'key': 'MatchStatus', 'value': '1'}]},\n    #                               {'database': 'TIGRFAMs', 'id': 'TIGR00636',\n    #                                'properties': [{'key': 'EntryName', 'value': 'PduO_Nterm'},\n    #                                               {'key': 'MatchStatus', 'value': '1'}]}\n    #                               ],\n    #  'sequence': {'value': 'MFTRRGDQGETDLANRARVGKDSPVVEVQGTIDELNSFIGYALVLSRWDDIRNDLFRIQNDLFVLGEDVSTGGKG'\n    #                        'RTVTREMIDYLEARVKEMKAEIGKIELFVVPGGSIESASLHMARAVSRRLERRIVAASKLTEINKNVLIYANRLS'\n    #                        'SILFMHALLSNKRLNIPEKIWSIHRVS', 'length': 177, 'molWeight': 20013,\n    #                        'crc64': '13D3B46CB3ED92F6', 'md5': '5D8AFDAE2BFCB93431C348E2A0172D96'},\n    #  'extraAttributes': {'countByCommentType': {'SIMILARITY': 1}, 'countByFeatureType': {'Domain': 1},\n    #                      'uniParcId': 'UPI000006403F'}\n    #  }\n    if uniprot_id and len(uniprot_id) in [6, 10]:\n        # query_url = f'https://rest.uniprot.org/uniprotkb/{uniprot_id}.json'\n        return connection_exception_handler(f'https://rest.uniprot.org/uniprotkb/{uniprot_id}.json')\n    else:\n        # logger.warning('UniProt ID \"%s\" is not of the required format and will not be found with the UniProt API'\n        #                % uniprot_id)\n        return None\n</code></pre>"},{"location":"reference/sequence/","title":"sequence","text":""},{"location":"reference/sequence/#sequence.protein_letters_alph1","title":"protein_letters_alph1  <code>module-attribute</code>","text":"<pre><code>protein_letters_alph1: str = join(get_args(protein_letters_alph1_literal))\n</code></pre> <p>ACDEFGHIKLMNPQRSTVWY</p>"},{"location":"reference/sequence/#sequence.protein_letters_alph1_extended","title":"protein_letters_alph1_extended  <code>module-attribute</code>","text":"<pre><code>protein_letters_alph1_extended: str = join(get_args(protein_letters_alph1_extended_literal))\n</code></pre> <p>ACDEFGHIKLMNPQRSTVWYBXZJUO</p>"},{"location":"reference/sequence/#sequence.protein_letters_extended_and_gap","title":"protein_letters_extended_and_gap  <code>module-attribute</code>","text":"<pre><code>protein_letters_extended_and_gap: str = join(get_args(protein_letters_alph1_extended_and_gap_literal))\n</code></pre> <p>ACDEFGHIKLMNPQRSTVWYBXZJUO-</p>"},{"location":"reference/sequence/#sequence.ProfileDict","title":"ProfileDict  <code>module-attribute</code>","text":"<pre><code>ProfileDict = dict[int, ProfileEntry]\n</code></pre> <p>{1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...}, 'type': 'W', 'info': 0.00, 'weight': 0.00}, {...}}</p>"},{"location":"reference/sequence/#sequence.MutationEntry","title":"MutationEntry  <code>module-attribute</code>","text":"<pre><code>MutationEntry = TypedDict('MutationEntry', {'to': protein_letters_literal, 'from': protein_letters_literal})\n</code></pre> <p>Mapping of a reference sequence amino acid type, 'to', and the resulting sequence amino acid type, 'from'.</p>"},{"location":"reference/sequence/#sequence.mutation_dictionary","title":"mutation_dictionary  <code>module-attribute</code>","text":"<pre><code>mutation_dictionary = dict[int, MutationEntry]\n</code></pre> <p>The mapping of a residue number to a mutation entry containing the reference, 'to', and sequence, 'from', amino acid  type.</p>"},{"location":"reference/sequence/#sequence.sequence_dictionary","title":"sequence_dictionary  <code>module-attribute</code>","text":"<pre><code>sequence_dictionary = dict[int, protein_letters_literal]\n</code></pre> <p>The mapping of a residue number to the corresponding amino acid type.</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment","title":"MultipleSequenceAlignment","text":"<pre><code>MultipleSequenceAlignment(alignment: MultipleSeqAlignment, aligned_sequence: str = None, alphabet: str = protein_letters_alph1_gaped, **kwargs)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>alignment</code>             (<code>MultipleSeqAlignment</code>)         \u2013          <p>A MultipleSeqAlignment object which contains an array-like object of SeqRecords</p> </li> <li> <code>aligned_sequence</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Provide the sequence on which the alignment is based, otherwise the first sequence will be used from the alignment</p> </li> <li> <code>alphabet</code>             (<code>str</code>, default:                 <code>protein_letters_alph1_gaped</code> )         \u2013          <p>'ACDEFGHIKLMNPQRSTVWY-'</p> </li> </ul> Sets <p>self.alignment - (Bio.Align.MultipleSeqAlignment) self.number_of_sequences - 214 self.query - 'MGSTHLVLK...' from aligned_sequence argument OR alignment argument, index 0 self.query_with_gaps - 'MGS--THLVLK...' self.counts - {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...} self.frequencies - {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...} self.observations - {1: 210, 2:211, ...}</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def __init__(self, alignment: MultipleSeqAlignment, aligned_sequence: str = None,\n             alphabet: str = protein_letters_alph1_gaped, **kwargs):\n    \"\"\"Take a Biopython MultipleSeqAlignment object and process for residue specific information. One-indexed\n\n    Args:\n        alignment: A MultipleSeqAlignment object which contains an array-like object of SeqRecords\n        aligned_sequence: Provide the sequence on which the alignment is based, otherwise the first\n            sequence will be used from the alignment\n        alphabet: 'ACDEFGHIKLMNPQRSTVWY-'\n\n    Sets:\n        self.alignment - (Bio.Align.MultipleSeqAlignment)\n        self.number_of_sequences - 214\n        self.query - 'MGSTHLVLK...' from aligned_sequence argument OR alignment argument, index 0\n        self.query_with_gaps - 'MGS--THLVLK...'\n        self.counts - {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...}\n        self.frequencies - {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...}\n        self.observations - {1: 210, 2:211, ...}\n    \"\"\"\n    # count_gaps: bool = False\n    # count_gaps: Whether gaps (-) should be counted in column weights\n    # if alignment is None:\n    #     raise NotImplementedError(\n    #         f\"Can't create a {MultipleSequenceAlignment.__name__} with alignment=None\")\n\n    self.alignment = alignment\n    self.alphabet = alphabet\n    if aligned_sequence is None:\n        self.query_aligned = str(alignment[0].seq)\n    else:\n        self.query_aligned = aligned_sequence\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.query","title":"query  <code>instance-attribute</code>","text":"<pre><code>query: str\n</code></pre> <p>The sequence used to perform the MultipleSequenceAlignment search</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.sequence_weights","title":"sequence_weights  <code>property</code> <code>writable</code>","text":"<pre><code>sequence_weights: list[float]\n</code></pre> <p>Weights for each sequence in the alignment. Default is based on the sequence \"surprise\", however provided weights can also be set</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.alphabet_length","title":"alphabet_length  <code>property</code>","text":"<pre><code>alphabet_length\n</code></pre> <p>The number of sequence characters in the character alphabet</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.counts_by_position","title":"counts_by_position  <code>property</code>","text":"<pre><code>counts_by_position: ndarray\n</code></pre> <p>The counts of each alphabet character for each residue position in the alignment with shape (number of residues, alphabet size)</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.gap_index","title":"gap_index  <code>property</code>","text":"<pre><code>gap_index: int\n</code></pre> <p>The index in the alphabet where the gap character resides</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.query_aligned","title":"query_aligned  <code>property</code> <code>writable</code>","text":"<pre><code>query_aligned: str\n</code></pre> <p>The sequence used to create the MultipleSequenceAlignment potentially containing gaps from alignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.query_length","title":"query_length  <code>property</code>","text":"<pre><code>query_length: int\n</code></pre> <p>The number of residues in the MultipleSequenceAlignment query</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.length","title":"length  <code>property</code>","text":"<pre><code>length: int\n</code></pre> <p>The number of sequences in the MultipleSequenceAlignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.number_of_positions","title":"number_of_positions  <code>property</code>","text":"<pre><code>number_of_positions: int\n</code></pre> <p>The number of residues plus gaps found in each sequence of the MultipleSequenceAlignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.sequences","title":"sequences  <code>property</code>","text":"<pre><code>sequences: Iterator[str]\n</code></pre> <p>Iterate over the sequences present in the alignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.sequence_identifiers","title":"sequence_identifiers  <code>property</code>","text":"<pre><code>sequence_identifiers: list[str]\n</code></pre> <p>Return the identifiers associated with each sequence in the alignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.alphabet_type","title":"alphabet_type  <code>property</code> <code>writable</code>","text":"<pre><code>alphabet_type: alphabet_types_literal\n</code></pre> <p>The type of alphabet that the alignment is mapped to numerically</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.query_indices","title":"query_indices  <code>property</code>","text":"<pre><code>query_indices: ndarray\n</code></pre> <p>View the query as a boolean array (1, sequence_length) where gap positions, \"-\", are False</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.sequence_indices","title":"sequence_indices  <code>property</code> <code>writable</code>","text":"<pre><code>sequence_indices: ndarray\n</code></pre> <p>View the alignment as a boolean array (length, number_of_positions) where gap positions, \"-\", are False</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.numerical_alignment","title":"numerical_alignment  <code>property</code>","text":"<pre><code>numerical_alignment: ndarray\n</code></pre> <p>Return the alignment as an integer array (length, number_of_positions) of the amino acid characters</p> <p>Maps the instance .alphabet characters to their resulting sequence index</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.array","title":"array  <code>property</code>","text":"<pre><code>array: ndarray\n</code></pre> <p>Return the alignment as a character array (length, number_of_positions) with numpy.string_ dtype</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.deletion_matrix","title":"deletion_matrix  <code>property</code>","text":"<pre><code>deletion_matrix: ndarray\n</code></pre> <p>Return the number of deletions at every query aligned sequence index for each sequence in the alignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.frequencies","title":"frequencies  <code>property</code>","text":"<pre><code>frequencies: ndarray\n</code></pre> <p>Access the per-residue, alphabet frequencies with shape (number of residues, alphabet characters). Bounded between 0 and 1</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.observations_by_position","title":"observations_by_position  <code>property</code>","text":"<pre><code>observations_by_position: ndarray\n</code></pre> <p>The number of sequences with observations at each residue position in the alignment</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.gaps_per_position","title":"gaps_per_position  <code>property</code>","text":"<pre><code>gaps_per_position: ndarray\n</code></pre> <p>The number of gaped letters at each position in the sequence with shape (number of residues,)</p>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.from_dictionary","title":"from_dictionary  <code>classmethod</code>","text":"<pre><code>from_dictionary(named_sequences: dict[str, str], **kwargs) -&gt; MultipleSequenceAlignment\n</code></pre> <p>Create a MultipleSequenceAlignment from a dictionary of named sequences</p> <p>Parameters:</p> <ul> <li> <code>named_sequences</code>             (<code>dict[str, str]</code>)         \u2013          <p>Where name and sequence must be a string, i.e. {'1': 'MNTEELQVAAFEI...', ...}</p> </li> </ul> <p>Returns:     The MultipleSequenceAlignment object for the provided sequences</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>@classmethod\ndef from_dictionary(cls, named_sequences: dict[str, str], **kwargs) -&gt; MultipleSequenceAlignment:\n    \"\"\"Create a MultipleSequenceAlignment from a dictionary of named sequences\n\n    Args:\n        named_sequences: Where name and sequence must be a string, i.e. {'1': 'MNTEELQVAAFEI...', ...}\n    Returns:\n        The MultipleSequenceAlignment object for the provided sequences\n    \"\"\"\n    return cls(alignment=MultipleSeqAlignment(\n        [SeqRecord(Seq(sequence), id=name)  # annotations={'molecule_type': 'Protein'},\n         for name, sequence in named_sequences.items()]), **kwargs)\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.from_seq_records","title":"from_seq_records  <code>classmethod</code>","text":"<pre><code>from_seq_records(seq_records: Iterable[SeqRecord], **kwargs) -&gt; MultipleSequenceAlignment\n</code></pre> <p>Create a MultipleSequenceAlignment from a SeqRecord Iterable</p> <p>Parameters:</p> <ul> <li> <code>seq_records</code>             (<code>Iterable[SeqRecord]</code>)         \u2013          <p>{name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>@classmethod\ndef from_seq_records(cls, seq_records: Iterable[SeqRecord], **kwargs) -&gt; MultipleSequenceAlignment:\n    \"\"\"Create a MultipleSequenceAlignment from a SeqRecord Iterable\n\n    Args:\n        seq_records: {name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}\n    \"\"\"\n    return cls(alignment=MultipleSeqAlignment(seq_records), **kwargs)\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.weight_alignment_by_sequence","title":"weight_alignment_by_sequence","text":"<pre><code>weight_alignment_by_sequence() -&gt; list[float]\n</code></pre> <p>Measure diversity/surprise when comparing a single alignment entry to the rest of the alignment</p> <p>Default means for weighting sequences. Important for creating representative sequence populations in the MSA as was described by Heinkoff and Heinkoff, 1994 (PMID: 7966282)</p> <p>Operation is: SUM(1 / (column_j_aa_representation * aa_ij_count))</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>Weight of each sequence in the MSA - [2.390, 2.90, 5.33, 1.123, ...]</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def weight_alignment_by_sequence(self) -&gt; list[float]:\n    \"\"\"Measure diversity/surprise when comparing a single alignment entry to the rest of the alignment\n\n    Default means for weighting sequences. Important for creating representative sequence populations in the MSA as\n    was described by Heinkoff and Heinkoff, 1994 (PMID: 7966282)\n\n    Operation is: SUM(1 / (column_j_aa_representation * aa_ij_count))\n\n    Returns:\n        Weight of each sequence in the MSA - [2.390, 2.90, 5.33, 1.123, ...]\n    \"\"\"\n    # create a 1/ obs * counts = positional_weights\n    #  alignment.number_of_positions - 0   1   2  ...\n    #      / obs 0 [[32]   count seq 0 '-' -  2   0   0  ...   [[ 64   0   0 ...]  \\\n    # 1 / |  obs 1  [33] * count seq 1 'A' - 10  10   0  ... =  [330 330   0 ...]   |\n    #      \\ obs 2  [33]   count seq 2 'C' -  8   8   1  ...    [270 270  33 ...]] /\n    #   ...   ...]               ...  ... ... ...\n    position_weights = 1 / (self.observations_by_position[None, :] * self.counts_by_position)\n    # take_along_axis from this with the transposed numerical_alignment (na) where each successive na idx\n    # is the sequence position at the na and therefore is grabbing the position_weights by that index\n    # finally sum along each sequence\n    # The position_weights count seq idx must be taken by a sequence index. This happens to be on NA axis 1\n    # at the moment so specified with .T and take using axis=0. Keeping both as axis=0 doen't index\n    # correctly. Maybe this is a case where 'F' array ordering is needed?\n    sequence_weights = np.take_along_axis(position_weights, self.numerical_alignment.T, axis=0).sum(axis=0)\n    logger.critical('New sequence_weights_', sequence_weights)\n\n    # # Old calculation\n    # counts_ = [[0 for _ in self.alphabet] for _ in range(self.number_of_positions)]  # list[list]\n    # for sequence in self.sequences:\n    #     for _count, aa in zip(counts_, sequence):\n    #         _count[numerical_translation_alph1_gaped[aa]] += 1\n    #         # self.counts[i][aa] += 1\n    #\n    # self._counts = counts_\n    # logger.critical('OLD self._counts', self._counts)\n    # self._observations = [sum(_aa_counts[:self.gap_index]) for _aa_counts in self._counts]  # list[list]\n\n    observations_by_position = self.observations_by_position\n    counts_by_position = self.counts_by_position\n    numerical_alignment = self.numerical_alignment\n    # sequence_weights_ = weight_sequences(self._counts, self.alignment, self.observations_by_position)\n    # sequence_weights_ = weight_sequences(counts_by_position, self.alignment, observations_by_position)\n    sequence_weights_ = []\n    for sequence_idx in range(self.length):\n        sequence_weights_.append(\n            (1 / (observations_by_position * counts_by_position[numerical_alignment[sequence_idx]])).sum())\n\n    logger.critical('OLD sequence_weights_', sequence_weights_)\n\n    return sequence_weights\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.update_counts_by_position_with_sequence_weights","title":"update_counts_by_position_with_sequence_weights","text":"<pre><code>update_counts_by_position_with_sequence_weights()\n</code></pre> <p>Overwrite the current counts with weighted counts</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def update_counts_by_position_with_sequence_weights(self):  # UNUSED\n    \"\"\"Overwrite the current counts with weighted counts\"\"\"\n    # Add each sequence weight to the indices indicated by the numerical_alignment\n    self._counts_by_position = np.zeros((self.number_of_positions, self.alphabet_length))\n    numerical_alignment = self.numerical_alignment\n    sequence_weights = self.sequence_weights\n    try:\n        for sequence_idx in range(self.length):\n            self._counts_by_position[:, numerical_alignment[sequence_idx]] += sequence_weights[sequence_idx]\n    except IndexError:  # sequence_weights is the wrong length\n        raise IndexError(\n            f\"Couldn't index the provided 'sequence_weights' with length {len(sequence_weights)}\")\n    logger.info('sequence_weight self.counts', self.counts_by_position)\n    logger.info('May need to refactor weight sequences() to MultipleSequenceAlignment. Take particular care in '\n                'putting the alignment back together after .insert()/.delete() &lt;- if written')\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.get_probabilities_from_profile","title":"get_probabilities_from_profile","text":"<pre><code>get_probabilities_from_profile(profile: numerical_profile) -&gt; ndarray\n</code></pre> <p>For each sequence in the alignment, extract the values from a profile corresponding to the amino acid type of each residue in each sequence</p> <p>Parameters:</p> <ul> <li> <code>profile</code>             (<code>numerical_profile</code>)         \u2013          <p>A profile of values with shape (length, alphabet_length) where length is the number_of_positions</p> </li> </ul> <p>Returns:     The array with shape (length, number_of_positions) with the value for each amino acid index in profile</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def get_probabilities_from_profile(self, profile: numerical_profile) -&gt; np.ndarray:\n    \"\"\"For each sequence in the alignment, extract the values from a profile corresponding to the amino acid type\n    of each residue in each sequence\n\n    Args:\n        profile: A profile of values with shape (length, alphabet_length) where length is the number_of_positions\n    Returns:\n        The array with shape (length, number_of_positions) with the value for each amino acid index in profile\n    \"\"\"\n    # transposed_alignment = self.numerical_alignment.T\n    return np.take_along_axis(profile, self.numerical_alignment.T, axis=1).T\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.insert","title":"insert","text":"<pre><code>insert(at: int, sequence: str, msa_index: bool = False)\n</code></pre> <p>Insert new sequence in the MultipleSequenceAlignment where the added sequence is added to all columns</p> <p>Parameters:</p> <ul> <li> <code>at</code>             (<code>int</code>)         \u2013          <p>The index to insert the sequence at. By default, the index is in reference to where self.query_indices are True, i.e. the query sequence</p> </li> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence to insert. Will be inserted for every sequence of the alignment</p> </li> <li> <code>msa_index</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the insertion index is in the frame of the entire multiple sequence alignment. Default, False, indicates the index is in the frame of the query sequence index, i.e. no gaps</p> </li> </ul> Sets <p>self.alignment: The existing alignment updated with the new sequence in alignment form</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def insert(self, at: int, sequence: str, msa_index: bool = False):\n    \"\"\"Insert new sequence in the MultipleSequenceAlignment where the added sequence is added to all columns\n\n    Args:\n        at: The index to insert the sequence at. By default, the index is in reference to where self.query_indices\n            are True, i.e. the query sequence\n        sequence: The sequence to insert. Will be inserted for every sequence of the alignment\n        msa_index: Whether the insertion index is in the frame of the entire multiple sequence alignment.\n            Default, False, indicates the index is in the frame of the query sequence index, i.e. no gaps\n\n    Sets:\n        self.alignment: The existing alignment updated with the new sequence in alignment form\n    \"\"\"\n    if msa_index:\n        at = at\n    else:\n        try:  # To get the index 'at' for those indices that are present in the query\n            at = np.flatnonzero(self.query_indices)[at]\n        except IndexError:  # This index is outside of query\n            if at &gt;= self.query_length:\n                # Treat as append\n                at = self.number_of_positions\n            else:\n                raise NotImplementedError(f\"Couldn't index with a negative index...\")\n    begin_slice = slice(at)\n    end_slice = slice(at, None)\n\n    logger.debug(f'Insertion is occurring at {self.__class__.__name__} index {at}')\n\n    new_sequence = Seq(sequence)\n    new_alignment = MultipleSeqAlignment(\n        [SeqRecord(new_sequence, id=id_)  # annotations={'molecule_type': 'Protein'},\n         for id_ in self.sequence_identifiers])\n\n    logger.debug(f'number of sequences in new_alignment: {len(new_alignment)}')\n    start_alignment_slice = self.alignment[:, begin_slice]\n    start_alignment_len = len(start_alignment_slice)\n    if start_alignment_len:\n        logger.debug(f'number of sequences in start_alignment_slice: {start_alignment_len}')\n        new_alignment = start_alignment_slice + new_alignment\n\n    end_alignment_slice = self.alignment[:, end_slice]\n    end_alignment_len = len(end_alignment_slice)\n    if end_alignment_len:\n        logger.debug(f'number of sequences in end_alignment_slice: {end_alignment_len}')\n        new_alignment = new_alignment + end_alignment_slice\n\n    # Set the alignment\n    self.alignment = new_alignment\n    self.query_aligned = str(new_alignment[0].seq)\n\n    # Update alignment dependent features\n    self.reset_state()\n    logger.debug(f'Inserted alignment has shape ({self.length}, {self.number_of_positions})')\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.reset_state","title":"reset_state","text":"<pre><code>reset_state()\n</code></pre> <p>Remove any state attributes</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def reset_state(self):\n    \"\"\"Remove any state attributes\"\"\"\n    for attr in ['_array', '_deletion_matrix', '_numerical_alignment', '_sequence_indices',\n                 '_sequence_identifiers', '_observations_by_position', '_counts_by_position', '_gaps_per_position',\n                 '_frequencies']:\n        try:\n            self.__delattr__(attr)\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"reference/sequence/#sequence.MultipleSequenceAlignment.pad_alignment","title":"pad_alignment","text":"<pre><code>pad_alignment(length, axis: int = 0)\n</code></pre> <p>Extend the alignment by a set length</p> <p>Parameters:</p> <ul> <li> <code>length</code>         \u2013          <p>The length to pad the alignment</p> </li> <li> <code>axis</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The axis to pad. 0 pads the sequences, 1 pads the residues</p> </li> </ul> Sets <p>self.alignment with the specified padding</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def pad_alignment(self, length, axis: int = 0):\n    \"\"\"Extend the alignment by a set length\n\n    Args:\n        length: The length to pad the alignment\n        axis: The axis to pad. 0 pads the sequences, 1 pads the residues\n\n    Sets:\n        self.alignment with the specified padding\n    \"\"\"\n    if axis == 0:\n        dummy_record = SeqRecord(Seq('-' * self.number_of_positions), id='dummy')\n        self.alignment.extend([dummy_record for _ in range(length)])\n        self.reset_state()\n    else:  # axis == 1\n        self.insert(self.number_of_positions, '-' * length, msa_index=True)\n\n    logger.debug(f'padded alignment has shape ({self.length}, {self.number_of_positions})')\n</code></pre>"},{"location":"reference/sequence/#sequence.create_numeric_translation_table","title":"create_numeric_translation_table","text":"<pre><code>create_numeric_translation_table(alphabet: Sequence[str], bytes_: bool = True) -&gt; dict[bytes | str, int]\n</code></pre> <p>Return the numeric translation from an alphabet to the integer position in that alphabet</p> <p>Parameters:</p> <ul> <li> <code>alphabet</code>             (<code>Sequence[str]</code>)         \u2013          <p>The alphabet to use. Example 'ARNDCQEGHILKMFPSTWYVBZX*'</p> </li> <li> <code>bytes_</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to map from byte characters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[bytes | str, int]</code>         \u2013          <p>The mapping from the character to the positional integer</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def create_numeric_translation_table(alphabet: Sequence[str], bytes_: bool = True) -&gt; dict[bytes | str, int]:\n    \"\"\"Return the numeric translation from an alphabet to the integer position in that alphabet\n\n    Args:\n        alphabet: The alphabet to use. Example 'ARNDCQEGHILKMFPSTWYVBZX*'\n        bytes_: Whether to map from byte characters\n\n    Returns:\n        The mapping from the character to the positional integer\n    \"\"\"\n    if bytes_:\n        alphabet = (char.encode() for char in alphabet)\n\n    return dict(zip(alphabet, count()))\n</code></pre>"},{"location":"reference/sequence/#sequence.get_numeric_translation_table","title":"get_numeric_translation_table","text":"<pre><code>get_numeric_translation_table(alphabet_type: alphabet_types_literal) -&gt; defaultdict[str, int] | dict[str, int]\n</code></pre> <p>Given an amino acid alphabet type, return the corresponding numerical translation table. If a table is passed, just return it</p> <p>Returns:</p> <ul> <li> <code>defaultdict[str, int] | dict[str, int]</code>         \u2013          <p>The integer mapping to the sequence of the requested alphabet</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def get_numeric_translation_table(alphabet_type: alphabet_types_literal) -&gt; defaultdict[str, int] | dict[str, int]:\n    \"\"\"Given an amino acid alphabet type, return the corresponding numerical translation table.\n    If a table is passed, just return it\n\n    Returns:\n        The integer mapping to the sequence of the requested alphabet\n    \"\"\"\n    # try:\n    #     match alphabet_type:\n    #         case 'protein_letters_alph1':\n    #             numeric_translation_table = numerical_translation_alph1_bytes\n    #         case 'protein_letters_alph3':\n    #             numeric_translation_table = numerical_translation_alph3_bytes\n    #         case 'protein_letters_alph1_gaped':\n    #             numeric_translation_table = numerical_translation_alph1_gaped_bytes\n    #         case 'protein_letters_alph3_gaped':\n    #             numeric_translation_table = numerical_translation_alph3_gaped_bytes\n    #         case 'protein_letters_alph1_unknown':\n    #             numeric_translation_table = numerical_translation_alph1_unknown_bytes\n    #         case 'protein_letters_alph3_unknown':\n    #             numeric_translation_table = numerical_translation_alph3_unknown_bytes\n    #         case 'protein_letters_alph1_unknown_gaped':\n    #             numeric_translation_table = numerical_translation_alph1_unknown_gaped_bytes\n    #         case 'protein_letters_alph3_unknown_gaped':\n    #             numeric_translation_table = numerical_translation_alph3_unknown_gaped_bytes\n    #         case _:\n    #             try:  # To see if we already have the alphabet, and just return defaultdict\n    #                 numeric_translation_table = alphabet_to_type[alphabet_type]\n    #             except KeyError:\n    #                 # raise ValueError(wrong_alphabet_type)\n    #                 logger.warning(wrong_alphabet_type)\n    #                 numeric_translation_table = create_numeric_translation_table(alphabet_type)\n    # except SyntaxError:  # python version not 3.10\n    if alphabet_type == 'protein_letters_alph1':\n        numeric_translation_table = numerical_translation_alph1_bytes\n    elif alphabet_type == 'protein_letters_alph3':\n        numeric_translation_table = numerical_translation_alph3_bytes\n    elif alphabet_type == 'protein_letters_alph1_gaped':\n        numeric_translation_table = numerical_translation_alph1_gaped_bytes\n    elif alphabet_type == 'protein_letters_alph3_gaped':\n        numeric_translation_table = numerical_translation_alph3_gaped_bytes\n    elif alphabet_type == 'protein_letters_alph1_unknown':\n        numeric_translation_table = numerical_translation_alph1_unknown_bytes\n    elif alphabet_type == 'protein_letters_alph3_unknown':\n        numeric_translation_table = numerical_translation_alph3_unknown_bytes\n    elif alphabet_type == 'protein_letters_alph1_unknown_gaped':\n        numeric_translation_table = numerical_translation_alph1_unknown_gaped_bytes\n    elif alphabet_type == 'protein_letters_alph3_unknown_gaped':\n        numeric_translation_table = numerical_translation_alph3_unknown_gaped_bytes\n    else:\n        try:  # To see if we already have the alphabet, and return the defaultdict\n            _type = alphabet_to_alphabet_type[alphabet_type]\n        except KeyError:\n            raise KeyError(\n                f\"The alphabet '{alphabet_type}' isn't an allowed alphabet_type.\"\n                f\" See {', '.join(alphabet_types)}\")\n            # raise ValueError(wrong_alphabet_type)\n        logger.warning(f\"{get_numeric_translation_table.__name__}: The alphabet_type '{alphabet_type}' \"\n                       \"isn't viable. Attempting to create it\")\n        numeric_translation_table = create_numeric_translation_table(alphabet_type)\n\n    return numeric_translation_table\n</code></pre>"},{"location":"reference/sequence/#sequence.generate_alignment","title":"generate_alignment","text":"<pre><code>generate_alignment(seq1: Sequence[str], seq2: Sequence[str], matrix: str = default_substitution_matrix_name, local: bool = False, top_alignment: bool = True, **alignment_kwargs) -&gt; Alignment | PairwiseAlignments\n</code></pre> <p>Use Biopython's pairwise2 to generate a sequence alignment</p> <p>Parameters:</p> <ul> <li> <code>seq1</code>             (<code>Sequence[str]</code>)         \u2013          <p>The first sequence to align</p> </li> <li> <code>seq2</code>             (<code>Sequence[str]</code>)         \u2013          <p>The second sequence to align</p> </li> <li> <code>matrix</code>             (<code>str</code>, default:                 <code>default_substitution_matrix_name</code> )         \u2013          <p>The matrix used to compare character similarities</p> </li> <li> <code>local</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run a local alignment. Only use for generally similar sequences!</p> </li> <li> <code>top_alignment</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Only include the highest scoring alignment</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>query_left_open_gap_score</code>         \u2013          <p>int = 0 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>query_left_extend_gap_score</code>         \u2013          <p>int = 0 - The score used for extending a gap in the alignment procedure</p> </li> <li> <code>target_left_open_gap_score</code>         \u2013          <p>int = 0 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>target_left_extend_gap_score</code>         \u2013          <p>int = 0 - The score used for extending a gap in the alignment procedure</p> </li> <li> <code>query_internal_open_gap_score</code>         \u2013          <p>int = -12 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>query_internal_extend_gap_score</code>         \u2013          <p>int = -1 - The score used for extending a gap in the alignment procedure</p> </li> <li> <code>target_internal_open_gap_score</code>         \u2013          <p>int = -12 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>target_internal_extend_gap_score</code>         \u2013          <p>int = -1 - The score used for extending a gap in the alignment procedure</p> </li> <li> <code>query_right_open_gap_score</code>         \u2013          <p>int = 0 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>query_right_extend_gap_score</code>         \u2013          <p>int = 0 - The score used for extending a gap in the alignment procedure</p> </li> <li> <code>target_right_open_gap_score</code>         \u2013          <p>int = 0 - The score used for opening a gap in the alignment procedure</p> </li> <li> <code>target_right_extend_gap_score</code>         \u2013          <p>int = 0 - The score used for extending a gap in the alignment procedure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Alignment | PairwiseAlignments</code>         \u2013          <p>The resulting alignment(s). Will be an Alignment object if top_alignment is True else PairwiseAlignments object</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def generate_alignment(seq1: Sequence[str], seq2: Sequence[str], matrix: str = default_substitution_matrix_name,\n                       local: bool = False, top_alignment: bool = True, **alignment_kwargs) \\\n        -&gt; Alignment | PairwiseAlignments:\n    \"\"\"Use Biopython's pairwise2 to generate a sequence alignment\n\n    Args:\n        seq1: The first sequence to align\n        seq2: The second sequence to align\n        matrix: The matrix used to compare character similarities\n        local: Whether to run a local alignment. Only use for generally similar sequences!\n        top_alignment: Only include the highest scoring alignment\n\n    Keyword Args:\n        query_left_open_gap_score: int = 0 - The score used for opening a gap in the alignment procedure\n        query_left_extend_gap_score: int = 0 - The score used for extending a gap in the alignment procedure\n        target_left_open_gap_score: int = 0 - The score used for opening a gap in the alignment procedure\n        target_left_extend_gap_score: int = 0 - The score used for extending a gap in the alignment procedure\n        query_internal_open_gap_score: int = -12 - The score used for opening a gap in the alignment procedure\n        query_internal_extend_gap_score: int = -1 - The score used for extending a gap in the alignment procedure\n        target_internal_open_gap_score: int = -12 - The score used for opening a gap in the alignment procedure\n        target_internal_extend_gap_score: int = -1 - The score used for extending a gap in the alignment procedure\n        query_right_open_gap_score: int = 0 - The score used for opening a gap in the alignment procedure\n        query_right_extend_gap_score: int = 0 - The score used for extending a gap in the alignment procedure\n        target_right_open_gap_score: int = 0 - The score used for opening a gap in the alignment procedure\n        target_right_extend_gap_score: int = 0 - The score used for extending a gap in the alignment procedure\n\n    Returns:\n        The resulting alignment(s). Will be an Alignment object if top_alignment is True else PairwiseAlignments object\n    \"\"\"\n    matrix_ = _substitution_matrices_cache.get(matrix)\n    if matrix_ is None:\n        try:  # To get the new matrix and store for future ops\n            matrix_ = _substitution_matrices_cache[matrix] = substitution_matrices.load(matrix)\n        except FileNotFoundError:  # Missing this\n            raise KeyError(\n                f\"Couldn't find the substitution matrix '{matrix}' \")\n\n    if local:\n        mode = 'local'\n    else:\n        mode = 'global'\n\n    if alignment_kwargs:\n        protein_alignment_variables_ = protein_alignment_variables.copy()\n        protein_alignment_variables_.update(**alignment_kwargs)\n    else:\n        protein_alignment_variables_ = protein_alignment_variables\n    # logger.debug(f'Generating sequence alignment between:\\n{seq1}\\n\\tAND:\\n{seq2}')\n    # Create sequence alignment\n    aligner = PairwiseAligner(mode=mode, substitution_matrix=matrix_, **protein_alignment_variables_)\n    try:\n        alignments = aligner.align(seq1, seq2)\n    except ValueError:  # sequence contains letters not in the alphabet\n        print(f'Sequence1: {seq1}')\n        print(f'Sequence2: {seq2}')\n        raise\n    first_alignment = alignments[0]\n    logger.debug(f'Found alignment with score: {alignments.score}\\n{first_alignment}')\n    # print(\"Number of alignments: %d\" % len(alignments))\n    #   Number of alignments: 1\n    # alignment = alignments[0]\n    # print(\"Score = %.1f\" % alignment.score)\n    #   Score = 13.0\n    # print(alignment)\n    #   target            0 KEVLA 5\n    #                     0 -|||- 5\n    #   query             0 -EVL- 3\n    # print(alignment.target)\n    # print(alignment.query)\n    # print(alignment.indices)  # returns array([[ 0,  1,  2,  3,  4], [ -1,  0,  1,  2, -1]]) where -1 are gaps\n    # This would be useful in checking for gaps during generate_mutations()\n    # print(alignment.inverse_indices)  # returns [array([ 0,  1,  2,  3,  4], [ 1,  2,  3]])\n    # where -1 are outside array and each index is the position in the alignment. This would be useful for instance with\n    # get_equivalent_indices() which is precalculated and now does this routine twice during Entity.__init__()\n    # logger.debug(f'Generated alignment:\\n{pairwise2.format_alignment(*align[0])}')\n\n    # return align[0] if top_alignment else align\n    return first_alignment if top_alignment else alignments\n</code></pre>"},{"location":"reference/sequence/#sequence.read_fasta_file","title":"read_fasta_file","text":"<pre><code>read_fasta_file(file_name: AnyStr, **kwargs) -&gt; Iterable[SeqRecord]\n</code></pre> <p>Opens a fasta file and return a parser object to load the sequences to SeqRecords</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file on disk</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterable[SeqRecord]</code>         \u2013          <p>An iterator of the sequences in the file [record1, record2, ...]</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def read_fasta_file(file_name: AnyStr, **kwargs) -&gt; Iterable[SeqRecord]:\n    \"\"\"Opens a fasta file and return a parser object to load the sequences to SeqRecords\n\n    Args:\n        file_name: The location of the file on disk\n\n    Returns:\n        An iterator of the sequences in the file [record1, record2, ...]\n    \"\"\"\n    return SeqIO.parse(file_name, 'fasta')\n</code></pre>"},{"location":"reference/sequence/#sequence.read_sequence_file","title":"read_sequence_file","text":"<pre><code>read_sequence_file(file_name: AnyStr, **kwargs) -&gt; Iterable[SeqRecord]\n</code></pre> <p>Opens a fasta file and return a parser object to load the sequences to SeqRecords</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file on disk</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterable[SeqRecord]</code>         \u2013          <p>An iterator of the sequences in the file [record1, record2, ...]</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def read_sequence_file(file_name: AnyStr, **kwargs) -&gt; Iterable[SeqRecord]:\n    \"\"\"Opens a fasta file and return a parser object to load the sequences to SeqRecords\n\n    Args:\n        file_name: The location of the file on disk\n\n    Returns:\n        An iterator of the sequences in the file [record1, record2, ...]\n    \"\"\"\n    raise NotImplementedError()\n    return SeqIO.parse(file_name, 'csv')\n</code></pre>"},{"location":"reference/sequence/#sequence.read_alignment","title":"read_alignment","text":"<pre><code>read_alignment(file_name: AnyStr, alignment_type: str = 'fasta', **kwargs) -&gt; MultipleSeqAlignment\n</code></pre> <p>Open an alignment file and parse the alignment to a Biopython MultipleSeqAlignment</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file on disk</p> </li> <li> <code>alignment_type</code>             (<code>str</code>, default:                 <code>'fasta'</code> )         \u2013          <p>The type of file that the alignment is stored in. Used for parsing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MultipleSeqAlignment</code>         \u2013          <p>The parsed alignment</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>@utils.handle_errors(errors=(FileNotFoundError,))\ndef read_alignment(file_name: AnyStr, alignment_type: str = 'fasta', **kwargs) -&gt; MultipleSeqAlignment:\n    \"\"\"Open an alignment file and parse the alignment to a Biopython MultipleSeqAlignment\n\n    Args:\n        file_name: The location of the file on disk\n        alignment_type: The type of file that the alignment is stored in. Used for parsing\n\n    Returns:\n        The parsed alignment\n    \"\"\"\n    return AlignIO.read(file_name, alignment_type)\n</code></pre>"},{"location":"reference/sequence/#sequence.write_fasta","title":"write_fasta","text":"<pre><code>write_fasta(sequence_records: Iterable[SeqRecord], file_name: AnyStr = None, name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write an iterator of SeqRecords to a .fasta file with fasta format. '.fasta' is appended if not specified in name</p> <p>Parameters:</p> <ul> <li> <code>sequence_records</code>             (<code>Iterable[SeqRecord]</code>)         \u2013          <p>The sequences to write. Should be Biopython SeqRecord format</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The explicit name of the file</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the file to output</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The name of the output file</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def write_fasta(sequence_records: Iterable[SeqRecord], file_name: AnyStr = None, name: str = None,\n                out_dir: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write an iterator of SeqRecords to a .fasta file with fasta format. '.fasta' is appended if not specified in name\n\n    Args:\n        sequence_records: The sequences to write. Should be Biopython SeqRecord format\n        file_name: The explicit name of the file\n        name: The name of the file to output\n        out_dir: The location on disk to output file\n\n    Returns:\n        The name of the output file\n    \"\"\"\n    if file_name is None:\n        file_name = os.path.join(out_dir, name)\n        if not file_name.endswith('.fasta'):\n            file_name = f'{file_name}.fasta'\n\n    SeqIO.write(sequence_records, file_name, 'fasta')\n\n    return file_name\n</code></pre>"},{"location":"reference/sequence/#sequence.write_sequence_to_fasta","title":"write_sequence_to_fasta","text":"<pre><code>write_sequence_to_fasta(sequence: str, file_name: AnyStr, name: str, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write an iterator of SeqRecords to a .fasta file with fasta format. '.fasta' is appended if not specified in name</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence to write</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name of the sequence. Will be used as the default file_name base name if file_name not provided</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>)         \u2013          <p>The explicit name of the file</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output the file. Only used if file_name not explicitly provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The path to the output file</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def write_sequence_to_fasta(sequence: str, file_name: AnyStr, name: str, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write an iterator of SeqRecords to a .fasta file with fasta format. '.fasta' is appended if not specified in name\n\n    Args:\n        sequence: The sequence to write\n        name: The name of the sequence. Will be used as the default file_name base name if file_name not provided\n        file_name: The explicit name of the file\n        out_dir: The location on disk to output the file. Only used if file_name not explicitly provided\n\n    Returns:\n        The path to the output file\n    \"\"\"\n    if file_name is None:\n        file_name = os.path.join(out_dir, name)\n        if not file_name.endswith('.fasta'):\n            file_name = f'{file_name}.fasta'\n\n    with open(file_name, 'w') as outfile:\n        outfile.write(f'&gt;{name}\\n{sequence}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/sequence/#sequence.concatenate_fasta_files","title":"concatenate_fasta_files","text":"<pre><code>concatenate_fasta_files(file_names: Iterable[AnyStr], out_path: str = 'concatenated_fasta') -&gt; AnyStr\n</code></pre> <p>Take multiple fasta files and concatenate into a single file</p> <p>Parameters:</p> <ul> <li> <code>file_names</code>             (<code>Iterable[AnyStr]</code>)         \u2013          <p>The name of the files to concatenate</p> </li> <li> <code>out_path</code>             (<code>str</code>, default:                 <code>'concatenated_fasta'</code> )         \u2013          <p>The location on disk to output file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The name of the output file</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def concatenate_fasta_files(file_names: Iterable[AnyStr], out_path: str = 'concatenated_fasta') -&gt; AnyStr:\n    \"\"\"Take multiple fasta files and concatenate into a single file\n\n    Args:\n        file_names: The name of the files to concatenate\n        out_path: The location on disk to output file\n\n    Returns:\n        The name of the output file\n    \"\"\"\n    seq_records = []\n    for file in file_names:\n        seq_records.extend(list(read_fasta_file(file)))\n    return write_fasta(seq_records, out_dir=out_path)\n</code></pre>"},{"location":"reference/sequence/#sequence.write_sequences","title":"write_sequences","text":"<pre><code>write_sequences(sequences: Sequence | dict[str, Sequence], names: Sequence = None, out_path: AnyStr = os.getcwd(), file_name: AnyStr = None, csv: bool = False) -&gt; AnyStr\n</code></pre> <p>Write a fasta file from sequence(s). If a single sequence is provided, pass as a string</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>Sequence | dict[str, Sequence]</code>)         \u2013          <p>If a list, can be list of tuples(name, sequence), or list[sequence] where names contain the corresponding sequence names. If dict, uses key as name, value as sequence. If str, treats as the sequence</p> </li> <li> <code>names</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>The name or names of the sequence record(s). If a single name, will be used as the default file_name base name if file_name not provided. Otherwise, will be used iteratively</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output file</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The explicit name of the file</p> </li> <li> <code>csv</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the file should be written as a .csv. Default is .fasta</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The name of the output file</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def write_sequences(sequences: Sequence | dict[str, Sequence], names: Sequence = None,\n                    out_path: AnyStr = os.getcwd(), file_name: AnyStr = None, csv: bool = False) -&gt; AnyStr:\n    \"\"\"Write a fasta file from sequence(s). If a single sequence is provided, pass as a string\n\n    Args:\n        sequences: If a list, can be list of tuples(name, sequence), or list[sequence] where names contain the\n            corresponding sequence names. If dict, uses key as name, value as sequence. If str, treats as the sequence\n        names: The name or names of the sequence record(s). If a single name, will be used as the default file_name\n            base name if file_name not provided. Otherwise, will be used iteratively\n        out_path: The location on disk to output file\n        file_name: The explicit name of the file\n        csv: Whether the file should be written as a .csv. Default is .fasta\n\n    Returns:\n        The name of the output file\n    \"\"\"\n    if file_name is None:\n        if isinstance(names, str):  # Not an iterable\n            file_name = os.path.join(out_path, names)\n        else:\n            raise ValueError(\n                f\"Must provide argument 'file_name' or 'names' as a str to {write_sequences.__name__}()\")\n\n    if csv:\n        start, sep = '', ','\n        extension = '.csv'\n    else:\n        start, sep = '&gt;', '\\n'\n        extension = '.fasta'\n\n    provided_extension = os.path.splitext(file_name)[-1]\n    if not file_name.endswith(extension) or provided_extension != extension:\n        file_name = f'{os.path.splitext(file_name)[0]}{extension}'\n\n    def data_dump():\n        return f\"{write_sequences.__name__} can't parse data to make fasta\\n\" \\\n               f'names={names}, sequences={sequences}, extension={extension}, out_path={out_path}, ' \\\n               f'file_name={file_name}'\n\n    with open(file_name, 'a') as outfile:\n        if isinstance(sequences, np.ndarray):\n            sequences = sequences.tolist()\n\n        if not sequences:\n            raise ValueError(f\"No sequences provided, couldn't write anything\")\n\n        if isinstance(sequences, list):\n            if isinstance(sequences[0], tuple):  # Where seq[0] is name, seq[1] is seq\n                formatted_sequence_gen = (f'{start}{name}{sep}{seq}' for name, seq, *_ in sequences)\n            elif isinstance(names, Sequence):\n                if isinstance(sequences[0], str):\n                    formatted_sequence_gen = (f'{start}{name}{sep}{seq}' for name, seq in zip(names, sequences))\n                elif isinstance(sequences[0], Sequence):\n                    formatted_sequence_gen = \\\n                        (f'{start}{name}{sep}{\"\".join(seq)}' for name, seq in zip(names, sequences))\n                else:\n                    raise TypeError(data_dump())\n            # elif isinstance(sequences[0], list):  # Where interior list is alphabet (AA or DNA)\n            #     for idx, seq in enumerate(sequences):\n            #         outfile.write(f'&gt;{name}_{idx}\\n')  # Write header\n            #         # Check if alphabet is 3 letter protein\n            #         outfile.write(f'{\" \".join(seq)}\\n' if len(seq[0]) == 3 else f'{\"\".join(seq)}\\n')\n            # elif isinstance(sequences[0], str):  # Likely 3 aa format...\n            #     outfile.write(f'&gt;{name}\\n{\" \".join(sequences)}\\n')\n            else:\n                raise TypeError(data_dump())\n        elif isinstance(sequences, dict):\n            formatted_sequence_gen = (f'{start}{name}{sep}{\"\".join(seq)}' for name, seq in sequences.items())\n        elif isinstance(sequences, tuple):  # Where seq[0] is name, seq[1] is seq\n            try:\n                name, seq, *_ = sequences\n            except ValueError:\n                raise ValueError(f\"When using a tuple, expected that the tuple contain (name, sequence) pairs\")\n            formatted_sequence_gen = (f'{start}{name}{sep}{seq}',)\n        elif isinstance(names, str):  # Assume sequences is a str or tuple\n            formatted_sequence_gen = (f'{start}{names}{sep}{\"\".join(sequences)}',)\n        else:\n            raise TypeError(data_dump())\n        outfile.write('%s\\n' % '\\n'.join(formatted_sequence_gen))\n\n    return file_name\n</code></pre>"},{"location":"reference/sequence/#sequence.hhblits","title":"hhblits","text":"<pre><code>hhblits(name: str, sequence_file: Sequence[str] = None, sequence: Sequence[str] = None, out_dir: AnyStr = os.getcwd(), threads: int = hhblits_threads, return_command: bool = False, **kwargs) -&gt; list[str] | None\n</code></pre> <p>Generate a position specific scoring matrix from HHblits using Hidden Markov Models</p> <p>When the command is run, it is possible to create six files in out_dir with the pattern '/outdir/name.' where the '' extensions are:  hhblits profile - .hmm</p> <p>hhblits resulting cluster description - .hrr</p> <p>sequence alignment in a3m format - .a3m</p> <p>sequence file - .seq (if sequence_file)</p> <p>sequence alignment in stockholm format - .sto (if return_command is False)</p> <p>sequence alignment in fasta format - .fasta (if return_command is False)</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name associated with the sequence</p> </li> <li> <code>sequence_file</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>The file containing the sequence to use</p> </li> <li> <code>sequence</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>The sequence to use. Used in place of sequence_file</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Disk location where generated files should be written</p> </li> <li> <code>threads</code>             (<code>int</code>, default:                 <code>hhblits_threads</code> )         \u2013          <p>Number of cpu's to use for the process</p> </li> <li> <code>return_command</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to simply return the hhblits command</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str] | None</code>         \u2013          <p>The command if return_command is True, otherwise None</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def hhblits(name: str, sequence_file: Sequence[str] = None, sequence: Sequence[str] = None,\n            out_dir: AnyStr = os.getcwd(), threads: int = hhblits_threads,\n            return_command: bool = False, **kwargs) -&gt; list[str] | None:\n    \"\"\"Generate a position specific scoring matrix from HHblits using Hidden Markov Models\n\n    When the command is run, it is possible to create six files in out_dir with the pattern '/outdir/name.*'\n    where the '*' extensions are:\n     hhblits profile - .hmm\n\n     hhblits resulting cluster description - .hrr\n\n     sequence alignment in a3m format - .a3m\n\n     sequence file - .seq (if sequence_file)\n\n     sequence alignment in stockholm format - .sto (if return_command is False)\n\n     sequence alignment in fasta format - .fasta (if return_command is False)\n\n    Args:\n        name: The name associated with the sequence\n        sequence_file: The file containing the sequence to use\n        sequence: The sequence to use. Used in place of sequence_file\n        out_dir: Disk location where generated files should be written\n        threads: Number of cpu's to use for the process\n        return_command: Whether to simply return the hhblits command\n\n    Raises:\n        RuntimeError if hhblits command is run and returns a non-zero exit code\n\n    Returns:\n        The command if return_command is True, otherwise None\n    \"\"\"\n    if sequence_file is None:\n        if sequence is None:\n            raise ValueError(\n                f\"{hhblits.__name__}: Can't proceed without argument 'sequence_file' or 'sequence'\")\n        else:\n            sequence_file = write_sequences((name, sequence), file_name=os.path.join(out_dir, f'{name}.seq'))\n    pssm_file = os.path.join(out_dir, f'{name}.hmm')\n    a3m_file = os.path.join(out_dir, f'{name}.a3m')\n    # Todo for higher performance set up https://www.howtoforge.com/storing-files-directories-in-memory-with-tmpfs\n    #  Create a ramdisk to store a database chunk to make hhblits/Jackhmmer run fast.\n    #  sudo mkdir -m 777 --parents /tmp/ramdisk\n    #  sudo mount -t tmpfs -o size=9G ramdisk /tmp/ramdisk\n    cmd = [putils.hhblits_exe, '-d', putils.uniclust_db, '-i', sequence_file, '-ohhm', pssm_file, '-oa3m', a3m_file,\n           '-hide_cons', '-hide_pred', '-hide_dssp', '-E', '1E-06', '-v', '1', '-cpu', str(threads)]\n\n    if return_command:\n        return cmd\n\n    logger.debug(f'{name} Profile Command: {subprocess.list2cmdline(cmd)}')\n    p = subprocess.Popen(cmd)\n    p.communicate()\n    if p.returncode != 0:\n        # temp_file = os.path.join(out_path, f'{self.name}.hold')\n        temp_file = Path(out_dir, f'{name}.hold')\n        temp_file.unlink(missing_ok=True)\n        # if os.path.exists(temp_file):  # remove hold file blocking progress\n        #     os.remove(temp_file)\n        raise utils.SymDesignException(\n            f'Profile generation for {name} got stuck. Found return code {p.returncode}')  #\n\n    # Preferred alignment type\n    msa_file = os.path.join(out_dir, f'{name}.sto')\n    p = subprocess.Popen([putils.reformat_msa_exe_path, a3m_file, msa_file, '-num', '-uc'])\n    p.communicate()\n    fasta_msa = os.path.join(out_dir, f'{name}.fasta')\n    p = subprocess.Popen([putils.reformat_msa_exe_path, a3m_file, fasta_msa, '-M', 'first', '-r'])\n    p.communicate()\n\n    return None\n</code></pre>"},{"location":"reference/sequence/#sequence.optimize_protein_sequence","title":"optimize_protein_sequence","text":"<pre><code>optimize_protein_sequence(sequence: str, species: optimization_species_literal = 'e_coli') -&gt; str\n</code></pre> <p>Optimize a sequence for expression in a desired organism</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence of interest</p> </li> <li> <code>species</code>             (<code>optimization_species_literal</code>, default:                 <code>'e_coli'</code> )         \u2013          <p>The species context to optimize nucleotide sequence usage</p> </li> </ul> <p>Returns:     The input sequence optimized to nucleotides for expression considerations</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def optimize_protein_sequence(sequence: str, species: optimization_species_literal = 'e_coli') -&gt; str:\n    \"\"\"Optimize a sequence for expression in a desired organism\n\n    Args:\n        sequence: The sequence of interest\n        species: The species context to optimize nucleotide sequence usage\n    Returns:\n        The input sequence optimized to nucleotides for expression considerations\n    \"\"\"\n    seq_length = len(sequence)\n    species = species.lower()\n    try:\n        from dnachisel import reverse_translate, AvoidHairpins, AvoidPattern, AvoidRareCodons, CodonOptimize, \\\n        DnaOptimizationProblem, EnforceGCContent, EnforceTranslation,  UniquifyAllKmers\n    except ModuleNotFoundError:\n        raise RuntimeError(\n            f\"Can't {optimize_protein_sequence.__name__} as the dependency `DnaChisel` isn`t available\")\n\n    try:\n        dna_sequence = reverse_translate(sequence)\n    except KeyError as error:\n        raise KeyError(\n            f'Warning an invalid character was found in the protein sequence: {error}')\n\n    problem = DnaOptimizationProblem(\n        sequence=dna_sequence, logger=None,  # max_random_iters=20000,\n        objectives=[CodonOptimize(species=species)],\n        # method='harmonize_rca')] &lt;- Useful for folding speed when original organism known\n        constraints=[EnforceGCContent(mini=0.25, maxi=0.65),  # Twist required\n                     EnforceGCContent(mini=0.35, maxi=0.65, window=50),  # Twist required\n                     AvoidHairpins(stem_size=20, hairpin_window=48),  # Efficient translate\n                     AvoidPattern('GGAGG', location=(1, seq_length, 1)),  # Ribosome bind\n                     AvoidPattern('TAAGGAG', location=(1, seq_length, 1)),  # Ribosome bind\n                     AvoidPattern('AAAAA', location=(1, seq_length, 0)),  # Terminator\n                     # AvoidPattern('TTTTT', location=(1, seq_length, 1)),  # Terminator\n                     AvoidPattern('GGGGGGGGGG', location=(1, seq_length, 0)),  # Homopoly\n                     # AvoidPattern('CCCCCCCCCC', location=(1, seq_length)),  # Homopoly\n                     UniquifyAllKmers(20),  # Twist required\n                     AvoidRareCodons(0.08, species=species),\n                     EnforceTranslation(),\n                     # EnforceMeltingTemperature(mini=10,maxi=62,location=(1, seq_length)),\n                     ])\n\n    # Solve constraints and solve in regard to the objective\n    problem.max_random_iters = 20000\n    problem.resolve_constraints()\n    problem.optimize()\n\n    # Display summaries of constraints that pass\n    # print(problem.constraints_text_summary())\n    # print(problem.objectives_text_summary())\n\n    # Get final sequence as string\n    final_sequence = problem.sequence\n    # Get final sequene as BioPython record\n    # final_record = problem.to_record(with_sequence_edits=True)\n\n    return final_sequence\n</code></pre>"},{"location":"reference/sequence/#sequence.make_mutations","title":"make_mutations","text":"<pre><code>make_mutations(sequence: Sequence, mutations: dict[int, dict[str, str]], find_orf: bool = True) -&gt; str\n</code></pre> <p>Modify a sequence to contain mutations specified by a mutation dictionary</p> <p>Assumes a zero-index sequence and zero-index mutations Args:     sequence: 'Wild-type' sequence to mutate     mutations: {mutation_index: {'from': AA, 'to': AA}, ...}     find_orf: Whether to find the correct ORF for the mutations and the seq Returns:     seq: The mutated sequence</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def make_mutations(sequence: Sequence, mutations: dict[int, dict[str, str]], find_orf: bool = True) -&gt; str:\n    \"\"\"Modify a sequence to contain mutations specified by a mutation dictionary\n\n    Assumes a zero-index sequence and zero-index mutations\n    Args:\n        sequence: 'Wild-type' sequence to mutate\n        mutations: {mutation_index: {'from': AA, 'to': AA}, ...}\n        find_orf: Whether to find the correct ORF for the mutations and the seq\n    Returns:\n        seq: The mutated sequence\n    \"\"\"\n    # Seq can be either list or string\n    if find_orf:\n        offset = find_orf_offset(sequence, mutations)\n        logger.info(f'Found ORF. Offset = {offset}')\n    else:\n        offset = 0  # ZERO_OFFSET\n\n    seq = sequence\n    index_errors = []\n    for key, mutation in mutations.items():\n        index = key + offset\n        try:\n            if seq[index] == mutation['from']:  # Adjust key for zero index slicing\n                seq = seq[:index] + mutation['to'] + seq[index + 1:]\n            else:  # Find correct offset, or mark mutation source as doomed\n                index_errors.append(key)\n        except IndexError:\n            logger.error(key + offset)\n    if index_errors:\n        logger.warning(f'{make_mutations.__name__} index errors: {\", \".join(map(str, index_errors))}')\n\n    return seq\n</code></pre>"},{"location":"reference/sequence/#sequence.find_orf_offset","title":"find_orf_offset","text":"<pre><code>find_orf_offset(sequence: Sequence, mutations: mutation_dictionary) -&gt; int\n</code></pre> <p>Using a sequence and mutation data, find the open reading frame that matches mutations closest</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence</code>)         \u2013          <p>Sequence to search for ORF in 1 letter format</p> </li> <li> <code>mutations</code>             (<code>mutation_dictionary</code>)         \u2013          <p>{mutation_index: {'from': AA, 'to': AA}, ...} zero-indexed sequence dictionary</p> </li> </ul> <p>Returns:     The zero-indexed integer to offset the provided sequence to best match the provided mutations</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def find_orf_offset(sequence: Sequence, mutations: mutation_dictionary) -&gt; int:\n    \"\"\"Using a sequence and mutation data, find the open reading frame that matches mutations closest\n\n    Args:\n        sequence: Sequence to search for ORF in 1 letter format\n        mutations: {mutation_index: {'from': AA, 'to': AA}, ...} zero-indexed sequence dictionary\n    Returns:\n        The zero-indexed integer to offset the provided sequence to best match the provided mutations\n    \"\"\"\n    def gen_offset_repr():\n        return f'Found the orf_offsets: {\",\".join(f\"{k}={v}\" for k, v in orf_offsets.items())}'\n\n    unsolvable = False\n    orf_start_idx = 0\n    orf_offsets = {idx: 0 for idx, aa in enumerate(sequence) if aa == 'M'}\n    methionine_positions = list(orf_offsets.keys())\n    while True:\n        if not orf_offsets:  # MET is missing for the sequnce/we haven't found the ORF start and need to scan a range\n            orf_offsets = {start_idx: 0 for start_idx in range(-30, 50)}\n\n        # Weight potential MET offsets by finding the one which gives the highest number correct mutation sites\n        for test_orf_index in orf_offsets:\n            for mutation_index, mutation in mutations.items():\n                try:\n                    if sequence[test_orf_index + mutation_index] == mutation['from']:\n                        orf_offsets[test_orf_index] += 1\n                except IndexError:  # We have reached the end of the sequence\n                    break\n\n        # logger.debug(gen_offset_repr())\n        max_count = max(list(orf_offsets.values()))\n        # Check if likely ORF has been identified (count &lt; number mutations/2). If not, MET is missing/not the ORF start\n        if max_count &lt; len(mutations) / 2:\n            if unsolvable:\n                raise RuntimeError(f\"Couldn't find a orf_offset max_count {max_count} &lt; {len(mutations) / 2} (half the \"\n                                   f\"mutations). The orf_start_idx={orf_start_idx} still\\n\\t{gen_offset_repr()}\")\n            orf_offsets = {}\n            unsolvable = True  # if we reach this spot again, the problem is deemed unsolvable\n        else:  # Find the index of the max_count\n            for idx, count_ in orf_offsets.items():\n                if max_count == count_:  # orf_offsets[offset]:\n                    orf_start_idx = idx  # Select the first occurrence of the max count\n                    break\n            else:\n                raise RuntimeError(f\"Couldn't find a orf_offset count == {max_count} (the max_count). \"\n                                   f\"The orf_start_idx={orf_start_idx} still\\n\\t{gen_offset_repr()}\")\n\n            # For cases where the orf doesn't begin on Met, try to find a prior Met. Otherwise, selects the id'd Met\n            closest_met = None\n            for met_index in methionine_positions:\n                if met_index &lt;= orf_start_idx:\n                    closest_met = met_index\n                else:  # We have passed the identified orf_start_idx\n                    if closest_met is not None:\n                        orf_start_idx = closest_met\n                    break\n            break\n\n    return orf_start_idx\n</code></pre>"},{"location":"reference/sequence/#sequence.generate_mutations","title":"generate_mutations","text":"<pre><code>generate_mutations(reference: Sequence, query: Sequence, offset: bool = True, keep_gaps: bool = False, remove_termini: bool = True, remove_query_gaps: bool = True, only_gaps: bool = False, zero_index: bool = False, return_all: bool = False, return_to: bool = False, return_from: bool = False) -&gt; mutation_dictionary | sequence_dictionary\n</code></pre> <p>Create mutation data in a typical A5K format. Integer indexed dictionary keys with the index matching reference sequence. Sequence mutations accessed by \"from\" and \"to\" keys. By default, only mutated positions are returned and all gaped sequences are excluded</p> <p>For PDB comparison, reference should be expression sequence (SEQRES), query should be atomic sequence (ATOM)</p> <p>Parameters:</p> <ul> <li> <code>reference</code>             (<code>Sequence</code>)         \u2013          <p>Reference sequence to align mutations against. Character values are returned to the \"from\" key</p> </li> <li> <code>query</code>             (<code>Sequence</code>)         \u2013          <p>Query sequence. Character values are returned to the \"to\" key</p> </li> <li> <code>offset</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether sequences are different lengths. Will create an alignment of the two sequences</p> </li> <li> <code>keep_gaps</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Return gaped indices, i.e. outside the aligned sequences or missing internal characters</p> </li> <li> <code>remove_termini</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Remove indices that are outside the reference sequence boundaries</p> </li> <li> <code>remove_query_gaps</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Remove indices where there are gaps present in the query sequence</p> </li> <li> <code>only_gaps</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Only include reference indices that are missing query residues. All \"to\" values will be a gap \"-\"</p> </li> <li> <code>zero_index</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return the indices zero-indexed (like python) or one-indexed</p> </li> <li> <code>return_all</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return all the indices and there corresponding mutational data</p> </li> <li> <code>return_to</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return only the 'to' amino acid type</p> </li> <li> <code>return_from</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return only the 'from' amino acid type</p> </li> </ul> <p>Returns:     Mutation index to mutations with format         {1: {'from': 'A', 'to': 'K'}, ...}         unless return_to or return_from is True, then         {1: 'K', ...} or {1: 'A', ...}, respectively</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def generate_mutations(reference: Sequence, query: Sequence, offset: bool = True, keep_gaps: bool = False,\n                       remove_termini: bool = True, remove_query_gaps: bool = True, only_gaps: bool = False,\n                       zero_index: bool = False,\n                       return_all: bool = False, return_to: bool = False, return_from: bool = False) \\\n        -&gt; mutation_dictionary | sequence_dictionary:\n    \"\"\"Create mutation data in a typical A5K format. Integer indexed dictionary keys with the index matching reference\n    sequence. Sequence mutations accessed by \"from\" and \"to\" keys. By default, only mutated positions are returned and\n    all gaped sequences are excluded\n\n    For PDB comparison, reference should be expression sequence (SEQRES), query should be atomic sequence (ATOM)\n\n    Args:\n        reference: Reference sequence to align mutations against. Character values are returned to the \"from\" key\n        query: Query sequence. Character values are returned to the \"to\" key\n        offset: Whether sequences are different lengths. Will create an alignment of the two sequences\n        keep_gaps: Return gaped indices, i.e. outside the aligned sequences or missing internal characters\n        remove_termini: Remove indices that are outside the reference sequence boundaries\n        remove_query_gaps: Remove indices where there are gaps present in the query sequence\n        only_gaps: Only include reference indices that are missing query residues. All \"to\" values will be a gap \"-\"\n        zero_index: Whether to return the indices zero-indexed (like python) or one-indexed\n        return_all: Whether to return all the indices and there corresponding mutational data\n        return_to: Whether to return only the 'to' amino acid type\n        return_from: Whether to return only the 'from' amino acid type\n    Returns:\n        Mutation index to mutations with format\n            {1: {'from': 'A', 'to': 'K'}, ...}\n            unless return_to or return_from is True, then\n            {1: 'K', ...} or {1: 'A', ...}, respectively\n    \"\"\"\n    if offset:\n        alignment = generate_alignment(reference, query)\n        align_seq_1, align_seq_2 = alignment\n    else:\n        align_seq_1, align_seq_2 = reference, query\n\n    idx_offset = 0 if zero_index else ZERO_OFFSET\n\n    # Get the first matching index of the reference sequence\n    starting_idx_of_seq1 = align_seq_1.find(reference[0])\n    # Ensure iteration sequence1/reference starts at idx 0       v\n    sequence_iterator = enumerate(zip(align_seq_1, align_seq_2), -starting_idx_of_seq1 + idx_offset)\n    # Extract differences from the alignment\n    if return_all:\n        mutations = {idx: {'from': char1, 'to': char2} for idx, (char1, char2) in sequence_iterator}\n    else:\n        mutations = {idx: {'from': char1, 'to': char2} for idx, (char1, char2) in sequence_iterator if char1 != char2}\n\n    # Find last index of reference (including internal gaps)\n    starting_key_of_seq1 = idx_offset\n    ending_key_of_seq1 = starting_key_of_seq1 + align_seq_1.rfind(reference[-1])\n    remove_mutation_list = []\n    if only_gaps:  # Remove the actual mutations, keep internal and external gap indices and the reference sequence\n        keep_gaps = True\n        remove_mutation_list.extend([entry for entry, mutation in mutations.items()\n                                     if mutation['from'] != '-' and mutation['to'] != '-'])\n    if keep_gaps:  # Leave all types of keep_gaps, otherwise check for each requested type\n        remove_termini = remove_query_gaps = False\n\n    if remove_termini:  # Remove indices outside of sequence 1\n        remove_mutation_list.extend([entry for entry in mutations\n                                     if entry &lt; starting_key_of_seq1 or entry &gt; ending_key_of_seq1])\n\n    if remove_query_gaps:  # Remove indices where sequence 2 is gaped\n        remove_mutation_list.extend([entry for entry, mutation in mutations.items()\n                                     if starting_key_of_seq1 &lt; entry &lt;= ending_key_of_seq1 and mutation['to'] == '-'])\n    for entry in remove_mutation_list:\n        mutations.pop(entry, None)\n\n    if return_to:\n        mutations = {idx: _mutation_dictionary['to'] for idx, _mutation_dictionary in mutations.items()}\n    elif return_from:\n        mutations = {idx: _mutation_dictionary['from'] for idx, _mutation_dictionary in mutations.items()}\n\n    return mutations\n</code></pre>"},{"location":"reference/sequence/#sequence.pdb_to_pose_offset","title":"pdb_to_pose_offset","text":"<pre><code>pdb_to_pose_offset(reference_sequence: dict[Any, Sequence]) -&gt; dict[Any, int]\n</code></pre> <p>Take a dictionary with chain name as keys and return the length of Pose numbering offset</p> <p>Parameters:</p> <ul> <li> <code>reference_sequence</code>             (<code>dict[Any, Sequence]</code>)         \u2013          <p>{key1: 'MSGKLDA...', ...} or {key2: {1: 'A', 2: 'S', ...}, ...}</p> </li> </ul> <p>Returns:     {key1: 0, key2: 123, ...}</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def pdb_to_pose_offset(reference_sequence: dict[Any, Sequence]) -&gt; dict[Any, int]:\n    \"\"\"Take a dictionary with chain name as keys and return the length of Pose numbering offset\n\n    Args:\n        reference_sequence: {key1: 'MSGKLDA...', ...} or {key2: {1: 'A', 2: 'S', ...}, ...}\n    Returns:\n        {key1: 0, key2: 123, ...}\n    \"\"\"\n    offset = {}\n    # prior_chain = None\n    prior_chains_len = prior_key = 0  # prior_key not used as 0 but to ensure initialized nonetheless\n    for idx, key in enumerate(reference_sequence):\n        if idx &gt; 0:\n            prior_chains_len += len(reference_sequence[prior_key])\n        offset[key] = prior_chains_len\n        # insert function here? Make this a decorator!?\n        prior_key = key\n\n    return offset\n</code></pre>"},{"location":"reference/sequence/#sequence.generate_multiple_mutations","title":"generate_multiple_mutations","text":"<pre><code>generate_multiple_mutations(reference: dict[str, str], sequences: dict[str, dict[str, str]], pose_num: bool = True) -&gt; dict[str, dict[str, dict[int]]]\n</code></pre> <p>Extract mutation data from multiple sequence dictionaries with regard to a reference. Default is Pose numbering</p> <p>Parameters:</p> <ul> <li> <code>reference</code>             (<code>dict[str, str]</code>)         \u2013          <p>{chain: sequence, ...} The reference sequence to compare sequences to</p> </li> <li> <code>sequences</code>             (<code>dict[str, dict[str, str]]</code>)         \u2013          <p>{pdb_code: {chain: sequence, ...}, ...}</p> </li> <li> <code>pose_num</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to return the mutations in Pose numbering with the first Entity as 1 and the second Entity as Entity1 last residue + 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict[str, dict[int]]]</code>         \u2013          <p>{pdb_code: {chain_id: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def generate_multiple_mutations(\n    reference: dict[str, str], sequences: dict[str, dict[str, str]], pose_num: bool = True\n) -&gt; dict[str, dict[str, dict[int, ]]]:\n    \"\"\"Extract mutation data from multiple sequence dictionaries with regard to a reference. Default is Pose numbering\n\n    Args:\n        reference: {chain: sequence, ...} The reference sequence to compare sequences to\n        sequences: {pdb_code: {chain: sequence, ...}, ...}\n        pose_num: Whether to return the mutations in Pose numbering with the first Entity as 1 and the\n            second Entity as Entity1 last residue + 1\n\n    Returns:\n        {pdb_code: {chain_id: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}\n    \"\"\"\n    # Add reference sequence mutations\n    mutations = {'reference': {chain: {sequence_idx: {'from': aa, 'to': aa}\n                                       for sequence_idx, aa in enumerate(ref_sequence, 1)}\n                               for chain, ref_sequence in reference.items()}}\n    #                         returns {1: {'from': 'A', 'to': 'K'}, ...}\n    # mutations = {pdb: {chain: generate_mutations(sequence, reference[chain], offset=False)\n    #                    for chain, sequence in chain_sequences.items()}\n    #              for pdb, chain_sequences in pdb_sequences.items()}\n    try:\n        for name, chain_sequences in sequences.items():\n            mutations[name] = {}\n            for chain, sequence in chain_sequences.items():\n                mutations[name][chain] = generate_mutations(reference[chain], sequence, offset=False)\n    except KeyError:\n        raise RuntimeError(\n            f\"The 'reference' and 'sequences' have different chains. Chain '{chain}' isn't in the reference\")\n    if pose_num:\n        offset_dict = pdb_to_pose_offset(reference)\n        # pose_mutations = {}\n        # for chain, offset in offset_dict.items():\n        #     for pdb_code in mutations:\n        #         if pdb_code not in pose_mutations:\n        #             pose_mutations[pdb_code] = {}\n        #         pose_mutations[pdb_code][chain] = {}\n        #         for mutation_idx in mutations[pdb_code][chain]:\n        #             pose_mutations[pdb_code][chain][mutation_idx + offset] = mutations[pdb_code][chain][mutation_idx]\n        # mutations = pose_mutations\n        mutations = {name: {chain: {idx + offset: mutation for idx, mutation in chain_mutations[chain].items()}\n                            for chain, offset in offset_dict.items()} for name, chain_mutations in mutations.items()}\n    return mutations\n</code></pre>"},{"location":"reference/sequence/#sequence.generate_mutations_from_reference","title":"generate_mutations_from_reference","text":"<pre><code>generate_mutations_from_reference(reference: Sequence[str], sequences: dict[str, Sequence[str]], **kwargs) -&gt; dict[str, mutation_dictionary | sequence_dictionary]\n</code></pre> <p>Generate mutation data from multiple alias mapped sequence dictionaries with regard to a single reference.</p> <p>Defaults to returning only mutations (return_all=False) and forgoes any sequence alignment (offset=False)</p> <p>Parameters:</p> <ul> <li> <code>reference</code>             (<code>Sequence[str]</code>)         \u2013          <p>The reference sequence to align each sequence against. Character values are returned to the \"from\" key</p> </li> <li> <code>sequences</code>             (<code>dict[str, Sequence[str]]</code>)         \u2013          <p>The template sequences to align, i.e. {alias: sequence, ...}. Character values are returned to the \"to\" key</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>offset</code>         \u2013          <p>bool = True - Whether sequences are different lengths. Will create an alignment of the two sequences</p> </li> <li> <code>keep_gaps</code>         \u2013          <p>bool = False - Return gaped indices, i.e. outside the aligned sequences or missing internal characters</p> </li> <li> <code>remove_termini</code>         \u2013          <p>bool = True - Remove indices that are outside the reference sequence boundaries</p> </li> <li> <code>remove_query_gaps</code>         \u2013          <p>bool = True - Remove indices where there are gaps present in the query sequence</p> </li> <li> <code>only_gaps</code>         \u2013          <p>bool = False - Only include reference indices that are missing query residues. All \"to\" values will be a gap \"-\"</p> </li> <li> <code>zero_index</code>         \u2013          <p>bool = False - Whether to return the indices zero-indexed (like python Sequence) or one-indexed</p> </li> <li> <code>return_all</code>         \u2013          <p>bool = False - Whether to return all the indices and there corresponding mutational data</p> </li> <li> <code>return_to</code>         \u2013          <p>bool = False - Whether to return only the \"to\" amino acid type</p> </li> <li> <code>return_from</code>         \u2013          <p>bool = False - Whether to return only the \"from\" amino acid type</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, mutation_dictionary | sequence_dictionary]</code>         \u2013          <p>{alias: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...} unless return_to or return_from is True, then {alias: {mutation_index: 'K', ...}, ...}</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def generate_mutations_from_reference(reference: Sequence[str], sequences: dict[str, Sequence[str]], **kwargs) -&gt; \\\n        dict[str, mutation_dictionary | sequence_dictionary]:\n    \"\"\"Generate mutation data from multiple alias mapped sequence dictionaries with regard to a single reference.\n\n    Defaults to returning only mutations (return_all=False) and forgoes any sequence alignment (offset=False)\n\n    Args:\n        reference: The reference sequence to align each sequence against.\n            Character values are returned to the \"from\" key\n        sequences: The template sequences to align, i.e. {alias: sequence, ...}.\n            Character values are returned to the \"to\" key\n\n    Keyword Args:\n        offset: bool = True - Whether sequences are different lengths. Will create an alignment of the two sequences\n        keep_gaps: bool = False - Return gaped indices, i.e. outside the aligned sequences or missing internal\n            characters\n        remove_termini: bool = True - Remove indices that are outside the reference sequence boundaries\n        remove_query_gaps: bool = True - Remove indices where there are gaps present in the query sequence\n        only_gaps: bool = False - Only include reference indices that are missing query residues.\n            All \"to\" values will be a gap \"-\"\n        zero_index: bool = False - Whether to return the indices zero-indexed (like python Sequence) or one-indexed\n        return_all: bool = False - Whether to return all the indices and there corresponding mutational data\n        return_to: bool = False - Whether to return only the \"to\" amino acid type\n        return_from: bool = False - Whether to return only the \"from\" amino acid type\n\n    Returns:\n        {alias: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...} unless return_to or return_from is True, then\n            {alias: {mutation_index: 'K', ...}, ...}\n    \"\"\"\n    # offset_value = kwargs.get('offset')\n    kwargs['offset'] = (kwargs.get('offset') or False)  # Default to False if there was no argument passed\n    mutations = {alias: generate_mutations(reference, sequence, **kwargs)  # offset=False,\n                 for alias, sequence in sequences.items()}\n\n    # Add the reference sequence to mutation data\n    if kwargs.get('return_to') or kwargs.get('return_from'):\n        mutations[putils.reference_name] = dict(enumerate(reference, 0 if kwargs.get('zero_index') else 1))\n    else:\n        mutations[putils.reference_name] = \\\n            {sequence_idx: {'from': aa, 'to': aa}\n             for sequence_idx, aa in enumerate(reference, 0 if kwargs.get('zero_index') else 1)}\n\n    return mutations\n</code></pre>"},{"location":"reference/sequence/#sequence.make_sequences_from_mutations","title":"make_sequences_from_mutations","text":"<pre><code>make_sequences_from_mutations(wild_type: str, pdb_mutations: dict, aligned: bool = False) -&gt; dict\n</code></pre> <p>Takes a list of sequence mutations and returns the mutated form on wildtype</p> <p>Parameters:</p> <ul> <li> <code>wild_type</code>             (<code>str</code>)         \u2013          <p>Sequence to mutate</p> </li> <li> <code>pdb_mutations</code>             (<code>dict</code>)         \u2013          <p>{name: {mutation_index: {'from': AA, 'to': AA}, ...}, ...}, ...}</p> </li> <li> <code>aligned</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the input sequences are already aligned</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>         \u2013          <p>{name: sequence, ...}</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def make_sequences_from_mutations(wild_type: str, pdb_mutations: dict, aligned: bool = False) -&gt; dict:\n    \"\"\"Takes a list of sequence mutations and returns the mutated form on wildtype\n\n    Args:\n        wild_type: Sequence to mutate\n        pdb_mutations: {name: {mutation_index: {'from': AA, 'to': AA}, ...}, ...}, ...}\n        aligned: Whether the input sequences are already aligned\n\n    Returns:\n        {name: sequence, ...}\n    \"\"\"\n    return {pdb: make_mutations(wild_type, mutations, find_orf=not aligned) for pdb, mutations in pdb_mutations.items()}\n</code></pre>"},{"location":"reference/sequence/#sequence.generate_sequences","title":"generate_sequences","text":"<pre><code>generate_sequences(wild_type_sequences: dict, all_design_mutations: dict) -&gt; dict\n</code></pre> <p>Separate chains from mutation dictionary and generate mutated sequences</p> <p>Parameters:</p> <ul> <li> <code>wild_type_sequences</code>             (<code>dict</code>)         \u2013          <p>{chain: sequence, ...}</p> </li> <li> <code>all_design_mutations</code>             (<code>dict</code>)         \u2013          <p>{'name': {chain: {mutation_index: {'from': AA, 'to': AA}, ...}, ...}, ...} Index so mutation_index starts at 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>         \u2013          <p>{chain: {name: sequence, ...}</p> </li> </ul> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def generate_sequences(wild_type_sequences: dict, all_design_mutations: dict) -&gt; dict:\n    \"\"\"Separate chains from mutation dictionary and generate mutated sequences\n\n    Args:\n        wild_type_sequences: {chain: sequence, ...}\n        all_design_mutations: {'name': {chain: {mutation_index: {'from': AA, 'to': AA}, ...}, ...}, ...}\n            Index so mutation_index starts at 1\n\n    Returns:\n        {chain: {name: sequence, ...}\n    \"\"\"\n    mutated_sequences = {}\n    for chain in wild_type_sequences:\n        # pdb_chain_mutations = {pdb: chain_mutations.get(chain) for pdb, chain_mutations in all_design_mutations.items()}\n        pdb_chain_mutations = {}\n        for pdb, chain_mutations in all_design_mutations.items():\n            if chain in chain_mutations:\n                pdb_chain_mutations[pdb] = all_design_mutations[pdb][chain]\n        mutated_sequences[chain] = make_sequences_from_mutations(wild_type_sequences[chain], pdb_chain_mutations,\n                                                                 aligned=True)\n    return mutated_sequences\n</code></pre>"},{"location":"reference/sequence/#sequence.numeric_to_sequence","title":"numeric_to_sequence","text":"<pre><code>numeric_to_sequence(numeric_sequence: ndarray, translation_table: dict[str, int] = None, alphabet_order: int = 1) -&gt; ndarray\n</code></pre> <p>Convert a numeric sequence array into a sequence array</p> <p>Parameters:</p> <ul> <li> <code>numeric_sequence</code>             (<code>ndarray</code>)         \u2013          <p>The sequence to convert</p> </li> <li> <code>translation_table</code>             (<code>dict[str, int]</code>, default:                 <code>None</code> )         \u2013          <p>If a translation table is provided, it will be used. If not, use alphabet_order</p> </li> <li> <code>alphabet_order</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The alphabetical order of the amino acid alphabet. Can be either 1 or 3</p> </li> </ul> <p>Returns:     The alphabetically encoded sequence where each entry along axis=-1 is the one letter amino acid</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def numeric_to_sequence(numeric_sequence: np.ndarray, translation_table: dict[str, int] = None,\n                        alphabet_order: int = 1) -&gt; np.ndarray:\n    \"\"\"Convert a numeric sequence array into a sequence array\n\n    Args:\n        numeric_sequence: The sequence to convert\n        translation_table: If a translation table is provided, it will be used. If not, use alphabet_order\n        alphabet_order: The alphabetical order of the amino acid alphabet. Can be either 1 or 3\n    Returns:\n        The alphabetically encoded sequence where each entry along axis=-1 is the one letter amino acid\n    \"\"\"\n    if translation_table is not None:\n        return np.vectorize(translation_table.__getitem__)(numeric_sequence)\n    else:\n        if alphabet_order == 1:\n            return np.vectorize(sequence_translation_alph1.__getitem__)(numeric_sequence)\n        elif alphabet_order == 3:\n            return np.vectorize(sequence_translation_alph3.__getitem__)(numeric_sequence)\n        else:\n            raise ValueError(f\"The 'alphabet_order' {alphabet_order} isn't valid. Choose from either 1 or 3\")\n</code></pre>"},{"location":"reference/sequence/#sequence.get_equivalent_indices","title":"get_equivalent_indices","text":"<pre><code>get_equivalent_indices(target: Sequence = None, query: Sequence = None, mutation_allowed: bool = False) -&gt; tuple[list[int], list[int]]\n</code></pre> <p>From two sequences, find the indices where both sequences are equal</p> <p>Parameters:</p> <ul> <li> <code>target</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>The first sequence to compare</p> </li> <li> <code>query</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>The second sequence to compare</p> </li> <li> <code>mutation_allowed</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether equivalent indices can exist at mutation sites</p> </li> </ul> <p>Returns:     The pair of indices where the sequences align.         Ex: sequence1 = A B C D E F ...             sequence2 = A B - D E F ...         returns        [0,1,  3,4,5, ...],                        [0,1,  2,3,4, ...]</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def get_equivalent_indices(target: Sequence = None, query: Sequence = None, mutation_allowed: bool = False) \\\n        -&gt; tuple[list[int], list[int]]:\n    \"\"\"From two sequences, find the indices where both sequences are equal\n\n    Args:\n        target: The first sequence to compare\n        query: The second sequence to compare\n        mutation_allowed: Whether equivalent indices can exist at mutation sites\n    Returns:\n        The pair of indices where the sequences align.\n            Ex: sequence1 = A B C D E F ...\n                sequence2 = A B - D E F ...\n            returns        [0,1,  3,4,5, ...],\n                           [0,1,  2,3,4, ...]\n    \"\"\"\n    # alignment: Sequence = None\n    #     alignment: An existing Bio.Align.Alignment object\n    # if alignment is None:\n    if target is not None and query is not None:\n        # # Get all mutations from the alignment of sequence1 and sequence2\n        mutations = generate_mutations(target, query, keep_gaps=True, return_all=True)\n        # alignment = generate_alignment(target, query)\n        # alignment.inverse_indices\n        # return\n    else:\n        raise ValueError(f\"Can't {get_equivalent_indices.__name__} without passing either 'alignment' or \"\n                         f\"'target' and 'query'\")\n    # else:  # Todo this may not be ever useful since the alignment needs to go into the generate_mutations()\n    #     raise NotImplementedError(f\"Set {get_equivalent_indices.__name__} up with an Alignment object from Bio.Align\")\n\n    target_mutations = ''.join([mutation['to'] for mutation in mutations.values()])\n    query_mutations = ''.join([mutation['from'] for mutation in mutations.values()])\n    logger.debug(f\"Sequence info:\\ntarget :{target_mutations}\\nquery  :{query_mutations}\")\n    # Get only those indices where there is an aligned aa on the opposite chain\n    sequence1_indices, sequence2_indices = [], []\n    # to_idx = from_idx = 0\n    to_idx, from_idx = count(), count()\n    # sequence1 'from' is fixed, sequence2 'to' is moving\n    for mutation_idx, mutation in enumerate(mutations.values()):\n        if mutation['to'] == mutation['from']:  # They are equal\n            sequence1_indices.append(next(from_idx))  # from_idx)\n            sequence2_indices.append(next(to_idx))  # to_idx)\n        elif mutation['from'] == '-':  # increment to_idx/fixed_idx\n            # to_idx += 1\n            next(to_idx)\n        elif mutation['to'] == '-':  # increment from_idx/moving_idx\n            # from_idx += 1\n            next(from_idx)\n        elif mutation['to'] != mutation['from']:\n            if mutation_allowed:\n                sequence1_indices.append(next(from_idx))\n                sequence2_indices.append(next(to_idx))\n            else:\n                next(from_idx)\n                next(to_idx)\n            # to_idx += 1\n            # from_idx += 1\n        else:  # What else is there\n            target_mutations = ''.join([mutation['to'] for mutation in mutations.values()])\n            query_mutations = ''.join([mutation['from'] for mutation in mutations.values()])\n            raise RuntimeError(f\"This should never be reached. Ran into error at index {mutation_idx}:\\n\"\n                               f\"{mutation}\\nSequence info:\\ntarget :{target_mutations}\\nquery  :{query_mutations}\")\n        # else:  # They are equal\n        #     sequence1_indices.append(next(from_idx))  # from_idx)\n        #     sequence2_indices.append(next(to_idx))  # to_idx)\n        #     # to_idx += 1\n        #     # from_idx += 1\n\n    return sequence1_indices, sequence2_indices\n</code></pre>"},{"location":"reference/sequence/#sequence.get_lod","title":"get_lod","text":"<pre><code>get_lod(frequencies: dict[protein_letters_literal, float], background: dict[protein_letters_literal, float], as_int: bool = True) -&gt; dict[str, int | float]\n</code></pre> <p>Get the log of the odds that an amino acid is in a frequency distribution compared to a background frequency</p> <p>Parameters:</p> <ul> <li> <code>frequencies</code>             (<code>dict[protein_letters_literal, float]</code>)         \u2013          <p>{'A': 0.11, 'C': 0.01, 'D': 0.034, ...}</p> </li> <li> <code>background</code>             (<code>dict[protein_letters_literal, float]</code>)         \u2013          <p>{'A': 0.10, 'C': 0.02, 'D': 0.04, ...}</p> </li> <li> <code>as_int</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to round the lod values to an integer</p> </li> </ul> <p>Returns:      The log of odds for each amino acid type {'A': 2, 'C': -9, 'D': -1, ...}</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def get_lod(frequencies: dict[protein_letters_literal, float],\n            background: dict[protein_letters_literal, float], as_int: bool = True) -&gt; dict[str, int | float]:\n    \"\"\"Get the log of the odds that an amino acid is in a frequency distribution compared to a background frequency\n\n    Args:\n        frequencies: {'A': 0.11, 'C': 0.01, 'D': 0.034, ...}\n        background: {'A': 0.10, 'C': 0.02, 'D': 0.04, ...}\n        as_int: Whether to round the lod values to an integer\n    Returns:\n         The log of odds for each amino acid type {'A': 2, 'C': -9, 'D': -1, ...}\n    \"\"\"\n    lods = {}\n    for aa, freq in frequencies.items():\n        try:  # Todo why is this 2. * the log2? I believe this is a heuristic of BLOSUM62. This should be removed...\n            lods[aa] = float(2. * log2(freq / background[aa]))  # + 0.0\n        except ValueError:  # math domain error\n            lods[aa] = -9\n        except KeyError:\n            if aa in protein_letters_alph1:\n                raise KeyError(f'{aa} was not in the background frequencies: {\", \".join(background)}')\n            else:  # We shouldn't worry about a missing value if it's not an amino acid\n                continue\n        except ZeroDivisionError:  # background is 0. We may need a pseudocount...\n            raise ZeroDivisionError(f'{aa} has a background frequency of 0. Consider adding a pseudocount')\n        # if lods[aa] &lt; -9:\n        #     lods[aa] = -9\n        # elif round_lod:\n        #     lods[aa] = round(lods[aa])\n\n    if as_int:\n        return {aa: (int(value) if value &gt;= -9 else -9) for aa, value in lods.items()}\n    else:  # ensure that -9 is the lowest value (formatting issues if 2 digits)\n        return {aa: (value if value &gt;= -9 else -9) for aa, value in lods.items()}\n</code></pre>"},{"location":"reference/sequence/#sequence.parse_pssm","title":"parse_pssm","text":"<pre><code>parse_pssm(file: AnyStr, **kwargs) -&gt; dict[int, dict[str, str | float | int | dict[str, int]]]\n</code></pre> <p>Take the contents of a pssm file, parse, and input into a pose profile dictionary.</p> <p>Resulting dictionary is indexed according to the values in the pssm file</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file on disk</p> </li> </ul> <p>Returns:     Dictionary containing residue indexed profile information         i.e. {1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},               2: {}, ...}</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def parse_pssm(file: AnyStr, **kwargs) -&gt; dict[int, dict[str, str | float | int | dict[str, int]]]:\n    \"\"\"Take the contents of a pssm file, parse, and input into a pose profile dictionary.\n\n    Resulting dictionary is indexed according to the values in the pssm file\n\n    Args:\n        file: The location of the file on disk\n    Returns:\n        Dictionary containing residue indexed profile information\n            i.e. {1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},\n                  2: {}, ...}\n    \"\"\"\n    with open(file, 'r') as f:\n        lines = f.readlines()\n\n    pose_dict = {}\n    for line in lines:\n        line_data = line.strip().split()\n        if len(line_data) == 44:\n            residue_number = int(line_data[0])\n            pose_dict[residue_number] = \\\n                dict(zip(protein_letters_alph3,\n                         [x / 100. for x in map(int, line_data[22:len(\n                             protein_letters_alph3) + 22])]))\n            # pose_dict[residue_number] = aa_counts_alph3.copy()\n            # for i, aa in enumerate(protein_letters_alph3, 22):\n            #     # Get normalized counts for pose_dict\n            #     pose_dict[residue_number][aa] = int(line_data[i]) / 100.\n\n            # for i, aa in enumerate(protein_letters_alph3, 2):\n            #     pose_dict[residue_number]['lod'][aa] = line_data[i]\n            pose_dict[residue_number]['lod'] = \\\n                dict(zip(protein_letters_alph3, line_data[2:len(\n                    protein_letters_alph3) + 2]))\n            pose_dict[residue_number]['type'] = line_data[1]\n            pose_dict[residue_number]['info'] = float(line_data[42])\n            pose_dict[residue_number]['weight'] = float(line_data[43])\n\n    return pose_dict\n</code></pre>"},{"location":"reference/sequence/#sequence.parse_hhblits_pssm","title":"parse_hhblits_pssm","text":"<pre><code>parse_hhblits_pssm(file: AnyStr, null_background: bool = True, **kwargs) -&gt; ProfileDict\n</code></pre> <p>Take contents of protein.hmm, parse file and input into pose_dict. File is Single AA code alphabetical order</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>)         \u2013          <p>The file to parse, typically with the extension '.hmm'</p> </li> <li> <code>null_background</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to use the null background for the specific protein</p> </li> </ul> <p>Returns:     Dictionary containing residue indexed profile information         Ex: {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...}, 'type': 'W', 'info': 0.00,                  'weight': 0.00}, {...}}</p> Source code in <code>symdesign/sequence/__init__.py</code> <pre><code>def parse_hhblits_pssm(file: AnyStr, null_background: bool = True, **kwargs) -&gt; ProfileDict:\n    \"\"\"Take contents of protein.hmm, parse file and input into pose_dict. File is Single AA code alphabetical order\n\n    Args:\n        file: The file to parse, typically with the extension '.hmm'\n        null_background: Whether to use the null background for the specific protein\n    Returns:\n        Dictionary containing residue indexed profile information\n            Ex: {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...}, 'type': 'W', 'info': 0.00,\n                     'weight': 0.00}, {...}}\n    \"\"\"\n    null_bg = latest_uniclust_background_frequencies\n\n    def to_freq(value: str) -&gt; float:\n        if value == '*':  # When frequency is zero\n            return 0.0001\n        else:\n            # Equation: value = -1000 * log_2(frequency)\n            return 2 ** (-int(value) / 1000)\n\n    with open(file, 'r') as f:\n        lines = f.readlines()\n\n    evolutionary_profile = {}\n    dummy = 0.\n    read = False\n    for line in lines:\n        if not read:\n            if line[0:1] == '#':\n                read = True\n        else:\n            if line[0:4] == 'NULL':\n                if null_background:  # Use the provided null background from the profile search\n                    null, *background_values = line.strip().split()\n                    # null = 'NULL', background_values = list[str] ['3706', '5728', ...]\n                    null_bg = {aa: to_freq(value) for value, aa in zip(background_values,\n                                                                       protein_letters_alph3)}\n\n            if len(line.split()) == 23:\n                residue_type, residue_number, *position_values = line.strip().split()\n                aa_freqs = {aa: to_freq(value) for value, aa in zip(position_values,\n                                                                    protein_letters_alph1)}\n\n                evolutionary_profile[int(residue_number)] = \\\n                    dict(lod=get_lod(aa_freqs, null_bg), type=residue_type, info=dummy, weight=dummy, **aa_freqs)\n\n    return evolutionary_profile\n</code></pre>"},{"location":"reference/sequence/constants/","title":"constants","text":""},{"location":"reference/sequence/constants/#sequence.constants.aa_molecular_weights","title":"aa_molecular_weights  <code>module-attribute</code>","text":"<pre><code>aa_molecular_weights = array([89.09, 121.15, 133.1, 147.13, 165.19, 75.07, 155.16, 131.17, 146.19, 131.17, 149.21, 132.12, 115.13, 146.15, 174.2, 105.09, 119.12, 117.15, 204.23, 181.19])\n</code></pre> <p>These values are for individual amino acids (from benchling), subtract h20_mass to get weight in a polymer</p>"},{"location":"reference/sequence/constants/#sequence.constants.aa_polymer_molecular_weights","title":"aa_polymer_molecular_weights  <code>module-attribute</code>","text":"<pre><code>aa_polymer_molecular_weights = array([71.0788, 103.1388, 115.0886, 129.1155, 147.1766, 57.0519, 137.1411, 113.1594, 128.1741, 113.1594, 131.1926, 114.1038, 97.1167, 128.1307, 156.1875, 87.0782, 101.1051, 99.1326, 186.2132, 163.176])\n</code></pre> <p>These values are for amino acids in a polypeptide. Add the weight of one water molecule to get the individual mass</p>"},{"location":"reference/sequence/constants/#sequence.constants.instability_array","title":"instability_array  <code>module-attribute</code>","text":"<pre><code>instability_array = array([[1.0, 44.94, -7.49, 1.0, 1.0, 1.0, -7.49, 1.0, 1.0, 1.0, 1.0, 1.0, 20.26, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 20.26, 1.0, 1.0, 1.0, 33.6, 1.0, 1.0, 20.26, 33.6, 1.0, 20.26, -6.54, 1.0, 1.0, 33.6, -6.54, 24.68, 1.0], [1.0, 1.0, 1.0, 1.0, -6.54, 1.0, 1.0, 1.0, -7.49, 1.0, 1.0, 1.0, 1.0, 1.0, -6.54, 20.26, -14.03, 1.0, 1.0, 1.0], [1.0, 44.94, 20.26, 33.6, 1.0, 1.0, -6.54, 20.26, 1.0, 1.0, 1.0, 1.0, 20.26, 20.26, 1.0, 20.26, 1.0, 1.0, -14.03, 1.0], [1.0, 1.0, 13.34, 1.0, 1.0, 1.0, 1.0, 1.0, -14.03, 1.0, 1.0, 1.0, 20.26, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 33.6], [-7.49, 1.0, 1.0, -6.54, 1.0, 13.34, 1.0, -7.49, -7.49, 1.0, 1.0, -7.49, 1.0, 1.0, 1.0, 1.0, -7.49, 1.0, 13.34, -7.49], [1.0, 1.0, 1.0, 1.0, -9.37, -9.37, 1.0, 44.94, 24.68, 1.0, 1.0, 24.68, -1.88, 1.0, 1.0, 1.0, -6.54, 1.0, -1.88, 44.94], [1.0, 1.0, 1.0, 44.94, 1.0, 1.0, 13.34, 1.0, -7.49, 20.26, 1.0, 1.0, -1.88, 1.0, 1.0, 1.0, 1.0, -7.49, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, -7.49, 1.0, -7.49, 1.0, -7.49, 33.6, 1.0, -6.54, 24.68, 33.6, 1.0, 1.0, -7.49, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -7.49, 1.0, 1.0, 1.0, 20.26, 33.6, 20.26, 1.0, 1.0, 1.0, 24.68, 1.0], [13.34, 1.0, 1.0, 1.0, 1.0, 1.0, 58.28, 1.0, 1.0, 1.0, -1.88, 1.0, 44.94, -6.54, -6.54, 44.94, -1.88, 1.0, 1.0, 24.68], [1.0, -1.88, 1.0, 1.0, -14.03, -14.03, 1.0, 44.94, 24.68, 1.0, 1.0, 1.0, -1.88, -6.54, 1.0, 1.0, -7.49, 1.0, -9.37, 1.0], [20.26, -6.54, -6.54, 18.38, 20.26, 1.0, 1.0, 1.0, 1.0, 1.0, -6.54, 1.0, 20.26, 20.26, -6.54, 20.26, 1.0, 20.26, -1.88, 1.0], [1.0, -6.54, 20.26, 20.26, -6.54, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 20.26, 20.26, 1.0, 44.94, 1.0, -6.54, 1.0, -6.54], [1.0, 1.0, 1.0, 1.0, 1.0, -7.49, 20.26, 1.0, 1.0, 1.0, 1.0, 13.34, 20.26, 20.26, 58.28, 44.94, 1.0, 1.0, 58.28, -6.54], [1.0, 33.6, 1.0, 20.26, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 44.94, 20.26, 20.26, 20.26, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 20.26, 13.34, -7.49, 1.0, 1.0, 1.0, 1.0, 1.0, -14.03, 1.0, -6.54, 1.0, 1.0, 1.0, 1.0, -14.03, 1.0], [1.0, 1.0, -14.03, 1.0, 1.0, -7.49, 1.0, 1.0, -1.88, 1.0, 1.0, 1.0, 20.26, 1.0, 1.0, 1.0, -7.49, 1.0, 1.0, -6.54], [-14.03, 1.0, 1.0, 1.0, 1.0, -9.37, 24.68, 1.0, 1.0, 13.34, 24.68, 13.34, 1.0, 1.0, 1.0, 1.0, -14.03, -7.49, 1.0, 1.0], [24.68, 1.0, 24.68, -6.54, 1.0, -7.49, 13.34, 1.0, 1.0, 1.0, 44.94, 1.0, 13.34, 1.0, -15.91, 1.0, -7.49, 1.0, -9.37, 13.34]])\n</code></pre> <p>Derived from Guruprasad, K., Reddy, B.V.B. and Pandit, M.W. (1990) Correlation between stability of a protein and its dipeptide composition: a novel approach for predicting in vivo stability of a protein from its primary sequence. Protein Eng. 4,155-161. Table III.</p>"},{"location":"reference/sequence/expression/","title":"expression","text":"<p>Add expression tags onto the termini of specific designs</p>"},{"location":"reference/sequence/expression/#sequence.expression.get_sequence_features","title":"get_sequence_features","text":"<pre><code>get_sequence_features(sequence: Sequence[str | int]) -&gt; dict[str, float]\n</code></pre> <p>For the specified amino acid sequence perform biochemical calculations related to solution properties</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str | int]</code>)         \u2013          <p>The sequence to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, float]</code>         \u2013          <p>A feature dictionary with each feature from the set below mapped to a float.</p> </li> <li> <code>dict[str, float]</code>         \u2013          <p>{'extinction_coefficient_reduced', 'extinction_coefficient_oxidized', 'instability_index', 'molecular_weight', 'isoelectric_point', 'absorbance_0.1_red', 'absorbance_0.1_ox', }</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def get_sequence_features(sequence: Sequence[str | int]) -&gt; dict[str, float]:\n    \"\"\"For the specified amino acid sequence perform biochemical calculations related to solution properties\n\n    Args:\n        sequence: The sequence to measure\n\n    Returns:\n        A feature dictionary with each feature from the set below mapped to a float.\n        {'extinction_coefficient_reduced',\n         'extinction_coefficient_oxidized',\n         'instability_index',\n         'molecular_weight',\n         'isoelectric_point',\n         'absorbance_0.1_red',\n         'absorbance_0.1_ox',\n         }\n    \"\"\"\n    sequence = format_sequence_to_numeric(sequence)\n    ox_coef, red_coef = molecular_extinction_coefficient(sequence)\n    mw = calculate_protein_molecular_weight(sequence)\n    abs_01_ox = ox_coef / mw\n    abs_01_red = red_coef / mw\n    return {\n        'number_of_residues': len(sequence),\n        'extinction_coefficient_reduced': red_coef,\n        'extinction_coefficient_oxidized': ox_coef,\n        'instability_index': calculate_instability_index(sequence),\n        'molecular_weight': mw,\n        'isoelectric_point': calculate_protein_isoelectric_point(sequence),\n        'absorbance_0.1_red': abs_01_red,\n        'absorbance_0.1_ox': abs_01_ox\n    }\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.calculate_protein_molecular_weight","title":"calculate_protein_molecular_weight","text":"<pre><code>calculate_protein_molecular_weight(sequence: Sequence[str | int]) -&gt; float\n</code></pre> <p>Find the total molecular mass for the amino acids present in a sequence</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str | int]</code>)         \u2013          <p>The sequence to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The molecular weight in daltons/atomic mass units</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def calculate_protein_molecular_weight(sequence: Sequence[str | int]) -&gt; float:\n    \"\"\"Find the total molecular mass for the amino acids present in a sequence\n\n    Args:\n        sequence: The sequence to measure\n\n    Returns:\n        The molecular weight in daltons/atomic mass units\n    \"\"\"\n    seq_index = format_sequence_to_numeric(sequence)\n\n    # Add h2o_mass as the n- and c-term are free\n    return aa_polymer_molecular_weights[seq_index].sum() + h2o_mass\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.calculate_protein_isoelectric_point","title":"calculate_protein_isoelectric_point","text":"<pre><code>calculate_protein_isoelectric_point(sequence: Sequence[str | int], threshold: float = 0.01) -&gt; float\n</code></pre> <p>Find the total isoelectric point (pI) for the amino acids present in a sequence</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str | int]</code>)         \u2013          <p>The sequence to measure</p> </li> <li> <code>threshold</code>             (<code>float</code>, default:                 <code>0.01</code> )         \u2013          <p>The pH unit value to consider the found pI passing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The molecular weight in daltons/atomic mass units</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def calculate_protein_isoelectric_point(sequence: Sequence[str | int], threshold: float = 0.01) -&gt; float:\n    \"\"\"Find the total isoelectric point (pI) for the amino acids present in a sequence\n\n    Args:\n        sequence: The sequence to measure\n        threshold: The pH unit value to consider the found pI passing\n\n    Returns:\n        The molecular weight in daltons/atomic mass units\n    \"\"\"\n    seq_index = format_sequence_to_numeric(sequence)\n    # # Take the mean of every individual pI\n    # return aa_isoelectric_point[seq_index].mean()\n\n    n_cys = seq_index.count(cys_index)\n    n_asp = seq_index.count(asp_index)\n    n_glu = seq_index.count(glu_index)\n    n_his = seq_index.count(his_index)\n    n_lys = seq_index.count(lys_index)\n    n_arg = seq_index.count(arg_index)\n    n_tyr = seq_index.count(tyr_index)\n\n    # def calculate_aa_charge_contribution(aa: str, pka_sol: float = 7.5):\n    #     negative_charged = {'C': 7.555, 'D': 3.872, 'E': 4.412, 'Y': 10.85}\n    #     positive_charged = {'H': 5.637, 'K': 9.052, 'R': 11.84}\n    #     pka_aa = positive_charged.get(aa)\n    #     if pka_aa:\n    #         pka_aa = 1 / (1 + 10**(pka_sol-pka_aa))  # positive\n    #     else:\n    #         pka_aa = -1 / (1 + 10**(negative_charged[aa]-pka_sol))  # negative\n\n    # Array version\n    # Make an array of the counts padding respective ends with n- and c-termini counts (i.e. 1)\n    positive_counts = np.array([1, n_his, n_lys, n_arg])\n    negative_counts = np.array([n_cys, n_asp, n_glu, n_tyr, 1])\n\n    iterative_multiplier = 1\n    delta_magnitude = 4\n    test_pi = 7.  # - delta_magnitude  # &lt;- ensures that the while loop search starts at 7.\n    # pka_aa_pos = 1 / (1 + 10 ** (test_pi - positive_charged_pka))  # positive\n    # pka_aa_neg = -1 / (1 + 10 ** (negative_charged_pka - test_pi))  # negative\n    # pos_charge = pka_aa_pos * positive_counts\n    # neg_charge = pka_aa_neg * negative_counts\n    # total_charge = neg_charge + pos_charge\n    # remaining_charge = abs(total_charge)\n    # negative_last = abs(neg_charge) &gt; abs(pos_charge)\n    # neg_charge = pos_charge = negative_last = 0\n    if threshold &lt; 0:\n        raise ValueError(\n            f\"The argument 'threshold' can't be lower than 0. Got {threshold}\")\n    remaining_charge = threshold + 1\n    direction = 0\n    count_ = count()\n    tested_pis = {}\n    while remaining_charge &gt;= threshold:\n        if next(count_) &gt; 0:\n            # Check which type of charge has a larger magnitude\n            if abs(neg_charge) &gt; abs(pos_charge):\n                # Estimation is more negative\n                direction = -1\n                if last_direction + direction == 0:\n                    # pass  # This was also negative last iteration, haven't gone far enough\n                # else:  # Searched this direction far enough, turn around and decrease step\n                    iterative_multiplier /= 2\n            else:  # Positive larger\n                direction = 1\n                if last_direction + direction == 0:\n                    # Searched this direction far enough, turn around and decrease step\n                    iterative_multiplier /= 2\n                # else:\n                #     pass  # This was positive last iteration, haven't gone far enough\n        last_direction = direction\n\n        # Calculate the error\n        pi_modifier = direction * delta_magnitude * iterative_multiplier\n        test_pi_ = pi_modifier + test_pi\n        if test_pi_ in tested_pis:\n            # This has already been checked. Cut the space in half again\n            iterative_multiplier /= 2\n            pi_modifier = direction * delta_magnitude * iterative_multiplier\n\n        test_pi += pi_modifier\n\n        pka_aa_pos = 1 / (1 + 10**(test_pi-positive_charged_pka))  # positive\n        pka_aa_neg = -1 / (1 + 10**(negative_charged_pka-test_pi))  # negative\n        # Multiply the individual contributions by the number of observations\n        pos_charge = (pka_aa_pos * positive_counts).sum()\n        neg_charge = (pka_aa_neg * negative_counts).sum()\n        total_charge = neg_charge + pos_charge\n        remaining_charge = abs(total_charge)\n        logger.debug(f'pI = {test_pi}, remainder = {remaining_charge}')  # Iteration {next(count_)}:\n        tested_pis[test_pi] = remaining_charge\n\n    return test_pi + (total_charge/2)  # Approximately the real pi\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.calculate_instability_index","title":"calculate_instability_index","text":"<pre><code>calculate_instability_index(sequence: Sequence[str | int]) -&gt; float\n</code></pre> <p>Find the total instability index for the amino acids present in a sequence. See PMID:2075190</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str | int]</code>)         \u2013          <p>The sequence to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The value of the stability index where a value less than 40 indicates stability</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def calculate_instability_index(sequence: Sequence[str | int]) -&gt; float:\n    \"\"\"Find the total instability index for the amino acids present in a sequence. See PMID:2075190\n\n    Args:\n        sequence: The sequence to measure\n\n    Returns:\n        The value of the stability index where a value less than 40 indicates stability\n    \"\"\"\n\n    seq_index = format_sequence_to_numeric(sequence)\n    return (instability_array[seq_index[:-1], seq_index[1:]].sum() * 10) / len(sequence)\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.molecular_extinction_coefficient","title":"molecular_extinction_coefficient","text":"<pre><code>molecular_extinction_coefficient(sequence: Sequence[str | int]) -&gt; tuple[float, float]\n</code></pre> <p>Calculate the molecular extinction coefficient for an amino acid sequence using the formula E(ProtOx) = Numb(Tyr) * Ext(Tyr) + Numb(Trp) * Ext(Trp) + Numb(Cystine) * Ext(Cystine) E(ProtRed) = Numb(Tyr) * Ext(Tyr) + Numb(Trp) * Ext(Trp)</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str | int]</code>)         \u2013          <p>The sequence to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, float]</code>         \u2013          <p>The pair of molecular extinction coefficients, first with all Cysteine oxidized, then reduced</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def molecular_extinction_coefficient(sequence: Sequence[str | int]) -&gt; tuple[float, float]:\n    \"\"\"Calculate the molecular extinction coefficient for an amino acid sequence using the formula\n    E(ProtOx) = Numb(Tyr) * Ext(Tyr) + Numb(Trp) * Ext(Trp) + Numb(Cystine) * Ext(Cystine)\n    E(ProtRed) = Numb(Tyr) * Ext(Tyr) + Numb(Trp) * Ext(Trp)\n\n    Args:\n        sequence: The sequence to measure\n\n    Returns:\n        The pair of molecular extinction coefficients, first with all Cysteine oxidized, then reduced\n    \"\"\"\n    seq_index = format_sequence_to_numeric(sequence)\n    n_tyr = seq_index.count(tyr_index)\n    n_trp = seq_index.count(trp_index)\n    n_cys = seq_index.count(cys_index)\n    coef_ox = n_tyr*ext_coef_tyr + n_trp*ext_coef_trp + n_cys*ext_coef_cys_ox\n    coef_red = n_tyr*ext_coef_tyr + n_trp*ext_coef_trp  # + n_cys*ext_coef_cys_red\n    return coef_ox, coef_red\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.find_matching_expression_tags","title":"find_matching_expression_tags","text":"<pre><code>find_matching_expression_tags(uniprot_id: str = None, entity_id: str = None, pdb_code: str = None, chain: str = None, **kwargs) -&gt; list | list[dict[str, str]]\n</code></pre> <p>Find matching expression tags by PDB ID reference</p> <p>Parameters:</p> <ul> <li> <code>uniprot_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The uniprot_id to query tags from</p> </li> <li> <code>entity_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          </li> <li> <code>pdb_code</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The pdb to query tags from. Requires chain argument as well</p> </li> <li> <code>chain</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain to query tags from. Requires pdb argument as well</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>alignment_length</code>         \u2013          <p>int = 12 - The length to slice the sequence plus any identified tags</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list | list[dict[str, str]]</code>         \u2013          <p>[{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...], or [] if none found</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def find_matching_expression_tags(uniprot_id: str = None, entity_id: str = None,\n                                  pdb_code: str = None, chain: str = None, **kwargs) -&gt; list | list[dict[str, str]]:\n    \"\"\"Find matching expression tags by PDB ID reference\n\n    Args:\n        uniprot_id: The uniprot_id to query tags from\n        entity_id:\n        pdb_code: The pdb to query tags from. Requires chain argument as well\n        chain: The chain to query tags from. Requires pdb argument as well\n\n    Keyword Args:\n        alignment_length: int = 12 - The length to slice the sequence plus any identified tags\n\n    Returns:\n        [{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...], or [] if none found\n    \"\"\"\n    #         (dict): {'n': {His Tag: 2}, 'c': {Spy Catcher: 1},\n    #                  'matching_tags': [{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...]}\n    matching_pdb_tags = []\n    if entity_id is None:\n        entity_ids = []\n        if pdb_code and chain:\n            uniprot_id = pull_uniprot_id_by_pdb(uniprot_pdb_d, pdb_code, chain=chain)\n            if uniprot_id is None:\n                logger.error(f\"The 'pdb_code'.'chain' combination '{pdb_code}.{chain}' found no valid identifiers\")\n                return matching_pdb_tags\n    else:\n        entity_ids = [entity_id]\n\n    if uniprot_id:\n        entity_ids = query.pdb.pdb_id_matching_uniprot_id(uniprot_id=uniprot_id)\n    elif entity_ids:\n        pass\n    else:\n        logger.error(\"One of 'uniprot_id' OR 'entity_id' OR 'pdb_code' and 'chain' is required\")\n        return matching_pdb_tags\n    # From PDB API\n    partner_sequences = [query.pdb.get_entity_reference_sequence(entity_id=entity_id) for entity_id in entity_ids]\n    for sequence in partner_sequences:\n        matching_pdb_tags.extend(find_expression_tags(sequence, **kwargs))\n\n    return matching_pdb_tags\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.report_termini_availability","title":"report_termini_availability","text":"<pre><code>report_termini_availability(matching_pdb_tags: list[dict[str, str]], n: bool = True, c: bool = True) -&gt; str\n</code></pre> <p>From a list of possible tags, solve for the tag with the most observations in the PDB. If there are discrepancies, query the user for a solution</p> <p>Parameters:</p> <ul> <li> <code>matching_pdb_tags</code>             (<code>list[dict[str, str]]</code>)         \u2013          <p>[{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...]</p> </li> <li> <code>n</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the n-termini can be tagged</p> </li> <li> <code>c</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the c-termini can be tagged</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The termini which should be tagger, either 'n', 'c' or 'skip' if the sequence shouldn't be used</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def report_termini_availability(matching_pdb_tags: list[dict[str, str]], n: bool = True, c: bool = True) -&gt; str:\n    \"\"\"From a list of possible tags, solve for the tag with the most observations in the PDB. If there are\n    discrepancies, query the user for a solution\n\n    Args:\n        matching_pdb_tags: [{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...]\n        n: Whether the n-termini can be tagged\n        c: Whether the c-termini can be tagged\n\n    Returns:\n        The termini which should be tagger, either 'n', 'c' or 'skip' if the sequence shouldn't be used\n    \"\"\"\n    final_tag = {'name': None, 'termini': None, 'sequence': None}\n    # Next, align all the tags to the reference sequence and tally the tag location and type\n    # {'n': {his_tag: 2}, 'c': {spy_catcher: 1}}\n    pdb_tag_tally: dict[str, dict[str, int]] = {'n': defaultdict(int), 'c': defaultdict(int)}\n    for partner_tag in matching_pdb_tags:\n        # if partner_pdb_tags:\n        #     for partner_tag in partner_pdb_tags:\n        partner_termini = partner_tag['termini']\n        partner_name = partner_tag['name']\n        # if partner_name in pdb_tag_tally[partner_termini]:\n        pdb_tag_tally[partner_termini][partner_name] += 1\n        # else:\n        #     pdb_tag_tally[partner_termini][partner_name] = 1\n\n    n_term = sum([pdb_tag_tally['n'][tag_name] for tag_name in pdb_tag_tally.get('n', {})])\n    c_term = sum([pdb_tag_tally['c'][tag_name] for tag_name in pdb_tag_tally.get('c', {})])\n\n    if n_term == 0 and c_term == 0:  # No tags found\n        evidence_string = 'Based on termini availability'\n    elif not c and not n:\n        evidence_string = 'Based on termini availability'\n    else:\n        evidence_string = 'Based on prior observations and available termini'\n\n    if n_term &gt; c_term and n or (n_term &lt; c_term and n and not c):\n        termini_string = 'the \\033[38;5;208mn\\033[0;0m termini is recommended to be tagged.'\n    elif n_term &lt; c_term and c or (n_term &gt; c_term and c and not n):\n        termini_string = 'the \\033[38;5;208mC\\033[0;0m termini is recommended to be tagged.'\n    elif not c and not n:\n        termini_string = '\\033[38;5;208mneither\\033[0;0m termini can be tagged.'\n    else:  # termini = 'Both'\n        if c and not n:\n            termini_string = 'the \\033[38;5;208mc\\033[0;0m termini can be tagged.'\n        elif not c and n:\n            termini_string = 'the \\033[38;5;208mn\\033[0;0m termini can be tagged.'\n        else:\n            termini_string = '\\033[38;5;208mboth\\033[0;0m termini can be tagged.'\n\n    termini_header = 'Termini', 'Tag name', 'Count'\n    formatted_tags = [(termini, name, counts) for termini, name_counts in pdb_tag_tally.items()\n                      for name, counts in name_counts.items()]\n    if formatted_tags:\n        observation_string = ' Observed tag options are as follows:\\n\\t%s\\n' % \\\n                             '\\n\\t'.join(utils.pretty_format_table(formatted_tags, header=termini_header))\n    else:\n        observation_string = ''\n\n    print(f'{observation_string}{evidence_string}, {termini_string}')\n    return utils.validate_input(f\"Which termini would you prefer? 'skip' discards this sequence\",\n                                ['n', 'c', 'skip'])\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.select_tags_for_sequence","title":"select_tags_for_sequence","text":"<pre><code>select_tags_for_sequence(sequence_id: str, matching_pdb_tags: list[dict[str, str]], preferred: str = None, n: bool = True, c: bool = True) -&gt; dict[str, str | None]\n</code></pre> <p>From a list of possible tags, solve for the tag with the most observations in the PDB. If there are discrepancies, query the user for a solution</p> <p>Parameters:</p> <ul> <li> <code>sequence_id</code>             (<code>str</code>)         \u2013          <p>The sequence identifier</p> </li> <li> <code>matching_pdb_tags</code>             (<code>list[dict[str, str]]</code>)         \u2013          <p>[{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...]</p> </li> <li> <code>preferred</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a preferred tag provided by the user</p> </li> <li> <code>n</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the n-termini can be tagged</p> </li> <li> <code>c</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the c-termini can be tagged</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str | None]</code>         \u2013          <p>{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def select_tags_for_sequence(sequence_id: str, matching_pdb_tags: list[dict[str, str]], preferred: str = None,\n                             n: bool = True, c: bool = True) -&gt; dict[str, str | None]:\n    \"\"\"From a list of possible tags, solve for the tag with the most observations in the PDB. If there are\n    discrepancies, query the user for a solution\n\n    Args:\n        sequence_id: The sequence identifier\n        matching_pdb_tags: [{'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}, ...]\n        preferred: The name of a preferred tag provided by the user\n        n: Whether the n-termini can be tagged\n        c: Whether the c-termini can be tagged\n\n    Returns:\n        {'name': 'his_tag', 'termini': 'n', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}\n    \"\"\"\n    final_tag = {'name': None, 'termini': None, 'sequence': None}\n    # Next, align all the tags to the reference sequence and tally the tag location and type\n    # {'n': {his_tag: 2}, 'c': {spy_catcher: 1}}\n    pdb_tag_tally: dict[str, dict[str, int]] = {'n': defaultdict(int), 'c': defaultdict(int)}\n    for partner_tag in matching_pdb_tags:\n        # if partner_pdb_tags:\n        #     for partner_tag in partner_pdb_tags:\n        partner_termini = partner_tag['termini']\n        partner_name = partner_tag['name']\n        # if partner_name in pdb_tag_tally[partner_termini]:\n        pdb_tag_tally[partner_termini][partner_name] += 1\n        # else:\n        #     pdb_tag_tally[partner_termini][partner_name] = 1\n\n    n_term = sum([pdb_tag_tally['n'][tag_name] for tag_name in pdb_tag_tally.get('n', {})])\n    c_term = sum([pdb_tag_tally['c'][tag_name] for tag_name in pdb_tag_tally.get('c', {})])\n    if n_term == 0 and c_term == 0:  # No tags found\n        return final_tag\n\n    termini_header = 'Termini', 'Tag name', 'Count'\n    formatted_tags = [(termini, name, counts) for termini, name_counts in pdb_tag_tally.items()\n                      for name, counts in name_counts.items()]\n    if n_term &gt; c_term and n or (n_term &lt; c_term and n and not c):\n        termini = 'n'\n    elif n_term &lt; c_term and c or (n_term &gt; c_term and c and not n):\n        termini = 'c'\n    elif not c and not n:\n        print(f'For sequence target {sequence_id}, \\033[38;5;208mneither\\033[0;0m termini is available for tagging.\\n'\n              'You can configure tags now regardless and modify the choice later, or skip tagging.\\n'\n              'The tag options are as follows:\\n\\t%s\\n' %\n              '\\n\\t'.join(utils.pretty_format_table(formatted_tags, header=termini_header)))\n        termini = utils.validate_input(f\"Which termini would you prefer [n/c]? To skip, input 'skip'\",\n                                       ['n', 'c', 'skip'])\n        if termini == 'skip':\n            return final_tag\n    else:  # termini = 'Both'\n        if c and not n:\n            termini = 'c'\n        elif not c and n:\n            termini = 'n'\n        else:\n            print(f'For sequence target {sequence_id}, \\033[38;5;208mboth\\033[0;0m termini are available and have the '\n                  f'same number of matched tags.\\nThe tag options are as follows:\\n\\t%s\\n' %\n                  '\\n\\t'.join(utils.pretty_format_table(formatted_tags, header=termini_header)))\n            termini = utils.validate_input(f\"Which termini would you prefer [n/c]? To skip, input 'skip'\",\n                                           ['n', 'c', 'skip'])\n            if termini == 'skip':\n                return final_tag\n\n    # Find the most common tag at the specific termini\n    max_tag_type = None\n    max_tag_count = 0\n    for tag_name, tag_count in pdb_tag_tally[termini].items():\n        if tag_count &gt; max_tag_count:\n            max_tag_count = tag_count\n            max_tag_type = tag_name\n\n    # Ensure at least one tag was found\n    if max_tag_type is None:\n        return final_tag\n    else:\n        all_tags = [max_tag_type]\n        # Check if there are equally represented tags\n        for tag_name, tag_count in pdb_tag_tally[termini].items():\n            if tag_name != max_tag_type and tag_count == max_tag_count:\n                all_tags.append(tag_name)\n\n    # Finally report results to the user and solve ambiguous tags\n    tag_type = all_tags[0]\n    custom = False\n    if preferred:\n        if preferred == tag_type:\n            default = 'y'\n        else:\n            return final_tag\n            # default = 'y'\n            # logger.info(\n            #     f\"The preferred tag '{preferred}' wasn't found from observations in the PDB. Using it anyway\")\n            # # default = input(f'If you would like to proceed with it anyway, enter \"y\"'\n            # #                 f'.{query.utils.input_string}').lower()\n            # tag_type = preferred\n            # print(f'For {sequence_id}, the tag options are:\\n\\t%s'\n            #       % '\\n\\t'.join(utils.pretty_format_table([tag.values() for tag in matching_pdb_tags],\n            #                                               header=matching_pdb_tags[0].keys())))\n            # # Solve for the termini\n            # print(f'For {sequence_id}, the termini tag options are:\\n\\t%s'\n            #       % '\\n\\t'.join(utils.pretty_format_table(formatted_tags, header=('Termini', 'Tag', 'Count'))))\n            # while True:\n            #     termini_input = input(f'What termini would you like to use [n/c]?{query.utils.input_string}') \\\n            #         .lower()\n            #     if termini_input in ['n', 'c']:\n            #         recommended_termini = termini_input\n            #         break\n            #     elif termini_input == 'none':\n            #         return final_tag\n            #     else:\n            #         print(f\"Input '{termini_input}' doesn't match available options. Please try again\")\n    else:\n        logger.info(f'For {sequence_id}, the \\033[38;5;208mrecommended\\033[0;0m tag options are:\\n'\n                    f'\\tTermini-{termini} Type-{tag_type}\\n'\n                    'If the Termini or Type is undesired, you can see the underlying options by specifying '\n                    f\"'options'. Otherwise, '{tag_type}' will be chosen\")\n        default = utils.validate_input('If you would like to proceed with the \\033[38;5;208mrecommended\\033[0;0m '\n                                       \"options, enter 'y', otherwise, specify 'options' to see all possibilities\",\n                                       ['y', 'options'])\n    if default == 'y':\n        final_tag_name = tag_type\n        final_tag_termini = termini\n    else:  # if default == 'options':\n        print(f'For sequence target {sequence_id}, all tag options are as follows:\\n%s\\n' %\n              '\\n\\t'.join(utils.pretty_format_table(formatted_tags, header=termini_header)))\n        print(f'All tags:\\n{matching_pdb_tags}\\n')\n        termini = utils.validate_input(\"Which termini would you prefer [n/c]? To skip, input 'skip'\",\n                                       ['n', 'c', 'skip'])\n        if termini == 'skip':\n            return final_tag\n        final_tag_termini = termini\n\n        print('\\n\\t%s' % '\\n\\t'.join([f'{i} - {tag}' for i, tag in enumerate(pdb_tag_tally[termini], 1)]))\n        number_termini_tags = len(pdb_tag_tally[termini])\n        # print(f'\\n\\t{number_termini_tags + 1} - CUSTOM')\n        tag_input = utils.validate_input('What tag would you like to use? Enter the number of the above options',\n                                         list(map(str, range(1, 1 + number_termini_tags))))  # 2 + number_termini_tags\n        tag_input = int(tag_input)\n        if tag_input &lt;= number_termini_tags:\n            final_tag_name = list(pdb_tag_tally[termini].keys())[tag_input - 1]\n        else:  # if tag_input == number_termini_tags + 1:\n            # Todo this isn't currently available\n            print('All available tags are:\\n\\t%s' %\n                  '\\n\\t'.join([f'{i} - {tag}' for i, tag in enumerate(expression_tags, 1)]))\n            number_expression_tags = len(expression_tags)\n            tag_input = utils.validate_input('What tag would you like to use? Enter the number of the above options',\n                                             list(map(str, range(1, 1 + number_expression_tags))))\n            tag_input = int(tag_input)\n            final_tag_name = list(expression_tags.keys())[tag_input - 1]\n\n            final_tag_sequence = expression_tags[final_tag_name]\n\n    # Solve for the desired tag\n    all_matching_tag_sequences = [tag['sequence'] for tag in matching_pdb_tags\n                                  if final_tag_name == tag['name'] and final_tag_termini == tag['termini']]\n    # Todo align multiple and choose the consensus\n    # all_alignments = []\n    # max_tag_idx, max_len = None, []  # 0\n    # for idx, (tag1, tag2) in enumerate(combinations(all_matching_tags, 2)):\n    #     alignment = generate_alignment(tag1, tag2)\n    #     all_alignments.append(alignment)\n    #     # if max_len &lt; alignment[4]:  # the length of alignment\n    #     max_len.append(alignment[4])\n    #     # have to find the alignment with the max length, then find which one of the sequences has the max length for\n    #     # multiple alignments, then need to select all alignments to this sequence to generate the MSA\n\n    if all_matching_tag_sequences:  # For now, grab the first available tag\n        logger.debug(f'Grabbing the first matching tag out of {len(all_matching_tag_sequences)} possible')\n        final_tag_sequence = all_matching_tag_sequences[0]\n    else:\n        logger.critical(f\"{select_tags_for_sequence.__name__}: This logic shouldn't have been possible\")\n        return final_tag\n\n    return dict(name=final_tag_name, termini=final_tag_termini, sequence=final_tag_sequence)\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.add_expression_tag","title":"add_expression_tag","text":"<pre><code>add_expression_tag(tag: str, sequence: str) -&gt; str\n</code></pre> <p>Add an expression tag to a sequence by aligning a tag specified with PDB reference sequence to the sequence</p> <p>Parameters:</p> <ul> <li> <code>tag</code>             (<code>str</code>)         \u2013          <p>The tag with additional PDB reference sequence appended</p> </li> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The final sequence with the tag added</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def add_expression_tag(tag: str, sequence: str) -&gt; str:\n    \"\"\"Add an expression tag to a sequence by aligning a tag specified with PDB reference sequence to the sequence\n\n    Args:\n        tag: The tag with additional PDB reference sequence appended\n        sequence: The sequence of interest\n\n    Returns:\n        The final sequence with the tag added\n    \"\"\"\n    if not tag:\n        return sequence\n    alignment = generate_alignment(tag, sequence)\n    tag_seq, seq = alignment\n    # print('Expression TAG alignment:', alignment[0])\n    # score = alignment.score\n    # # tag_seq, seq, score, *_ = alignment\n    # # score = alignment[2]  # first alignment, grab score value\n    # # print('Expression TAG alignment score:', score)\n    # # if score == 0:  # TODO find the correct score for a bad alignment to indicate there was no productive alignment?\n    # #     # alignment[0][4]:  # the length of alignment\n    # #     # match_score = score / len(sequence)  # could also use which ever sequence is greater\n    # #     return sequence\n    # # print(alignment[0])\n    # # print(tag_seq)\n    # # print(seq)\n    # # starting_index_of_seq2 = seq.find(sequence[0])\n    # # i = -starting_index_of_seq2 + zero_offset  # make 1 index so residue value starts at 1\n    final_seq = ''\n    for seq1_aa, seq2_aa in zip(tag_seq, seq):\n        if seq2_aa == '-':\n            # if seq1_aa in protein_letters_alph1:\n            final_seq += seq1_aa\n        else:\n            final_seq += seq2_aa\n\n    return final_seq\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.find_expression_tags","title":"find_expression_tags","text":"<pre><code>find_expression_tags(sequence: str, alignment_length: int = 12) -&gt; list | list[dict[str, str]]\n</code></pre> <p>Find all expression_tags on an input sequence from a reference set of expression_tags. Returns the matching tag sequence with additional protein sequence context equal to the passed alignment_length</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'</p> </li> <li> <code>alignment_length</code>             (<code>int</code>, default:                 <code>12</code> )         \u2013          <p>The length to slice the sequence plus any identified tags</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list | list[dict[str, str]]</code>         \u2013          <p>A list of the available tags with a featured dictionary for each tag. Formatted as - {'name': str, 'termini': 'n'/'c', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}. Returns [] if no tags are found</p> </li> </ul> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def find_expression_tags(sequence: str, alignment_length: int = 12) -&gt; list | list[dict[str, str]]:\n    \"\"\"Find all expression_tags on an input sequence from a reference set of expression_tags. Returns the matching tag\n    sequence with additional protein sequence context equal to the passed alignment_length\n\n    Args:\n        sequence: The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'\n        alignment_length: The length to slice the sequence plus any identified tags\n\n    Returns:\n        A list of the available tags with a featured dictionary for each tag. Formatted as -\n            {'name': str, 'termini': 'n'/'c', 'sequence': 'MSGHHHHHHGKLKPNDLRI'}. Returns [] if no tags are found\n    \"\"\"\n    half_sequence_length = len(sequence) / 2\n    matching_tags = []\n    for name, tag_sequence in expression_tags.items():\n        tag_index = sequence.find(tag_sequence)\n        if tag_index == -1:  # No match was found\n            continue\n        # Save the tag name, the termini of the sequence it is closest to, and the source sequence context\n        tag_length = len(tag_sequence)\n        alignment_size = tag_length + alignment_length\n        if tag_index &lt; half_sequence_length:\n            termini = 'n'\n            matching_sequence = sequence[tag_index:tag_index + alignment_size]\n        else:\n            termini = 'c'\n            final_tag_index = tag_index + tag_length\n            matching_sequence = sequence[final_tag_index - alignment_size:final_tag_index]\n        matching_tags.append({'name': name, 'termini': termini, 'sequence': matching_sequence})\n\n    return matching_tags\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.remove_internal_tags","title":"remove_internal_tags","text":"<pre><code>remove_internal_tags(sequence: str, tag_names: list[str] = None) -&gt; str\n</code></pre> <p>Remove matching tag sequences only, from the specified sequence</p> <p>Defaults to known tags in constants.expression_tags</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'</p> </li> <li> <code>tag_names</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>If only certain tags should be removed, a list with the names of known tags</p> </li> </ul> <p>Returns:     'MSGGKLKPNDLRI...' The modified sequence without the tag</p> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def remove_internal_tags(sequence: str, tag_names: list[str] = None) -&gt; str:\n    \"\"\"Remove matching tag sequences only, from the specified sequence\n\n    Defaults to known tags in constants.expression_tags\n\n    Args:\n        sequence: The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'\n        tag_names: If only certain tags should be removed, a list with the names of known tags\n    Returns:\n        'MSGGKLKPNDLRI...' The modified sequence without the tag\n    \"\"\"\n    if tag_names:\n        _expression_tags = {tag_name: expression_tags[tag_name] for tag_name in tag_names}\n    else:\n        _expression_tags = expression_tags\n\n    for name, tag_sequence in _expression_tags.items():\n        tag_index = sequence.find(tag_sequence)\n        if tag_index == -1:  # No match was found\n            continue\n\n        # Excise the tag from the source sequence\n        sequence = sequence[:tag_index] + sequence[tag_index + len(tag_sequence):]\n\n    return sequence\n</code></pre>"},{"location":"reference/sequence/expression/#sequence.expression.remove_terminal_tags","title":"remove_terminal_tags","text":"<pre><code>remove_terminal_tags(sequence: str, tag_names: list[str] = None, termini: termini_literal = None) -&gt; str\n</code></pre> <p>Remove matching tag sequences and any remaining termini from the specified sequence</p> <p>Defaults to known tags in constants.expression_tags</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>str</code>)         \u2013          <p>The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'</p> </li> <li> <code>tag_names</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>If only certain tags should be removed, a list with the names of known tags</p> </li> <li> <code>termini</code>             (<code>termini_literal</code>, default:                 <code>None</code> )         \u2013          <p>Pass 'n' or 'c' if particular termini should be cleaned of tags</p> </li> </ul> <p>Returns:     'GGKLKPNDLRI...' The modified sequence without the tagged termini</p> Source code in <code>symdesign/sequence/expression.py</code> <pre><code>def remove_terminal_tags(sequence: str, tag_names: list[str] = None, termini: termini_literal = None) -&gt; str:\n    \"\"\"Remove matching tag sequences and any remaining termini from the specified sequence\n\n    Defaults to known tags in constants.expression_tags\n\n    Args:\n        sequence: The sequence of interest i.e. 'MSGHHHHHHGKLKPNDLRI...'\n        tag_names: If only certain tags should be removed, a list with the names of known tags\n        termini: Pass 'n' or 'c' if particular termini should be cleaned of tags\n    Returns:\n        'GGKLKPNDLRI...' The modified sequence without the tagged termini\n    \"\"\"\n    if tag_names:\n        _expression_tags = {tag_name: expression_tags[tag_name] for tag_name in tag_names}\n    else:\n        _expression_tags = expression_tags\n\n    if termini is None:  # Use the half_sequence_length, and hard code n_term\n        half_sequence_length = len(sequence) / 2\n        n_term = True  # c_term = True\n    else:  # Hard code the half_sequence_length as always true\n        half_sequence_length = len(sequence)\n        if termini == 'n':\n            n_term = True\n            # c_term = False\n        elif termini == 'c':\n            n_term = False\n            # c_term = True\n        else:\n            raise ValueError(f\"Must pass either 'n' or 'c' for the argument 'termini'\")\n\n    for name, tag_sequence in _expression_tags.items():\n        tag_index = sequence.find(tag_sequence)\n        if tag_index == -1:  # No match was found\n            continue\n\n        # Remove from one end based on termini proximity\n        if n_term and tag_index &lt; half_sequence_length:  # termini = 'n'\n            sequence = sequence[tag_index + len(tag_sequence):]\n        else:  # termini = 'c'\n            sequence = sequence[:tag_index]\n\n    return sequence\n</code></pre>"},{"location":"reference/structure/","title":"structure","text":""},{"location":"reference/structure/#structure--provide-objects-for-manipulation-and-modeling-of-protein-structures","title":"Provide objects for manipulation and modeling of protein structures","text":"<p>This module allows you to load protein structural files, manipulate their  coordinates, and measure aspects of their position, relationships, and  shape properties. Additionally, identify structural symmetry and inherently model symmetric relationships, including their interfaces.</p>"},{"location":"reference/structure/#structure--structure","title":"structure","text":"<ul> <li>An Atom has a Coordinate.</li> </ul> <pre><code>|Atom|                                                         &lt;- Structure\n |xyz|                                                         &lt;- Coordinate\n</code></pre> <ul> <li>A Residue ContainsAtoms and ContainsCoordinates.</li> </ul> <pre><code>|Residue - - - - - - |                                         &lt;- Structure\n [Atom|Atom|Atom|Atom]                                         &lt;- Structure\n</code></pre> <ul> <li>A Structure ContainsResidues.</li> </ul> <pre><code>|Structure - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n [Residue|Residue|Residue|Residue|Residue|Residue|Residue|...] &lt;- Structure\n</code></pre> <ul> <li>A Structure also ContainsAtoms and ContainsCoordinates.</li> </ul> <pre><code>|Structure - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n [Atom|Atom|Atom|Atom|Atom|Atom|Atom|Atom|Atom|Atom|Atom|Atom] &lt;- Structure\n [xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz|xyz] &lt;- Coordinate\n</code></pre> <ul> <li>A Chain is an instance of a Structure.</li> </ul> <pre><code>|Chain - - - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n</code></pre> <ul> <li>A Complex is a Structure that ContainsChains.</li> </ul> <pre><code>|Complex - - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n/Structure * - * - * - * - * - * - * - * - * - * - * - * - * \\ &lt;- Structure\n [Chain- - - - - - - - - - - - |Chain- - - - - - - - - - - - ] &lt;- Structure\n</code></pre>"},{"location":"reference/structure/#structure--loading-a-structure","title":"Loading a Structure","text":""},{"location":"reference/structure/#structure--sequence","title":"sequence","text":"<ul> <li>Every Structure is a GeneProduct.</li> </ul> <pre><code>|Structure - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n/GeneProduct-&gt; MPEAIRELNGHILFNCKALVDTGSSYPKQCDAKTGMIALQRPESA \\ &lt;- Sequence(Protein)\n</code></pre> <ul> <li>Each GeneProduct represents a Gene.</li> </ul> <pre><code>|GeneProduct - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Sequence\n/Gene-&gt; ATGCCTGAAGCTATTCGTGAATTAAATGGTCATATTTTATTTAATTGTAAAG \\ &lt;- Sequence(DNA)\n/ CTTTAGTTGATACTGGTTCTTCTTATCCTAAACAATGTGATGCTAAAACTGGTATGAT \\\n/ TGCTTTACAACGTCCTGAATCTGCT - - - -  - - - - - - - - - - - - \\\n</code></pre> <ul> <li>An Entity maps a biological instance of a Structure which ContainsChains(1-N) and is a Complex, to a single GeneProduct.</li> </ul> <pre><code>|Entity- - - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n/GeneProduct * - * - * - * - * - * - * - * - * - * - * - * - \\ &lt;- Seq\n ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n [Chain- - - - - - - - - - - - - - - - - - - - - - - - - - - ] &lt;- Structure\n /GeneProduct- - - - - - - - - - - - - - - - - - - - - - - - \\ &lt;- Seq\n [Chain- - - - - - - - - - - - |Chain- - - - - - - - - - - - ] &lt;- Structure\n /GeneProduct- - - - - - - - - |GeneProduct- - - - - - - - - \\ &lt;- Seq\n [Chain- - - - - - - |Chain- - - - - - - |Chain- - - - - - - ] &lt;- Structure\n /GeneProduct- - - - |GeneProduct- - - - |GeneProduct- - - - \\ &lt;- Seq\n [Chain- - - - -|Chain - - - - |Chain- - - - -|Chain - - - - ] &lt;- Structure\n /GeneProduct- -|GeneProduct - |GeneProduct- -|GeneProduct - \\ &lt;- Seq\n [Chain- - |Chain- - |Chain- - |Chain- - |Chain- - |Chain- - | &lt;- Structure\n /GeneProd.|GeneProd.|GeneProd.|GeneProd.|GeneProd.|GeneProd.\\ &lt;- Seq\n</code></pre> <ul> <li>As a result of GeneProduct duplication, an Entity can be a SymmetricModel.</li> </ul>"},{"location":"reference/structure/#structure--working-with-interfaces","title":"Working with interfaces","text":"<ul> <li>A Complex ContainsGeneProducts.</li> </ul> <pre><code>|Complex- - - - - - - - - - - - - - - - - - - - - - - - - - -| &lt;- Structure\n [Entity- - - - - - - - - - - - - - - - - - - - - - - - - - -] &lt;- Seq\n [Entity- - - - - - - - -||Entity- - - - - - - - - - - - - - ] &lt;- Seq\n</code></pre>"},{"location":"reference/structure/#structure--working-with-pieces-of-structure","title":"Working with pieces of Structure","text":"<ul> <li>A Fragment ContainsResidues, but only a few and thus is a small Structure representation.</li> </ul> <pre><code>|Fragment - - - - - - - - - - - - - - - -|                     &lt;- Structure\n [Residue|Residue|Residue|Residue|Residue]                     &lt;- Structure\n</code></pre>"},{"location":"reference/structure/#structure--each-fragment-can-map-to-other-fragment-instances-so-that-larger-structure-can-be-broken-down-into-pairs-of-fragment-such-fragment-overlap-with-neighboring-fragment-such-as-those-that-aare-mapped-to-a-separate-sliding-window-register","title":"Each Fragment can map to other Fragment instances so that larger Structure can be broken down into pairs of Fragment. Such Fragment overlap with neighboring Fragment such as those that aare mapped to a separate \"sliding window\" register.","text":"<pre><code>|Structure - - - - - - - - - - - - - - - - - - - - - - - - - | &lt;- Structure\n Fragment- - - |Fragment - - -|Fragment- - - |Fragment - - - | &lt;- Structure\n    |Fragment- - - |Fragment - - -|Fragment- - - |             &lt;- Structure\n        |Fragment- - - |Fragment - - -|Fragment- - - |         &lt;- Structure\n            |Fragment- - - |Fragment - - -|Fragment- - - |     &lt;- Structure\n</code></pre> <p>The module contains the following classes:</p> <ul> <li><code>Pose()</code> - Create a symmetry-aware object to manipulate a collection of Entity instances</li> <li><code>Model()</code> - Create an object to manipulate a collection of Chain instances</li> <li><code>Entity()</code> - Create a symmetry-aware object to manipulate structurally homologous Chain instances</li> </ul> <p>The module contains the following functions:</p> <ul> <li><code>()</code> - Returns</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from symdesign import structure\n&gt;&gt;&gt;\n&gt;&gt;&gt; structure_file = Path(Path.cwd(), 'tests', '1xyz.pdb1')\n&gt;&gt;&gt; # structure_file = Path(Path.cwd(), 'tests', '1xyz-assembly1.cif')\n&gt;&gt;&gt; pose_1xyz = structure.model.Pose.from_file(structure_file)\n&gt;&gt;&gt; # Transform the pose to the coordinate origin\n&gt;&gt;&gt; pose_1xyz.transform(translation=-pose_1xyz.center_of_mass)\n&gt;&gt;&gt; chain_a_1xyz = pose_1xyz.get_chain('A')\n&gt;&gt;&gt; for residue in chain_a_1xyz.residues:\n...     print(f'Residue {residue.type}{residue.number} has a B-factor of {residue.b_factor} and {residue.sasa} '\n...           '$\\AA$\u00b2 of solvent accessible surface area')\nResidue M1 has a B-factor of 47.08 and 34.02 A2 of solvent accessible surface area\nResidue V2 has a B-factor of 54.11 and 12.83 A2 of solvent accessible surface area\n...\n&gt;&gt;&gt;\n&gt;&gt;&gt; chain_a_1xyz_per_residue_sasa = sum([residue.sasa for residue in chain_a_1xyz.residues])\n&gt;&gt;&gt; chain_a_1xyz_per_residue_sasa == chain_a_1xyz.sasa\nTrue\n&gt;&gt;&gt; for entity_idx, entity in enumerate(pose_1xyz.entities, 1):\n...     print(f'The Entity #{entity_idx} is {entity.name}')\n...     print(entity == chain_a_1xyz)\n</code></pre> <p>Structure initialization arguments Process various types of Structure containers to update the Model with the corresponding information</p> <p>Parameters:</p> <ul> <li> <code>entities</code>         \u2013          <p>bool | list[Entity] | Structures = True - Whether to create Entity instances from passed Structure container instances, or existing Entity instances to create the Model with</p> </li> <li> <code>entity_names</code>         \u2013          <p>Sequence = None - Names explicitly passed for the Entity instances. Length must equal number of entities. Names will take precedence over query_by_sequence if passed</p> </li> <li> <code>chains</code>         \u2013          <p>bool | list[Chain] | Structures = True - Whether to create Chain instances from passed Structure container instances, or existing Chain instances to create the Model with</p> </li> <li> <code>query_by_sequence</code>         \u2013          <p>bool = True - Whether the PDB API should be queried for an Entity name by matching sequence. Only used if entity_names not provided</p> </li> <li> <code>structure</code>         \u2013          <p>ContainsResidues = None - Create the instance based on an existing Structure instance</p> </li> <li> <code>fragment_db</code>         \u2013          <p>FragmentDatabase = None - The identity of the FragmentDatabase to use for Fragment based operations</p> </li> <li> <code>residues</code>         \u2013          <p>list[Residue] | Residues = None - The Residue instances which should constitute a new instance</p> </li> <li> <code>residue_indices</code>         \u2013          <p>list[int] = None - The indices which specify the particular Residue instances to make this ContainsResidues instance. Used with a parent to specify a subdivision of a larger structure</p> </li> <li> <code>atoms</code>         \u2013          <p>list[Atom] | Atoms = None - The Atom instances which should constitute a new Structure instance</p> </li> <li> <code>parent</code>         \u2013          <p>StructureBase = None - If another Structure object created this Structure instance, pass the 'parent' instance. Will take ownership over Structure containers (coords, atoms, residues) for dependent Structures</p> </li> <li> <code>log</code>         \u2013          <p>Log | Logger | bool = True - The Log or Logger instance, or the name for the logger to handle parent Structure logging. None or False prevents logging while any True assignment enables it</p> </li> <li> <code>coords</code>         \u2013          <p>Coords | np.ndarray | list[list[float]] = None - When setting up a parent Structure instance, the coordinates of that Structure</p> </li> <li> <code>name</code>         \u2013          <p>str = None - The identifier for the Structure instance</p> </li> <li> <code>biological_assembly</code>         \u2013          <p>str | None = None - The integer of the biological assembly (as indicated by PDB AssemblyID format)</p> </li> <li> <code>cryst_record</code>         \u2013          <p>str | None - The string specifying how the molecule is situated in a lattice</p> </li> <li> <code>entity_info</code>         \u2013          <p>dict[str, dict[dict | list | str]] - A mapping of the metadata to their distinct molecular identifiers</p> </li> <li> <code>file_path</code>         \u2013          <p>AnyStr | None - The location on disk where the file was accessed</p> </li> <li> <code>reference_sequence</code>         \u2013          <p>str | dict[str, str] = None - The reference sequence according to expression sequence or reference database</p> </li> <li> <code>resolution</code>         \u2013          <p>float | None = None: The level of detail available from an experimental dataset contributing to the sharpness with which structural data can contribute towards building a model</p> </li> </ul>"},{"location":"reference/structure/base/","title":"base","text":""},{"location":"reference/structure/base/#structure.base.DEFAULT_SS_COIL_IDENTIFIER","title":"DEFAULT_SS_COIL_IDENTIFIER  <code>module-attribute</code>","text":"<pre><code>DEFAULT_SS_COIL_IDENTIFIER = 'C'\n</code></pre> <p>Secondary structure identifier mapping Stride B/b:Isolated bridge E:Strand/Extended conformation  G:3-10 helix H:Alpha helix I:PI-helix T:Turn C:Coil (none of the above) SS_TURN_IDENTIFIERS = 'T' SS_DISORDER_IDENTIFIERS = 'C'</p> <p>DSSP B:beta bridge E:strand/beta bulge G:310 helix H:\u03b1 helix I:\u03c0 helix T:turns S:high curvature (where the angle between i-2, i, and i+2 is at least 70\u00b0) \" \"(space):loop SS_DISORDER_IDENTIFIERS = ' ' SS_TURN_IDENTIFIERS = 'TS'</p>"},{"location":"reference/structure/base/#structure.base.alpha_helix_15_atoms","title":"alpha_helix_15_atoms  <code>module-attribute</code>","text":"<pre><code>alpha_helix_15_atoms = Atoms([Atom(0, 1, 'N', ' ', 'ALA', 'A', 1, ' ', 27.128, 20.897, 37.943, 1.0, 0.0, 'N', ''), Atom(1, 2, 'CA', ' ', 'ALA', 'A', 1, ' ', 27.933, 21.94, 38.546, 1.0, 0.0, 'C', ''), Atom(2, 3, 'C', ' ', 'ALA', 'A', 1, ' ', 28.402, 22.92, 37.481, 1.0, 0.0, 'C', ''), Atom(3, 4, 'O', ' ', 'ALA', 'A', 1, ' ', 28.303, 24.132, 37.663, 1.0, 0.0, 'O', ''), Atom(4, 5, 'CB', ' ', 'ALA', 'A', 1, ' ', 29.162, 21.356, 39.234, 1.0, 0.0, 'C', ''), Atom(5, 6, 'N', ' ', 'ALA', 'A', 2, ' ', 28.914, 22.392, 36.367, 1.0, 0.0, 'N', ''), Atom(6, 7, 'CA', ' ', 'ALA', 'A', 2, ' ', 29.395, 23.219, 35.278, 1.0, 0.0, 'C', ''), Atom(7, 8, 'C', ' ', 'ALA', 'A', 2, ' ', 28.286, 24.142, 34.793, 1.0, 0.0, 'C', ''), Atom(8, 9, 'O', ' ', 'ALA', 'A', 2, ' ', 28.508, 25.337, 34.61, 1.0, 0.0, 'O', ''), Atom(9, 10, 'CB', ' ', 'ALA', 'A', 2, ' ', 29.857, 22.365, 34.102, 1.0, 0.0, 'C', ''), Atom(10, 11, 'N', ' ', 'ALA', 'A', 3, ' ', 27.092, 23.583, 34.584, 1.0, 0.0, 'N', ''), Atom(11, 12, 'CA', ' ', 'ALA', 'A', 3, ' ', 25.956, 24.355, 34.121, 1.0, 0.0, 'C', ''), Atom(12, 13, 'C', ' ', 'ALA', 'A', 3, ' ', 25.681, 25.505, 35.079, 1.0, 0.0, 'C', ''), Atom(13, 14, 'O', ' ', 'ALA', 'A', 3, ' ', 25.488, 26.639, 34.648, 1.0, 0.0, 'O', ''), Atom(14, 15, 'CB', ' ', 'ALA', 'A', 3, ' ', 24.703, 23.49, 34.038, 1.0, 0.0, 'C', ''), Atom(15, 16, 'N', ' ', 'ALA', 'A', 4, ' ', 25.662, 25.208, 36.38, 1.0, 0.0, 'N', ''), Atom(16, 17, 'CA', ' ', 'ALA', 'A', 4, ' ', 25.411, 26.214, 37.393, 1.0, 0.0, 'C', ''), Atom(17, 18, 'C', ' ', 'ALA', 'A', 4, ' ', 26.424, 27.344, 37.27, 1.0, 0.0, 'C', ''), Atom(18, 19, 'O', ' ', 'ALA', 'A', 4, ' ', 26.055, 28.516, 37.29, 1.0, 0.0, 'O', ''), Atom(19, 20, 'CB', ' ', 'ALA', 'A', 4, ' ', 25.519, 25.624, 38.794, 1.0, 0.0, 'C', ''), Atom(20, 21, 'N', ' ', 'ALA', 'A', 5, ' ', 27.704, 26.987, 37.142, 1.0, 0.0, 'N', ''), Atom(21, 22, 'CA', ' ', 'ALA', 'A', 5, ' ', 28.764, 27.968, 37.016, 1.0, 0.0, 'C', ''), Atom(22, 23, 'C', ' ', 'ALA', 'A', 5, ' ', 28.497, 28.876, 35.825, 1.0, 0.0, 'C', ''), Atom(23, 24, 'O', ' ', 'ALA', 'A', 5, ' ', 28.602, 30.096, 35.937, 1.0, 0.0, 'O', ''), Atom(24, 25, 'CB', ' ', 'ALA', 'A', 5, ' ', 30.115, 27.292, 36.812, 1.0, 0.0, 'C', ''), Atom(25, 26, 'N', ' ', 'ALA', 'A', 6, ' ', 28.151, 28.278, 34.682, 1.0, 0.0, 'N', ''), Atom(26, 27, 'CA', ' ', 'ALA', 'A', 6, ' ', 27.871, 29.032, 33.478, 1.0, 0.0, 'C', ''), Atom(27, 28, 'C', ' ', 'ALA', 'A', 6, ' ', 26.759, 30.04, 33.737, 1.0, 0.0, 'C', ''), Atom(28, 29, 'O', ' ', 'ALA', 'A', 6, ' ', 26.876, 31.205, 33.367, 1.0, 0.0, 'O', ''), Atom(29, 30, 'CB', ' ', 'ALA', 'A', 6, ' ', 27.429, 28.113, 32.344, 1.0, 0.0, 'C', ''), Atom(30, 31, 'N', ' ', 'ALA', 'A', 7, ' ', 25.678, 29.586, 34.376, 1.0, 0.0, 'N', ''), Atom(31, 32, 'CA', ' ', 'ALA', 'A', 7, ' ', 24.552, 30.444, 34.682, 1.0, 0.0, 'C', ''), Atom(32, 33, 'C', ' ', 'ALA', 'A', 7, ' ', 25.013, 31.637, 35.507, 1.0, 0.0, 'C', ''), Atom(33, 34, 'O', ' ', 'ALA', 'A', 7, ' ', 24.652, 32.773, 35.212, 1.0, 0.0, 'O', ''), Atom(34, 35, 'CB', ' ', 'ALA', 'A', 7, ' ', 23.489, 29.693, 35.478, 1.0, 0.0, 'C', ''), Atom(35, 36, 'N', ' ', 'ALA', 'A', 8, ' ', 25.814, 31.374, 36.543, 1.0, 0.0, 'N', ''), Atom(36, 37, 'CA', ' ', 'ALA', 'A', 8, ' ', 26.321, 32.423, 37.405, 1.0, 0.0, 'C', ''), Atom(37, 38, 'C', ' ', 'ALA', 'A', 8, ' ', 27.081, 33.454, 36.583, 1.0, 0.0, 'C', ''), Atom(38, 39, 'O', ' ', 'ALA', 'A', 8, ' ', 26.874, 34.654, 36.745, 1.0, 0.0, 'O', ''), Atom(39, 40, 'CB', ' ', 'ALA', 'A', 8, ' ', 27.193, 31.773, 38.487, 1.0, 0.0, 'C', ''), Atom(40, 41, 'N', ' ', 'ALA', 'A', 9, ' ', 27.963, 32.98, 35.7, 1.0, 0.0, 'N', ''), Atom(41, 42, 'CA', ' ', 'ALA', 'A', 9, ' ', 28.75, 33.859, 34.858, 1.0, 0.0, 'C', ''), Atom(42, 43, 'C', ' ', 'ALA', 'A', 9, ' ', 27.834, 34.759, 34.042, 1.0, 0.0, 'C', ''), Atom(43, 44, 'O', ' ', 'ALA', 'A', 9, ' ', 28.052, 35.967, 33.969, 1.0, 0.0, 'O', ''), Atom(44, 45, 'CB', ' ', 'ALA', 'A', 9, ' ', 29.621, 33.061, 33.894, 1.0, 0.0, 'C', ''), Atom(45, 46, 'N', ' ', 'ALA', 'A', 10, ' ', 26.807, 34.168, 33.427, 1.0, 0.0, 'N', ''), Atom(46, 47, 'CA', ' ', 'ALA', 'A', 10, ' ', 25.864, 34.915, 32.62, 1.0, 0.0, 'C', ''), Atom(47, 48, 'C', ' ', 'ALA', 'A', 10, ' ', 25.23, 36.024, 33.448, 1.0, 0.0, 'C', ''), Atom(48, 49, 'O', ' ', 'ALA', 'A', 10, ' ', 25.146, 37.165, 33.001, 1.0, 0.0, 'O', ''), Atom(49, 50, 'CB', ' ', 'ALA', 'A', 10, ' ', 24.752, 34.012, 32.097, 1.0, 0.0, 'C', ''), Atom(50, 51, 'N', ' ', 'ALA', 'A', 11, ' ', 24.783, 35.683, 34.66, 1.0, 0.0, 'N', ''), Atom(51, 52, 'CA', ' ', 'ALA', 'A', 11, ' ', 24.16, 36.646, 35.544, 1.0, 0.0, 'C', ''), Atom(52, 53, 'C', ' ', 'ALA', 'A', 11, ' ', 25.104, 37.812, 35.797, 1.0, 0.0, 'C', ''), Atom(53, 54, 'O', ' ', 'ALA', 'A', 11, ' ', 24.699, 38.97, 35.714, 1.0, 0.0, 'O', ''), Atom(54, 55, 'CB', ' ', 'ALA', 'A', 11, ' ', 23.81, 36.012, 36.887, 1.0, 0.0, 'C', ''), Atom(55, 56, 'N', ' ', 'ALA', 'A', 12, ' ', 26.365, 37.503, 36.107, 1.0, 0.0, 'N', ''), Atom(56, 57, 'CA', ' ', 'ALA', 'A', 12, ' ', 27.361, 38.522, 36.37, 1.0, 0.0, 'C', ''), Atom(57, 58, 'C', ' ', 'ALA', 'A', 12, ' ', 27.477, 39.461, 35.177, 1.0, 0.0, 'C', ''), Atom(58, 59, 'O', ' ', 'ALA', 'A', 12, ' ', 27.485, 40.679, 35.342, 1.0, 0.0, 'O', ''), Atom(59, 60, 'CB', ' ', 'ALA', 'A', 12, ' ', 28.73, 37.9, 36.625, 1.0, 0.0, 'C', ''), Atom(60, 61, 'N', ' ', 'ALA', 'A', 13, ' ', 27.566, 38.89, 33.974, 1.0, 0.0, 'N', ''), Atom(61, 62, 'CA', ' ', 'ALA', 'A', 13, ' ', 27.68, 39.674, 32.761, 1.0, 0.0, 'C', ''), Atom(62, 63, 'C', ' ', 'ALA', 'A', 13, ' ', 26.504, 40.634, 32.645, 1.0, 0.0, 'C', ''), Atom(63, 64, 'O', ' ', 'ALA', 'A', 13, ' ', 26.69, 41.815, 32.36, 1.0, 0.0, 'O', ''), Atom(64, 65, 'CB', ' ', 'ALA', 'A', 13, ' ', 27.69, 38.779, 31.527, 1.0, 0.0, 'C', ''), Atom(65, 66, 'N', ' ', 'ALA', 'A', 14, ' ', 25.291, 40.121, 32.868, 1.0, 0.0, 'N', ''), Atom(66, 67, 'CA', ' ', 'ALA', 'A', 14, ' ', 24.093, 40.932, 32.789, 1.0, 0.0, 'C', ''), Atom(67, 68, 'C', ' ', 'ALA', 'A', 14, ' ', 24.193, 42.112, 33.745, 1.0, 0.0, 'C', ''), Atom(68, 69, 'O', ' ', 'ALA', 'A', 14, ' ', 23.905, 43.245, 33.367, 1.0, 0.0, 'O', ''), Atom(69, 70, 'CB', ' ', 'ALA', 'A', 14, ' ', 22.856, 40.12, 33.158, 1.0, 0.0, 'C', ''), Atom(70, 71, 'N', ' ', 'ALA', 'A', 15, ' ', 24.604, 41.841, 34.986, 1.0, 0.0, 'N', ''), Atom(71, 72, 'CA', ' ', 'ALA', 'A', 15, ' ', 24.742, 42.878, 35.989, 1.0, 0.0, 'C', ''), Atom(72, 73, 'C', ' ', 'ALA', 'A', 15, ' ', 25.691, 43.96, 35.497, 1.0, 0.0, 'C', ''), Atom(73, 74, 'O', ' ', 'ALA', 'A', 15, ' ', 25.39, 45.147, 35.602, 1.0, 0.0, 'O', ''), Atom(74, 75, 'CB', ' ', 'ALA', 'A', 15, ' ', 24.418, 41.969, 34.808, 1.0, 0.0, 'C', '')])\n</code></pre> <p>Caution. After the first time this helix is created as a Structure, the alpha_helix_15_atoms Atom instances will have  their .coords set to the alpha_helix_15 'parent' Structure instance. Transformation of this Structure causes the  starting coordinates in each Atom instance in alpha_helix_15_atoms to be transformed as well. Making new structures  from this Atoms container is sufficient to detach them, but the coordinates are moved from program start after usage the first time. This behavior isn't ideal, but a consequence of construction of Atom instances from the singular  attribute basis with the intention to be reused</p>"},{"location":"reference/structure/base/#structure.base.ArrayIndexer","title":"ArrayIndexer  <code>module-attribute</code>","text":"<pre><code>ArrayIndexer = Type[Union[Sequence[int], Sequence[bool], slice, None]]\n</code></pre> <p>Where integers, slices <code>:</code>, ellipsis <code>...</code>, None and integer or boolean arrays are valid indices</p>"},{"location":"reference/structure/base/#structure.base.Log","title":"Log","text":"<pre><code>Log(log: Logger | None = logging.getLogger('null'))\n</code></pre> <p>Responsible for StructureBase logging operations</p> <p>Parameters:</p> <ul> <li> <code>log</code>             (<code>Logger | None</code>, default:                 <code>getLogger('null')</code> )         \u2013          <p>The logging.Logger to handle StructureBase logging. If None is passed a Logger with NullHandler is used</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, log: Logger | None = logging.getLogger('null')):\n    \"\"\"Construct the instance\n\n    Args:\n        log: The logging.Logger to handle StructureBase logging. If None is passed a Logger with NullHandler is used\n    \"\"\"\n    self.log = log\n</code></pre>"},{"location":"reference/structure/base/#structure.base.SymmetryBase","title":"SymmetryBase","text":"<pre><code>SymmetryBase(rotation_matrices: ndarray = None, translation_matrices: ndarray = None, **kwargs)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Adds functionality for symmetric manipulation of Structure instances</p> <p>Collects known keyword arguments for all derived classes to protect <code>object</code>. Should always be the last class in the method resolution order of derived classes.</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, rotation_matrices: np.ndarray = None, translation_matrices: np.ndarray = None, **kwargs):\n    self._symmetry = self._symmetric_dependents = None\n    try:\n        super().__init__(**kwargs)  # SymmetryBase\n    except TypeError:\n        raise TypeError(\n            f\"The argument(s) passed to the {self.__class__.__name__} instance weren't recognized and aren't \"\n            f\"accepted by the object class: {', '.join(kwargs.keys())}\\n\\nIt's likely that your class MRO is \"\n            \"insufficient for your passed arguments or you have passed invalid arguments\")\n</code></pre>"},{"location":"reference/structure/base/#structure.base.SymmetryBase.symmetry","title":"symmetry  <code>property</code> <code>writable</code>","text":"<pre><code>symmetry: str | None\n</code></pre> <p>The symmetry of the Structure described by its Sch\u00f6nflies notation</p>"},{"location":"reference/structure/base/#structure.base.SymmetryBase.symmetric_dependents","title":"symmetric_dependents  <code>property</code> <code>writable</code>","text":"<pre><code>symmetric_dependents: list[StructureBase] | list\n</code></pre> <p>Access the symmetrically dependent Structure instances</p>"},{"location":"reference/structure/base/#structure.base.SymmetryBase.reset_symmetry_state","title":"reset_symmetry_state","text":"<pre><code>reset_symmetry_state() -&gt; None\n</code></pre> <p>Remove any state variable associated with the instance</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reset_symmetry_state(self) -&gt; None:\n    \"\"\"Remove any state variable associated with the instance\"\"\"\n    # self.log.debug(f\"Removing symmetric attributes from {repr(self)}\")\n    for attribute in self.symmetry_state_attributes:\n        try:\n            delattr(self, attribute)\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"reference/structure/base/#structure.base.SymmetryBase.is_symmetric","title":"is_symmetric","text":"<pre><code>is_symmetric() -&gt; bool\n</code></pre> <p>Query whether the Structure is symmetric. Returns True if self.symmetry is not None</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_symmetric(self) -&gt; bool:\n    \"\"\"Query whether the Structure is symmetric. Returns True if self.symmetry is not None\"\"\"\n    return self._symmetry is not None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.SymmetryBase.has_symmetric_dependents","title":"has_symmetric_dependents","text":"<pre><code>has_symmetric_dependents() -&gt; bool\n</code></pre> <p>Evaluates to True if the Structure has symmetrically dependent children</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def has_symmetric_dependents(self) -&gt; bool:\n    \"\"\"Evaluates to True if the Structure has symmetrically dependent children\"\"\"\n    return True if self._symmetric_dependents else False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureMetadata","title":"StructureMetadata","text":"<pre><code>StructureMetadata(biological_assembly: str | int = None, cryst_record: str = None, entity_info: dict[str, dict[dict | list | str]] = None, file_path: AnyStr = None, reference_sequence: str | dict[str, str] = None, resolution: float = None, **kwargs)\n</code></pre> <p>Contains all metadata available from structure file parsing</p> <p>Parameters:</p> <ul> <li> <code>biological_assembly</code>             (<code>str | int</code>, default:                 <code>None</code> )         \u2013          <p>The integer of the biological assembly (as indicated by PDB AssemblyID format)</p> </li> <li> <code>cryst_record</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The string specifying how the molecule is situated in a lattice</p> </li> <li> <code>entity_info</code>             (<code>dict[str, dict[dict | list | str]]</code>, default:                 <code>None</code> )         \u2013          <p>A mapping of the metadata to their distinct molecular identifiers</p> </li> <li> <code>file_path</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The location on disk where the file was accessed</p> </li> <li> <code>reference_sequence</code>             (<code>str | dict[str, str]</code>, default:                 <code>None</code> )         \u2013          <p>The reference sequence (according to expression sequence or reference database)</p> </li> <li> <code>resolution</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The level of detail available from an experimental dataset contributing to the sharpness with which structural data can contribute towards building a model</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, biological_assembly: str | int = None, cryst_record: str = None,\n             entity_info: dict[str, dict[dict | list | str]] = None, file_path: AnyStr = None,\n             reference_sequence: str | dict[str, str] = None, resolution: float = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        biological_assembly: The integer of the biological assembly (as indicated by PDB AssemblyID format)\n        cryst_record: The string specifying how the molecule is situated in a lattice\n        entity_info: A mapping of the metadata to their distinct molecular identifiers\n        file_path: The location on disk where the file was accessed\n        reference_sequence: The reference sequence (according to expression sequence or reference database)\n        resolution: The level of detail available from an experimental dataset contributing to the sharpness with\n            which structural data can contribute towards building a model\n    \"\"\"\n    if biological_assembly is None:\n        self.biological_assembly = biological_assembly\n    else:\n        self.biological_assembly = str(biological_assembly)\n    self.cryst_record = cryst_record\n    self.entity_info = entity_info\n    self.file_path = file_path\n    self.reference_sequence = reference_sequence\n    self.resolution = resolution\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin","title":"CoordinateOpsMixin","text":"<p>             Bases: <code>ABC</code></p>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.coords","title":"coords  <code>abstractmethod</code> <code>property</code> <code>writable</code>","text":"<pre><code>coords\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.distance_from_reference","title":"distance_from_reference","text":"<pre><code>distance_from_reference(reference: ndarray = utils.symmetry.origin, measure: str = 'mean', **kwargs) -&gt; float\n</code></pre> <p>From a Structure, find the furthest coordinate from the origin (default) or from a reference.</p> <p>Parameters:</p> <ul> <li> <code>reference</code>             (<code>ndarray</code>, default:                 <code>origin</code> )         \u2013          <p>The reference where the point should be measured from. Default is origin</p> </li> <li> <code>measure</code>             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>The measurement to take with respect to the reference. Could be 'mean', 'min', 'max', or any numpy function to describe computed distance scalars</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The distance from the reference point to the furthest point</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def distance_from_reference(\n    self, reference: np.ndarray = utils.symmetry.origin, measure: str = 'mean', **kwargs\n) -&gt; float:\n    \"\"\"From a Structure, find the furthest coordinate from the origin (default) or from a reference.\n\n    Args:\n        reference: The reference where the point should be measured from. Default is origin\n        measure: The measurement to take with respect to the reference. Could be 'mean', 'min', 'max', or any\n            numpy function to describe computed distance scalars\n\n    Returns:\n        The distance from the reference point to the furthest point\n    \"\"\"\n    if reference is None:\n        reference = utils.symmetry.origin\n\n    return getattr(np, measure)(np.linalg.norm(self.coords - reference, axis=1))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.translate","title":"translate","text":"<pre><code>translate(translation: list[float] | ndarray, **kwargs) -&gt; None\n</code></pre> <p>Perform a translation to the Structure ensuring only the Structure container of interest is translated ensuring the underlying coords are not modified</p> <p>Parameters:</p> <ul> <li> <code>translation</code>             (<code>list[float] | ndarray</code>)         \u2013          <p>The first translation to apply, expected array shape (3,)</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def translate(self, translation: list[float] | np.ndarray, **kwargs) -&gt; None:\n    \"\"\"Perform a translation to the Structure ensuring only the Structure container of interest is translated\n    ensuring the underlying coords are not modified\n\n    Args:\n        translation: The first translation to apply, expected array shape (3,)\n    \"\"\"\n    self._transforming = True\n    self.coords += translation\n    self._transforming = False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.rotate","title":"rotate","text":"<pre><code>rotate(rotation: list[list[float]] | ndarray, **kwargs) -&gt; None\n</code></pre> <p>Perform a rotation to the Structure ensuring only the Structure container of interest is rotated ensuring the underlying coords are not modified</p> <p>Parameters:</p> <ul> <li> <code>rotation</code>             (<code>list[list[float]] | ndarray</code>)         \u2013          <p>The first rotation to apply, expected array shape (3, 3)</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def rotate(self, rotation: list[list[float]] | np.ndarray, **kwargs) -&gt; None:\n    \"\"\"Perform a rotation to the Structure ensuring only the Structure container of interest is rotated ensuring the\n    underlying coords are not modified\n\n    Args:\n        rotation: The first rotation to apply, expected array shape (3, 3)\n    \"\"\"\n    self._transforming = True\n    self.coords = np.matmul(self.coords, np.transpose(rotation))  # Allows a list to be passed\n    # self.coords = np.matmul(self.coords, rotation.swapaxes(-2, -1))  # Essentially a transpose\n    self._transforming = False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.transform","title":"transform","text":"<pre><code>transform(rotation: list[list[float]] | ndarray = None, translation: list[float] | ndarray = None, rotation2: list[list[float]] | ndarray = None, translation2: list[float] | ndarray = None, **kwargs) -&gt; None\n</code></pre> <p>Perform a specific transformation to the Structure ensuring only the Structure container of interest is transformed ensuring the underlying coords are not modified</p> <p>Transformation proceeds by matrix multiplication and vector addition with the order of operations as: rotation, translation, rotation2, translation2</p> <p>Parameters:</p> <ul> <li> <code>rotation</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected array shape (3,)</p> </li> <li> <code>rotation2</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation2</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected array shape (3,)</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def transform(\n    self, rotation: list[list[float]] | np.ndarray = None, translation: list[float] | np.ndarray = None,\n    rotation2: list[list[float]] | np.ndarray = None, translation2: list[float] | np.ndarray = None, **kwargs\n) -&gt; None:\n    \"\"\"Perform a specific transformation to the Structure ensuring only the Structure container of interest is\n    transformed ensuring the underlying coords are not modified\n\n    Transformation proceeds by matrix multiplication and vector addition with the order of operations as:\n    rotation, translation, rotation2, translation2\n\n    Args:\n        rotation: The first rotation to apply, expected array shape (3, 3)\n        translation: The first translation to apply, expected array shape (3,)\n        rotation2: The second rotation to apply, expected array shape (3, 3)\n        translation2: The second translation to apply, expected array shape (3,)\n    \"\"\"\n    if rotation is not None:  # Required for np.ndarray or None checks\n        new_coords = np.matmul(self.coords, np.transpose(rotation))  # Allows list to be passed...\n        # new_coords = np.matmul(self.coords, rotation.swapaxes(-2, -1))  # Essentially a transpose\n    else:\n        new_coords = self.coords  # No need to copy as this is a view\n\n    if translation is not None:  # Required for np.ndarray or None checks\n        new_coords += translation\n\n    if rotation2 is not None:  # Required for np.ndarray or None checks\n        np.matmul(new_coords, np.transpose(rotation2), out=new_coords)  # Allows list to be passed...\n        # np.matmul(new_coords, rotation2.swapaxes(-2, -1), out=new_coords)  # Essentially a transpose\n\n    if translation2 is not None:  # Required for np.ndarray or None checks\n        new_coords += translation2\n\n    self._transforming = True\n    self.coords = new_coords\n    self._transforming = False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.copy","title":"copy  <code>abstractmethod</code>","text":"<pre><code>copy()\n</code></pre> Source code in <code>symdesign/structure/base.py</code> <pre><code>@abc.abstractmethod\ndef copy(self):\n    \"\"\"\"\"\"\n</code></pre>"},{"location":"reference/structure/base/#structure.base.CoordinateOpsMixin.get_transformed_copy","title":"get_transformed_copy","text":"<pre><code>get_transformed_copy(rotation: list[list[float]] | ndarray = None, translation: list[float] | ndarray = None, rotation2: list[list[float]] | ndarray = None, translation2: list[float] | ndarray = None) -&gt; StructureBase\n</code></pre> <p>Make a semi-deep copy of the Structure object with the coordinates transformed in cartesian space</p> <p>Transformation proceeds by matrix multiplication and vector addition with the order of operations as: rotation, translation, rotation2, translation2</p> <p>Parameters:</p> <ul> <li> <code>rotation</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected array shape (3,)</p> </li> <li> <code>rotation2</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation2</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected array shape (3,)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>StructureBase</code>         \u2013          <p>A transformed copy of the original object</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_transformed_copy(\n    self, rotation: list[list[float]] | np.ndarray = None, translation: list[float] | np.ndarray = None,\n    rotation2: list[list[float]] | np.ndarray = None, translation2: list[float] | np.ndarray = None\n) -&gt; StructureBase:  # Todo -&gt; Self in python 3.11\n    \"\"\"Make a semi-deep copy of the Structure object with the coordinates transformed in cartesian space\n\n    Transformation proceeds by matrix multiplication and vector addition with the order of operations as:\n    rotation, translation, rotation2, translation2\n\n    Args:\n        rotation: The first rotation to apply, expected array shape (3, 3)\n        translation: The first translation to apply, expected array shape (3,)\n        rotation2: The second rotation to apply, expected array shape (3, 3)\n        translation2: The second translation to apply, expected array shape (3,)\n\n    Returns:\n        A transformed copy of the original object\n    \"\"\"\n    if rotation is not None:  # Required for np.ndarray or None checks\n        new_coords = np.matmul(self.coords, np.transpose(rotation))  # Allows list to be passed...\n        # new_coords = np.matmul(self.coords, rotation.swapaxes(-2, -1))  # Essentially a transpose\n    else:\n        new_coords = self.coords  # No need to copy as this is a view\n\n    if translation is not None:  # Required for np.ndarray or None checks\n        new_coords += translation\n\n    if rotation2 is not None:  # Required for np.ndarray or None checks\n        np.matmul(new_coords, np.transpose(rotation2), out=new_coords)  # Allows list to be passed...\n        # np.matmul(new_coords, rotation2.swapaxes(-2, -1), out=new_coords)  # Essentially a transpose\n\n    if translation2 is not None:  # Required for np.ndarray or None checks\n        new_coords += translation2\n\n    new_structure = self.copy()\n    new_structure._transforming = True\n    new_structure.coords = new_coords\n    new_structure._transforming = False\n\n    return new_structure\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBase","title":"StructureBase","text":"<pre><code>StructureBase(parent: StructureBase = None, log: Log | Logger | bool = True, coords: ndarray | Coordinates | list[list[float]] = None, name: str = None, file_path: AnyStr = None, biological_assembly: str | int = None, cryst_record: str = None, resolution: float = None, entity_info: dict[str, dict[dict | list | str]] = None, reference_sequence: str | dict[str, str] = None, **kwargs)\n</code></pre> <p>             Bases: <code>SymmetryBase</code>, <code>CoordinateOpsMixin</code>, <code>ABC</code></p> <p>Manipulates Coordinates and Log instances as well as the .atom_indices.</p> <p>Additionally. sorts through parent Structure and dependent Structure hierarchies during Structure subclass creation.</p> <p>Parameters:</p> <ul> <li> <code>parent</code>             (<code>StructureBase</code>, default:                 <code>None</code> )         \u2013          <p>If another Structure object created this Structure instance, pass the 'parent' instance. Will take ownership over Structure containers (coords, atoms, residues) for dependent Structures</p> </li> <li> <code>log</code>             (<code>Log | Logger | bool</code>, default:                 <code>True</code> )         \u2013          <p>The Log or Logger instance, or the name for the logger to handle parent Structure logging. None or False prevents logging while any True assignment enables it</p> </li> <li> <code>coords</code>             (<code>ndarray | Coordinates | list[list[float]]</code>, default:                 <code>None</code> )         \u2013          <p>When setting up a parent Structure instance, the coordinates of that Structure</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The identifier for the Structure instance</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, parent: StructureBase = None, log: Log | Logger | bool = True,\n             coords: np.ndarray | Coordinates | list[list[float]] = None, name: str = None,\n             # metadata: StructureMetadata = None,\n             file_path: AnyStr = None, biological_assembly: str | int = None,\n             cryst_record: str = None, resolution: float = None,\n             entity_info: dict[str, dict[dict | list | str]] = None,\n             reference_sequence: str | dict[str, str] = None,\n             # entity_info=None,\n             **kwargs):\n    # These shouldn't be passed as they should be stripped by prior constructors...\n    # entity_names=None, rotation_matrices=None, translation_matrices=None,\n    # metadata=None, pose_format=None, query_by_sequence=True, rename_chains=None\n    \"\"\"Construct the instance\n\n    Args:\n        parent: If another Structure object created this Structure instance, pass the 'parent' instance. Will take\n            ownership over Structure containers (coords, atoms, residues) for dependent Structures\n        log: The Log or Logger instance, or the name for the logger to handle parent Structure logging.\n            None or False prevents logging while any True assignment enables it\n        coords: When setting up a parent Structure instance, the coordinates of that Structure\n        name: The identifier for the Structure instance\n    \"\"\"\n    self.name = name if name not in [None, False] else f'Unnamed_{self.__class__.__name__}'\n    if parent is not None:  # Initialize StructureBase from parent\n        self._parent = parent\n    else:  # This is the parent\n        self._parent_ = None\n        self.metadata = StructureMetadata(\n            biological_assembly=biological_assembly, cryst_record=cryst_record, entity_info=entity_info,\n            file_path=file_path, reference_sequence=reference_sequence, resolution=resolution\n        )\n        # Initialize Log\n        if log:\n            if isinstance(log, Log):  # Initialized Log\n                self._log = log\n            elif isinstance(log, Logger):  # logging.Logger object\n                self._log = Log(log)\n            elif isinstance(log, str):\n                self._log = Log(utils.start_log(name=f'{__name__}.{self.name}', level=log))\n            else:  # log is True or some other type:  # Use the module logger\n                self._log = Log(logger)\n        else:  # When explicitly passed as None or False, uses the null logger\n            self._log = null_struct_log  # Log()\n\n        # Initialize Coordinates\n        if coords is None:  # Check this first\n            # Most init occurs from Atom instances that are their own parent until another StructureBase adopts them\n            self._coords = Coordinates()\n        elif isinstance(coords, Coordinates):\n            self._coords = coords\n        else:  # Create a Coordinates instance. This assumes the dimensions are correct. Coordinates() handles if not\n            self._coords = Coordinates(coords)\n\n    # try:\n    super().__init__(**kwargs)  # StructureBase\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBase.parent","title":"parent  <code>property</code>","text":"<pre><code>parent: StructureBase | None\n</code></pre> <p>Return the instance's 'parent' StructureBase which is responsible for manipulation of Structure containers</p>"},{"location":"reference/structure/base/#structure.base.StructureBase.log","title":"log  <code>property</code> <code>writable</code>","text":"<pre><code>log: Logger\n</code></pre> <p>The StructureBase Logger</p>"},{"location":"reference/structure/base/#structure.base.StructureBase.coords","title":"coords  <code>property</code> <code>writable</code>","text":"<pre><code>coords: ndarray\n</code></pre> <p>The coordinates for the Atoms in the StructureBase object</p>"},{"location":"reference/structure/base/#structure.base.StructureBase.is_dependent","title":"is_dependent","text":"<pre><code>is_dependent() -&gt; bool\n</code></pre> <p>Is this instance a dependent on a parent StructureBase?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_dependent(self) -&gt; bool:\n    \"\"\"Is this instance a dependent on a parent StructureBase?\"\"\"\n    return self._parent_ is not None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBase.is_parent","title":"is_parent","text":"<pre><code>is_parent() -&gt; bool\n</code></pre> <p>Is this instance a parent?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_parent(self) -&gt; bool:\n    \"\"\"Is this instance a parent?\"\"\"\n    return self._parent_ is None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBase.make_parent","title":"make_parent","text":"<pre><code>make_parent()\n</code></pre> <p>Remove this instance from its parent, making it a parent in the process</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def make_parent(self):\n    \"\"\"Remove this instance from its parent, making it a parent in the process\"\"\"\n    # Set parent explicitly as None\n    self.__setattr__(_parent_variable, None)\n    # Create a new, Coords instance detached from the parent\n    self._coords = Coordinates(self.coords)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBase.reset_state","title":"reset_state","text":"<pre><code>reset_state()\n</code></pre> <p>Removes attributes that are valid for the current state. Additionally, resets the symmetry state if symmetric</p> <p>This is useful for transfer of ownership, or changes in the state that should be overwritten</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reset_state(self):\n    \"\"\"Removes attributes that are valid for the current state. Additionally, resets the symmetry state if symmetric\n\n    This is useful for transfer of ownership, or changes in the state that should be overwritten\n    \"\"\"\n    # self.log.debug(f'Resetting {repr(self)} state_attributes')\n    for attr in self.state_attributes:\n        try:\n            self.__delattr__(attr)\n        except AttributeError:\n            continue\n\n    self.reset_symmetry_state()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom","title":"Atom","text":"<pre><code>Atom(index: int = None, number: int = None, atom_type: str = None, alt_location: str = ' ', residue_type: str = None, chain_id: str = None, residue_number: int = None, code_for_insertion: str = ' ', x: float = None, y: float = None, z: float = None, occupancy: float = None, b_factor: float = None, element: str = None, charge: str = None, coords: list[float] = None, **kwargs)\n</code></pre> <p>             Bases: <code>CoordinateOpsMixin</code></p> <p>Contains Atom metadata and a single coordinate position</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The zero-indexed number to describe this Atom instance's position in a StructureBaseContainer</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The integer number describing this Atom instances position in comparison to others instances</p> </li> <li> <code>atom_type</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The characters describing the classification of atom via membership to a molecule</p> </li> <li> <code>alt_location</code>             (<code>str</code>, default:                 <code>' '</code> )         \u2013          <p>Whether the observation of the Atom has alternative evidences</p> </li> <li> <code>residue_type</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The characters describing the molecular unit to which this Atom belongs</p> </li> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The identifier which signifies this Atom instances membership to a larger polymer</p> </li> <li> <code>residue_number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The integer of the polymer that this Atom belongs too</p> </li> <li> <code>code_for_insertion</code>             (<code>str</code>, default:                 <code>' '</code> )         \u2013          <p>Whether the Atom is included in a residue which is inserted according to a common numbering scheme</p> </li> <li> <code>x</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The x axis value of the instances coordinate position, i.e., it's x coordinate</p> </li> <li> <code>y</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The y axis value of the instances coordinate position, i.e., it's y coordinate</p> </li> <li> <code>z</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The z axis value of the instances coordinate position, i.e., it's z coordinate</p> </li> <li> <code>occupancy</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The fraction of the time this Atom is observed in the experiment</p> </li> <li> <code>b_factor</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The thermal fluctuation for the Atom OR a value for which structural representation should account</p> </li> <li> <code>element</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The atomic symbol for the Atom</p> </li> <li> <code>charge</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The numeric value for the electronic charge of the Atom</p> </li> <li> <code>coords</code>             (<code>list[float]</code>, default:                 <code>None</code> )         \u2013          <p>The set of values of the x, y, and z coordinate position</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, index: int = None, number: int = None, atom_type: str = None, alt_location: str = ' ',\n             residue_type: str = None, chain_id: str = None, residue_number: int = None,\n             code_for_insertion: str = ' ', x: float = None, y: float = None, z: float = None,\n             occupancy: float = None, b_factor: float = None, element: str = None, charge: str = None,\n             coords: list[float] = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        index: The zero-indexed number to describe this Atom instance's position in a StructureBaseContainer\n        number: The integer number describing this Atom instances position in comparison to others instances\n        atom_type: The characters describing the classification of atom via membership to a molecule\n        alt_location: Whether the observation of the Atom has alternative evidences\n        residue_type: The characters describing the molecular unit to which this Atom belongs\n        chain_id: The identifier which signifies this Atom instances membership to a larger polymer\n        residue_number: The integer of the polymer that this Atom belongs too\n        code_for_insertion: Whether the Atom is included in a residue which is inserted according to a common\n            numbering scheme\n        x: The x axis value of the instances coordinate position, i.e., it's x coordinate\n        y: The y axis value of the instances coordinate position, i.e., it's y coordinate\n        z: The z axis value of the instances coordinate position, i.e., it's z coordinate\n        occupancy: The fraction of the time this Atom is observed in the experiment\n        b_factor: The thermal fluctuation for the Atom OR a value for which structural representation should account\n        element: The atomic symbol for the Atom\n        charge: The numeric value for the electronic charge of the Atom\n        coords: The set of values of the x, y, and z coordinate position\n        **kwargs:\n    \"\"\"\n    # super().__init__(**kwargs)  # Atom\n    self.index = index\n    # self._atom_indices = [index]\n    self.number = number\n    self._type = atom_type\n    # Comply with special .pdb formatting syntax by padding type with a space if len(atom_type) == 4\n    self._type_str = f'{\"\" if atom_type[3:] else \" \"}{atom_type:&lt;3s}'\n    self.alt_location = alt_location\n    self.residue_type = residue_type\n    self.chain_id = chain_id\n    self.residue_number = residue_number\n    self.code_for_insertion = code_for_insertion\n    if coords is not None:\n        self._coords_ = coords\n    elif x is not None and y is not None and z is not None:\n        self._coords_ = [x, y, z]\n    else:\n        self._coords_ = []\n    self.occupancy = occupancy\n    self.b_factor = b_factor\n    self.element = element\n    self.charge = charge\n    self.sasa = None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>This can't currently be set</p>"},{"location":"reference/structure/base/#structure.base.Atom.atom_indices","title":"atom_indices  <code>property</code>","text":"<pre><code>atom_indices: list[int]\n</code></pre> <p>The index of the Atom in the Atoms/Coords container</p>"},{"location":"reference/structure/base/#structure.base.Atom.coords","title":"coords  <code>property</code> <code>writable</code>","text":"<pre><code>coords: ndarray\n</code></pre> <p>The coordinates for the Atom. Array is 1 dimensional in contrast to other .coords properties</p>"},{"location":"reference/structure/base/#structure.base.Atom.center_of_mass","title":"center_of_mass  <code>property</code>","text":"<pre><code>center_of_mass: ndarray\n</code></pre> <p>The center of mass (the Atom coordinates). Provided for compatibility with StructureBase API</p>"},{"location":"reference/structure/base/#structure.base.Atom.radius","title":"radius  <code>property</code>","text":"<pre><code>radius: float\n</code></pre> <p>The width of the Atom</p>"},{"location":"reference/structure/base/#structure.base.Atom.x","title":"x  <code>property</code> <code>writable</code>","text":"<pre><code>x: float\n</code></pre> <p>Access the value for the x coordinate</p>"},{"location":"reference/structure/base/#structure.base.Atom.y","title":"y  <code>property</code> <code>writable</code>","text":"<pre><code>y: float\n</code></pre> <p>Access the value for the y coordinate</p>"},{"location":"reference/structure/base/#structure.base.Atom.z","title":"z  <code>property</code> <code>writable</code>","text":"<pre><code>z: float\n</code></pre> <p>Access the value for the z coordinate</p>"},{"location":"reference/structure/base/#structure.base.Atom.without_coordinates","title":"without_coordinates  <code>classmethod</code>","text":"<pre><code>without_coordinates(idx, number, atom_type, alt_location, residue_type, chain_id, residue_number, code_for_insertion, occupancy, b_factor, element, charge)\n</code></pre> <p>Initialize Atom record data without coordinates. Performs all type casting</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>@classmethod\ndef without_coordinates(cls, idx, number, atom_type, alt_location, residue_type, chain_id, residue_number,\n                        code_for_insertion, occupancy, b_factor, element, charge):\n    \"\"\"Initialize Atom record data without coordinates. Performs all type casting\"\"\"\n    return cls(index=idx, number=int(number), atom_type=atom_type, alt_location=alt_location,\n               residue_type=residue_type, chain_id=chain_id, residue_number=int(residue_number),\n               code_for_insertion=code_for_insertion, occupancy=float(occupancy), b_factor=float(b_factor),\n               element=element, charge=charge, coords=[])  # Use a list for speed\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_dependent","title":"is_dependent","text":"<pre><code>is_dependent() -&gt; bool\n</code></pre> <p>Is this instance a dependent on a parent StructureBase?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_dependent(self) -&gt; bool:\n    \"\"\"Is this instance a dependent on a parent StructureBase?\"\"\"\n    return True\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_parent","title":"is_parent","text":"<pre><code>is_parent() -&gt; bool\n</code></pre> <p>Is this instance a parent?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_parent(self) -&gt; bool:\n    \"\"\"Is this instance a parent?\"\"\"\n    return False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.make_parent","title":"make_parent","text":"<pre><code>make_parent()\n</code></pre> <p>Remove this instance from its parent, making it a parent in the process</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def make_parent(self):\n    \"\"\"Remove this instance from its parent, making it a parent in the process\"\"\"\n    # super().make_parent()  # When subclassing StructureBase\n    # Create a new, Coords instance detached from the parent\n    self._coords = Coordinates(self.coords)\n    self.index = 0\n    self.reset_state()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.reset_state","title":"reset_state","text":"<pre><code>reset_state()\n</code></pre> <p>Remove attributes that are valid for the current state</p> <p>This is useful for transfer of ownership, or changes in the state that should be overwritten</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reset_state(self):\n    \"\"\"Remove attributes that are valid for the current state\n\n    This is useful for transfer of ownership, or changes in the state that should be overwritten\n    \"\"\"\n    for attr in self.state_attributes:\n        try:\n            self.__delattr__(attr)\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_backbone_and_cb","title":"is_backbone_and_cb","text":"<pre><code>is_backbone_and_cb() -&gt; bool\n</code></pre> <p>Is the Atom is a backbone or CB Atom? Includes N, CA, C, O, and CB</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_backbone_and_cb(self) -&gt; bool:\n    \"\"\"Is the Atom is a backbone or CB Atom? Includes N, CA, C, O, and CB\"\"\"\n    return self._type in protein_backbone_and_cb_atom_types\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_backbone","title":"is_backbone","text":"<pre><code>is_backbone() -&gt; bool\n</code></pre> <p>Is the Atom is a backbone Atom? These include N, CA, C, and O</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_backbone(self) -&gt; bool:\n    \"\"\"Is the Atom is a backbone Atom? These include N, CA, C, and O\"\"\"\n    return self._type in protein_backbone_atom_types\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_cb","title":"is_cb","text":"<pre><code>is_cb(gly_ca: bool = True) -&gt; bool\n</code></pre> <p>Is the Atom a CB atom? Default returns True if Glycine and Atom is CA</p> <p>Parameters:</p> <ul> <li> <code>gly_ca</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to include Glycine CA in the boolean evaluation</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_cb(self, gly_ca: bool = True) -&gt; bool:\n    \"\"\"Is the Atom a CB atom? Default returns True if Glycine and Atom is CA\n\n    Args:\n        gly_ca: Whether to include Glycine CA in the boolean evaluation\n    \"\"\"\n    if gly_ca:\n        return self._type == 'CB' or (self.residue_type == 'GLY' and self._type == 'CA')\n    else:\n        #                                         When Rosetta assigns, it is this   v, but PDB assigns this v\n        return self._type == 'CB' or (self.residue_type == 'GLY' and (self._type == '2HA' or self._type == 'HA3'))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_ca","title":"is_ca","text":"<pre><code>is_ca() -&gt; bool\n</code></pre> <p>Is the Atom a CA atom?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_ca(self) -&gt; bool:\n    \"\"\"Is the Atom a CA atom?\"\"\"\n    return self._type == 'CA'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.is_heavy","title":"is_heavy","text":"<pre><code>is_heavy() -&gt; bool\n</code></pre> <p>Is the Atom a heavy atom?</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_heavy(self) -&gt; bool:\n    \"\"\"Is the Atom a heavy atom?\"\"\"\n    return 'H' not in self._type\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.get_atom_record","title":"get_atom_record","text":"<pre><code>get_atom_record() -&gt; str\n</code></pre> <p>Provide the Atom as an Atom record string</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The archived .pdb formatted ATOM records for the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_atom_record(self) -&gt; str:\n    \"\"\"Provide the Atom as an Atom record string\n\n    Returns:\n        The archived .pdb formatted ATOM records for the Structure\n    \"\"\"\n    x, y, z = list(self.coords)\n    # Add 1 to the self.index since this is 0 indexed\n    return f'ATOM  {self.index + 1:5d} {self._type_str}{self.alt_location:1s}{self.residue_type:3s}' \\\n           f'{self.chain_id:&gt;2s}{self.residue_number:4d}{self.code_for_insertion:1s}   ' \\\n           f'{x:8.3f}{y:8.3f}{z:8.3f}{self.occupancy:6.2f}{self.b_factor:6.2f}          ' \\\n           f'{self.element:&gt;2s}{self.charge:2s}'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atom.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Represent Atom in PDB format</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Represent Atom in PDB format\"\"\"\n    # Use self.type_str to comply with the PDB format specifications because of the atom type field\n    # ATOM     32  CG2 VAL A 132       9.902  -5.550   0.695  1.00 17.48           C  &lt;-- PDB format\n    # Checks if len(atom.type)=4 with slice v. If not insert a space\n    return f'ATOM  {\"{}\"} {self._type_str}{self.alt_location:1s}{\"{}\"}{\"{}\"}{\"{}\"}' \\\n           f'{self.code_for_insertion:1s}   {\"{}\"}{self.occupancy:6.2f}{self.b_factor:6.2f}          ' \\\n           f'{self.element:&gt;2s}{self.charge:2s}'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer","title":"StructureBaseContainer","text":"<pre><code>StructureBaseContainer(structs: Sequence[StructureBase] | ndarray = None)\n</code></pre> <p>             Bases: <code>Generic[_StructType]</code></p> <p>Container for a StructureBase instances</p> <p>Parameters:</p> <ul> <li> <code>structs</code>             (<code>Sequence[StructureBase] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The StructureBase instances to store. Should be a homogeneous Sequence</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, structs: Sequence[StructureBase] | np.ndarray = None):\n    \"\"\"Construct the instance\n\n    Args:\n        structs: The StructureBase instances to store. Should be a homogeneous Sequence\n    \"\"\"\n    if structs is None:\n        structs = []\n    elif not isinstance(structs, (np.ndarray, list)):\n        structs = list(structs)\n        # raise TypeError(\n        #     f\"Can't initialize {self.__class__.__name__} with {type(structs).__name__}. Type must be a \"\n        #     f'numpy.ndarray or list[{StructureBase.__name__}]')\n\n    self.structs = np.array(structs, dtype=np.object_)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.reindex","title":"reindex  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>reindex\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.are_dependents","title":"are_dependents","text":"<pre><code>are_dependents() -&gt; bool\n</code></pre> <p>Check if any of the StructureBase instances are dependents of another StructureBase</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def are_dependents(self) -&gt; bool:\n    \"\"\"Check if any of the StructureBase instances are dependents of another StructureBase\"\"\"\n    for struct in self:\n        if struct.is_dependent():\n            return True\n    return False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.append","title":"append","text":"<pre><code>append(new_structures: list[_StructType] | ndarray)\n</code></pre> <p>Append additional StructureBase instances to the StructureBaseContainer</p> <p>Parameters:</p> <ul> <li> <code>new_structures</code>             (<code>list[_StructType] | ndarray</code>)         \u2013          <p>The Structure instances to append</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def append(self, new_structures: list[_StructType] | np.ndarray):\n    \"\"\"Append additional StructureBase instances to the StructureBaseContainer\n\n    Args:\n        new_structures: The Structure instances to append\n    \"\"\"\n    self.structs = np.concatenate((self.structs, new_structures))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.delete","title":"delete","text":"<pre><code>delete(indices: Sequence[int])\n</code></pre> <p>Delete StructureBase instances from the StructureBaseContainer</p> <p>Parameters:</p> <ul> <li> <code>indices</code>             (<code>Sequence[int]</code>)         \u2013          <p>The indices to delete</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def delete(self, indices: Sequence[int]):\n    \"\"\"Delete StructureBase instances from the StructureBaseContainer\n\n    Args:\n        indices: The indices to delete\n    \"\"\"\n    self.structs = np.delete(self.structs, indices)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.insert","title":"insert","text":"<pre><code>insert(at: int, new_structures: list[_StructType] | ndarray)\n</code></pre> <p>Insert StructureBase instances into the StructureBaseContainer</p> <p>Parameters:</p> <ul> <li> <code>at</code>             (<code>int</code>)         \u2013          <p>The index to perform the insert at</p> </li> <li> <code>new_structures</code>             (<code>list[_StructType] | ndarray</code>)         \u2013          <p>The Structure instances to insert</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def insert(self, at: int, new_structures: list[_StructType] | np.ndarray):\n    \"\"\"Insert StructureBase instances into the StructureBaseContainer\n\n    Args:\n        at: The index to perform the insert at\n        new_structures: The Structure instances to insert\n    \"\"\"\n    self.structs = np.concatenate((\n        self.structs[:at],\n        new_structures if isinstance(new_structures, Iterable) else [new_structures],\n        self.structs[at:]\n    ))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.set","title":"set","text":"<pre><code>set(new_structures: list[_StructType] | ndarray)\n</code></pre> <p>Set the StructureBaseContainer with new StructureBase instances</p> <p>Parameters:</p> <ul> <li> <code>new_structures</code>             (<code>list[_StructType] | ndarray</code>)         \u2013          <p>The new instances which should make up the container</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set(self, new_structures: list[_StructType] | np.ndarray):\n    \"\"\"Set the StructureBaseContainer with new StructureBase instances\n\n    Args:\n        new_structures: The new instances which should make up the container\n    \"\"\"\n    self.structs = np.array(new_structures)\n    self.reindex()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.reset_state","title":"reset_state","text":"<pre><code>reset_state()\n</code></pre> <p>Remove any attributes from the Structure instances that are part of the current state</p> <p>This is useful for transfer of ownership, or changes in the state that need to be overwritten</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reset_state(self):\n    \"\"\"Remove any attributes from the Structure instances that are part of the current state\n\n    This is useful for transfer of ownership, or changes in the state that need to be overwritten\n    \"\"\"\n    for struct in self:\n        struct.reset_state()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureBaseContainer.set_attributes","title":"set_attributes","text":"<pre><code>set_attributes(**kwargs)\n</code></pre> <p>Set Structure attributes passed by keyword to their corresponding value</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_attributes(self, **kwargs):\n    \"\"\"Set Structure attributes passed by keyword to their corresponding value\"\"\"\n    for struct in self:\n        for key, value in kwargs.items():\n            setattr(struct, key, value)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atoms","title":"Atoms","text":"<pre><code>Atoms(structs: Sequence[StructureBase] | ndarray = None)\n</code></pre> <p>             Bases: <code>StructureBaseContainer</code></p> <p>Parameters:</p> <ul> <li> <code>structs</code>             (<code>Sequence[StructureBase] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The StructureBase instances to store. Should be a homogeneous Sequence</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, structs: Sequence[StructureBase] | np.ndarray = None):\n    \"\"\"Construct the instance\n\n    Args:\n        structs: The StructureBase instances to store. Should be a homogeneous Sequence\n    \"\"\"\n    if structs is None:\n        structs = []\n    elif not isinstance(structs, (np.ndarray, list)):\n        structs = list(structs)\n        # raise TypeError(\n        #     f\"Can't initialize {self.__class__.__name__} with {type(structs).__name__}. Type must be a \"\n        #     f'numpy.ndarray or list[{StructureBase.__name__}]')\n\n    self.structs = np.array(structs, dtype=np.object_)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Atoms.reindex","title":"reindex","text":"<pre><code>reindex(start_at: int = 0)\n</code></pre> <p>Set each Atom instance index according to incremental Atoms/Coords index</p> <p>Parameters:</p> <ul> <li> <code>start_at</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The index to start reindexing at. Must be [0, 'inf']</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reindex(self, start_at: int = 0):\n    \"\"\"Set each Atom instance index according to incremental Atoms/Coords index\n\n    Args:\n        start_at: The index to start reindexing at. Must be [0, 'inf']\n    \"\"\"\n    if start_at == 0:\n        _start_at = start_at\n    else:\n        _start_at = start_at - 1\n        if start_at &lt; 0:\n            _start_at += len(self)\n    try:\n        prior_struct, *other_structs = self.structs[_start_at:]\n    except ValueError:  # Not enough values to unpack as the index didn't slice anything\n        raise IndexError(\n            f'{self.reindex.__name__}: {start_at=} is outside of the {self.__class__.__name__} indices with '\n            f'size {len(self)}')\n    else:\n        if start_at == 0:\n            prior_struct.index = 0\n\n        for idx, struct in enumerate(other_structs, prior_struct.index + 1):\n            struct.index = idx\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms","title":"ContainsAtoms","text":"<pre><code>ContainsAtoms(atoms: list[Atom] | Atoms = None, atom_indices: list[int] = None, **kwargs)\n</code></pre> <p>             Bases: <code>StructureBase</code>, <code>ABC</code></p> <p>Parameters:</p> <ul> <li> <code>atoms</code>             (<code>list[Atom] | Atoms</code>, default:                 <code>None</code> )         \u2013          <p>Atom instances to initialize the instance</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, atoms: list[Atom] | Atoms = None, atom_indices: list[int] = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        atoms: Atom instances to initialize the instance\n    \"\"\"\n    super().__init__(**kwargs)  # ContainsAtoms\n    if self.is_parent():\n        if atoms is not None:\n            self._assign_atoms(atoms)\n        else:  # Create an empty container\n            # try:\n            #     self._atoms\n            # except AttributeError:\n            self._atoms = Atoms()\n    elif atom_indices is not None:\n        try:\n            atom_indices[0]\n        except (TypeError, IndexError):\n            raise ValueError(\n                f\"The {self.__class__.__name__} wasn't passed 'atom_indices' which are required for initialization\")\n\n        if not isinstance(atom_indices, list):\n            atom_indices = list(atom_indices)\n        self._atom_indices = atom_indices\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.atoms","title":"atoms  <code>property</code>","text":"<pre><code>atoms: list[Atom] | None\n</code></pre> <p>Return the Atom instances in the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.atom_indices","title":"atom_indices  <code>property</code>","text":"<pre><code>atom_indices: list[int]\n</code></pre> <p>The Atoms/Coords indices which the StructureBase has access to</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.number_of_atoms","title":"number_of_atoms  <code>property</code>","text":"<pre><code>number_of_atoms: int\n</code></pre> <p>The number of atoms/coordinates in the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_and_cb_indices","title":"backbone_and_cb_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>backbone_and_cb_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase backbone and CB Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_indices","title":"backbone_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>backbone_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase backbone and CB Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.ca_indices","title":"ca_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ca_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase CA Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.cb_indices","title":"cb_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>cb_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase CB Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.heavy_indices","title":"heavy_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>heavy_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase heavy (non-hydrogen) Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.side_chain_indices","title":"side_chain_indices  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>side_chain_indices: list[int]\n</code></pre> <p>The indices that index the StructureBase side-chain Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_atoms","title":"backbone_atoms  <code>property</code>","text":"<pre><code>backbone_atoms: list[Atom]\n</code></pre> <p>Returns backbone Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_and_cb_atoms","title":"backbone_and_cb_atoms  <code>property</code>","text":"<pre><code>backbone_and_cb_atoms: list[Atom]\n</code></pre> <p>Returns backbone and CB Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.ca_atoms","title":"ca_atoms  <code>property</code>","text":"<pre><code>ca_atoms: list[Atom]\n</code></pre> <p>Returns CA Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.cb_atoms","title":"cb_atoms  <code>property</code>","text":"<pre><code>cb_atoms: list[Atom]\n</code></pre> <p>Returns CB Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.heavy_atoms","title":"heavy_atoms  <code>property</code>","text":"<pre><code>heavy_atoms: list[Atom]\n</code></pre> <p>Returns heavy Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.side_chain_atoms","title":"side_chain_atoms  <code>property</code>","text":"<pre><code>side_chain_atoms: list[Atom]\n</code></pre> <p>Returns side chain Atom instances from the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.center_of_mass","title":"center_of_mass  <code>property</code>","text":"<pre><code>center_of_mass: ndarray\n</code></pre> <p>The center of mass for the Atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.radius","title":"radius  <code>property</code>","text":"<pre><code>radius: float\n</code></pre> <p>The furthest point from the center of mass of the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.radius_of_gyration","title":"radius_of_gyration  <code>property</code>","text":"<pre><code>radius_of_gyration: float\n</code></pre> <p>The measurement of the implied radius (Angstroms) affecting how the StructureBase diffuses through solution</p> Satisfies the equation <p>Rg = SQRT(SUM|i-&gt;N(Ri**2)/N)</p> <p>Where:     - Ri is the radius of the point i from the center of mass point     - N is the total number of points</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_coords","title":"backbone_coords  <code>property</code>","text":"<pre><code>backbone_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the StructureBase with backbone atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.backbone_and_cb_coords","title":"backbone_and_cb_coords  <code>property</code>","text":"<pre><code>backbone_and_cb_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the StructureBase with backbone and CB atom coordinates. Includes glycine CA</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.ca_coords","title":"ca_coords  <code>property</code>","text":"<pre><code>ca_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the Structure with CA atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.cb_coords","title":"cb_coords  <code>property</code>","text":"<pre><code>cb_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the Structure with CB atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.heavy_coords","title":"heavy_coords  <code>property</code>","text":"<pre><code>heavy_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the StructureBase with heavy atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.side_chain_coords","title":"side_chain_coords  <code>property</code>","text":"<pre><code>side_chain_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the StructureBase with side chain atom coordinates</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.start_index","title":"start_index  <code>property</code>","text":"<pre><code>start_index: int\n</code></pre> <p>The first atomic index of the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.end_index","title":"end_index  <code>property</code>","text":"<pre><code>end_index: int\n</code></pre> <p>The last atomic index of the StructureBase</p>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.make_parent","title":"make_parent","text":"<pre><code>make_parent()\n</code></pre> <p>Remove this instance from its parent, making it a parent in the process</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def make_parent(self):\n    \"\"\"Remove this instance from its parent, making it a parent in the process\"\"\"\n    super().make_parent()\n    # Populate the Structure with its existing instances removed of any indexing\n    self._assign_atoms(self.atoms)\n    self.reset_state()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.get_base_containers","title":"get_base_containers","text":"<pre><code>get_base_containers() -&gt; dict[str, Any]\n</code></pre> <p>Access each of the constituent structure container attributes</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>The instance structural containers as a dictionary with attribute as key and container as value</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_base_containers(self) -&gt; dict[str, Any]:\n    \"\"\"Access each of the constituent structure container attributes\n\n    Returns:\n        The instance structural containers as a dictionary with attribute as key and container as value\n    \"\"\"\n    return dict(coords=self._coords, atoms=self._atoms)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.neighboring_atom_indices","title":"neighboring_atom_indices","text":"<pre><code>neighboring_atom_indices(distance: float = 8.0, **kwargs) -&gt; list[int]\n</code></pre> <p>Returns the Atom instances in the Structure</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>The distance to measure neighbors by</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The sorted atom_indices which are in contact with this instance, however, do not belong to the instance</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def neighboring_atom_indices(self, distance: float = 8., **kwargs) -&gt; list[int]:  # np.ndarray:\n    \"\"\"Returns the Atom instances in the Structure\n\n    Args:\n        distance: The distance to measure neighbors by\n\n    Returns:\n        The sorted atom_indices which are in contact with this instance, however, do not belong to the instance\n    \"\"\"\n    parent_coords = self._coords.coords\n    atom_indices = self.atom_indices\n\n    # Create a \"self.coords\" and modify only coordinates in the ContainsAtoms instance\n    modified_self_coords = parent_coords.copy()\n    # Translate each self coordinate\n    modified_self_coords[atom_indices] = parent_coords.max(axis=0) + [1000, 1000, 1000]\n    modified_coords_balltree = BallTree(modified_self_coords)\n\n    # Query for neighbors of the self coordinates but excluding the self indices\n    coords = parent_coords[atom_indices]\n    query = modified_coords_balltree.query_radius(coords, distance)\n\n    return sorted({idx for contacts in query.tolist() for idx in contacts.tolist()})\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.atom","title":"atom","text":"<pre><code>atom(atom_number: int) -&gt; Atom | None\n</code></pre> <p>Returns the Atom specified by atom number if a matching Atom is found, otherwise None</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def atom(self, atom_number: int) -&gt; Atom | None:\n    \"\"\"Returns the Atom specified by atom number if a matching Atom is found, otherwise None\"\"\"\n    for atom in self.atoms:\n        if atom.number == atom_number:\n            return atom\n    return None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.renumber_atoms","title":"renumber_atoms","text":"<pre><code>renumber_atoms(at: int = 1)\n</code></pre> <p>Renumber all Atom objects sequentially starting with 1</p> <p>Parameters:</p> <ul> <li> <code>at</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number to start renumbering at</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def renumber_atoms(self, at: int = 1):\n    \"\"\"Renumber all Atom objects sequentially starting with 1\n\n    Args:\n        at: The number to start renumbering at\n    \"\"\"\n    for idx, atom in enumerate(self.atoms, at):\n        atom.number = idx\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.reset_indices","title":"reset_indices","text":"<pre><code>reset_indices()\n</code></pre> <p>Reset the indices attached to the instance</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reset_indices(self):\n    \"\"\"Reset the indices attached to the instance\"\"\"\n    for attr in self._indices_attributes:\n        try:\n            delattr(self, attr)\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.format_header","title":"format_header","text":"<pre><code>format_header(**kwargs) -&gt; str\n</code></pre> <p>Returns the base .pdb formatted header</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The .pdb file header string</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def format_header(self, **kwargs) -&gt; str:\n    \"\"\"Returns the base .pdb formatted header\n\n    Returns:\n        The .pdb file header string\n    \"\"\"\n    # XXXX should be substituted for the PDB code. Ex:\n    # f'HEADER    VIRAL PROTEIN                           28-MAY-21   7OP2              \\n'\n    # f'HEADER    VIRAL PROTEIN/DE NOVO PROTEIN           11-MAY-12   4ATZ              \\n'\n    return \\\n        f'HEADER    {self.name[:40]:&lt;40s}{utils.short_start_date.upper():&lt;12s}{\"XXXX\":&lt;18s}\\n' \\\n        'EXPDTA    THEORETICAL MODEL                                                     \\n' \\\n        'REMARK 220                                                                      \\n' \\\n        'REMARK 220 EXPERIMENTAL DETAILS                                                 \\n' \\\n        'REMARK 220  EXPERIMENT TYPE                : THEORETICAL MODELLING              \\n' \\\n        f'REMARK 220  DATE OF DATA COLLECTION        : {utils.long_start_date:&lt;35s}\\n' \\\n        f'REMARK 220 REMARK: MODEL GENERATED BY {putils.program_name.upper():&lt;50s}\\n' \\\n        f'REMARK 220         VERSION {putils.commit_short:&lt;61s}\\n'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.get_atom_record","title":"get_atom_record  <code>abstractmethod</code>","text":"<pre><code>get_atom_record(**kwargs) -&gt; str\n</code></pre> <p>Provide the Structure Atom instances as a .pdb file string</p> <p>Other Parameters:</p> <ul> <li> <code>chain_id</code>         \u2013          <p>str = None - The chain ID to use</p> </li> <li> <code>atom_offset</code>         \u2013          <p>int = 0 - How much to offset the atom number by. Default returns one-indexed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The archived .pdb formatted ATOM records for the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>@abc.abstractmethod\ndef get_atom_record(self, **kwargs) -&gt; str:\n    \"\"\"Provide the Structure Atom instances as a .pdb file string\n\n    Keyword Args:\n        chain_id: str = None - The chain ID to use\n        atom_offset: int = 0 - How much to offset the atom number by. Default returns one-indexed\n\n    Returns:\n        The archived .pdb formatted ATOM records for the Structure\n    \"\"\"\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.write","title":"write","text":"<pre><code>write(out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Write Atom instances to a file specified by out_path or with a passed file_handle</p> <p>If a file_handle is passed, no header information will be written. Arguments are mutually exclusive</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>bytes | str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the Structure object should be written to disk</p> </li> <li> <code>file_handle</code>             (<code>IO</code>, default:                 <code>None</code> )         \u2013          <p>Used to write Structure details to an open FileObject</p> </li> <li> <code>header</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string that is desired at the top of the file</p> </li> </ul> <p>Keyword Args     chain_id: str = None - The chain ID to use     atom_offset: int = 0 - How much to offset the atom index by. Default uses one-indexed atom numbers</p> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The name of the written file if out_path is used</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def write(\n    self, out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, **kwargs\n) -&gt; AnyStr | None:\n    \"\"\"Write Atom instances to a file specified by out_path or with a passed file_handle\n\n    If a file_handle is passed, no header information will be written. Arguments are mutually exclusive\n\n    Args:\n        out_path: The location where the Structure object should be written to disk\n        file_handle: Used to write Structure details to an open FileObject\n        header: A string that is desired at the top of the file\n\n    Keyword Args\n        chain_id: str = None - The chain ID to use\n        atom_offset: int = 0 - How much to offset the atom index by. Default uses one-indexed atom numbers\n\n    Returns:\n        The name of the written file if out_path is used\n    \"\"\"\n    if file_handle:\n        file_handle.write(f'{self.get_atom_record(**kwargs)}\\n')\n        return None\n    else:  # out_path always has default argument current working directory\n        _header = self.format_header()\n        if header is not None:\n            if not isinstance(header, str):\n                header = str(header)\n            _header += (header if header[-2:] == '\\n' else f'{header}\\n')\n\n        with open(out_path, 'w') as outfile:\n            outfile.write(_header)\n            outfile.write(f'{self.get_atom_record(**kwargs)}\\n')\n        return out_path\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.get_atoms","title":"get_atoms","text":"<pre><code>get_atoms(numbers: Container = None, **kwargs) -&gt; list[Atom]\n</code></pre> <p>Retrieves Atom instances. Returns all by default unless a list of numbers is specified</p> <p>Parameters:</p> <ul> <li> <code>numbers</code>             (<code>Container</code>, default:                 <code>None</code> )         \u2013          <p>The Atom numbers of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Atom]</code>         \u2013          <p>The requested Atom objects</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_atoms(self, numbers: Container = None, **kwargs) -&gt; list[Atom]:\n    \"\"\"Retrieves Atom instances. Returns all by default unless a list of numbers is specified\n\n    Args:\n        numbers: The Atom numbers of interest\n\n    Returns:\n        The requested Atom objects\n    \"\"\"\n    if numbers is not None:\n        if isinstance(numbers, Container):\n            return [atom for atom in self.atoms if atom.number in numbers]\n        else:\n            self.log.error(f'The passed numbers type \"{type(numbers).__name__}\" must be a Container. Returning'\n                           f' all Atom instances instead')\n    return self.atoms\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsAtoms.set_atoms_attributes","title":"set_atoms_attributes","text":"<pre><code>set_atoms_attributes(**kwargs)\n</code></pre> <p>Set attributes specified by key, value pairs for Atoms in the Structure</p> <p>Other Parameters:</p> <ul> <li> <code>numbers</code>         \u2013          <p>Container[int] = None - The Atom numbers of interest</p> </li> <li> <code>pdb</code>         \u2013          <p>bool = False - Whether to search for numbers as they were parsed (if True)</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_atoms_attributes(self, **kwargs):\n    \"\"\"Set attributes specified by key, value pairs for Atoms in the Structure\n\n    Keyword Args:\n        numbers: Container[int] = None - The Atom numbers of interest\n        pdb: bool = False - Whether to search for numbers as they were parsed (if True)\n    \"\"\"\n    for atom in self.get_atoms(**kwargs):\n        for kwarg, value in kwargs.items():\n            setattr(atom, kwarg, value)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue","title":"Residue","text":"<pre><code>Residue(**kwargs)\n</code></pre> <p>             Bases: <code>ContainsAtoms</code>, <code>ResidueFragment</code></p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # Residue\n    if self.is_parent():\n        # Setting up a parent (independent) Residue\n        self._ensure_valid_residue()\n        self.start_index = 0\n\n    self.delegate_atoms()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.index","title":"index  <code>property</code>","text":"<pre><code>index\n</code></pre> <p>The Residue index in a Residues container</p>"},{"location":"reference/structure/base/#structure.base.Residue.range","title":"range  <code>property</code>","text":"<pre><code>range: list[int]\n</code></pre> <p>The range of indices corresponding to the Residue atoms</p>"},{"location":"reference/structure/base/#structure.base.Residue.type1","title":"type1  <code>property</code>","text":"<pre><code>type1: str\n</code></pre> <p>Access the one character representation of the amino acid type</p>"},{"location":"reference/structure/base/#structure.base.Residue.backbone_indices","title":"backbone_indices  <code>property</code> <code>writable</code>","text":"<pre><code>backbone_indices: list[int]\n</code></pre> <p>The indices that index the Residue backbone Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.backbone_and_cb_indices","title":"backbone_and_cb_indices  <code>property</code> <code>writable</code>","text":"<pre><code>backbone_and_cb_indices: list[int]\n</code></pre> <p>The indices that index the Residue backbone and CB Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.ca_indices","title":"ca_indices  <code>property</code>","text":"<pre><code>ca_indices: list[int]\n</code></pre> <p>Return the index of the CA Atom as a list in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.cb_indices","title":"cb_indices  <code>property</code>","text":"<pre><code>cb_indices: list[int]\n</code></pre> <p>Return the index of the CB Atom as a list in the Residue Atoms/Coords. Will return CA index if Glycine</p>"},{"location":"reference/structure/base/#structure.base.Residue.side_chain_indices","title":"side_chain_indices  <code>property</code> <code>writable</code>","text":"<pre><code>side_chain_indices: list[int]\n</code></pre> <p>The indices that index the Residue side chain Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.heavy_indices","title":"heavy_indices  <code>property</code> <code>writable</code>","text":"<pre><code>heavy_indices: list[int]\n</code></pre> <p>The indices that index the Residue heavy (non-hydrogen) Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.n","title":"n  <code>property</code>","text":"<pre><code>n: Atom | None\n</code></pre> <p>Return the amide N Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.n_coords","title":"n_coords  <code>property</code>","text":"<pre><code>n_coords: ndarray | None\n</code></pre> <p>Return the amide N Atom coordinate</p>"},{"location":"reference/structure/base/#structure.base.Residue.n_atom_index","title":"n_atom_index  <code>property</code>","text":"<pre><code>n_atom_index: int | None\n</code></pre> <p>Return the index of the amide N Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.n_index","title":"n_index  <code>property</code>","text":"<pre><code>n_index: int | None\n</code></pre> <p>Return the index of the amide N Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.h","title":"h  <code>property</code>","text":"<pre><code>h: Atom | None\n</code></pre> <p>Return the amide H Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.h_coords","title":"h_coords  <code>property</code>","text":"<pre><code>h_coords: ndarray | None\n</code></pre> <p>Return the amide H Atom coordinate</p>"},{"location":"reference/structure/base/#structure.base.Residue.h_atom_index","title":"h_atom_index  <code>property</code>","text":"<pre><code>h_atom_index: int | None\n</code></pre> <p>Return the index of the amide H Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.h_index","title":"h_index  <code>property</code>","text":"<pre><code>h_index: int | None\n</code></pre> <p>Return the index of the amide H Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.ca","title":"ca  <code>property</code>","text":"<pre><code>ca: Atom | None\n</code></pre> <p>Return the CA Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.ca_coords","title":"ca_coords  <code>property</code>","text":"<pre><code>ca_coords: ndarray | None\n</code></pre> <p>Return the CA Atom coordinate</p>"},{"location":"reference/structure/base/#structure.base.Residue.ca_atom_index","title":"ca_atom_index  <code>property</code>","text":"<pre><code>ca_atom_index: int | None\n</code></pre> <p>Return the index of the CA Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.ca_index","title":"ca_index  <code>property</code>","text":"<pre><code>ca_index: int | None\n</code></pre> <p>Return the index of the CA Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.cb","title":"cb  <code>property</code>","text":"<pre><code>cb: Atom | None\n</code></pre> <p>Return the CB Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.cb_coords","title":"cb_coords  <code>property</code>","text":"<pre><code>cb_coords: ndarray | None\n</code></pre> <p>Return the CB Atom coordinate</p>"},{"location":"reference/structure/base/#structure.base.Residue.cb_atom_index","title":"cb_atom_index  <code>property</code>","text":"<pre><code>cb_atom_index: int | None\n</code></pre> <p>Return the index of the CB Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.cb_index","title":"cb_index  <code>property</code>","text":"<pre><code>cb_index: int | None\n</code></pre> <p>Return the index of the CB Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.c","title":"c  <code>property</code>","text":"<pre><code>c: Atom | None\n</code></pre> <p>Return the carbonyl C Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.c_coords","title":"c_coords  <code>property</code>","text":"<pre><code>c_coords: ndarray | None\n</code></pre> <p>Return the carbonyl C Atom coordinate</p>"},{"location":"reference/structure/base/#structure.base.Residue.c_atom_index","title":"c_atom_index  <code>property</code>","text":"<pre><code>c_atom_index: int | None\n</code></pre> <p>Return the index of the carbonyl C Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.c_index","title":"c_index  <code>property</code>","text":"<pre><code>c_index: int | None\n</code></pre> <p>Return the index of the carbonyl C Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.o","title":"o  <code>property</code>","text":"<pre><code>o: Atom | None\n</code></pre> <p>Return the carbonyl O Atom object</p>"},{"location":"reference/structure/base/#structure.base.Residue.o_coords","title":"o_coords  <code>property</code>","text":"<pre><code>o_coords: ndarray | None\n</code></pre> <p>Return the carbonyl O Atom coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.o_atom_index","title":"o_atom_index  <code>property</code>","text":"<pre><code>o_atom_index: int | None\n</code></pre> <p>Return the index of the carbonyl C Atom in the Structure Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.o_index","title":"o_index  <code>property</code>","text":"<pre><code>o_index: int | None\n</code></pre> <p>Return the index of the carbonyl O Atom in the Residue Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.Residue.prev_residue","title":"prev_residue  <code>property</code>","text":"<pre><code>prev_residue: Residue | None\n</code></pre> <p>The previous Residue in the Structure if this Residue is part of a polymer</p>"},{"location":"reference/structure/base/#structure.base.Residue.next_residue","title":"next_residue  <code>property</code>","text":"<pre><code>next_residue: Residue | None\n</code></pre> <p>The next Residue in the Structure if this Residue is part of a polymer</p>"},{"location":"reference/structure/base/#structure.base.Residue.local_density","title":"local_density  <code>property</code> <code>writable</code>","text":"<pre><code>local_density: float\n</code></pre> <p>Describes how many heavy Atoms are within a distance (default = 12 Angstroms) of Residue heavy Atoms</p>"},{"location":"reference/structure/base/#structure.base.Residue.secondary_structure","title":"secondary_structure  <code>property</code> <code>writable</code>","text":"<pre><code>secondary_structure: str\n</code></pre> <p>Return the secondary structure designation as defined by a secondary structure calculation</p>"},{"location":"reference/structure/base/#structure.base.Residue.sasa","title":"sasa  <code>property</code> <code>writable</code>","text":"<pre><code>sasa: float\n</code></pre> <p>Return the solvent accessible surface area as calculated by a solvent accessible surface area calculator</p>"},{"location":"reference/structure/base/#structure.base.Residue.sasa_apolar","title":"sasa_apolar  <code>property</code> <code>writable</code>","text":"<pre><code>sasa_apolar: float\n</code></pre> <p>Return apolar solvent accessible surface area as calculated by a solvent accessible surface area calculator</p>"},{"location":"reference/structure/base/#structure.base.Residue.sasa_polar","title":"sasa_polar  <code>property</code> <code>writable</code>","text":"<pre><code>sasa_polar: float\n</code></pre> <p>Return polar solvent accessible surface area as calculated by a solvent accessible surface area calculator</p>"},{"location":"reference/structure/base/#structure.base.Residue.relative_sasa","title":"relative_sasa  <code>property</code>","text":"<pre><code>relative_sasa: float\n</code></pre> <p>The solvent accessible surface area relative to the standard surface accessibility of the Residue type</p>"},{"location":"reference/structure/base/#structure.base.Residue.spatial_aggregation_propensity","title":"spatial_aggregation_propensity  <code>property</code> <code>writable</code>","text":"<pre><code>spatial_aggregation_propensity: float\n</code></pre> <p>The Residue contact order, which describes how far away each Residue makes contacts in the polymer chain</p>"},{"location":"reference/structure/base/#structure.base.Residue.contact_order","title":"contact_order  <code>property</code> <code>writable</code>","text":"<pre><code>contact_order: float\n</code></pre> <p>The Residue contact order, which describes how far away each Residue makes contacts in the polymer chain</p>"},{"location":"reference/structure/base/#structure.base.Residue.start_index","title":"start_index","text":"<pre><code>start_index(index: int)\n</code></pre> <p>Set Residue atom_indices starting with atom_indices[0] as start_index. Creates remainder incrementally and updates individual Atom instance .index accordingly</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>@ContainsAtoms.start_index.setter\ndef start_index(self, index: int):\n    \"\"\"Set Residue atom_indices starting with atom_indices[0] as start_index. Creates remainder incrementally and\n    updates individual Atom instance .index accordingly\n    \"\"\"\n    self._atom_indices = list(range(index, index + self.number_of_atoms))\n    for atom, index in zip(self._atoms[self._atom_indices], self._atom_indices):\n        atom.index = index\n    # Clear all the indices attributes for this Residue\n    self.reset_indices()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.delegate_atoms","title":"delegate_atoms","text":"<pre><code>delegate_atoms()\n</code></pre> <p>Set the Residue atoms from a parent StructureBase</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def delegate_atoms(self):\n    \"\"\"Set the Residue atoms from a parent StructureBase\"\"\"\n    side_chain_indices, heavy_indices = [], []\n    # try:\n    #     for idx, atom in enumerate(self.atoms):\n    #         match atom.type:  # Todo python 3.10\n    #             case 'N':\n    #                 self._n_index = idx\n    #                 if self.number is None:\n    #                     self.chain_id = atom.chain_id\n    #                     self.number = atom.residue_number\n    #                     self.type = atom.residue_type\n    #                else:\n    #                    raise stutils.ConstructionError(\n    #                        f\"Couldn't create a {self.__class__.__name__} with multiple 'N' Atom instances\"\n    #                    )\n    #             case 'CA':\n    #                 self._ca_index = idx\n    #             case 'CB':\n    #                 self._cb_index = idx\n    #             case 'C':\n    #                 self._c_index = idx\n    #             case 'O':\n    #                 self._o_index = idx\n    #             case 'H':\n    #                 self._h_index = idx\n    #             case _:\n    #                 side_chain_indices.append(idx)\n    #                 if 'H' not in atom.type:\n    #                     heavy_indices.append(idx)\n    # except SyntaxError:  # python version not 3.10\n    for idx, atom in enumerate(self.atoms):\n        atom_type = atom.type\n        if atom_type == 'N':\n            self._n_index = idx\n            if self.number is None:\n                self.chain_id = atom.chain_id\n                self.number = atom.residue_number\n                self.type = atom.residue_type\n            else:\n                raise stutils.ConstructionError(\n                    f\"Couldn't create a {self.__class__.__name__} with multiple 'N' Atom instances\"\n                )\n        elif atom_type == 'CA':\n            self._ca_index = idx\n        elif atom_type == 'CB':\n            self._cb_index = idx\n        elif atom_type == 'C':\n            self._c_index = idx\n        elif atom_type == 'O':\n            self._o_index = idx\n        elif atom_type == 'H':  # 1H or H1\n            self._h_index = idx\n        # elif atom_type == 'OXT':\n        #     self._oxt_index = idx\n        # elif atom_type == 'H2':\n        #     self._h2_index = idx\n        # elif atom_type == 'H3':\n        #     self._h3_index = idx\n        else:\n            side_chain_indices.append(idx)\n            if 'H' not in atom_type:\n                heavy_indices.append(idx)\n\n    # Construction ensures proper order for _bb_indices even if out of order\n    # Important this order is correct for ProteinMPNN\n    self.backbone_indices = [getattr(self, f'_{index}_index', None) for index in ['n', 'ca', 'c', 'o']]\n    cb_index = getattr(self, '_cb_index', None)\n    if cb_index:\n        cb_indices = [cb_index]\n    else:\n        if self.type == 'GLY':  # Set _cb_index, but don't include in backbone_and_cb_indices\n            self._cb_index = getattr(self, '_ca_index')\n        cb_indices = []\n\n    # By using private backbone_indices variable v, None is removed\n    self.backbone_and_cb_indices = self._bb_indices + cb_indices\n    self.heavy_indices = self._bb_and_cb_indices + heavy_indices\n    self.side_chain_indices = side_chain_indices\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.contains_hydrogen","title":"contains_hydrogen","text":"<pre><code>contains_hydrogen() -&gt; bool\n</code></pre> <p>Returns whether the Residue contains hydrogen atoms</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def contains_hydrogen(self) -&gt; bool:  # in Structure too\n    \"\"\"Returns whether the Residue contains hydrogen atoms\"\"\"\n    return self.heavy_indices != self._atom_indices\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.is_n_termini","title":"is_n_termini","text":"<pre><code>is_n_termini() -&gt; bool\n</code></pre> <p>Returns whether the Residue is the n-termini of the parent Structure</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_n_termini(self) -&gt; bool:\n    \"\"\"Returns whether the Residue is the n-termini of the parent Structure\"\"\"\n    return self.prev_residue is None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.is_c_termini","title":"is_c_termini","text":"<pre><code>is_c_termini() -&gt; bool\n</code></pre> <p>Returns whether the Residue is the c-termini of the parent Structure</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_c_termini(self) -&gt; bool:\n    \"\"\"Returns whether the Residue is the c-termini of the parent Structure\"\"\"\n    return self.next_residue is None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.get_upstream","title":"get_upstream","text":"<pre><code>get_upstream(number: int = None) -&gt; list[Residue]\n</code></pre> <p>Get the Residues upstream of (n-terminal to) the current Residue</p> <p>Parameters:</p> <ul> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of residues to retrieve. If not provided gets all</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The Residue instances in n- to c-terminal order</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_upstream(self, number: int = None) -&gt; list[Residue]:\n    \"\"\"Get the Residues upstream of (n-terminal to) the current Residue\n\n    Args:\n        number: The number of residues to retrieve. If not provided gets all\n\n    Returns:\n        The Residue instances in n- to c-terminal order\n    \"\"\"\n    if number is None:\n        number = sys.maxsize\n    elif number == 0:\n        raise ValueError(\"Can't get 0 upstream residues. 1 or more must be specified\")\n\n    last_prev_residue = self.prev_residue\n    prev_residues = [last_prev_residue]\n    idx = 0\n    try:\n        for idx in range(abs(number) - 1):\n            prev_residue = last_prev_residue.prev_residue\n            prev_residues.append(prev_residue)\n            last_prev_residue = prev_residue\n    except AttributeError:  # Hit a termini, where prev_residue is None\n        # logger.debug(f'Stopped at {idx=} with {repr(prev_residues)}. Popping the last')\n        prev_residues.pop(idx)\n    else:  # For the edge case where the last added residue is a termini, strip from results\n        if last_prev_residue is None:\n            prev_residues.pop()\n\n    return prev_residues[::-1]\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.get_downstream","title":"get_downstream","text":"<pre><code>get_downstream(number: int = None) -&gt; list[Residue]\n</code></pre> <p>Get the Residues downstream of (c-terminal to) the current Residue</p> <p>Parameters:</p> <ul> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of residues to retrieve. If not provided gets all</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The Residue instances in n- to c-terminal order</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_downstream(self, number: int = None) -&gt; list[Residue]:\n    \"\"\"Get the Residues downstream of (c-terminal to) the current Residue\n\n    Args:\n        number: The number of residues to retrieve. If not provided gets all\n\n    Returns:\n        The Residue instances in n- to c-terminal order\n    \"\"\"\n    if number is None:\n        number = sys.maxsize\n    elif number == 0:\n        raise ValueError(\"Can't get 0 downstream residues. 1 or more must be specified\")\n\n    last_next_residue = self.next_residue\n    next_residues = [last_next_residue]\n    idx = 0\n    try:\n        for idx in range(abs(number) - 1):\n            next_residue = last_next_residue.next_residue\n            next_residues.append(next_residue)\n            last_next_residue = next_residue\n    except AttributeError:  # Hit a termini, where next_residue is None\n        # logger.debug(f'Stopped at {idx=} with {repr(next_residues)}. Popping the last')\n        next_residues.pop(idx)\n    else:  # For the edge case where the last added residue is a termini, strip from results\n        if last_next_residue is None:\n            next_residues.pop()\n\n    return next_residues\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.get_neighbors","title":"get_neighbors","text":"<pre><code>get_neighbors(distance: float = 8.0, **kwargs) -&gt; list[Residue]\n</code></pre> <p>If this Residue instance is part of a polymer, find neighboring Residue instances defined by a distance</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>The distance to measure neighbors by</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The Residue instances that are within the distance to this Residue</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_neighbors(self, distance: float = 8., **kwargs) -&gt; list[Residue]:\n    \"\"\"If this Residue instance is part of a polymer, find neighboring Residue instances defined by a distance\n\n    Args:\n        distance: The distance to measure neighbors by\n\n    Returns:\n        The Residue instances that are within the distance to this Residue\n    \"\"\"\n    try:\n        return self.parent.get_residues_by_atom_indices(self.neighboring_atom_indices(distance=distance, **kwargs))\n    except AttributeError:  # This Residue is the parent\n        return [self]\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.mutation_possibilities_from_directive","title":"mutation_possibilities_from_directive","text":"<pre><code>mutation_possibilities_from_directive(directive: directives = None, background: set[str] = None, special: bool = False, **kwargs) -&gt; set[protein_letters3_alph1_literal] | set\n</code></pre> <p>Select mutational possibilities for each Residue based on the Residue and a directive</p> <p>Parameters:</p> <ul> <li> <code>directive</code>             (<code>directives</code>, default:                 <code>None</code> )         \u2013          <p>Where the choice is one of 'special', 'same', 'different', 'charged', 'polar', 'apolar', 'hydrophobic', 'aromatic', 'hbonding', 'branched'</p> </li> <li> <code>background</code>             (<code>set[str]</code>, default:                 <code>None</code> )         \u2013          <p>The background amino acids to compare possibilities against</p> </li> <li> <code>special</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to include special residues</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>set[protein_letters3_alph1_literal] | set</code>         \u2013          <p>The possible amino acid types available given the mutational directive</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def mutation_possibilities_from_directive(\n    self, directive: directives = None, background: set[str] = None, special: bool = False, **kwargs\n) -&gt; set[protein_letters3_alph1_literal] | set:\n    \"\"\"Select mutational possibilities for each Residue based on the Residue and a directive\n\n    Args:\n        directive: Where the choice is one of 'special', 'same', 'different', 'charged', 'polar', 'apolar',\n            'hydrophobic', 'aromatic', 'hbonding', 'branched'\n        background: The background amino acids to compare possibilities against\n        special: Whether to include special residues\n\n    Returns:\n        The possible amino acid types available given the mutational directive\n    \"\"\"\n    if not directive or directive not in mutation_directives:\n        self.log.debug(f'{self.mutation_possibilities_from_directive.__name__}: The mutation directive {directive} '\n                       f'is not a valid directive yet. Possible directives are: {\", \".join(mutation_directives)}')\n        return set()\n        # raise TypeError('%s: The mutation directive %s is not a valid directive yet. Possible directives are: %s'\n        #                 % (self.mutation_possibilities_from_directive.__name__, directive,\n        #                    ', '.join(mutation_directives)))\n\n    current_properties = residue_properties[self.type]\n    if directive == 'same':\n        properties = current_properties\n    elif directive == 'different':  # hmm not right -&gt; .difference({hbonding, branched}) &lt;- for ex. polar if apolar\n        properties = set(aa_by_property.keys()).difference(current_properties)\n    else:\n        properties = [directive]\n    available_aas = set(aa for prop in properties for aa in aa_by_property[prop])\n\n    if directive != 'special' and not special:\n        available_aas = available_aas.difference(aa_by_property['special'])\n    if background:\n        available_aas = background.intersection(available_aas)\n\n    return available_aas\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.distance","title":"distance","text":"<pre><code>distance(other: Residue, dtype: str = 'ca') -&gt; float\n</code></pre> <p>Return the distance from this Residue to another specified by atom type \"dtype\"</p> <p>Parameters:</p> <ul> <li> <code>other</code>             (<code>Residue</code>)         \u2013          <p>The other Residue to measure against</p> </li> <li> <code>dtype</code>             (<code>str</code>, default:                 <code>'ca'</code> )         \u2013          <p>The Atom type to perform the measurement with</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The Euclidean distance between the specified Atom type</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def distance(self, other: Residue, dtype: str = 'ca') -&gt; float:\n    \"\"\"Return the distance from this Residue to another specified by atom type \"dtype\"\n\n    Args:\n        other: The other Residue to measure against\n        dtype: The Atom type to perform the measurement with\n\n    Returns:\n        The Euclidean distance between the specified Atom type\n    \"\"\"\n    return np.linalg.norm(getattr(self, f'.{dtype}_coords') - getattr(other, f'.{dtype}_coords'))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.get_atom_record","title":"get_atom_record","text":"<pre><code>get_atom_record(chain_id: str = None, atom_offset: int = 0, **kwargs) -&gt; str\n</code></pre> <p>Provide the Structure Atoms as a PDB file string</p> <p>Parameters:</p> <ul> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain ID to use</p> </li> <li> <code>atom_offset</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>How much to offset the atom number by. Default returns one-indexed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The archived .pdb formatted ATOM records for the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_atom_record(self, chain_id: str = None, atom_offset: int = 0, **kwargs) -&gt; str:\n    \"\"\"Provide the Structure Atoms as a PDB file string\n\n    Args:\n        chain_id: The chain ID to use\n        atom_offset: How much to offset the atom number by. Default returns one-indexed\n\n    Returns:\n        The archived .pdb formatted ATOM records for the Structure\n    \"\"\"\n    return f'{self.__str__(chain_id=chain_id, atom_offset=atom_offset, **kwargs)}\\n'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residue.__str__","title":"__str__","text":"<pre><code>__str__(chain_id: str = None, atom_offset: int = 0, **kwargs) -&gt; str\n</code></pre> <p>Format the Residue into the contained Atoms. The Atom number is truncated at 5 digits for PDB compliant formatting</p> <p>Parameters:</p> <ul> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain ID to use</p> </li> <li> <code>atom_offset</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>How much to offset the atom number by. Default returns one-indexed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The archived .pdb formatted ATOM records for the Residue</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __str__(self, chain_id: str = None, atom_offset: int = 0, **kwargs) -&gt; str:\n    #         type=None, number=None\n    \"\"\"Format the Residue into the contained Atoms. The Atom number is truncated at 5 digits for PDB compliant\n    formatting\n\n    Args:\n        chain_id: The chain ID to use\n        atom_offset: How much to offset the atom number by. Default returns one-indexed\n\n    Returns:\n        The archived .pdb formatted ATOM records for the Residue\n    \"\"\"\n    #     pdb: Whether the Residue representation should use the number at file parsing\n    # format the string returned from each Atom, such as\n    #  'ATOM  %s  CG2 %s %s%s    %s  1.00 17.48           C  0'\n    #       AtomIdx  TypeChNumberCoords\n    # To\n    #  'ATOM     32  CG2 VAL A 132       9.902  -5.550   0.695  1.00 17.48           C  0'\n    # self.type, self.alt_location, self.code_for_insertion, self.occupancy, self.b_factor,\n    #                     self.element, self.charge)\n    # res_str = self.residue_string(**kwargs)\n    res_str = format(self.type, '3s'), format(chain_id or self.chain_id, '&gt;2s'), format(self.number, '4d')\n    # Add 1 to make index one-indexed\n    offset = 1 + atom_offset\n    # Limit atom_index with atom_index_slice to keep ATOM record correct length v\n    return '\\n'.join(atom.__str__().format(format(atom_idx + offset, '5d')[atom_index_slice],\n                                           *res_str, '{:8.3f}{:8.3f}{:8.3f}'.format(*coord))\n                     for atom, atom_idx, coord in zip(self.atoms, self._atom_indices, self.coords.tolist()))\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residues","title":"Residues","text":"<pre><code>Residues(structs: Sequence[StructureBase] | ndarray = None)\n</code></pre> <p>             Bases: <code>StructureBaseContainer</code></p> <p>Parameters:</p> <ul> <li> <code>structs</code>             (<code>Sequence[StructureBase] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The StructureBase instances to store. Should be a homogeneous Sequence</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, structs: Sequence[StructureBase] | np.ndarray = None):\n    \"\"\"Construct the instance\n\n    Args:\n        structs: The StructureBase instances to store. Should be a homogeneous Sequence\n    \"\"\"\n    if structs is None:\n        structs = []\n    elif not isinstance(structs, (np.ndarray, list)):\n        structs = list(structs)\n        # raise TypeError(\n        #     f\"Can't initialize {self.__class__.__name__} with {type(structs).__name__}. Type must be a \"\n        #     f'numpy.ndarray or list[{StructureBase.__name__}]')\n\n    self.structs = np.array(structs, dtype=np.object_)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residues.reindex","title":"reindex","text":"<pre><code>reindex(start_at: int = 0)\n</code></pre> <p>Index the Residue instances and their corresponding Atom/Coords indices according to their position</p> <p>Parameters:</p> <ul> <li> <code>start_at</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The index to start reindexing at. Must be [0, 'inf']</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reindex(self, start_at: int = 0):\n    \"\"\"Index the Residue instances and their corresponding Atom/Coords indices according to their position\n\n    Args:\n        start_at: The index to start reindexing at. Must be [0, 'inf']\n    \"\"\"\n    self.set_index(start_at=start_at)\n    self.reindex_atoms(start_at=start_at)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residues.set_index","title":"set_index","text":"<pre><code>set_index(start_at: int = 0)\n</code></pre> <p>Index the Residue instances according to their position in the Residues container</p> <p>Parameters:</p> <ul> <li> <code>start_at</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The Residue index to start reindexing at</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_index(self, start_at: int = 0):\n    \"\"\"Index the Residue instances according to their position in the Residues container\n\n    Args:\n        start_at: The Residue index to start reindexing at\n    \"\"\"\n    for idx, struct in enumerate(self.structs[start_at:].tolist(), start_at):\n        struct._index = idx\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Residues.reindex_atoms","title":"reindex_atoms","text":"<pre><code>reindex_atoms(start_at: int = 0)\n</code></pre> <p>Index the Residue instances Atoms/Coords indices according to incremental Atoms/Coords index. Responsible for updating member Atom.index attributes as well</p> <p>Parameters:</p> <ul> <li> <code>start_at</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The index to start reindexing at. Must be [0, 'inf']</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def reindex_atoms(self, start_at: int = 0):\n    \"\"\"Index the Residue instances Atoms/Coords indices according to incremental Atoms/Coords index. Responsible\n    for updating member Atom.index attributes as well\n\n    Args:\n        start_at: The index to start reindexing at. Must be [0, 'inf']\n    \"\"\"\n    if start_at == 0:\n        _start_at = start_at\n    else:\n        _start_at = start_at - 1\n        if start_at &lt; 0:\n            _start_at += len(self)\n\n    struct: Residue\n    prior_struct: Residue\n    try:\n        # prior_struct, *other_structs = self.structs[start_at - 1:]\n        prior_struct, *other_structs = self[_start_at:]\n    except ValueError:  # Not enough values to unpack as the index didn't slice anything\n        raise IndexError(\n            f'{self.reindex_atoms.__name__}: {start_at=} is outside of the allowed {self.__class__.__name__} '\n            f'indices with size {len(self)}')\n    else:\n        if start_at == 0:\n            prior_struct.start_index = start_at\n\n        for struct in other_structs:\n            struct.start_index = prior_struct.end_index + 1\n            prior_struct = struct\n</code></pre>"},{"location":"reference/structure/base/#structure.base.StructureIndexMixin","title":"StructureIndexMixin","text":"<p>             Bases: <code>ABC</code></p>"},{"location":"reference/structure/base/#structure.base.StructureIndexMixin.is_parent","title":"is_parent  <code>abstractmethod</code>","text":"<pre><code>is_parent() -&gt; bool\n</code></pre> Source code in <code>symdesign/structure/base.py</code> <pre><code>@abc.abstractmethod\ndef is_parent(self) -&gt; bool:\n    \"\"\"\"\"\"\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues","title":"ContainsResidues","text":"<pre><code>ContainsResidues(structure: ContainsResidues = None, residues: list[Residue] | Residues = None, residue_indices: list[int] = None, pose_format: bool = False, fragment_db: FragmentDatabase = None, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsAtoms</code>, <code>StructureIndexMixin</code></p> <p>Structure object handles Atom/Residue/Coords manipulation of all Structure containers. Must pass parent and residue_indices, atoms and coords, or residues to initialize</p> <p>Polymer/Chain designation. This designation essentially means it contains Residue instances in a Residues object</p> <p>Parameters:</p> <ul> <li> <code>structure</code>             (<code>ContainsResidues</code>, default:                 <code>None</code> )         \u2013          <p>Create the instance based on an existing Structure instance</p> </li> <li> <code>residues</code>             (<code>list[Residue] | Residues</code>, default:                 <code>None</code> )         \u2013          <p>The Residue instances which should constitute a new Structure instance</p> </li> <li> <code>residue_indices</code>             (<code>list[int]</code>, default:                 <code>None</code> )         \u2013          <p>The indices which specify the particular Residue instances to make this Structure instance. Used with a parent to specify a subdivision of a larger Structure</p> </li> <li> <code>pose_format</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to initialize with continuous Residue numbering from 1 to N</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atoms</code>         \u2013          <p>list[Atom] | Atoms = None - The Atom instances which should constitute a new Structure instance</p> </li> <li> <code>parent</code>         \u2013          <p>StructureBase = None - If another Structure object created this Structure instance, pass the 'parent' instance. Will take ownership over Structure containers (coords, atoms, residues) for dependent Structures</p> </li> <li> <code>log</code>         \u2013          <p>Log | Logger | bool = True - The Log or Logger instance, or the name for the logger to handle parent Structure logging. None or False prevents logging while any True assignment enables it</p> </li> <li> <code>coords</code>         \u2013          <p>Coords | np.ndarray | list[list[float]] = None - When setting up a parent Structure instance, the coordinates of that Structure</p> </li> <li> <code>name</code>         \u2013          <p>str = None - The identifier for the Structure instance</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, structure: ContainsResidues = None,\n             residues: list[Residue] | Residues = None, residue_indices: list[int] = None,\n             pose_format: bool = False, fragment_db: fragment.db.FragmentDatabase = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        structure: Create the instance based on an existing Structure instance\n        residues: The Residue instances which should constitute a new Structure instance\n        residue_indices: The indices which specify the particular Residue instances to make this Structure instance.\n            Used with a parent to specify a subdivision of a larger Structure\n        pose_format: Whether to initialize with continuous Residue numbering from 1 to N\n\n    Keyword Args:\n        atoms: list[Atom] | Atoms = None - The Atom instances which should constitute a new Structure instance\n        parent: StructureBase = None - If another Structure object created this Structure instance, pass the\n            'parent' instance. Will take ownership over Structure containers (coords, atoms, residues) for\n            dependent Structures\n        log: Log | Logger | bool = True - The Log or Logger instance, or the name for the logger to handle parent\n            Structure logging. None or False prevents logging while any True assignment enables it\n        coords: Coords | np.ndarray | list[list[float]] = None - When setting up a parent Structure instance, the\n            coordinates of that Structure\n        name: str = None - The identifier for the Structure instance\n    \"\"\"\n    if structure:\n        if isinstance(structure, ContainsResidues):\n            model_kwargs = structure.get_base_containers()\n            for key, value in model_kwargs.items():\n                if key in kwargs:\n                    logger.warning(f\"Passing an argument for '{key}' while providing the 'model' argument \"\n                                   f\"overwrites the '{key}' argument from the 'model'\")\n            new_model_kwargs = {**model_kwargs, **kwargs}\n            super().__init__(**new_model_kwargs)\n        else:\n            raise NotImplementedError(\n                f\"Setting {self.__class__.__name__} with model={type(structure).__name__} isn't supported\")\n    else:\n        super().__init__(**kwargs)  # ContainsResidues\n\n    # self._coords_indexed_residues_ = None\n    # self._residue_indices = None\n    # self.secondary_structure = None\n    # self.nucleotides_present = False\n    self._fragment_db = fragment_db\n    self.sasa = None\n    self.ss_sequence_indices = []\n    self.ss_type_sequence = []\n\n    if self.is_dependent():\n        try:\n            residue_indices[0]\n        except TypeError:\n            if isinstance(self, Structures):  # Structures handles this itself\n                return\n            raise stutils.ConstructionError(\n                f\"The {self.__class__.__name__} wasn't passed 'residue_indices' which are required for \"\n                f\"initialization\"\n            )\n\n        if not isinstance(residue_indices, list):\n            residue_indices = list(residue_indices)\n        # Must set this before setting _atom_indices\n        self._residue_indices = residue_indices\n        # Get the atom_indices from the provided residues\n        self._atom_indices = [idx for residue in self.residues for idx in residue.atom_indices]\n    # Setting up a parent Structure\n    elif residues:  # Assume the passed residues aren't bound to an existing Structure\n        self._assign_residues(residues)\n    elif self.atoms:\n        # Assume ContainsAtoms initialized .atoms. Make Residue instances, Residues\n        self._create_residues()\n    else:  # Set up an empty Structure or let subclass handle population\n        return\n\n    if pose_format:\n        self.pose_numbering()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.ss_sequence_indices","title":"ss_sequence_indices  <code>instance-attribute</code>","text":"<pre><code>ss_sequence_indices: list[int] = []\n</code></pre> <p>Index which indicates the Residue membership to the secondary structure type element sequence</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.ss_type_sequence","title":"ss_type_sequence  <code>instance-attribute</code>","text":"<pre><code>ss_type_sequence: list[str] = []\n</code></pre> <p>The ordered secondary structure type sequence which contains one character/secondary structure element</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.fragment_db","title":"fragment_db  <code>property</code> <code>writable</code>","text":"<pre><code>fragment_db: FragmentDatabase\n</code></pre> <p>The FragmentDatabase used to create Fragment instances</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.sequence","title":"sequence  <code>property</code> <code>writable</code>","text":"<pre><code>sequence: str\n</code></pre> <p>Holds the Structure amino acid sequence</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.residue_indices","title":"residue_indices  <code>property</code>","text":"<pre><code>residue_indices: list[int] | None\n</code></pre> <p>Return the residue indices which belong to the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.residues","title":"residues  <code>property</code>","text":"<pre><code>residues: list[Residue] | None\n</code></pre> <p>Return the Residue instances in the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.alphafold_atom_mask","title":"alphafold_atom_mask  <code>property</code>","text":"<pre><code>alphafold_atom_mask: ndarray\n</code></pre> <p>Return an Alphafold mask describing which Atom positions have Coord data</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.alphafold_coords","title":"alphafold_coords  <code>property</code>","text":"<pre><code>alphafold_coords: ndarray\n</code></pre> <p>Return a view of the Coords from the StructureBase in Alphafold coordinate format</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.coords_indexed_residues","title":"coords_indexed_residues  <code>property</code>","text":"<pre><code>coords_indexed_residues: list[Residue]\n</code></pre> <p>Returns the Residue associated with each Coord in the Structure</p> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>Each Residue which owns the corresponding index in the .atoms/.coords attribute</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.backbone_coords_indexed_residues","title":"backbone_coords_indexed_residues  <code>property</code>","text":"<pre><code>backbone_coords_indexed_residues: list[Residue]\n</code></pre> <p>Returns the Residue associated with each backbone Atom/Coord in the Structure</p> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>Each Residue which owns the corresponding index in the .atoms/.coords attribute</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.backbone_and_cb_coords_indexed_residues","title":"backbone_and_cb_coords_indexed_residues  <code>property</code>","text":"<pre><code>backbone_and_cb_coords_indexed_residues: list[Residue]\n</code></pre> <p>Returns the Residue associated with each backbone and CB Atom/Coord in the Structure</p> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>Each Residue which owns the corresponding index in the .atoms/.coords attribute</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.heavy_coords_indexed_residues","title":"heavy_coords_indexed_residues  <code>property</code>","text":"<pre><code>heavy_coords_indexed_residues: list[Residue]\n</code></pre> <p>Returns the Residue associated with each heavy Atom/Coord in the Structure</p> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>Each Residue which owns the corresponding index in the .atoms/.coords attribute</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.side_chain_coords_indexed_residues","title":"side_chain_coords_indexed_residues  <code>property</code>","text":"<pre><code>side_chain_coords_indexed_residues: list[Residue]\n</code></pre> <p>Returns the Residue associated with each side chain Atom/Coord in the Structure</p> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>Each Residue which owns the corresponding index in the .atoms/.coords attribute</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.number_of_residues","title":"number_of_residues  <code>property</code>","text":"<pre><code>number_of_residues: int\n</code></pre> <p>Access the number of Residues in the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.backbone_indices","title":"backbone_indices  <code>property</code>","text":"<pre><code>backbone_indices: list[int]\n</code></pre> <p>The indices that index the Structure backbone Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.backbone_and_cb_indices","title":"backbone_and_cb_indices  <code>property</code>","text":"<pre><code>backbone_and_cb_indices: list[int]\n</code></pre> <p>The indices that index the Structure backbone and CB Atoms. Inherently gets CA of Residue instances missing CB</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.cb_indices","title":"cb_indices  <code>property</code>","text":"<pre><code>cb_indices: list[int]\n</code></pre> <p>The indices that index the Structure CB Atoms. Inherently gets CA of Residue instances missing CB</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.ca_indices","title":"ca_indices  <code>property</code>","text":"<pre><code>ca_indices: list[int]\n</code></pre> <p>The indices that index the Structure CA Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.heavy_indices","title":"heavy_indices  <code>property</code>","text":"<pre><code>heavy_indices: list[int]\n</code></pre> <p>The indices that index the Structure heavy Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.side_chain_indices","title":"side_chain_indices  <code>property</code>","text":"<pre><code>side_chain_indices: list[int]\n</code></pre> <p>The indices that index the Structure side chain Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.helix_cb_indices","title":"helix_cb_indices  <code>property</code>","text":"<pre><code>helix_cb_indices: list[int]\n</code></pre> <p>The indices that index the Structure helical CB Atoms/Coords</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.n_terminal_residue","title":"n_terminal_residue  <code>property</code>","text":"<pre><code>n_terminal_residue: Residue\n</code></pre> <p>Retrieve the Residue from the n-termini</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.c_terminal_residue","title":"c_terminal_residue  <code>property</code>","text":"<pre><code>c_terminal_residue: Residue\n</code></pre> <p>Retrieve the Residue from the c-termini</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.relative_sasa","title":"relative_sasa  <code>property</code>","text":"<pre><code>relative_sasa: list[float]\n</code></pre> <p>Return a per-residue array of the relative solvent accessible area for the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.sasa_apolar","title":"sasa_apolar  <code>property</code>","text":"<pre><code>sasa_apolar: list[float]\n</code></pre> <p>Return a per-residue array of the polar (hydrophilic) solvent accessible area for the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.sasa_polar","title":"sasa_polar  <code>property</code>","text":"<pre><code>sasa_polar: list[float]\n</code></pre> <p>Return a per-residue array of the apolar (hydrophobic) solvent accessible area for the Structure</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.surface_residues","title":"surface_residues  <code>property</code>","text":"<pre><code>surface_residues: list[Residue]\n</code></pre> <p>Get the Residue instances that reside on the surface of the molecule</p> <p>Parameters:</p> <ul> <li> <code>relative_sasa_thresh</code>         \u2013          <p>The relative area threshold that the Residue should have before it is considered 'surface'. Default cutoff is based on Levy, E. 2010</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The surface Residue instances</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.interior_residues","title":"interior_residues  <code>property</code>","text":"<pre><code>interior_residues: list[Residue]\n</code></pre> <p>Get the Residue instances that reside in the interior of the molecule</p> <p>Parameters:</p> <ul> <li> <code>relative_sasa_thresh</code>         \u2013          <p>The relative area threshold that the Residue should fall below before it is considered 'interior'. Default cutoff is based on Levy, E. 2010</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The interior Residue instances</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.secondary_structure","title":"secondary_structure  <code>property</code> <code>writable</code>","text":"<pre><code>secondary_structure: str\n</code></pre> <p>The Structure secondary structure assignment as provided by the DEFAULT_SS_PROGRAM</p>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.spatial_aggregation_propensity","title":"spatial_aggregation_propensity  <code>property</code>","text":"<pre><code>spatial_aggregation_propensity: ndarray\n</code></pre> <p>Return the spatial aggregation propensity on a per-residue basis</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The array of floats representing the spatial aggregation propensity for each Residue in the Structure</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.contact_order","title":"contact_order  <code>property</code> <code>writable</code>","text":"<pre><code>contact_order: ndarray\n</code></pre> <p>Return the contact order on a per-Residue basis</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The array of floats representing the contact order for each Residue in the Structure</p> </li> </ul>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.from_residues","title":"from_residues  <code>classmethod</code>","text":"<pre><code>from_residues(residues: list[Residue] | Residues, **kwargs)\n</code></pre> <p>Initialize from existing Residue instances</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>@classmethod\ndef from_residues(cls, residues: list[Residue] | Residues, **kwargs):\n    \"\"\"Initialize from existing Residue instances\"\"\"\n    return cls(residues=residues, **kwargs)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.from_structure","title":"from_structure  <code>classmethod</code>","text":"<pre><code>from_structure(structure: ContainsResidues, **kwargs)\n</code></pre> <p>Initialize from an existing Structure</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>@classmethod\ndef from_structure(cls, structure: ContainsResidues, **kwargs):\n    \"\"\"Initialize from an existing Structure\"\"\"\n    return cls(structure=structure, **kwargs)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.assign_residues_from_structures","title":"assign_residues_from_structures","text":"<pre><code>assign_residues_from_structures(structures: Iterable[ContainsResidues])\n</code></pre> <p>Initialize the instance from existing structure instance attributes, .coords, and .atoms, and .residues</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def assign_residues_from_structures(self, structures: Iterable[ContainsResidues]):\n    \"\"\"Initialize the instance from existing structure instance attributes, .coords, and .atoms, and .residues\"\"\"\n    atoms, residues, coords = [], [], []\n    for structure in structures:\n        atoms.extend(structure.atoms)\n        residues.extend(structure.residues)\n        coords.append(structure.coords)\n\n    self._assign_residues(residues, atoms=atoms, coords=coords)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.make_parent","title":"make_parent","text":"<pre><code>make_parent()\n</code></pre> <p>Remove this instance from its parent, making it a parent in the process</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def make_parent(self):\n    \"\"\"Remove this instance from its parent, making it a parent in the process\"\"\"\n    super(ContainsAtoms, ContainsAtoms).make_parent(self)\n    # Populate the Structure with its existing instances removed of any indexing\n    self._assign_residues(self.residues, atoms=self.atoms)\n    self.reset_state()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_base_containers","title":"get_base_containers","text":"<pre><code>get_base_containers() -&gt; dict[str, Any]\n</code></pre> <p>Returns the instance structural containers as a dictionary with attribute as key and container as value</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_base_containers(self) -&gt; dict[str, Any]:\n    \"\"\"Returns the instance structural containers as a dictionary with attribute as key and container as value\"\"\"\n    return dict(coords=self._coords, atoms=self._atoms, residues=self._residues)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.contains_hydrogen","title":"contains_hydrogen","text":"<pre><code>contains_hydrogen() -&gt; bool\n</code></pre> <p>Returns whether the Structure contains hydrogen atoms</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def contains_hydrogen(self) -&gt; bool:  # in Residue too\n    \"\"\"Returns whether the Structure contains hydrogen atoms\"\"\"\n    try:\n        return self._contains_hydrogen\n    except AttributeError:\n        # Sample 20 residues from various \"locations\".\n        # If H is still present but not found, there is an anomaly in the Structure\n        slice_amount = max(int(self.number_of_residues / 20), 1)\n        for residue in self.residues[::slice_amount]:\n            if residue.contains_hydrogen():\n                self._contains_hydrogen = True\n                break\n        else:\n            self._contains_hydrogen = False\n\n    return self._contains_hydrogen\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_coords_subset","title":"get_coords_subset","text":"<pre><code>get_coords_subset(residue_numbers: Container[int] = None, indices: Iterable[int] = None, start: int = None, end: int = None, dtype: coords_type_literal = 'ca') -&gt; ndarray\n</code></pre> <p>Return a view of a subset of the Coords from the Structure specified by a range of Residue numbers</p> <p>Parameters:</p> <ul> <li> <code>residue_numbers</code>             (<code>Container[int]</code>, default:                 <code>None</code> )         \u2013          <p>The Residue numbers to return</p> </li> <li> <code>indices</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>Residue indices of interest</p> </li> <li> <code>start</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The Residue number to start at. Inclusive</p> </li> <li> <code>end</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The Residue number to end at. Inclusive</p> </li> <li> <code>dtype</code>             (<code>coords_type_literal</code>, default:                 <code>'ca'</code> )         \u2013          <p>The type of coordinates to get</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The specific coordinates from the Residue instances with the specified dtype</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_coords_subset(self, residue_numbers: Container[int] = None, indices: Iterable[int] = None,\n                      start: int = None, end: int = None, dtype: stutils.coords_type_literal = 'ca') -&gt; np.ndarray:\n    \"\"\"Return a view of a subset of the Coords from the Structure specified by a range of Residue numbers\n\n    Args:\n        residue_numbers: The Residue numbers to return\n        indices: Residue indices of interest\n        start: The Residue number to start at. Inclusive\n        end: The Residue number to end at. Inclusive\n        dtype: The type of coordinates to get\n\n    Returns:\n        The specific coordinates from the Residue instances with the specified dtype\n    \"\"\"\n    if indices is None:\n        if residue_numbers is None:\n            if start is not None and end is not None:\n                residue_numbers = list(range(start, end + 1))\n            else:\n                raise ValueError(\n                    f\"{self.get_coords_subset.__name__}: Must provide either 'indices', 'residue_numbers' or \"\n                    f\"'start' and 'end'\")\n        residues = self.get_residues(residue_numbers)\n    else:\n        residues = self.get_residues(indices=indices)\n\n    coords_type = 'coords' if dtype == 'all' else f'{dtype}_coords'\n    return np.concatenate([getattr(residue, coords_type) for residue in residues])\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.set_residues_attributes","title":"set_residues_attributes","text":"<pre><code>set_residues_attributes(**kwargs)\n</code></pre> <p>Set attributes specified by key, value pairs for Residues in the Structure</p> <p>Other Parameters:</p> <ul> <li> <code>numbers</code>         \u2013          <p>Container[int] = None - The Atom numbers of interest</p> </li> <li> <code>pdb</code>         \u2013          <p>bool = False - Whether to search for numbers as they were parsed (if True)</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_residues_attributes(self, **kwargs):\n    \"\"\"Set attributes specified by key, value pairs for Residues in the Structure\n\n    Keyword Args:\n        numbers: Container[int] = None - The Atom numbers of interest\n        pdb: bool = False - Whether to search for numbers as they were parsed (if True)\n    \"\"\"\n    for residue in self.get_residues(**kwargs):\n        for kwarg, value in kwargs.items():\n            setattr(residue, kwarg, value)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_residue_atom_indices","title":"get_residue_atom_indices","text":"<pre><code>get_residue_atom_indices(**kwargs) -&gt; list[int]\n</code></pre> <p>Returns Atom indices for Residue instances. Returns all by default unless numbers are specified, returns only those Residue instance selected by number.</p> <p>Other Parameters:</p> <ul> <li> <code>numbers</code>         \u2013          <p>Container[int] = None - The Residue numbers of interest</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_residue_atom_indices(self, **kwargs) -&gt; list[int]:\n    \"\"\"Returns Atom indices for Residue instances. Returns all by default unless numbers are specified, returns only\n    those Residue instance selected by number.\n\n    Keyword Args:\n        numbers: Container[int] = None - The Residue numbers of interest\n    \"\"\"\n    atom_indices = []\n    for residue in self.get_residues(**kwargs):\n        atom_indices.extend(residue.atom_indices)\n    return atom_indices\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_residues_by_atom_indices","title":"get_residues_by_atom_indices","text":"<pre><code>get_residues_by_atom_indices(atom_indices: ArrayIndexer) -&gt; list[Residue]\n</code></pre> <p>Retrieve Residues in the Structure specified by Atom indices</p> <p>Parameters:</p> <ul> <li> <code>atom_indices</code>             (<code>ArrayIndexer</code>)         \u2013          <p>The atom indices to retrieve Residue objects by</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The sorted, unique Residue instances corresponding to the provided atom_indices</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_residues_by_atom_indices(self, atom_indices: ArrayIndexer) -&gt; list[Residue]:\n    \"\"\"Retrieve Residues in the Structure specified by Atom indices\n\n    Args:\n        atom_indices: The atom indices to retrieve Residue objects by\n\n    Returns:\n        The sorted, unique Residue instances corresponding to the provided atom_indices\n    \"\"\"\n    if self.is_parent():\n        _struct = self\n    else:\n        _struct = self.parent\n\n    all_residues = _struct._coords_indexed_residues[atom_indices].tolist()\n    return sorted(set(all_residues), key=lambda residue: residue.index)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.renumber","title":"renumber","text":"<pre><code>renumber()\n</code></pre> <p>Change the Atom and Residue numbering. Access the readtime Residue number in Residue.pdb_number attribute</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def renumber(self):\n    \"\"\"Change the Atom and Residue numbering. Access the readtime Residue number in Residue.pdb_number attribute\"\"\"\n    self.renumber_atoms()\n    self.pose_numbering()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.pose_numbering","title":"pose_numbering","text":"<pre><code>pose_numbering()\n</code></pre> <p>Change the Residue numbering to start at 1. Access the readtime Residue number in .pdb_number attribute</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def pose_numbering(self):\n    \"\"\"Change the Residue numbering to start at 1. Access the readtime Residue number in .pdb_number attribute\"\"\"\n    for idx, residue in enumerate(self.residues, 1):\n        residue.number = idx\n    self.log.debug(f'{self.name} was formatted in Pose numbering (residues now 1 to {self.number_of_residues})')\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.renumber_residues","title":"renumber_residues","text":"<pre><code>renumber_residues(index: int = 0, at: int = 1)\n</code></pre> <p>Renumber Residue objects sequentially starting with \"at\"</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>The index to start the renumbering process</p> </li> <li> <code>at</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The number to start renumbering at</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def renumber_residues(self, index: int = 0, at: int = 1):\n    \"\"\"Renumber Residue objects sequentially starting with \"at\"\n\n    Args:\n        index: The index to start the renumbering process\n        at: The number to start renumbering at\n    \"\"\"\n    for idx, residue in enumerate(self.residues[index:], at):\n        residue.number = idx\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_residues","title":"get_residues","text":"<pre><code>get_residues(numbers: Container[int] = None, indices: Iterable[int] = None, **kwargs) -&gt; list[Residue]\n</code></pre> <p>Returns Residue instances as specified. Returns all by default</p> <p>Parameters:</p> <ul> <li> <code>numbers</code>             (<code>Container[int]</code>, default:                 <code>None</code> )         \u2013          <p>Residue numbers of interest</p> </li> <li> <code>indices</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>Residue indices of interest for the Structure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The requested Residue instances, sorted in the order they appear in the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_residues(self, numbers: Container[int] = None, indices: Iterable[int] = None, **kwargs) -&gt; list[Residue]:\n    \"\"\"Returns Residue instances as specified. Returns all by default\n\n    Args:\n        numbers: Residue numbers of interest\n        indices: Residue indices of interest for the Structure\n\n    Returns:\n        The requested Residue instances, sorted in the order they appear in the Structure\n    \"\"\"\n    residues = self.residues\n    if numbers is not None:\n        if isinstance(numbers, Container):\n            residues = [residue for residue in residues if residue.number in numbers]\n        else:\n            self.log.warning(f'The passed numbers type \"{type(numbers).__name__}\" must be a Container. Returning '\n                             f'all Residue instances instead')\n    elif indices is not None:\n        try:\n            residues = [residues[idx] for idx in indices]\n        except IndexError:\n            number_of_residues = self.number_of_residues\n            for idx in indices:\n                if idx &lt; 0 or idx &gt;= number_of_residues:\n                    raise IndexError(\n                        f'The residue index {idx} is out of bounds for the {self.__class__.__name__} {self.name} '\n                        f'with size of {number_of_residues} residues')\n    return residues\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.residue","title":"residue","text":"<pre><code>residue(residue_number: int) -&gt; Residue | None\n</code></pre> <p>Retrieve the specified Residue</p> <p>Parameters:</p> <ul> <li> <code>residue_number</code>             (<code>int</code>)         \u2013          <p>The number of the Residue to search for</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def residue(self, residue_number: int) -&gt; Residue | None:\n    \"\"\"Retrieve the specified Residue\n\n    Args:\n        residue_number: The number of the Residue to search for\n    \"\"\"\n    for residue in self.residues:\n        if residue.number == residue_number:\n            return residue\n    return None\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.add_ideal_helix","title":"add_ideal_helix","text":"<pre><code>add_ideal_helix(termini: termini_literal = 'n', length: int = 5, alignment_length: int = 5)\n</code></pre> <p>Add an ideal helix to a termini given by a certain length</p> <p>Parameters:</p> <ul> <li> <code>termini</code>             (<code>termini_literal</code>, default:                 <code>'n'</code> )         \u2013          <p>The termini to add the ideal helix to</p> </li> <li> <code>length</code>             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>The length of the addition, where viable values are [1-10] residues additions</p> </li> <li> <code>alignment_length</code>             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>The number of residues used to calculation overlap of the target to the ideal helix</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def add_ideal_helix(self, termini: stutils.termini_literal = 'n', length: int = 5, alignment_length: int = 5):\n    \"\"\"Add an ideal helix to a termini given by a certain length\n\n    Args:\n        termini: The termini to add the ideal helix to\n        length: The length of the addition, where viable values are [1-10] residues additions\n        alignment_length: The number of residues used to calculation overlap of the target to the ideal helix\n    \"\"\"\n    maximum_extension_length = 15 - alignment_length\n    if length &gt; maximum_extension_length:\n        number_of_iterations, length = divmod(length, maximum_extension_length)\n        # First add the remainder 'length' with the specified alignment length,\n        self.add_ideal_helix(termini=termini, length=length, alignment_length=alignment_length)\n        # Then perform 10 residue extensions until complete\n        # Using the default 10 and 5 alignment to prevent any errors from alignment of shorter amount from\n        # propagating to the ideal addition and creating ideal \"kinks\"\n        addition_count = count()\n        while next(addition_count) != number_of_iterations:\n            self.add_ideal_helix(termini=termini, length=10)\n\n        return\n    elif length &lt; 1:\n        return\n    else:\n        self.log.debug(f'Adding {length} residue ideal helix to {termini}-terminus of {self.name}')\n\n    alpha_helix_15_struct = ContainsResidues.from_atoms(alpha_helix_15_atoms)\n\n    if termini == 'n':\n        residue = self.n_terminal_residue\n        residue_index = residue.index\n        fixed_coords = self.get_coords_subset(\n            indices=list(range(residue_index, residue_index + alignment_length)), dtype='backbone')\n\n        ideal_end_index = alpha_helix_15_struct.c_terminal_residue.index + 1\n        ideal_start_index = ideal_end_index - alignment_length\n        moving_coords = alpha_helix_15_struct.get_coords_subset(\n            indices=list(range(ideal_start_index, ideal_end_index)), dtype='backbone')\n        rmsd, rot, tx = superposition3d(fixed_coords, moving_coords)\n        alpha_helix_15_struct.transform(rotation=rot, translation=tx)\n\n        # Add residues\n        helix_add_start = ideal_start_index - length\n        # Exclude ideal_start_index from residue selection\n        add_residues = alpha_helix_15_struct.get_residues(indices=list(range(helix_add_start, ideal_start_index)))\n    elif termini == 'c':\n        residue = self.c_terminal_residue\n        residue_index = residue.index + 1\n        fixed_coords = self.get_coords_subset(\n            indices=list(range(residue_index - alignment_length, residue_index)), dtype='backbone')\n\n        ideal_start_index = alpha_helix_15_struct.n_terminal_residue.index\n        ideal_end_index = ideal_start_index + alignment_length\n        moving_coords = alpha_helix_15_struct.get_coords_subset(\n            indices=list(range(ideal_start_index, ideal_end_index)), dtype='backbone')\n        rmsd, rot, tx = superposition3d(fixed_coords, moving_coords)\n        alpha_helix_15_struct.transform(rotation=rot, translation=tx)\n\n        # Add residues\n        helix_add_end = ideal_end_index + length\n        # Leave out the residue with ideal_end_index, and include the helix_add_end number\n        add_residues = alpha_helix_15_struct.get_residues(indices=list(range(ideal_end_index, helix_add_end)))\n    else:\n        raise ValueError(\n            f\"'termini' must be either 'n' or 'c', not {termini}\")\n\n    self.insert_residues(residue_index, add_residues, chain_id=residue.chain_id)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_residue_atoms","title":"get_residue_atoms","text":"<pre><code>get_residue_atoms(**kwargs) -&gt; list[Atom]\n</code></pre> <p>Return the Atoms contained in the Residue objects matching a set of residue numbers</p> <p>Other Parameters:</p> <ul> <li> <code>numbers</code>         \u2013          <p>Container[int] = None \u2013 Residue numbers of interest</p> </li> <li> <code>indices</code>         \u2013          <p>Iterable[int] = None \u2013 Residue indices of interest for the Structure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Atom]</code>         \u2013          <p>The Atom instances belonging to the Residue instances</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_residue_atoms(self, **kwargs) -&gt; list[Atom]:\n    \"\"\"Return the Atoms contained in the Residue objects matching a set of residue numbers\n\n    Keyword Args:\n        numbers: Container[int] = None \u2013 Residue numbers of interest\n        indices: Iterable[int] = None \u2013 Residue indices of interest for the Structure\n\n    Returns:\n        The Atom instances belonging to the Residue instances\n    \"\"\"\n    atoms = []\n    for residue in self.get_residues(**kwargs):\n        atoms.extend(residue.atoms)\n    return atoms\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.mutate_residue","title":"mutate_residue","text":"<pre><code>mutate_residue(residue: Residue = None, index: int = None, number: int = None, to: str = 'A', **kwargs) -&gt; list[int] | list\n</code></pre> <p>Mutate a specific Residue to a new residue type. Type can be 1 or 3 letter format</p> <p>Parameters:</p> <ul> <li> <code>residue</code>             (<code>Residue</code>, default:                 <code>None</code> )         \u2013          <p>A Residue instance to mutate</p> </li> <li> <code>index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>A Residue index to select the Residue instance of interest</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>A Residue number to select the Residue instance of interest</p> </li> <li> <code>to</code>             (<code>str</code>, default:                 <code>'A'</code> )         \u2013          <p>The type of amino acid to mutate to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int] | list</code>         \u2013          <p>The indices of the Atoms being removed from the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def mutate_residue(self, residue: Residue = None, index: int = None, number: int = None, to: str = 'A', **kwargs) \\\n        -&gt; list[int] | list:\n    \"\"\"Mutate a specific Residue to a new residue type. Type can be 1 or 3 letter format\n\n    Args:\n        residue: A Residue instance to mutate\n        index: A Residue index to select the Residue instance of interest\n        number: A Residue number to select the Residue instance of interest\n        to: The type of amino acid to mutate to\n\n    Returns:\n        The indices of the Atoms being removed from the Structure\n    \"\"\"\n    if index is not None:\n        try:\n            residue = self.residues[index]\n        except IndexError:\n            raise IndexError(\n                f'The residue index {index} is out of bounds for the {self.__class__.__name__} '\n                f'{self.name} with size of {self.number_of_residues} residues')\n    elif number is not None:\n        residue = self.residue(number)\n\n    if residue is None:\n        raise ValueError(\n            f\"Can't {self.mutate_residue.__name__} without Residue instance, index, or number\")\n    elif self.is_dependent():\n        _parent = self.parent\n        self.log.debug(f\"{self.mutate_residue.__name__} can't be performed on a dependent StructureBase. \"\n                       f\"Calling on the {self.__class__.__name__}.parent {repr(_parent)}\")\n        # Ensure the deletion is done by the Structure parent to account for everything correctly\n        return _parent.mutate_residue(residue, to=to)\n\n    to = protein_letters_1to3.get(to.upper(), to.upper())\n    if residue.type == to:  # No mutation necessary\n        return []\n    else:\n        try:\n            protein_letters_3to1_extended[to]\n        except KeyError:\n            raise KeyError(\n                f\"The mutation type '{to}' isn't a viable Residue type\")\n    self.log.debug(f'Mutating {residue.type}{residue.number}{to}')\n    residue.type = to\n    for atom in residue.atoms:\n        atom.residue_type = to\n\n    # Find the corresponding Residue Atom indices to delete\n    delete_indices = residue.side_chain_indices\n    if not delete_indices:  # There are no indices\n        return []\n    else:  # Clear all state variables for all Residue instances\n        self._residues.reset_state()\n        # residue.side_chain_indices = []\n\n    # Remove indices from the Residue, and Structure atom_indices\n    residue_atom_indices = residue.atom_indices\n    residue_atom_delete_index = residue_atom_indices.index(delete_indices[0])\n    _atom_indices = self._atom_indices\n    structure_atom_delete_index = _atom_indices.index(delete_indices[0])\n    for _ in iter(delete_indices):\n        residue_atom_indices.pop(residue_atom_delete_index)\n        _atom_indices.pop(structure_atom_delete_index)\n\n    self._offset_indices(start_at=structure_atom_delete_index, offset=-len(delete_indices), dtype='atom')\n\n    # Re-index all succeeding Atom and Residue instance indices\n    self._coords.delete(delete_indices)\n    self._atoms.delete(delete_indices)\n    # self._atoms.reindex(start_at=structure_atom_delete_index)\n    self._residues.reindex_atoms(start_at=residue.index)\n\n    # Reissue the atom assignments for the Residue\n    residue.delegate_atoms()\n    self.reset_state()\n\n    return delete_indices\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.delete_residues","title":"delete_residues","text":"<pre><code>delete_residues(residues: Iterable[Residue] = None, indices: Iterable[int] = None, numbers: Container[int] = None, **kwargs) -&gt; list[Residue] | list\n</code></pre> <p>Deletes Residue instances from the Structure</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>Iterable[Residue]</code>, default:                 <code>None</code> )         \u2013          <p>Residue instances to delete</p> </li> <li> <code>indices</code>             (<code>Iterable[int]</code>, default:                 <code>None</code> )         \u2013          <p>Residue indices to select the Residue instances of interest</p> </li> <li> <code>numbers</code>             (<code>Container[int]</code>, default:                 <code>None</code> )         \u2013          <p>Residue numbers to select the Residue instances of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue] | list</code>         \u2013          <p>Each deleted Residue</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def delete_residues(self, residues: Iterable[Residue] = None, indices: Iterable[int] = None,\n                    numbers: Container[int] = None, **kwargs) -&gt; list[Residue] | list:\n    \"\"\"Deletes Residue instances from the Structure\n\n    Args:\n        residues: Residue instances to delete\n        indices: Residue indices to select the Residue instances of interest\n        numbers: Residue numbers to select the Residue instances of interest\n\n    Returns:\n        Each deleted Residue\n    \"\"\"\n    if indices is not None:\n        residues = self.get_residues(indices=indices)\n    elif numbers is not None:\n        residues = self.get_residues(numbers=numbers)\n\n    if residues is None:\n        raise ValueError(\n            f\"Can't {self.delete_residues.__name__} without Residue instances. Provide with indices, numbers, or \"\n            \"residues\")\n    elif not residues:\n        self.log.debug(f'{self.delete_residues.__name__}: No residues found')\n        return []\n    elif self.is_dependent():  # Call on the parent\n        _parent = self.parent\n        self.log.debug(f\"{self.delete_residues.__name__} can't be performed on a dependent StructureBase. \"\n                       f\"Calling on the {self.__class__.__name__}.parent {repr(_parent)}\")\n        # Ensure the deletion is done by the Structure parent to account for everything correctly\n        return _parent.delete_residues(residues=residues)\n\n    # Find the Residue, Atom indices to delete\n    atom_indices = []\n    for residue in residues:\n        self.log.debug(f'Deleting {residue.type}{residue.number}')\n        atom_indices.extend(residue.atom_indices)\n\n    if not atom_indices:\n        return []  # There are no indices for the Residue instances\n    else:  # Find the Residue indices to delete\n        residue_indices = []\n        for residue in residues:\n            residue_indices.append(residue.index)\n\n    # Remove indices from the Residue, and Structure atom_indices\n    self._delete_indices(atom_indices, dtype='atom')\n    self._delete_indices(residue_indices, dtype='residue')\n    # Re-index all succeeding Atom and Residue instance indices\n    self._coords.delete(atom_indices)\n    self._atoms.delete(atom_indices)\n    self._residues.delete(residue_indices)\n    self._residues.reindex(start_at=residue_indices[0])\n\n    # Clear state variables for remaining Residue instances. Residue deletion affected remaining attrs and indices\n    self._residues.reset_state()\n    # Reindex the coords/residues map\n    self.reset_state()\n\n    return residues\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.insert_residue_type","title":"insert_residue_type","text":"<pre><code>insert_residue_type(index: int, residue_type: str, chain_id: str = None) -&gt; Residue\n</code></pre> <p>Insert a standard Residue type into the Structure at the origin. No structural alignment is performed!</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>)         \u2013          <p>The residue index where a new Residue should be inserted into the Structure</p> </li> <li> <code>residue_type</code>             (<code>str</code>)         \u2013          <p>Either the 1 or 3 letter amino acid code for the residue in question</p> </li> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain identifier to associate the new Residue with</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Residue</code>         \u2013          <p>The newly inserted Residue object</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def insert_residue_type(self, index: int, residue_type: str, chain_id: str = None) -&gt; Residue:\n    \"\"\"Insert a standard Residue type into the Structure at the origin. No structural alignment is performed!\n\n    Args:\n        index: The residue index where a new Residue should be inserted into the Structure\n        residue_type: Either the 1 or 3 letter amino acid code for the residue in question\n        chain_id: The chain identifier to associate the new Residue with\n\n    Returns:\n        The newly inserted Residue object\n    \"\"\"\n    if self.is_dependent():  # Call on the parent\n        _parent = self.parent\n        self.log.debug(f\"{self.insert_residue_type.__name__} can't be performed on a dependent StructureBase. \"\n                       f\"Calling on the {self.__class__.__name__}.parent {repr(_parent)}\")\n        # Ensure the deletion is done by the Structure parent to account for everything correctly\n        if chain_id is None:\n            chain_id = getattr(self, 'chain_id', None)\n        return _parent.insert_residue_type(index, residue_type, chain_id=chain_id)\n\n    self.log.debug(f'Inserting {residue_type} into index {index} of {self.name}')\n\n    # Convert incoming amino acid to an index to select the stutils.reference_residue.\n    # protein_letters_alph1 has a matching index\n    reference_index = \\\n        protein_letters_alph1.find(protein_letters_3to1_extended.get(residue_type, residue_type.upper()))\n    if reference_index == -1:\n        raise IndexError(\n            f\"{self.insert_residue_type.__name__} of residue_type '{residue_type}' isn't allowed\")\n    if index &lt; 0:\n        raise IndexError(\n            f\"{self.insert_residue_type.__name__} at index {index} &lt; 0 isn't allowed\")\n\n    # Grab the reference atom coordinates and push into the atom list\n    new_residue = reference_residues[reference_index].copy()\n\n    # Find the prior and next Residue, atom_start_index (starting atom in new Residue index)\n    residues = self.residues\n    if index == 0:  # n-termini = True\n        prev_residue = None\n        atom_start_index = 0\n    else:\n        prev_residue = residues[index - 1]\n        atom_start_index = prev_residue.end_index + 1\n\n    try:\n        next_residue = residues[index]\n    except IndexError:  # c_termini = True\n        next_residue = None\n        if not prev_residue:  # Insertion on an empty Structure? block for now to simplify chain identification\n            raise stutils.DesignError(\n                f\"Can't {self.insert_residue_type.__name__} for an empty {self.__class__.__name__} class\")\n\n    # Set found attributes\n    new_residue._insert = True\n\n    # Insert the new_residue coords, atoms, and residues into the Structure\n    self._coords.insert(atom_start_index, new_residue.coords)\n    self._atoms.insert(atom_start_index, new_residue.atoms)\n    self._residues.insert(index, [new_residue])\n    # After coords, atoms, residues insertion into \"_\" containers, set parent to self\n    new_residue._parent = self\n\n    # Reformat indices\n    # new_residue.start_index = atom_start_index\n    # self._atoms.reindex(start_at=atom_start_index)  # Called in self._residues.reindex\n    self._residues.reindex(start_at=index)  # .set_index()\n    # Insert new_residue index and atom_indices into Structure indices\n    self._insert_indices(index, [index], dtype='residue')\n    self._insert_indices(atom_start_index, new_residue.atom_indices, dtype='atom')\n\n    # Set the new chain_id. Must occur after self._residue_indices update if chain isn't provided\n    if chain_id is None:  # Try to solve without it...\n        if prev_residue and next_residue:\n            if prev_residue.chain_id == next_residue.chain_id:\n                chain_id = prev_residue.chain_id\n            else:  # There is a discrepancy which means this is an internal termini\n                raise stutils.DesignError(chain_assignment_error)\n        # This can be solved as it represents an absolute termini case\n        elif prev_residue:\n            chain_id = prev_residue.chain_id\n        else:\n            chain_id = next_residue.chain_id\n    new_residue.chain_id = chain_id\n\n    # Solve the Residue number and renumber Structure if there is overlap\n    if prev_residue and next_residue:\n        tentative_number = prev_residue.number + 1\n        if tentative_number == next_residue.number:\n            # There is a conflicting insertion\n            # The prev_residue could also be inserted and needed to be numbered lower\n            try:  # Check residue._insert\n                prev_residue._insert\n            except AttributeError:  # Not inserted. Renumber all subsequent\n                # self.renumber_residues()\n                residues_renumber = residues[index:]\n                for residue in residues_renumber:\n                    residue.number = residue.number + 1\n            else:\n                for residue in prev_residue.get_upstream() + [prev_residue]:\n                    residue.number = residue.number - 1\n\n        new_residue.number = tentative_number\n    elif prev_residue:\n        new_residue.number = prev_residue.number + 1\n    else:  # next_residue\n        # This cautionary note may not apply anymore\n        #  Subtracting one may not be enough if this insert_residue_type() is part of a set of inserts and all\n        #  n-terminal insertions are being conducted before this next_residue. Clean this in the first check of\n        #  this logic block\n        new_residue.number = next_residue.number - 1\n\n    try:\n        secondary_structure = self._secondary_structure\n    except AttributeError:  # When not set yet\n        self.calculate_secondary_structure()  # The new_residue will be included\n    else:  # Insert the new ss with a coiled assumption\n        # ASSUME the insertion is disordered and coiled segment\n        new_residue.secondary_structure = DEFAULT_SS_COIL_IDENTIFIER\n        self._secondary_structure = \\\n            secondary_structure[:index] + DEFAULT_SS_COIL_IDENTIFIER + secondary_structure[index:]\n\n    # Reindex the coords/residues map\n    self.reset_state()\n\n    return new_residue\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.insert_residues","title":"insert_residues","text":"<pre><code>insert_residues(index: int, new_residues: Iterable[Residue], chain_id: str = None) -&gt; list[Residue]\n</code></pre> <p>Insert Residue instances into the Structure at the origin. No structural alignment is performed!</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>)         \u2013          <p>The index to perform the insertion at</p> </li> <li> <code>new_residues</code>             (<code>Iterable[Residue]</code>)         \u2013          <p>The Residue instances to insert</p> </li> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain identifier to associate the new Residue instances with</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The inserted Residue instances</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def insert_residues(self, index: int, new_residues: Iterable[Residue], chain_id: str = None) -&gt; list[Residue]:\n    \"\"\"Insert Residue instances into the Structure at the origin. No structural alignment is performed!\n\n    Args:\n        index: The index to perform the insertion at\n        new_residues: The Residue instances to insert\n        chain_id: The chain identifier to associate the new Residue instances with\n\n    Returns:\n        The inserted Residue instances\n    \"\"\"\n    if not new_residues:\n        return []\n\n    if self.is_dependent():  # Call on the parent\n        _parent = self.parent\n        self.log.debug(f\"{self.insert_residues.__name__} can't be performed on a dependent StructureBase. \"\n                       f\"Calling on the {self.__class__.__name__}.parent {repr(_parent)}\")\n        # Ensure the deletion is done by the Structure parent to account for everything correctly\n        if chain_id is None:\n            chain_id = getattr(self, 'chain_id', None)\n        return _parent.insert_residues(index, new_residues, chain_id=chain_id)\n\n    # Make a copy of the Residue instances\n    new_residues = [residue.copy() for residue in new_residues]\n    number_new_residues = len(new_residues)\n\n    # Find the prior and next Residue, atom_start_index (starting atom in new Residue index)\n    residues = self.residues\n    if index == 0:  # n-termini = True\n        prev_residue = _prev_residue = None\n        atom_start_index = 0\n    else:\n        prev_residue = _prev_residue = residues[index - 1]\n        atom_start_index = prev_residue.end_index + 1\n\n    try:\n        next_residue = residues[index]\n    except IndexError:  # c_termini = True\n        next_residue = None\n        if not prev_residue:  # Insertion on an empty Structure? block for now to simplify chain identification\n            raise stutils.DesignError(\n                f\"Can't {self.insert_residue_type.__name__} for an empty {self.__class__.__name__} class\")\n\n    # Set found attributes\n    # prev_residue, *other_residues = new_residues\n    for residue in new_residues:\n        residue._insert = True\n\n    # Insert the new_residue coords, atoms, and residues into the Structure\n    self._coords.insert(atom_start_index, np.concatenate([residue.coords for residue in new_residues]))\n    self._atoms.insert(atom_start_index, [atom for residue in new_residues for atom in residue.atoms])\n    self._residues.insert(index, new_residues)\n    # After coords, atoms, residues insertion into \"_\" containers, set parent to self\n    for residue in new_residues:\n        residue._parent = self\n\n    # Reformat indices\n    # new_residue.start_index = atom_start_index\n    # self._atoms.reindex(start_at=atom_start_index)  # Called in self._residues.reindex\n    self._residues.reindex(start_at=index)  # .set_index()\n    # Insert new_residue index and atom_indices into Structure indices\n    new_residue_atom_indices = list(range(atom_start_index, new_residues[-1].end_index + 1))\n    self._insert_indices(atom_start_index, new_residue_atom_indices, dtype='atom')\n    new_residue_indices = list(range(index, index + number_new_residues))\n    self._insert_indices(index, new_residue_indices, dtype='residue')\n\n    # Set the new chain_id. Must occur after self._residue_indices update if chain isn't provided\n    if chain_id is None:  # Try to solve without it...\n        if prev_residue and next_residue:\n            if prev_residue.chain_id == next_residue.chain_id:\n                chain_id = prev_residue.chain_id\n            else:  # There is a discrepancy which means this is an internal termini\n                raise stutils.DesignError(chain_assignment_error)\n        # This can be solved as it represents an absolute termini case\n        elif prev_residue:\n            chain_id = prev_residue.chain_id\n        else:\n            chain_id = next_residue.chain_id\n    for residue in new_residues:\n        residue.chain_id = chain_id\n\n    # Solve the Residue number and renumber Structure if there is overlap\n    if prev_residue and next_residue:\n        first_number = prev_residue.number + 1\n        tentative_residue_numbers = list(range(first_number, first_number + number_new_residues))\n        if next_residue.number in tentative_residue_numbers:\n            # There is a conflicting insertion. Correct existing residue numbers\n            # The prev_residue could also be inserted and needed to be numbered lower\n            try:  # Check residue._insert\n                prev_residue._insert\n            except AttributeError:  # Not inserted. Renumber all subsequent\n                # self.renumber_residues()\n                residues_renumber = residues[index:]\n                for residue in residues_renumber:\n                    residue.number = residue.number + number_new_residues\n            else:\n                for residue in prev_residue.get_upstream() + [prev_residue]:\n                    residue.number = residue.number - number_new_residues\n        # Set the new numbers\n        for residue, new_number in zip(new_residues, tentative_residue_numbers):\n            residue.number = new_number\n    elif prev_residue:\n        _prev_residue = prev_residue\n        for residue in new_residues:\n            residue.number = _prev_residue.number + 1\n            _prev_residue = residue\n    else:  # next_residue\n        # This cautionary note may not apply to insert_residues(\n        #  Subtracting one may not be enough if insert_residues() is part of a set of inserts and all\n        #  n-terminal insertions are being conducted before this next_residue. Clean this in the first check of\n        #  this logic block\n        _prev_residue = next_residue\n        for residue in reversed(new_residues):\n            residue.number = _prev_residue.number - 1\n            _prev_residue = residue\n\n    # Reindex the coords/residues map\n    self.reset_state()\n\n    return new_residues\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.delete_termini","title":"delete_termini","text":"<pre><code>delete_termini(how: str = 'unstructured', termini: termini_literal = None)\n</code></pre> <p>Remove Residue instances from the Structure termini that are not helices</p> <p>Uses the default secondary structure prediction program's SS_HELIX_IDENTIFIERS (typically 'H') to search for non-conforming secondary structure</p> <p>Parameters:</p> <ul> <li> <code>how</code>             (<code>str</code>, default:                 <code>'unstructured'</code> )         \u2013          <p>How should termini be trimmed? Either 'unstructured' (default) or 'to_helices' can be used. If 'unstructured',     will use 'SS_DISORDER_IDENTIFIERS' (typically coil) to detect disorder. Function will then remove any     disordered segments, as well as turns ('T') that exist between disordered segments If 'to_helices',     will use 'SS_HELIX_IDENTIFIERS' (typically 'H') to remove any non-conforming secondary structure     elements until a helix is reached</p> </li> <li> <code>termini</code>             (<code>termini_literal</code>, default:                 <code>None</code> )         \u2013          <p>If a specific termini should be targeted, which one?</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def delete_termini(self, how: str = 'unstructured', termini: stutils.termini_literal = None):\n    \"\"\"Remove Residue instances from the Structure termini that are not helices\n\n    Uses the default secondary structure prediction program's SS_HELIX_IDENTIFIERS (typically 'H') to search for\n    non-conforming secondary structure\n\n    Args:\n        how: How should termini be trimmed? Either 'unstructured' (default) or 'to_helices' can be used.\n            If 'unstructured',\n                will use 'SS_DISORDER_IDENTIFIERS' (typically coil) to detect disorder. Function will then remove any\n                disordered segments, as well as turns ('T') that exist between disordered segments\n            If 'to_helices',\n                will use 'SS_HELIX_IDENTIFIERS' (typically 'H') to remove any non-conforming secondary structure\n                elements until a helix is reached\n        termini: If a specific termini should be targeted, which one?\n    \"\"\"\n    if termini is None or termini.lower() == 'nc':\n        termini_ = 'nc'\n    elif termini in 'NnCc':  # Only one of the two\n        termini_ = termini.lower()\n    else:\n        raise ValueError(\n            f\"'termini' must be one of 'n' or 'c', not '{termini}\")\n\n    secondary_structure = working_secondary_structure = self.secondary_structure\n    number_of_residues = self.number_of_residues\n    # no_nterm_disorder_ss = secondary_structure.lstrip(SS_DISORDER_IDENTIFIERS)\n    # n_removed_nterm_res = number_of_residues - len(no_nterm_disorder_ss)\n    # no_cterm_disorder_ss = secondary_structure.rstrip(SS_DISORDER_IDENTIFIERS)\n    # n_removed_cterm_res = number_of_residues - len(no_cterm_disorder_ss)\n    # sasa = self.relative_sasa\n    # self.log.debug(f'Found n-term relative sasa {sasa[:n_removed_nterm_res + 10]}')\n    # self.log.debug(f'Found c-term relative sasa {sasa[-(n_removed_cterm_res + 10):]}')\n\n    n_removed_nterm_res = 0\n    # Remove coils. Find the next coil. If only ss present is (T)urn, then remove that as well and start again\n    for idx, termini in enumerate('NC'):\n        if idx == 0:  # n-termini\n            self.log.debug(f'Starting N-term is: {working_secondary_structure[:15]}')\n            possible_secondary_structure = working_secondary_structure\n        else:  # c-termini\n            self.log.debug(f'N-term is: {working_secondary_structure[:15]}')\n            # Get the number of n-termini removed\n            n_removed_nterm_res = number_of_residues - len(working_secondary_structure)\n            # Reverse the sequence to get the c-termini first\n            possible_secondary_structure = working_secondary_structure[::-1]\n            self.log.debug(f'Starting C-term (reversed) is: {possible_secondary_structure[:15]}')\n\n        if how == 'to_helices':\n            ss_helix_index = possible_secondary_structure.find(SS_HELIX_IDENTIFIERS)\n            working_secondary_structure = working_secondary_structure[ss_helix_index:]\n        else:  # how == 'unstructured'\n            ss_disorder_index = possible_secondary_structure.find(SS_DISORDER_IDENTIFIERS)\n            while ss_disorder_index == 0:  # Go again\n                # Remove DISORDER ss\n                working_secondary_structure = possible_secondary_structure.lstrip(SS_DISORDER_IDENTIFIERS)\n                # Next try to remove TURN ss. Only remove if it is between DISORDER segments\n                possible_secondary_structure = working_secondary_structure.lstrip(SS_TURN_IDENTIFIERS)\n                ss_disorder_index = possible_secondary_structure.find(SS_DISORDER_IDENTIFIERS)\n\n    self.log.debug(f'C-term (reversed) is: {working_secondary_structure[:15]}')\n\n    residues = self.residues\n    _delete_residues = []\n    context_length = 10\n    if 'n' in termini_ and n_removed_nterm_res:\n        old_ss = secondary_structure[:n_removed_nterm_res + context_length]\n        self.log.debug(f'Found N-term secondary_structure {secondary_structure[:n_removed_nterm_res + 5]}')\n        self.log.info(f\"Removing {n_removed_nterm_res} N-terminal residues. Resulting secondary structure:\\n\"\n                      f\"\\told : {old_ss}...\\n\"\n                      f\"\\tnew : {'-' * n_removed_nterm_res}{old_ss[n_removed_nterm_res:]}...\")\n        _delete_residues += residues[:n_removed_nterm_res]\n\n    # Get the number of c-termini removed\n    n_removed_cterm_res = number_of_residues - len(working_secondary_structure) - n_removed_nterm_res\n    if 'c' in termini_ and n_removed_cterm_res:\n        c_term_index = number_of_residues - n_removed_cterm_res\n        old_ss = secondary_structure[c_term_index - context_length:]\n        self.log.debug(f'Found C-term secondary_structure {secondary_structure[-(n_removed_cterm_res + 5):]}')\n        self.log.info(f\"Removing {n_removed_cterm_res} C-terminal residues. Resulting secondary structure:\\n\"\n                      f\"\\told :...{old_ss}\\n\"\n                      f\"\\tnew :...{old_ss[:-n_removed_cterm_res]}{'-' * n_removed_cterm_res}\")\n        _delete_residues += residues[c_term_index:]\n\n    self.delete_residues(_delete_residues)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.local_density","title":"local_density","text":"<pre><code>local_density(residues: list[Residue] = None, residue_numbers: list[int] = None, distance: float = 12.0) -&gt; list[float]\n</code></pre> <p>Return the number of Atoms within 'distance' Angstroms of each Atom in the requested Residues</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>list[Residue]</code>, default:                 <code>None</code> )         \u2013          <p>The Residues to include in the calculation</p> </li> <li> <code>residue_numbers</code>             (<code>list[int]</code>, default:                 <code>None</code> )         \u2013          <p>The numbers of the Residues to include in the calculation</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>12.0</code> )         \u2013          <p>The cutoff distance with which Atoms should be included in local density</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>An array like containing the local density around each requested Residue</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def local_density(self, residues: list[Residue] = None, residue_numbers: list[int] = None, distance: float = 12.) \\\n        -&gt; list[float]:\n    \"\"\"Return the number of Atoms within 'distance' Angstroms of each Atom in the requested Residues\n\n    Args:\n        residues: The Residues to include in the calculation\n        residue_numbers: The numbers of the Residues to include in the calculation\n        distance: The cutoff distance with which Atoms should be included in local density\n\n    Returns:\n        An array like containing the local density around each requested Residue\n    \"\"\"\n    if residues:\n        coords = []\n        for residue in residues:\n            coords.extend(residue.heavy_coords)\n        coords_indexed_residues = [residue for residue in residues for _ in residue.heavy_indices]\n    elif residue_numbers:\n        coords = []\n        residues = self.get_residues(numbers=residue_numbers)\n        for residue in residues:\n            coords.extend(residue.heavy_coords)\n        coords_indexed_residues = [residue for residue in residues for _ in residue.heavy_indices]\n    else:  # use all Residue instances\n        residues = self.residues\n        coords = self.heavy_coords\n        coords_indexed_residues = self.heavy_coords_indexed_residues\n\n    # in case this was already called, we should set all to 0.\n    if residues[0].local_density &gt; 0:\n        for residue in residues:\n            residue.local_density = 0.\n\n    all_atom_tree = BallTree(coords)\n    all_atom_counts_query = all_atom_tree.query_radius(coords, distance, count_only=True)\n    # residue_neighbor_counts, current_residue = 0, coords_indexed_residues[0]\n    current_residue = coords_indexed_residues[0]\n    for residue, atom_neighbor_counts in zip(coords_indexed_residues, all_atom_counts_query.tolist()):\n        if residue == current_residue:\n            current_residue.local_density += atom_neighbor_counts\n        else:  # We have a new residue, find the average\n            current_residue.local_density /= current_residue.number_of_heavy_atoms\n            current_residue = residue\n            current_residue.local_density += atom_neighbor_counts\n    # Ensure the last residue is calculated\n    current_residue.local_density /= current_residue.number_of_heavy_atoms  # Find the average\n\n    return [residue.local_density for residue in self.residues]\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.is_clash","title":"is_clash","text":"<pre><code>is_clash(measure: coords_type_literal = stutils.default_clash_criteria, distance: float = stutils.default_clash_distance, warn: bool = False, silence_exceptions: bool = False, report_hydrogen: bool = False) -&gt; bool\n</code></pre> <p>Check if the Structure contains any self clashes. If clashes occur with the Backbone, return True. Reports the Residue where the clash occurred and the clashing Atoms</p> <p>Parameters:</p> <ul> <li> <code>measure</code>             (<code>coords_type_literal</code>, default:                 <code>default_clash_criteria</code> )         \u2013          <p>The atom type to measure clashing by</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>default_clash_distance</code> )         \u2013          <p>The distance which clashes should be checked</p> </li> <li> <code>warn</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to emit warnings about identified clashes. Output grouped into measure vs non-measure</p> </li> <li> <code>silence_exceptions</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to silence the raised ClashError and Return True instead</p> </li> <li> <code>report_hydrogen</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to report clashing hydrogen atoms</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if the Structure clashes, False if not</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_clash(self, measure: stutils.coords_type_literal = stutils.default_clash_criteria,\n             distance: float = stutils.default_clash_distance,\n             warn: bool = False, silence_exceptions: bool = False,\n             report_hydrogen: bool = False) -&gt; bool:\n    \"\"\"Check if the Structure contains any self clashes. If clashes occur with the Backbone, return True. Reports\n    the Residue where the clash occurred and the clashing Atoms\n\n    Args:\n        measure: The atom type to measure clashing by\n        distance: The distance which clashes should be checked\n        warn: Whether to emit warnings about identified clashes. Output grouped into measure vs non-measure\n        silence_exceptions: Whether to silence the raised ClashError and Return True instead\n        report_hydrogen: Whether to report clashing hydrogen atoms\n\n    Raises:\n        ClashError if the Structure has an identified clash\n\n    Returns:\n        True if the Structure clashes, False if not\n    \"\"\"\n    if measure == 'backbone_and_cb':\n        other = 'non-cb sidechain'\n    elif measure == 'heavy':\n        other = 'hydrogen'\n        report_hydrogen = True\n    elif measure == 'backbone':\n        other = 'sidechain'\n    elif measure == 'cb':\n        other = 'non-cb'\n    elif measure == 'ca':\n        other = 'non-ca'\n    else:  # measure == 'all'\n        other = 'solvent'  # this should never appear unless someone added solvent parsing\n\n    coords_type = 'coords' if measure == 'all' else f'{measure}_coords'\n    # cant use heavy_coords as the Residue.atom_indices aren't offset for the BallTree made from them...\n    # another option is to only care about heavy atoms on the residues...\n    # if self.contains_hydrogen():\n    #     atom_tree = BallTree(self.heavy_coords)\n    #     coords_indexed_residues = self.heavy_coords_indexed_residues\n    #     atoms = self.heavy_atoms\n    # else:\n\n    # Set up the query indices. BallTree is faster upon timeit with 131 msec/loop\n    atom_tree = BallTree(self.coords)\n    atoms = self.atoms\n    measured_clashes, other_clashes = [], []\n\n    def return_true(): return True\n\n    clashes = False\n    _any_clashes: Callable[[Iterable[int]], bool]\n    \"\"\"Local helper to separate clash reporting from clash generation\"\"\"\n    clash_msg = f'{self.name} contains Residue {measure} atom clashes at a {distance} A distance'\n    if warn:\n\n        def _any_clashes(_clash_indices: Iterable[int]) -&gt; bool:\n            new_clashes = any(_clash_indices)\n            if new_clashes:\n                for clashing_idx in _clash_indices:\n                    atom = atoms[clashing_idx]\n                    if getattr(atom, f'is_{measure}', return_true)():\n                        measured_clashes.append((residue, atom))\n                    elif report_hydrogen:  # Report all clashes, no need to check\n                        other_clashes.append((residue, atom))\n                    elif atom.is_heavy():  # Check if atom is a heavy atom then report if it is\n                        other_clashes.append((residue, atom))\n\n            return clashes or new_clashes\n\n        # Using _any_clashes to set the global clashes\n        #     clashes = _any_clashes(): ... return clashes\n        # While checking global clashes against new_clashes\n    else:  # Raise a ClashError to defer to caller\n        def _any_clashes(_clash_indices: Iterable[int]) -&gt; bool:\n            for clashing_idx in _clash_indices:\n                if getattr(atoms[clashing_idx], f'is_{measure}', return_true)():\n                    raise stutils.ClashError(clash_msg)\n\n            return clashes\n\n    residues = self.residues\n    # Check first and last residue with different considerations given covalent bonding\n    try:\n        residue, next_residue = residues[:2]\n        # residue, next_residue, *other_residues = self.residues\n    except ValueError:  # Can't unpack\n        # residues &lt; 2. Insufficient to check clashing\n        return False\n\n    # Query each residue with requested coords_type against the atom_tree\n    residue_atom_contacts = atom_tree.query_radius(getattr(residue, coords_type), distance)\n    # residue_atom_contacts returns as ragged nested array, (array of different sized array)\n    # Reduce the dimensions to all contacts\n    all_contacts = {atom_contact for residue_contacts in residue_atom_contacts.tolist()\n                    for atom_contact in residue_contacts.tolist()}\n    try:\n        # Subtract the N and C atoms from the adjacent residues for each residue as these are within a bond\n        clashes = _any_clashes(\n            all_contacts.difference(residue.atom_indices + [next_residue.n_atom_index]))\n        prev_residue = residue\n        residue = next_residue\n\n        # Perform routine for all middle residues\n        for next_residue in residues[2:]:\n            residue_atom_contacts = atom_tree.query_radius(getattr(residue, coords_type), distance)\n            all_contacts = {atom_contact for residue_contacts in residue_atom_contacts.tolist()\n                            for atom_contact in residue_contacts.tolist()}\n            clashes = _any_clashes(\n                all_contacts.difference(\n                    [prev_residue.o_atom_index, prev_residue.c_atom_index, next_residue.n_atom_index]\n                    + residue.atom_indices\n                ))\n            prev_residue = residue\n            residue = next_residue\n\n        residue_atom_contacts = atom_tree.query_radius(getattr(residue, coords_type), distance)\n        all_contacts = {atom_contact for residue_contacts in residue_atom_contacts.tolist()\n                        for atom_contact in residue_contacts.tolist()}\n        clashes = _any_clashes(\n            all_contacts.difference([prev_residue.o_atom_index, prev_residue.c_atom_index]\n                                    + residue.atom_indices)\n        )\n    except stutils.ClashError as error:  # Raised by _any_clashes()\n        if silence_exceptions:\n            return True\n        else:\n            raise error\n    else:\n        if clashes:\n            if measured_clashes:\n                bb_info = '\\n\\t'.join(f'Chain {residue.chain_id} {residue.number:5d}: {atom.get_atom_record()}'\n                                      for residue, atom in measured_clashes)\n                self.log.error(f'{self.name} contains {len(measured_clashes)} {measure} clashes from the following '\n                               f'Residues to the corresponding Atom:\\n\\t{bb_info}')\n                raise stutils.ClashError(clash_msg)\n            if other_clashes:\n                sc_info = '\\n\\t'.join(f'Chain {residue.chain_id} {residue.number:5d}: {atom.get_atom_record()}'\n                                      for residue, atom in other_clashes)\n                self.log.warning(f'{self.name} contains {len(other_clashes)} {other} clashes between the '\n                                 f'following Residues:\\n\\t{sc_info}')\n    return False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_sasa","title":"get_sasa","text":"<pre><code>get_sasa(probe_radius: float = 1.4, atom: bool = True, **kwargs)\n</code></pre> <p>Use FreeSASA to calculate the surface area of residues in the Structure object.</p> <p>Parameters:</p> <ul> <li> <code>probe_radius</code>             (<code>float</code>, default:                 <code>1.4</code> )         \u2013          <p>The radius which surface area should be generated</p> </li> <li> <code>atom</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> </ul> Sets <p>self.sasa, self.residue(s).sasa</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_sasa(self, probe_radius: float = 1.4, atom: bool = True, **kwargs):\n    \"\"\"Use FreeSASA to calculate the surface area of residues in the Structure object.\n\n    Args:\n        probe_radius: The radius which surface area should be generated\n        atom: Whether the output should be generated for each atom. If False, will be generated for each Residue\n\n    Sets:\n        self.sasa, self.residue(s).sasa\n    \"\"\"\n    if atom:\n        out_format = 'pdb'\n    # --format=pdb --depth=atom\n    # REMARK 999 This PDB file was generated by FreeSASA 2.0.\n    # REMARK 999 In the ATOM records temperature factors have been\n    # REMARK 999 replaced by the SASA of the atom, and the occupancy\n    # REMARK 999 by the radius used in the calculation.\n    # MODEL        1                                        [radii][sasa]\n    # ATOM   2557  C   PHE C 113      -2.627 -17.654  13.108  1.61  1.39\n    # ATOM   2558  O   PHE C 113      -2.767 -18.772  13.648  1.42 39.95\n    # ATOM   2559  CB  PHE C 113      -1.255 -16.970  11.143  1.88 13.46\n    # ATOM   2560  CG  PHE C 113      -0.886 -17.270   9.721  1.61  1.98\n    # ATOM   2563 CE1  PHE C 113      -0.041 -18.799   8.042  1.76 28.76\n    # ATOM   2564 CE2  PHE C 113      -0.694 -16.569   7.413  1.76  2.92\n    # ATOM   2565  CZ  PHE C 113      -0.196 -17.820   7.063  1.76  4.24\n    # ATOM   2566 OXT  PHE C 113      -2.515 -16.590  13.750  1.46 15.09\n    # ...\n    # TER    7913      GLU A 264\n    # ENDMDL EOF\n    # if residue:\n    else:\n        out_format = 'seq'\n    # --format=seq\n    # Residues in ...\n    # SEQ A    1 MET :   74.46\n    # SEQ A    2 LYS :   96.30\n    # SEQ A    3 VAL :    0.00\n    # SEQ A    4 VAL :    0.00\n    # SEQ A    5 VAL :    0.00\n    # SEQ A    6 GLN :    0.00\n    # SEQ A    7 ILE :    0.00\n    # SEQ A    8 LYS :    0.87\n    # SEQ A    9 ASP :    1.30\n    # SEQ A   10 PHE :   64.55\n    # ...\n    # \\n EOF\n    if self.contains_hydrogen():\n        include_hydrogen = ['--hydrogen']  # the addition of hydrogen changes results quite a bit\n    else:\n        include_hydrogen = []\n    cmd = [putils.freesasa_exe_path, f'--format={out_format}', '--probe-radius', str(probe_radius),\n           '-c', putils.freesasa_config_path, '--n-threads=2'] + include_hydrogen\n    self.log.debug(f'FreeSASA:\\n{subprocess.list2cmdline(cmd)}')\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate(input=self.get_atom_record().encode('utf-8'))\n    # if err:  # usually results from Hydrogen atoms, silencing\n    #     self.log.warning('\\n%s' % err.decode('utf-8'))\n    sasa_output = out.decode('utf-8').split('\\n')\n    if_idx = 0\n    if atom:\n        # slice removes first REMARK, MODEL and final TER, MODEL regardless of # of chains, TER inclusion\n        # since return_atom_record doesn't have models, these won't be present and no option to freesasa about model\n        # would be provided with above subprocess call\n        atoms = self.atoms\n        for line_split in map(str.split, sasa_output[5:-2]):  # Does slice remove need for if line[0] == 'ATOM'?\n            if line_split[0] == 'ATOM':  # This line appears necessary as MODEL can be added if MODEL is written\n                atoms[if_idx].sasa = float(line_split[-1])\n                if_idx += 1\n    else:\n        seq_slice = slice(3)\n        sasa_slice = slice(16, None)\n        residues = self.residues\n        for idx, line in enumerate(sasa_output[1:-1]):  # Does slice remove the need for if line[:3] == 'SEQ'?\n            if line[seq_slice] == 'SEQ':  # Doesn't seem that this equality is sufficient ^\n                residues[if_idx].sasa = float(line[sasa_slice])\n                if_idx += 1\n    try:\n        self.sasa = sum([residue.sasa for residue in self.residues])\n    except RecursionError:\n        self.log.error('RecursionError measuring SASA')\n        os.makedirs(putils.sasa_debug_dir, exist_ok=True)\n        self.write(out_path=os.path.join(putils.sasa_debug_dir, f'SASA-INPUT-{self.name}.pdb'))\n        with open(os.path.join(putils.sasa_debug_dir, f'SASA-OUTPUT-{self.name}.pdb'), 'w') as f:\n            f.write('%s\\n' % '\\n'.join(sasa_output))\n\n        raise stutils.DesignError(\n            \"Measurement of SASA isn't working, probably due to a missing Atom. Debug files written to \"\n            f'{putils.sasa_debug_dir}')\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_surface_area_residues","title":"get_surface_area_residues","text":"<pre><code>get_surface_area_residues(residues: list[Residue] = None, dtype: sasa_types_literal = 'polar', **kwargs) -&gt; float\n</code></pre> <p>Get the surface area for specified residues</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>list[Residue]</code>, default:                 <code>None</code> )         \u2013          <p>The Residues to sum. If not provided, will be retrieved by <code>.get_residues()</code></p> </li> <li> <code>dtype</code>             (<code>sasa_types_literal</code>, default:                 <code>'polar'</code> )         \u2013          <p>The type of area classification to query.</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> <li> <code>numbers</code>         \u2013          <p>Container[int] = None \u2013 Residue numbers of interest</p> </li> <li> <code>indices</code>         \u2013          <p>Iterable[int] = None \u2013 Residue indices of interest for the Structure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>Angstrom^2 of surface area</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_surface_area_residues(\n    self, residues: list[Residue] = None, dtype: sasa_types_literal = 'polar', **kwargs\n) -&gt; float:\n    \"\"\"Get the surface area for specified residues\n\n    Args:\n        residues: The Residues to sum. If not provided, will be retrieved by `.get_residues()`\n        dtype: The type of area classification to query.\n\n    Keyword Args:\n        atom: bool = True - Whether the output should be generated for each atom.\n            If False, will be generated for each Residue\n        probe_radius: float = 1.4 - The radius which surface area should be generated\n        numbers: Container[int] = None \u2013 Residue numbers of interest\n        indices: Iterable[int] = None \u2013 Residue indices of interest for the Structure\n\n    Returns:\n        Angstrom^2 of surface area\n    \"\"\"\n    if not self.sasa:\n        self.get_sasa(**kwargs)\n\n    if not residues:\n        residues = self.get_residues(**kwargs)\n\n    sasa_dtype = f'sasa_{dtype}'\n    try:\n        return sum([getattr(residue, sasa_dtype) for residue in residues])\n    except AttributeError:\n        raise ValueError(\n            f\" {dtype=} is an invalid 'sasa_dtype'. Viable types are {', '.join(sasa_types)}\"\n        )\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.errat","title":"errat","text":"<pre><code>errat(out_path: AnyStr = os.getcwd()) -&gt; tuple[float, ndarray]\n</code></pre> <p>Find the overall and per residue Errat accuracy for the given Structure</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The path where Errat files should be written</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, ndarray]</code>         \u2013          <p>Overall Errat score, Errat value/residue array</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def errat(self, out_path: AnyStr = os.getcwd()) -&gt; tuple[float, np.ndarray]:\n    \"\"\"Find the overall and per residue Errat accuracy for the given Structure\n\n    Args:\n        out_path: The path where Errat files should be written\n\n    Returns:\n        Overall Errat score, Errat value/residue array\n    \"\"\"\n    # name = 'errat_input-%s-%d.pdb' % (self.name, random() * 100000)\n    # current_struc_file = self.write(out_path=os.path.join(out_path, name))\n    # errat_cmd = [putils.errat_exe_path, os.path.splitext(name)[0], out_path]  # for writing file first\n    # os.system('rm %s' % current_struc_file)\n    out_path = out_path if out_path[-1] == os.sep else out_path + os.sep  # errat needs trailing \"/\"\n    errat_cmd = [putils.errat_exe_path, out_path]  # for passing atoms by stdin\n    # p = subprocess.Popen(errat_cmd, stdin=subprocess.PIPE, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    # out, err = p.communicate(input=self.get_atom_record().encode('utf-8'))\n    # logger.info(self.get_atom_record()[:120])\n    iteration = 1\n    all_residue_scores = []\n    number_of_residues = self.number_of_residues\n    while iteration &lt; 5:\n        p = subprocess.run(errat_cmd, input=self.get_atom_record(), encoding='utf-8', capture_output=True)\n        all_residue_scores = p.stdout.strip().split('\\n')\n        # Subtract one due to the addition of overall score\n        if len(all_residue_scores) - 1 == number_of_residues:\n            break\n        iteration += 1\n\n    if iteration == 5:\n        error = p.stderr.strip().split('\\n')\n        self.log.debug(f\"{self.errat.__name__} couldn't generate the correct output length. \"\n                       f'({len(all_residue_scores) - 1}) != number_of_residues ({number_of_residues}). Got stderr:'\n                       f'\\n{error}')\n    # errat_output_file = os.path.join(out_path, '%s.ps' % name)\n    # errat_output_file = os.path.join(out_path, 'errat.ps')\n    # else:\n    # print(subprocess.list2cmdline(['grep', 'Overall quality factor**: ', errat_output_file]))\n    # p = subprocess.Popen(['grep', 'Overall quality factor', errat_output_file],\n    #                      stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n    # errat_out, errat_err = p.communicate()\n    try:\n        # overall_score = set(errat_out.decode().split('\\n'))\n        # all_residue_scores = list(map(str.strip, errat_out.split('\\n'), 'Residue '))\n        # all_residue_scores = errat_out.split('\\n')\n        overall_score = all_residue_scores.pop(-1)\n        return float(overall_score.split()[-1]), \\\n            np.array([float(score[-1]) for score in map(str.split, all_residue_scores)])\n    except (IndexError, AttributeError, ValueError):  # ValueError when returning text instead of float\n        self.log.warning(f'{self.name}: Failed to generate ERRAT measurement. Errat returned: {all_residue_scores}')\n        return 0., np.array([0. for _ in range(number_of_residues)])\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.stride","title":"stride","text":"<pre><code>stride(to_file: AnyStr = None, **kwargs)\n</code></pre> <p>Calculates the secondary structure using the program Stride</p> <p>Args     to_file: The location of a file to save the Stride output</p> Sets <p>Each contained Residue instance <code>.secondary_structure</code> attribute</p> Source code in <code>symdesign/structure/base.py</code> <pre><code>def stride(self, to_file: AnyStr = None, **kwargs):\n    \"\"\"Calculates the secondary structure using the program Stride\n\n    Args\n        to_file: The location of a file to save the Stride output\n\n    Sets:\n        Each contained Residue instance `.secondary_structure` attribute\n    \"\"\"\n    # REM  -------------------- Secondary structure summary -------------------  XXXX\n    # REM                .         .         .         .         .               XXXX\n    # SEQ  1    IVQQQNNLLRAIEAQQHLLQLTVWGIKQLQAGGWMEWDREINNYTSLIHS   50          XXXX\n    # STR       HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHH  HHHHHHHHHHHHHHHHH               XXXX\n    # REM                                                                        XXXX\n    # SEQ  51   LIEESQN                                              57          XXXX\n    # STR       HHHHHH                                                           XXXX\n    # REM                                                                        XXXX\n    # LOC  AlphaHelix   ILE     3 A      ALA     33 A                            XXXX\n    # LOC  AlphaHelix   TRP    41 A      GLN     63 A                            XXXX\n    # REM                                                                        XXXX\n    # REM  --------------- Detailed secondary structure assignment-------------  XXXX\n    # REM                                                                        XXXX\n    # REM  |---Residue---|    |--Structure--|   |-Phi-|   |-Psi-|  |-Area-|      XXXX\n    # ASG  ILE A    3    1    H    AlphaHelix    360.00    -29.07     180.4      XXXX\n    # ASG  VAL A    4    2    H    AlphaHelix    -64.02    -45.93      99.8      XXXX\n    # ASG  GLN A    5    3    H    AlphaHelix    -61.99    -39.37      82.2      XXXX\n\n    # ASG    Detailed secondary structure assignment\n    # Format:\n    #  5-8  Residue type\n    #  9-10 Protein chain identifier\n    #  11-15 PDB residue number\n    #  16-20 Ordinal residue number\n    #  24-25 One letter secondary structure code **)\n    #  26-39 Full secondary structure name\n    #  42-49 Phi angle\n    #  52-59 Psi angle\n    #  61-69 Residue solvent accessible area\n    #\n    # -rId1Id2..  Read only Chains Id1, Id2 ...\n    # -cId1Id2..  Process only Chains Id1, Id2 ...\n\n    # The Stride based secondary structure names of each unique element where possible values are\n    #  H:Alpha helix,\n    #  G:3-10 helix,\n    #  I:PI-helix,\n    #  E:Extended conformation,\n    #  B/b:Isolated bridge,\n    #  T:Turn,\n    #  C:Coil (none of the above)'\n    current_struc_file = self.write(out_path=f'stride_input-{self.name}-{random() * 100000}.pdb')\n    p = subprocess.Popen([putils.stride_exe_path, current_struc_file],\n                         stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n    out, err = p.communicate()\n    struct_file = Path(current_struc_file)\n    struct_file.unlink(missing_ok=True)\n    # self.log.debug(f'Stride file is at: {current_struc_file}')\n\n    if out:\n        if to_file:\n            with open(to_file, 'wb') as f:\n                f.write(out)\n        stride_output = out.decode('utf-8').split('\\n')\n    else:\n        self.log.warning(f'{self.name}: No secondary structure assignment found with Stride')\n        return\n\n    residue_idx = count()\n    residues = self.residues\n    for line in stride_output:\n        # residue_idx = int(line[10:15])\n        if line[0:3] == 'ASG':\n            # residue_idx = int(line[15:20])  # one-indexed, use in Structure version...\n            # line[10:15].strip().isdigit():  # residue number -&gt; line[10:15].strip().isdigit():\n            # self.chain(line[9:10]).residue(int(line[10:15].strip())).secondary_structure = line[24:25]\n            residues[next(residue_idx)].secondary_structure = line[24:25]\n\n    self.secondary_structure = ''.join(residue.secondary_structure for residue in residues)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.is_termini_helical","title":"is_termini_helical","text":"<pre><code>is_termini_helical(termini: termini_literal = 'n', window: int = 5) -&gt; bool\n</code></pre> <p>Using assigned secondary structure, probe for helical termini using a segment of 'window' residues. Will remove any disordered residues from the specified termini before checking, with the assumption that the disordered terminal residues are not integral to the structure</p> <p>Parameters:</p> <ul> <li> <code>termini</code>             (<code>termini_literal</code>, default:                 <code>'n'</code> )         \u2013          <p>Either 'n' or 'c' should be specified</p> </li> <li> <code>window</code>             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>The segment size to search</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if the specified terminus has a stretch of helical residues the length of the window</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def is_termini_helical(self, termini: stutils.termini_literal = 'n', window: int = 5) -&gt; bool:\n    \"\"\"Using assigned secondary structure, probe for helical termini using a segment of 'window' residues. Will\n    remove any disordered residues from the specified termini before checking, with the assumption that the\n    disordered terminal residues are not integral to the structure\n\n    Args:\n        termini: Either 'n' or 'c' should be specified\n        window: The segment size to search\n\n    Returns:\n        True if the specified terminus has a stretch of helical residues the length of the window\n    \"\"\"\n    # Strip \"disorder\" from the termini, then use the window to compare against the secondary structure\n    search_window = window * 2\n    if termini in 'Nn':\n        term_window = self.secondary_structure.lstrip(SS_DISORDER_IDENTIFIERS)[:search_window]\n    elif termini in 'Cc':\n        term_window = self.secondary_structure.rstrip(SS_DISORDER_IDENTIFIERS)[-search_window:]\n    else:\n        raise ValueError(\n            f\"The termini value {termini} isn't allowed. Must indicate one of {get_args(stutils.termini_literal)}\")\n\n    if 'H' * window in term_window:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.calculate_secondary_structure","title":"calculate_secondary_structure","text":"<pre><code>calculate_secondary_structure(**kwargs)\n</code></pre> <p>Perform the secondary structure calculation for the Structure using the DEFAULT_SS_PROGRAM</p> <p>Other Parameters:</p> <ul> <li> <code>to_file</code>         \u2013          <p>AnyStr = None - The location of a file to save secondary structure calculations</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def calculate_secondary_structure(self, **kwargs):\n    \"\"\"Perform the secondary structure calculation for the Structure using the DEFAULT_SS_PROGRAM\n\n    Keyword Args:\n        to_file: AnyStr = None - The location of a file to save secondary structure calculations\n    \"\"\"\n    self.__getattribute__(DEFAULT_SS_PROGRAM)(**kwargs)  # self.stride()\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.termini_proximity_from_reference","title":"termini_proximity_from_reference","text":"<pre><code>termini_proximity_from_reference(termini: termini_literal = 'n', reference: ndarray = utils.symmetry.origin, **kwargs) -&gt; float\n</code></pre> <p>Finds the orientation of the termini from the origin (default) or from a reference point</p> <p>Parameters:</p> <ul> <li> <code>termini</code>             (<code>termini_literal</code>, default:                 <code>'n'</code> )         \u2013          <p>Either 'n' or 'c' should be specified</p> </li> <li> <code>reference</code>             (<code>ndarray</code>, default:                 <code>origin</code> )         \u2013          <p>The reference where the point should be measured from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>When compared to the reference, 1 if the termini is more than halfway from the center of the Structure and -1 if the termini is less than halfway from the center of the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def termini_proximity_from_reference(self, termini: stutils.termini_literal = 'n',\n                                     reference: np.ndarray = utils.symmetry.origin, **kwargs) -&gt; float:\n    \"\"\"Finds the orientation of the termini from the origin (default) or from a reference point\n\n    Args:\n        termini: Either 'n' or 'c' should be specified\n        reference: The reference where the point should be measured from\n\n    Returns:\n        When compared to the reference, 1 if the termini is more than halfway from the center of the Structure and\n            -1 if the termini is less than halfway from the center of the Structure\n    \"\"\"\n    if termini == 'n':\n        residue_coords = self.residues[0].n_coords\n    elif termini == 'c':\n        residue_coords = self.residues[-1].c_coords\n    else:\n        raise ValueError(\n            f\"'termini' must be either 'n' or 'c', not {termini}\")\n\n    if reference is None:\n        reference = utils.symmetry.origin\n\n    max_distance = self.distance_from_reference(reference=reference, measure='max')\n    min_distance = self.distance_from_reference(reference=reference, measure='min')\n    coord_distance = np.linalg.norm(residue_coords - reference)\n    if abs(coord_distance - max_distance) &lt; abs(coord_distance - min_distance):\n        return 1  # Termini further from the reference\n    else:\n        return -1  # Termini closer to the reference\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_atom_record","title":"get_atom_record","text":"<pre><code>get_atom_record(**kwargs) -&gt; str\n</code></pre> <p>Provides the Structure as a 'PDB' formatted string of Atom records</p> <p>Other Parameters:</p> <ul> <li> <code>chain_id</code>         \u2013          <p>str = None - The chain ID to use</p> </li> <li> <code>atom_offset</code>         \u2013          <p>int = 0 - How much to offset the atom number by. Default returns one-indexed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The archived .pdb formatted ATOM records for the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_atom_record(self, **kwargs) -&gt; str:\n    \"\"\"Provides the Structure as a 'PDB' formatted string of Atom records\n\n    Keyword Args:\n        chain_id: str = None - The chain ID to use\n        atom_offset: int = 0 - How much to offset the atom number by. Default returns one-indexed\n\n    Returns:\n        The archived .pdb formatted ATOM records for the Structure\n    \"\"\"\n    return '\\n'.join(residue.__str__(**kwargs) for residue in self.residues)\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_fragments","title":"get_fragments","text":"<pre><code>get_fragments(residues: list[Residue] = None, residue_numbers: list[int] = None, fragment_db: FragmentDatabase = None, **kwargs) -&gt; list[MonoFragment]\n</code></pre> <p>From the Structure, find Residues with a matching fragment type as identified in a fragment library</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>list[Residue]</code>, default:                 <code>None</code> )         \u2013          <p>The specific Residues to search for</p> </li> <li> <code>residue_numbers</code>             (<code>list[int]</code>, default:                 <code>None</code> )         \u2013          <p>The specific residue numbers to search for</p> </li> <li> <code>fragment_db</code>             (<code>FragmentDatabase</code>, default:                 <code>None</code> )         \u2013          <p>The FragmentDatabase with representative fragment types  to query against</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[MonoFragment]</code>         \u2013          <p>The MonoFragments found on the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_fragments(self, residues: list[Residue] = None, residue_numbers: list[int] = None,\n                  fragment_db: fragment.db.FragmentDatabase = None, **kwargs) -&gt; list[fragment.MonoFragment]:\n    \"\"\"From the Structure, find Residues with a matching fragment type as identified in a fragment library\n\n    Args:\n        residues: The specific Residues to search for\n        residue_numbers: The specific residue numbers to search for\n        fragment_db: The FragmentDatabase with representative fragment types  to query against\n\n    Returns:\n        The MonoFragments found on the Structure\n    \"\"\"\n    if not residues and not residue_numbers:\n        return []\n\n    if fragment_db is None:\n        fragment_db = self.fragment_db\n        if fragment_db is None:\n            raise ValueError(\"Can't assign fragments without passing 'fragment_db' or setting .fragment_db\")\n        self.log.warning(f\"Without passing 'fragment_db', using the existing .fragment_db={repr(fragment_db)}\")\n\n    try:\n        fragment_db.representatives\n    except AttributeError:\n        raise TypeError(\n            f\"The passed fragment_db is not of the required type \"\n            f\"'{fragment.db.FragmentDatabase.__class__.__name__}'\")\n\n    fragment_length = fragment_db.fragment_length\n    fragment_range = range(*fragment_db.fragment_range)\n    fragments = []\n    for residue_number in residue_numbers:\n        frag_residues = self.get_residues(numbers=[residue_number + i for i in fragment_range])\n\n        if len(frag_residues) == fragment_length:\n            new_fragment = fragment.MonoFragment(residues=frag_residues, fragment_db=fragment_db, **kwargs)\n            if new_fragment.i_type:\n                fragments.append(new_fragment)\n\n    return fragments\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.get_fragment_residues","title":"get_fragment_residues","text":"<pre><code>get_fragment_residues(residues: list[Residue] = None, residue_numbers: list[int] = None, fragment_db: FragmentDatabase = None, rmsd_thresh: float = fragment.Fragment.rmsd_thresh, **kwargs) -&gt; list | list[Residue]\n</code></pre> <p>Assigns a Fragment type to Residue instances identified from a FragmentDatabase, and returns them</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>list[Residue]</code>, default:                 <code>None</code> )         \u2013          <p>The specific Residues to search for</p> </li> <li> <code>residue_numbers</code>             (<code>list[int]</code>, default:                 <code>None</code> )         \u2013          <p>The specific residue numbers to search for</p> </li> <li> <code>fragment_db</code>             (<code>FragmentDatabase</code>, default:                 <code>None</code> )         \u2013          <p>The FragmentDatabase with representative fragment types to query the Residue against</p> </li> <li> <code>rmsd_thresh</code>             (<code>float</code>, default:                 <code>rmsd_thresh</code> )         \u2013          <p>The threshold for which a rmsd should fail to produce a fragment match</p> </li> </ul> Sets <p>Each Fragment Residue instance self.guide_coords, self.i_type</p> <p>Returns:</p> <ul> <li> <code>list | list[Residue]</code>         \u2013          <p>The Residue instances that match Fragment representatives from the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def get_fragment_residues(self, residues: list[Residue] = None, residue_numbers: list[int] = None,\n                          fragment_db: fragment.db.FragmentDatabase = None,\n                          rmsd_thresh: float = fragment.Fragment.rmsd_thresh, **kwargs) -&gt; list | list[Residue]:\n    \"\"\"Assigns a Fragment type to Residue instances identified from a FragmentDatabase, and returns them\n\n    Args:\n        residues: The specific Residues to search for\n        residue_numbers: The specific residue numbers to search for\n        fragment_db: The FragmentDatabase with representative fragment types to query the Residue against\n        rmsd_thresh: The threshold for which a rmsd should fail to produce a fragment match\n\n    Sets:\n        Each Fragment Residue instance self.guide_coords, self.i_type\n\n    Returns:\n        The Residue instances that match Fragment representatives from the Structure\n    \"\"\"\n    if fragment_db is None:\n        fragment_db = self.fragment_db\n        if fragment_db is None:\n            raise ValueError(\"Can't assign fragments without passing 'fragment_db' or setting .fragment_db\")\n        self.log.warning(f\"Without passing 'fragment_db', using the existing .fragment_db={repr(fragment_db)}\")\n\n    try:\n        fragment_db.representatives\n    except AttributeError:\n        raise TypeError(\n            f\"The passed fragment_db is not of the required type \"\n            f\"'{fragment.db.FragmentDatabase.__class__.__name__}'\")\n\n    if residue_numbers is not None:\n        residues = self.get_residues(numbers=residue_numbers)\n\n    # Get iterable of residues\n    residues = self.residues if residues is None else residues\n\n    # Get neighboring ca coords on each side by retrieving flanking residues. If not fragment_length, remove\n    fragment_length = fragment_db.fragment_length\n    frag_lower_range, frag_upper_range = fragment_db.fragment_range\n\n    # Iterate over the residues in reverse to remove any indices that are missing and convert to coordinates\n    viable_residues = []\n    residues_ca_coords = []\n    for residue in residues:\n        residue_set = \\\n            residue.get_upstream(frag_lower_range) + [residue] + residue.get_downstream(frag_upper_range-1)\n        if len(residue_set) == fragment_length:\n            residues_ca_coords.append([residue.ca_coords for residue in residue_set])\n            viable_residues.append(residue)\n\n    residue_ca_coords = np.array(residues_ca_coords)\n\n    # Solve for fragment type (secondary structure classification could be used too)\n    found_fragments = []\n    for idx, residue in enumerate(viable_residues):\n        min_rmsd = float('inf')\n        residue_ca_coord_set = residue_ca_coords[idx]\n        for fragment_type, representative in fragment_db.representatives.items():\n            rmsd, rot, tx = superposition3d(residue_ca_coord_set, representative.ca_coords)\n            if rmsd &lt;= rmsd_thresh and rmsd &lt;= min_rmsd:\n                residue.frag_type = fragment_type\n                min_rmsd = rmsd\n\n        if residue.frag_type:\n            residue.fragment_db = fragment_db\n            residue._fragment_coords = fragment_db.representatives[residue.frag_type].backbone_coords\n            found_fragments.append(residue)\n\n    return found_fragments\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.find_fragments","title":"find_fragments","text":"<pre><code>find_fragments(fragment_db: FragmentDatabase = None, **kwargs) -&gt; list[tuple[GhostFragment, Fragment, float]]\n</code></pre> <p>Search Residue instances to find Fragment instances that are neighbors, returning all Fragment pairs. By default, returns all Residue instances neighboring FragmentResidue instances</p> <p>Parameters:</p> <ul> <li> <code>fragment_db</code>             (<code>FragmentDatabase</code>, default:                 <code>None</code> )         \u2013          <p>The FragmentDatabase with representative fragment types to query the Residue against</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>residues</code>         \u2013          <p>list[Residue] = None - The specific Residues to search for</p> </li> <li> <code>residue_numbers</code>         \u2013          <p>list[int] = None - The specific residue numbers to search for</p> </li> <li> <code>rmsd_thresh</code>         \u2013          <p>float = fragment.Fragment.rmsd_thresh - The threshold for which a rmsd should fail to produce a fragment match</p> </li> <li> <code>distance</code>         \u2013          <p>float = 8.0 - The distance to query for neighboring fragments</p> </li> <li> <code>min_match_value</code>         \u2013          <p>float = 2 - The minimum value which constitutes an acceptable fragment z_score</p> </li> <li> <code>clash_coords</code>         \u2013          <p>np.ndarray = None \u2013 The coordinates to use for checking for GhostFragment clashes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[tuple[GhostFragment, Fragment, float]]</code>         \u2013          <p>The GhostFragment, Fragment pairs, along with their match score</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def find_fragments(self, fragment_db: fragment.db.FragmentDatabase = None, **kwargs) \\\n        -&gt; list[tuple[fragment.GhostFragment, fragment.Fragment, float]]:\n    \"\"\"Search Residue instances to find Fragment instances that are neighbors, returning all Fragment pairs.\n    By default, returns all Residue instances neighboring FragmentResidue instances\n\n    Args:\n        fragment_db: The FragmentDatabase with representative fragment types to query the Residue against\n\n    Keyword Args:\n        residues: list[Residue] = None - The specific Residues to search for\n        residue_numbers: list[int] = None - The specific residue numbers to search for\n        rmsd_thresh: float = fragment.Fragment.rmsd_thresh - The threshold for which a rmsd should fail to produce\n            a fragment match\n        distance: float = 8.0 - The distance to query for neighboring fragments\n        min_match_value: float = 2 - The minimum value which constitutes an acceptable fragment z_score\n        clash_coords: np.ndarray = None \u2013 The coordinates to use for checking for GhostFragment clashes\n\n    Returns:\n        The GhostFragment, Fragment pairs, along with their match score\n    \"\"\"\n    if fragment_db is None:\n        fragment_db = self.fragment_db\n        if fragment_db is None:\n            raise ValueError(\"Can't assign fragments without passing 'fragment_db' or setting .fragment_db\")\n        self.log.warning(f\"Without passing 'fragment_db', using the existing .fragment_db={repr(fragment_db)}\")\n\n    fragment_time_start = time.time()\n    frag_residues = self.get_fragment_residues(fragment_db=fragment_db, **kwargs)\n    self.log.info(f'Found {len(frag_residues)} fragments on {self.name}')\n\n    frag_residue_indices = [residue.index for residue in frag_residues]\n    all_fragment_pairs = []\n    for frag_residue in frag_residues:\n        # frag_neighbors = frag_residue.get_residue_neighbors(**kwargs)\n        neighbors = self.get_residues_by_atom_indices(frag_residue.neighboring_atom_indices(**kwargs))\n        # THIS GETS NON-SELF FRAGMENTS TOO\n        # frag_neighbors = [residue for residue in neighbors if residue.frag_type]\n        # if not frag_neighbors:\n        #     continue\n        frag_neighbors = [residue for residue in neighbors if residue.index in frag_residue_indices]\n        if not frag_neighbors:\n            continue\n        all_fragment_pairs.extend(fragment.find_fragment_overlap([frag_residue], frag_neighbors, **kwargs))\n\n    self.log.debug(f'Took {time.time() - fragment_time_start:.8f}s')\n    return all_fragment_pairs\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.spatial_aggregation_propensity_per_residue","title":"spatial_aggregation_propensity_per_residue","text":"<pre><code>spatial_aggregation_propensity_per_residue(distance: float = 5.0, **kwargs) -&gt; list[float]\n</code></pre> <p>Calculate the spatial aggregation propensity on a per-residue basis using calculated heavy atom contacts to define which Residue instances are in contact.</p> <p>Caution: Contrasts with published method due to use of relative sasa for each Residue instance instead of relative sasa for each Atom instance</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>5.0</code> )         \u2013          <p>The distance in angstroms to measure Atom instances in contact</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>The floats representing the spatial aggregation propensity for each Residue in the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def spatial_aggregation_propensity_per_residue(self, distance: float = 5., **kwargs) -&gt; list[float]:\n    \"\"\"Calculate the spatial aggregation propensity on a per-residue basis using calculated heavy atom contacts to\n    define which Residue instances are in contact.\n\n    Caution: Contrasts with published method due to use of relative sasa for each Residue instance instead of\n    relative sasa for each Atom instance\n\n    Args:\n        distance: The distance in angstroms to measure Atom instances in contact\n\n    Keyword Args:\n        probe_radius: float = 1.4 - The radius which surface area should be generated\n\n    Returns:\n        The floats representing the spatial aggregation propensity for each Residue in the Structure\n    \"\"\"\n    # SASA Keyword args that are not reported as available\n    # atom: bool = True - Whether the output should be generated for each atom.\n    #     If False, will be generated for each Residue\n    if not self.sasa:\n        self.get_sasa(**kwargs)\n    # Set up the hydrophobicity parameters\n    hydrophobicity_ = hydrophobicity_values_glycine_centered['black_and_mould']\n    # Get heavy Atom coordinates\n    heavy_coords = self.heavy_coords\n    # Make and query a tree\n    tree = BallTree(heavy_coords)\n    query = tree.query_radius(heavy_coords, distance)\n\n    residues = self.residues\n    # In case this was already called, all should be set to 0.0\n    for residue in residues:\n        residue.spatial_aggregation_propensity = 0.\n        # Set the hydrophobicity_ attribute in a first pass to reduce repetitive lookups\n        residue.hydrophobicity_ = hydrophobicity_[residue.type]\n\n    heavy_atom_coords_indexed_residues = self.heavy_coords_indexed_residues\n    contacting_pairs = set((heavy_atom_coords_indexed_residues[idx1], heavy_atom_coords_indexed_residues[idx2])\n                           for idx2, contacts in enumerate(query.tolist()) for idx1 in contacts.tolist())\n    # Residue.spatial_aggregation_propensity starts as 0., so we are adding any observation to that attribute\n    for residue1, residue2 in contacting_pairs:\n        # Multiply suggested hydrophobicity value by the Residue.relative_sasa\n        # Only set on residue1 as this is the \"center\" of the calculation\n        residue1.spatial_aggregation_propensity = residue2.hydrophobicity_ * residue2.relative_sasa\n\n    return [residue.spatial_aggregation_propensity for residue in residues]\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.contact_order_per_residue","title":"contact_order_per_residue","text":"<pre><code>contact_order_per_residue(sequence_distance_cutoff: int = 2, distance: float = 6.0) -&gt; list[float]\n</code></pre> <p>Calculate the contact order on a per-residue basis using calculated heavy atom contacts</p> <p>Parameters:</p> <ul> <li> <code>sequence_distance_cutoff</code>             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>The residue spacing required to count a contact as a true contact</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>6.0</code> )         \u2013          <p>The distance in angstroms to measure Atom instances in contact</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>The floats representing the contact order for each Residue in the Structure</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def contact_order_per_residue(self, sequence_distance_cutoff: int = 2, distance: float = 6.) -&gt; list[float]:\n    \"\"\"Calculate the contact order on a per-residue basis using calculated heavy atom contacts\n\n    Args:\n        sequence_distance_cutoff: The residue spacing required to count a contact as a true contact\n        distance: The distance in angstroms to measure Atom instances in contact\n\n    Returns:\n        The floats representing the contact order for each Residue in the Structure\n    \"\"\"\n    # Get heavy Atom coordinates\n    heavy_coords = self.heavy_coords\n    # Make and query a tree\n    tree = BallTree(heavy_coords)\n    query = tree.query_radius(heavy_coords, distance)\n\n    residues = self.residues\n    # In case this was already called, we should set all to 0.0\n    for residue in residues:\n        residue.contact_order = 0.\n\n    heavy_atom_coords_indexed_residues = self.heavy_coords_indexed_residues\n    contacting_pairs = set((heavy_atom_coords_indexed_residues[idx1], heavy_atom_coords_indexed_residues[idx2])\n                           for idx2, contacts in enumerate(query.tolist()) for idx1 in contacts.tolist())\n    # Residue.contact_order starts as 0., so we are adding any observation to that attribute\n    for residue1, residue2 in contacting_pairs:\n        # Calculate using number since index might not actually specify the intended distance\n        residue_sequence_distance = abs(residue1.number - residue2.number)\n        if residue_sequence_distance &gt;= sequence_distance_cutoff:\n            # Only set on residue1 so that we don't overcount\n            residue1.contact_order += residue_sequence_distance\n\n    number_residues = len(residues)\n    for residue in residues:\n        residue.contact_order /= number_residues\n\n    return [residue.contact_order for residue in residues]\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.format_resfile_from_directives","title":"format_resfile_from_directives","text":"<pre><code>format_resfile_from_directives(residue_directives: dict[int | Residue, str], include: dict[int | Residue, set[str]] = None, background: dict[int | Residue, set[str]] = None, **kwargs) -&gt; list[str]\n</code></pre> <p>Format Residue mutational potentials given Residues/residue numbers and corresponding mutation directive. Optionally, include specific amino acids and limit to a specific background. Both dictionaries accessed by same keys as residue_directives</p> <p>Parameters:</p> <ul> <li> <code>residue_directives</code>             (<code>dict[int | Residue, str]</code>)         \u2013          <p>{Residue object: 'mutational_directive', ...}</p> </li> <li> <code>include</code>             (<code>dict[int | Residue, set[str]]</code>, default:                 <code>None</code> )         \u2013          <p>Include a set of specific amino acids for each residue</p> </li> <li> <code>background</code>             (<code>dict[int | Residue, set[str]]</code>, default:                 <code>None</code> )         \u2013          <p>The background amino acids to compare possibilities against</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>special</code>         \u2013          <p>bool = False - Whether to include special residues</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>For each Residue, returns the string formatted for a resfile with a 'PIKAA' and amino acid type string</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def format_resfile_from_directives(self, residue_directives: dict[int | Residue, str],\n                                   include: dict[int | Residue, set[str]] = None,\n                                   background: dict[int | Residue, set[str]] = None, **kwargs) -&gt; list[str]:\n    \"\"\"Format Residue mutational potentials given Residues/residue numbers and corresponding mutation directive.\n    Optionally, include specific amino acids and limit to a specific background. Both dictionaries accessed by same\n    keys as residue_directives\n\n    Args:\n        residue_directives: {Residue object: 'mutational_directive', ...}\n        include: Include a set of specific amino acids for each residue\n        background: The background amino acids to compare possibilities against\n\n    Keyword Args:\n        special: bool = False - Whether to include special residues\n\n    Returns:\n        For each Residue, returns the string formatted for a resfile with a 'PIKAA' and amino acid type string\n    \"\"\"\n    if background is None:\n        background = {}\n    if include is None:\n        include = {}\n\n    res_file_lines = []\n    residues = self.residues\n    for residue_index, directive in residue_directives.items():\n        if isinstance(residue_index, Residue):\n            residue = residue_index\n            residue_index = residue.index\n        else:\n            residue = residues[residue_index]\n\n        allowed_aas = residue.mutation_possibilities_from_directive(\n            directive, background=background.get(residue_index), **kwargs)\n        allowed_aas = {protein_letters_3to1_extended[aa] for aa in allowed_aas}\n        allowed_aas = allowed_aas.union(include.get(residue_index, {}))\n        res_file_lines.append(f'{residue.number} {residue.chain_id} PIKAA {\"\".join(sorted(allowed_aas))}')\n\n    return res_file_lines\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.make_resfile","title":"make_resfile","text":"<pre><code>make_resfile(residue_directives: dict[Residue | int, str], out_path: AnyStr = os.getcwd(), header: list[str] = None, **kwargs) -&gt; AnyStr\n</code></pre> <p>Format a resfile for the Rosetta Packer from Residue mutational directives</p> <p>Parameters:</p> <ul> <li> <code>residue_directives</code>             (<code>dict[Residue | int, str]</code>)         \u2013          <p>{Residue/int: 'mutational_directive', ...}</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Directory to write the file</p> </li> <li> <code>header</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>A header to constrain all Residues for packing</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>include</code>         \u2013          <p>dict[Residue | int, set[str]] = None - Include a set of specific amino acids for each residue</p> </li> <li> <code>background</code>         \u2013          <p>dict[Residue | int, set[str]] = None - The background amino acids to compare possibilities</p> </li> <li> <code>special</code>         \u2013          <p>bool = False - Whether to include special residues</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The path to the resfile</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def make_resfile(self, residue_directives: dict[Residue | int, str], out_path: AnyStr = os.getcwd(),\n                 header: list[str] = None, **kwargs) -&gt; AnyStr:\n    \"\"\"Format a resfile for the Rosetta Packer from Residue mutational directives\n\n    Args:\n        residue_directives: {Residue/int: 'mutational_directive', ...}\n        out_path: Directory to write the file\n        header: A header to constrain all Residues for packing\n\n    Keyword Args:\n        include: dict[Residue | int, set[str]] = None - Include a set of specific amino acids for each residue\n        background: dict[Residue | int, set[str]] = None - The background amino acids to compare possibilities\n        special: bool = False - Whether to include special residues\n\n    Returns:\n        The path to the resfile\n    \"\"\"\n    residue_lines = self.format_resfile_from_directives(residue_directives, **kwargs)\n    res_file = os.path.join(out_path, f'{self.name}.resfile')\n    with open(res_file, 'w') as f:\n        # Format the header\n        f.write('%s\\n' % ('\\n'.join(header + ['start']) if header else 'start'))\n        # Start the body\n        f.write('%s\\n' % '\\n'.join(residue_lines))\n\n    return res_file\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.set_b_factor_by_attribute","title":"set_b_factor_by_attribute","text":"<pre><code>set_b_factor_by_attribute(dtype: residue_attributes_literal)\n</code></pre> <p>Set the b-factor entry for every Residue to a Residue attribute</p> <p>Parameters:</p> <ul> <li> <code>dtype</code>             (<code>residue_attributes_literal</code>)         \u2013          <p>The attribute of interest</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_b_factor_by_attribute(self, dtype: residue_attributes_literal):\n    \"\"\"Set the b-factor entry for every Residue to a Residue attribute\n\n    Args:\n        dtype: The attribute of interest\n    \"\"\"\n    if isinstance(dtype, str):\n        # self.set_residues_attributes(b_factor=dtype)\n        for residue in self.residues:\n            residue.b_factor = getattr(residue, dtype)\n    else:\n        raise TypeError(\n            f\"The type '{dtype.__class__.__name__}' isn't a string. To {self.set_b_factor_by_attribute.__name__}, \"\n            \"you must provide 'dtype' as a string specifying a Residue attribute\")\n</code></pre>"},{"location":"reference/structure/base/#structure.base.ContainsResidues.set_b_factor_data","title":"set_b_factor_data","text":"<pre><code>set_b_factor_data(values: Iterable[float])\n</code></pre> <p>Set the b-factor entry for every Residue to a value from an array-like</p> <p>Parameters:</p> <ul> <li> <code>values</code>             (<code>Iterable[float]</code>)         \u2013          <p>Array-like of integer types to set each Residue instance 'b_factor' attribute to</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def set_b_factor_data(self, values: Iterable[float]):\n    \"\"\"Set the b-factor entry for every Residue to a value from an array-like\n\n    Args:\n        values: Array-like of integer types to set each Residue instance 'b_factor' attribute to\n    \"\"\"\n    if isinstance(values, Iterable):\n        values = list(values)\n        if len(values) != self.number_of_residues:\n            raise ValueError(\n                f\"Can't provide a array-like of values with length {len(values)} != {self.number_of_residues}, the \"\n                \"number of residues\")\n        for residue, value in zip(self.residues, values):\n            residue.b_factor = value\n    else:\n        raise TypeError(\n            f\"The type '{values.__class__.__name__}' isn't an Iterable. To {self.set_b_factor_data.__name__}, you \"\n            \"must provide the 'values' as an Iterable of integer type with length = number_of_residues\")\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Structures","title":"Structures","text":"<pre><code>Structures(structures: Iterable[ContainsResidues], dtype: str = None, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsResidues</code>, <code>UserList</code></p> <p>A view of a set of Structure instances. This isn't used at the moment</p> <p>Parameters:</p> <ul> <li> <code>structures</code>             (<code>Iterable[ContainsResidues]</code>)         \u2013          <p>The Iterable of Structure to set the Structures with</p> </li> <li> <code>dtype</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If an empty Structures, the specific subclass of Structure that Structures contains</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def __init__(self, structures: Iterable[ContainsResidues], dtype: str = None, **kwargs):\n    \"\"\"Pass the parent Structure with parent= to initialize .log, .coords, .atoms, and .residues\n\n    Args:\n        structures: The Iterable of Structure to set the Structures with\n        dtype: If an empty Structures, the specific subclass of Structure that Structures contains\n    \"\"\"\n    super().__init__(initlist=structures, **kwargs)  # initlist sets UserList.data to Iterable[Structure]\n    raise NotImplementedError(\"This class isn't functional yet\")\n    if self.is_parent():\n        raise stutils.ConstructionError(\n            f\"Couldn't create {Structures.__name__} without passing 'parent' argument\"\n        )\n\n    if not self.data:  # Set up an empty Structures\n        self.dtype = dtype if dtype else 'Structure'\n    elif all([True if isinstance(structure, ContainsResidues) else False for structure in self]):\n        # self.data = [structure for structure in structures]\n        self._atom_indices = []\n        for structure in self:\n            self._atom_indices.extend(structure.atom_indices)\n        self._residue_indices = []\n        for structure in self:\n            self._residue_indices.extend(structure.residue_indices)\n\n        self.dtype = dtype if dtype else type(self.data[0]).__name__\n    else:\n        raise ValueError(\n            f\"Can't set {self.__class__.__name__} by passing '{', '.join(type(structure) for structure in self)}, \"\n            f'must set with type [Structure, ...] or an empty constructor. Ex: Structures()')\n\n    # Overwrite attributes in Structure\n    self.name = f'{self.parent.name}-{dtype}Selection'\n</code></pre>"},{"location":"reference/structure/base/#structure.base.Structures.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype: str\n</code></pre> <p>The type of Structure in instance</p>"},{"location":"reference/structure/base/#structure.base.Structures.structures","title":"structures  <code>property</code>","text":"<pre><code>structures: list[ContainsResidues]\n</code></pre> <p>Returns the underlying data in Structures</p>"},{"location":"reference/structure/base/#structure.base.parse_seqres","title":"parse_seqres","text":"<pre><code>parse_seqres(seqres_lines: list[str]) -&gt; dict[str, str]\n</code></pre> <p>Convert SEQRES information to single amino acid dictionary format</p> <p>Parameters:</p> <ul> <li> <code>seqres_lines</code>             (<code>list[str]</code>)         \u2013          <p>The list of lines containing SEQRES information</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str]</code>         \u2013          <p>The mapping of each chain to its reference sequence</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def parse_seqres(seqres_lines: list[str]) -&gt; dict[str, str]:  # list[str]:\n    \"\"\"Convert SEQRES information to single amino acid dictionary format\n\n    Args:\n        seqres_lines: The list of lines containing SEQRES information\n\n    Returns:\n        The mapping of each chain to its reference sequence\n    \"\"\"\n    # SEQRES   1 A  182  THR THR ALA SER THR SER GLN VAL ARG GLN ASN TYR HIS\n    # SEQRES   2 A  182  GLN ASP SER GLU ALA ALA ILE ASN ARG GLN ILE ASN LEU\n    # SEQRES   3 A  182  GLU LEU TYR ALA SER TYR VAL TYR LEU SER MET SER TYR\n    # SEQRES ...\n    # SEQRES  16 C  201  SER TYR ILE ALA GLN GLU\n    # In order to account for MultiModel files where the chain names are all the same, using the parsed order\n    # instead of a dictionary as later entries would overwrite earlier ones making them inaccurate\n    # If the file is screwed up in that it has chains in a different order than the seqres, then this wouldn't work\n    # I am opting for the standard .pdb file format and if it is messed up this is the users problem\n    reference_sequence = {}\n    for line in seqres_lines:\n        chain, length, *sequence = line.split()\n        if chain in reference_sequence:\n            reference_sequence[chain].extend(list(sequence))\n        else:\n            reference_sequence[chain] = list(sequence)\n\n    # Format the sequences as a one AA letter list\n    reference_sequences = {}  # []\n    for chain, sequence in reference_sequence.items():\n        # Ensure we parse selenomethionine correctly\n        one_letter_sequence = [protein_letters_3to1_extended_mse.get(aa, '-')\n                               for aa in sequence]\n        reference_sequences[chain] = ''.join(one_letter_sequence)\n        # reference_sequences.append(''.join(one_letter_sequence))\n\n    return reference_sequences\n</code></pre>"},{"location":"reference/structure/base/#structure.base.read_pdb_file","title":"read_pdb_file","text":"<pre><code>read_pdb_file(file: AnyStr = None, pdb_lines: Iterable[str] = None, separate_coords: bool = True, **kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Reads .pdb file and returns structural information pertaining to parsed file</p> <p>By default, returns the coordinates as a separate numpy.ndarray which is parsed directly by StructureBase. This will be associated with each Atom however, separate parsing is done for efficiency. To include coordinate info with the individual Atom instances, pass separate_coords=False. (Not recommended)</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The path to the file to parse</p> </li> <li> <code>pdb_lines</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>If lines are already read, provide the lines instead</p> </li> <li> <code>separate_coords</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to separate parsed coordinates from Atom instances. Will be returned as two separate entries in the parsed dictionary, otherwise returned with coords=None</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>The dictionary containing all the parsed structural information</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def read_pdb_file(file: AnyStr = None, pdb_lines: Iterable[str] = None, separate_coords: bool = True, **kwargs) -&gt; \\\n        dict[str, Any]:\n    \"\"\"Reads .pdb file and returns structural information pertaining to parsed file\n\n    By default, returns the coordinates as a separate numpy.ndarray which is parsed directly by StructureBase. This will\n    be associated with each Atom however, separate parsing is done for efficiency. To include coordinate info with the\n    individual Atom instances, pass separate_coords=False. (Not recommended)\n\n    Args:\n        file: The path to the file to parse\n        pdb_lines: If lines are already read, provide the lines instead\n        separate_coords: Whether to separate parsed coordinates from Atom instances. Will be returned as two separate\n            entries in the parsed dictionary, otherwise returned with coords=None\n\n    Returns:\n        The dictionary containing all the parsed structural information\n    \"\"\"\n    if pdb_lines:\n        # path, extension = None, None\n        assembly: str | None = None\n        name = None\n    elif file is not None:\n        with open(file, 'r') as f:\n            pdb_lines = f.readlines()\n        path, extension = os.path.splitext(file)\n        name = os.path.basename(path)\n\n        if extension[-1].isdigit():\n            # If last character is not a letter, then the file is an assembly, or the extension was provided weird\n            assembly: str | None = extension.translate(utils.keep_digit_table)\n        else:\n            assembly = None\n    else:\n        raise ValueError(\n            f\"{read_pdb_file.__name__}: Must provide the argument 'file' or 'pdb_lines'\")\n\n    # type to info index:   1    2    3    4    5    6    7     11     12   13   14\n    # eventual type:      int, str, str, str, str, int, str, float, float, str, str]] = []\n    temp_info: list[tuple[str, str, str, str, str, str, str, str, str, str, str]] = []\n    # type to info index:   1    2    3    4    5    6    7 8,9,10     11     12   13   14\n    # fields w/ coords   [int, str, str, str, str, int, str, float, float, float, str, str]\n    coords: list[list[float]] = []\n    cryst_record: str = None\n    dbref: dict[str, dict[str, str]] = {}\n    entity_info: dict[str, dict[str, dict | list | str]] = {}\n    header: list = []\n    resolution: float | None = None\n    seq_res_lines: list[str] = []\n    biomt = []\n\n    entity = None\n    current_operation = -1\n    alt_loc_str = ' '\n    # for line_tokens in map(str.split, pdb_lines):\n    #     # 0       1       2          3             4             5      6               7                   8  9\n    #     # remark, number, atom_type, alt_location, residue_type, chain, residue_number, code_for_insertion, x, y,\n    #     #     10 11   12         13       14\n    #     #     z, occ, temp_fact, element, charge = \\\n    #     #     line[6:11].strip(), int(line[6:11]), line[12:16].strip(), line[16:17].strip(), line[17:20].strip(),\n    #     #     line[21:22], int(line[22:26]), line[26:27].strip(), float(line[30:38]), float(line[38:46]), \\\n    #     #     float(line[46:54]), float(line[54:60]), float(line[60:66]), line[76:78].strip(), line[78:80].strip()\n    for line in pdb_lines:\n        remark = line[slice_remark]\n        if remark == 'ATOM  ' or line[slice_residue_type] == 'MSE' and remark == 'HETATM':\n            # if remove_alt_location and alt_location not in ['', 'A']:\n            if line[slice_alt_location] not in [alt_loc_str, 'A']:\n                continue\n            # number = int(line[slice_number])\n            residue_type = line[slice_residue_type].strip()\n            if residue_type == 'MSE':\n                residue_type = 'MET'\n                atom_type = line[slice_atom_type].strip()\n                if atom_type == 'SE':\n                    atom_type = 'SD'  # change type from Selenium to Sulfur delta\n            else:\n                atom_type = line[slice_atom_type].strip()\n            # prepare line information for population of Atom objects\n            temp_info.append((line[slice_number], atom_type, alt_loc_str, residue_type, line[slice_chain],\n                              line[slice_residue_number], line[slice_code_for_insertion].strip(),\n                              line[slice_occ], line[slice_temp_fact],\n                              line[slice_element].strip(), line[slice_charge].strip()))\n            # temp_info.append((int(line[slice_number]), atom_type, alt_loc_str, residue_type, line[slice_chain],\n            #                   int(line[slice_residue_number]), line[slice_code_for_insertion].strip(),\n            #                   float(line[slice_occ]), float(line[slice_temp_fact]),\n            #                   line[slice_element].strip(), line[slice_charge].strip()))\n            # Prepare the atomic coordinates for addition to numpy array\n            coords.append([float(line[slice_x]), float(line[slice_y]), float(line[slice_z])])\n        elif remark == 'SEQRES':\n            seq_res_lines.append(line[11:])\n        elif remark == 'REMARK':\n            header.append(line.strip())\n            remark_number = line[slice_number]\n            # elif line[:18] == 'REMARK 350   BIOMT':\n            if remark_number == ' 350 ':  # 6:11  '   BIOMT'\n                # integration of the REMARK 350 BIOMT\n                # REMARK 350\n                # REMARK 350 BIOMOLECULE: 1\n                # REMARK 350 AUTHOR DETERMINED BIOLOGICAL UNIT: TRIMERIC\n                # REMARK 350 SOFTWARE DETERMINED QUATERNARY STRUCTURE: TRIMERIC\n                # REMARK 350 SOFTWARE USED: PISA\n                # REMARK 350 TOTAL BURIED SURFACE AREA: 6220 ANGSTROM**2\n                # REMARK 350 SURFACE AREA OF THE COMPLEX: 28790 ANGSTROM**2\n                # REMARK 350 CHANGE IN SOLVENT FREE ENERGY: -42.0 KCAL/MOL\n                # REMARK 350 APPLY THE FOLLOWING TO CHAINS: A, B, C\n                # REMARK 350   BIOMT1   1  1.000000  0.000000  0.000000        0.00000\n                # REMARK 350   BIOMT2   1  0.000000  1.000000  0.000000        0.00000\n                # REMARK 350   BIOMT3   1  0.000000  0.000000  1.000000        0.00000\n                try:\n                    _, _, biomt_indicator, operation_number, x, y, z, tx = line.split()\n                except ValueError:  # Not enough values to unpack\n                    continue\n                if biomt_indicator == 'BIOMT':\n                    if operation_number != current_operation:  # Reached a new transformation matrix\n                        current_operation = operation_number\n                        biomt.append([])\n                    # Add the transformation to the current matrix\n                    biomt[-1].append(list(map(float, (x, y, z, tx))))\n            elif remark_number == '   2 ':  # 6:11 ' RESOLUTION'\n                try:\n                    resolution = float(line[22:30].strip().split()[0])\n                except (IndexError, ValueError):\n                    resolution = None\n        elif 'DBREF' in remark:\n            header.append(line.strip())\n            chain = line[12:14].strip().upper()\n            if line[5:6] == '2':\n                db_accession_id = line[18:40].strip()\n            else:\n                db = line[26:33].strip()\n                if line[5:6] == '1':  # skip grabbing db_accession_id until DBREF2\n                    continue\n                db_accession_id = line[33:42].strip()\n            dbref[chain] = {'db': db, 'accession': db_accession_id}  # implies each chain has only one id\n        elif remark == 'COMPND' and 'MOL_ID' in line:\n            header.append(line.strip())\n            entity = line[line.rfind(':') + 1: line.rfind(';')].strip()\n        elif remark == 'COMPND' and 'CHAIN' in line and entity:  # retrieve from standard .pdb file notation\n            header.append(line.strip())\n            # entity number (starting from 1) = {'chains' : {A, B, C}}\n            entity_info[f'{name}_{entity}'] = \\\n                {'chains': list(map(str.strip, line[line.rfind(':') + 1:].strip().rstrip(';').split(',')))}\n            entity = None\n        elif remark == 'SCALE ':\n            header.append(line.strip())\n        elif remark == 'CRYST1':\n            header.append(line.strip())\n            cryst_record = line  # Don't .strip() so '\\n' is attached for output\n            # uc_dimensions, space_group = parse_cryst_record(cryst_record)\n            # cryst = {'space': space_group, 'a_b_c': tuple(uc_dimensions[:3]), 'ang_a_b_c': tuple(uc_dimensions[3:])}\n\n    if not temp_info:\n        if file:\n            raise ValueError(\n                f'The file {file} has no ATOM records')\n        else:\n            raise ValueError(\"The provided 'pdb_lines' have no ATOM records\")\n\n    # Combine entity_info with the reference_sequence info and dbref info\n    if seq_res_lines:\n        reference_sequence = parse_seqres(seq_res_lines)\n    else:\n        reference_sequence = None\n\n    for entity_name, info in entity_info.items():\n        # Grab the first chain from the identified chains, and use it to grab the reference sequence\n        chain = info['chains'][0]\n        try:\n            info['reference_sequence'] = reference_sequence[chain]  # Used when parse_seqres returns dict[str, str]\n        except TypeError:  # This is None\n            pass\n        try:\n            info['dbref'] = dbref[chain]\n        except KeyError:  # Keys are missing\n            pass\n\n    # # Convert the incrementing reference sequence to a list of the sequences\n    # reference_sequence = list(reference_sequence.values())\n\n    if biomt:\n        biomt = np.array(biomt, dtype=float)\n        rotation_matrices = biomt[:, :, :3]\n        translation_matrices = biomt[:, :, 3:].squeeze()\n    else:\n        rotation_matrices = translation_matrices = None\n\n    parsed_info = \\\n        dict(atoms=[Atom.without_coordinates(idx, *info) for idx, info in enumerate(temp_info)]\n             if separate_coords else\n             # Initialize with individual coords. Not sure why anyone would do this, but include for compatibility\n             [Atom(number=int(number), atom_type=atom_type, alt_location=alt_location, residue_type=residue_type,\n                   chain_id=chain_id, residue_number=int(residue_number), code_for_insertion=code_for_insertion,\n                   coords=coords[idx], occupancy=float(occupancy), b_factor=float(b_factor), element=element,\n                   charge=charge)\n              for idx, (number, atom_type, alt_location, residue_type, chain_id, residue_number, code_for_insertion,\n                        occupancy, b_factor, element, charge)\n              in enumerate(temp_info)],\n             biological_assembly=assembly,\n             rotation_matrices=rotation_matrices,\n             translation_matrices=translation_matrices,\n             coords=coords if separate_coords else None,\n             cryst_record=cryst_record,\n             entity_info=entity_info,\n             name=name,\n             resolution=resolution,\n             reference_sequence=reference_sequence,\n             )\n    # Explicitly overwrite any parsing if argument was passed to caller\n    parsed_info.update(**kwargs)\n    return parsed_info\n</code></pre>"},{"location":"reference/structure/base/#structure.base.read_mmcif_file","title":"read_mmcif_file","text":"<pre><code>read_mmcif_file(file: AnyStr = None, **kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Reads .cif file and returns structural information pertaining to parsed file</p> <p>By default, returns the coordinates as a separate numpy.ndarray which is parsed directly by StructureBase. This will be associated with each Atom however, separate parsing is done for efficiency. To include coordinate info with the individual Atom instances, pass separate_coords=False. (Not recommended)</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The path to the file to parse</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>The dictionary containing all the parsed structural information</p> </li> </ul> Source code in <code>symdesign/structure/base.py</code> <pre><code>def read_mmcif_file(file: AnyStr = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Reads .cif file and returns structural information pertaining to parsed file\n\n    By default, returns the coordinates as a separate numpy.ndarray which is parsed directly by StructureBase. This will\n    be associated with each Atom however, separate parsing is done for efficiency. To include coordinate info with the\n    individual Atom instances, pass separate_coords=False. (Not recommended)\n\n    Args:\n        file: The path to the file to parse\n\n    Returns:\n        The dictionary containing all the parsed structural information\n    \"\"\"\n    # if lines:\n    #     # path, extension = None, None\n    #     assembly: str | None = None\n    #     name = None\n    # el\n    if file is not None:\n        path, extension = os.path.splitext(file)\n        name = os.path.basename(path)\n        ignore_fields = []\n        data: dict[str, dict[str, Any]] = cif_reader.read(file, ignore=ignore_fields)\n        if extension[-1].isdigit():\n            # If last character is not a letter, then the file is an assembly, or the extension was provided weird\n            assembly: str | None = extension.translate(utils.keep_digit_table)\n        elif 'assembly' in name:\n            assembly = name[name.find('assembly'):].translate(utils.keep_digit_table)\n        else:\n            assembly = None\n    else:\n        raise ValueError(\n            f\"{read_mmcif_file.__name__}: Must provide the argument 'file'\"\n        )\n\n    #  name = kwargs.pop('name', None)\n    #  if not name:\n    #      name = os.path.basename(os.path.splitext(file)[0])\n    #\n    #  input(data.keys())\n    #  for k, v_ in data.items():\n    #      # input(f'{list(v_.keys())}')\n    #      # ['_entry', '_audit_conform', '_database_2', '_pdbx_database_PDB_obs_spr', '_pdbx_database_related',\n    #      #  '_pdbx_database_status', '_audit_author', '_citation', '_citation_author', '_cell', '_symmetry',\n    #      #  '_entity', '_entity_poly', '_entity_poly_seq', '_entity_src_gen', '_struct_ref', '_struct_ref_seq',\n    #      #  '_struct_ref_seq_dif', '_chem_comp', '_exptl', '_exptl_crystal', '_exptl_crystal_grow', '_diffrn',\n    #      #  '_diffrn_detector', '_diffrn_radiation', '_diffrn_radiation_wavelength', '_diffrn_source', '_reflns',\n    #      #  '_reflns_shell', '_refine', '_refine_hist', '_refine_ls_restr', '_refine_ls_shell', '_pdbx_refine',\n    #      #  '_struct', '_struct_keywords', '_struct_asym', '_struct_biol', '_struct_conf', '_struct_conf_type',\n    #      #  '_struct_mon_prot_cis', '_struct_sheet', '_struct_sheet_order', '_struct_sheet_range',\n    #      #  '_pdbx_struct_sheet_hbond', '_atom_sites', '_atom_type', '_atom_site', '_atom_site_anisotrop',\n    #      #  '_pdbx_poly_seq_scheme', '_pdbx_struct_assembly', '_pdbx_struct_assembly_gen',\n    #      #  '_pdbx_struct_assembly_prop', '_pdbx_struct_oper_list', '_pdbx_audit_revision_history',\n    #      #  '_pdbx_audit_revision_details', '_pdbx_audit_revision_group', '_pdbx_refine_tls',\n    #      #  '_pdbx_refine_tls_group', '_pdbx_phasing_MR', '_phasing', '_software', '_pdbx_validate_torsion',\n    #      #  '_pdbx_unobs_or_zero_occ_atoms', '_pdbx_unobs_or_zero_occ_residues', '_space_group_symop']\n    #      for idx, (k, v) in enumerate(v_.items()):\n    #          # if k == '_atom_sites':\n    #          #     input(v.keys())\n    #          # if k in ['_database_2', '_pdbx_database_PDB_obs_spr', '_pdbx_database_related', '_pdbx_database_status']:\n    #          #     print('Database key', k)\n    #          #     input(v)\n    #          # if k in ['_entity', '_entity_poly', '_entity_poly_seq', '_struct_ref']:  # '_entity_src_gen', '_struct_ref_seq'\n    #          #     print('Sequence key', k)\n    #          #     input(v)\n    #          # if k in ['_cell', '_symmetry',\n    #          #          # '_space_group_symop'\n    #          #          ]:\n    #          #     print('Symmetry key', k)\n    #          #     input(v)\n    #          # if k in ['_reflns', '_reflns_shell', '_refine', '_refine_hist', '_refine_ls_restr', '_refine_ls_shell', '_pdbx_refine',]:\n    #          #     print('Diffraction key', k)\n    #          #     input(v)\n    #          # if k in [\n    #          #     # '_struct', '_struct_keywords', '_struct_asym', '_struct_biol',\n    #          #     '_pdbx_struct_assembly', '_pdbx_struct_assembly_gen',\n    #          #     # '_pdbx_struct_assembly_prop',\n    #          #     '_pdbx_struct_oper_list',\n    #          # ]:\n    #          #     print('struct key', k)\n    #          #     input(v)\n    #          # if k == '_atom_site':\n    #          #     \"\"\"\n    #          #     key group_PDB\n    #          #     values ['ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM', 'ATOM']\n    #          #     key id\n    #          #     values ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n    #          #     key type_symbol\n    #          #     values ['N', 'C', 'C', 'O', 'C', 'O', 'N', 'C', 'C', 'O']\n    #          #     key label_atom_id\n    #          #     values ['N', 'CA', 'C', 'O', 'CB', 'OG', 'N', 'CA', 'C', 'O']\n    #          #     key label_alt_id\n    #          #     values ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.']\n    #          #     key label_comp_id\n    #          #     values ['SER', 'SER', 'SER', 'SER', 'SER', 'SER', 'VAL', 'VAL', 'VAL', 'VAL']\n    #          #     key label_asym_id\n    #          #     values ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n    #          #     key label_entity_id\n    #          #     values ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n    #          #     key label_seq_id\n    #          #     values ['3', '3', '3', '3', '3', '3', '4', '4', '4', '4']\n    #          #     key pdbx_PDB_ins_code\n    #          #     values ['?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n    #          #     key Cartn_x\n    #          #     values ['25.947', '25.499', '24.208', '23.310', '26.585', '27.819', '24.126', '22.943', '22.353', '23.081']\n    #          #     key Cartn_y\n    #          #     values ['8.892', '10.149', '9.959', '10.800', '10.734', '10.839', '8.851', '8.533', '7.200', '6.224']\n    #          #     key Cartn_z\n    #          #     values ['43.416', '42.828', '42.038', '42.084', '41.925', '42.615', '41.310', '40.519', '40.967', '41.152']\n    #          #     key occupancy\n    #          #     values ['1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00', '1.00']\n    #          #     key B_iso_or_equiv\n    #          #     values ['67.99', '79.33', '61.67', '60.15', '81.39', '86.58', '62.97', '56.54', '56.17', '85.78']\n    #          #     key pdbx_formal_charge\n    #          #     values ['?', '?', '?', '?', '?', '?', '?', '?', '?', '?']\n    #          #     key auth_seq_id\n    #          #     values ['3', '3', '3', '3', '3', '3', '4', '4', '4', '4']\n    #          #     key auth_comp_id\n    #          #     values ['SER', 'SER', 'SER', 'SER', 'SER', 'SER', 'VAL', 'VAL', 'VAL', 'VAL']\n    #          #     key auth_asym_id\n    #          #     values ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n    #          #     key auth_atom_id\n    #          #     values ['N', 'CA', 'C', 'O', 'CB', 'OG', 'N', 'CA', 'C', 'O']\n    #          #     key pdbx_PDB_model_num\n    #          #     values ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n    #          #     \"\"\"\n    #          #     for k__, v__ in v.items():\n    #          #         # ['group_PDB', 'id', 'type_symbol', 'label_atom_id', 'label_alt_id', 'label_comp_id', 'label_asym_id', 'label_entity_id', 'label_seq_id', 'pdbx_PDB_ins_code', 'Cartn_x', 'Cartn_y', 'Cartn_z', 'occupancy', 'B_iso_or_equiv', 'pdbx_formal_charge', 'auth_seq_id', 'auth_comp_id', 'auth_asym_id', 'auth_atom_id', 'pdbx_PDB_model_num']\n    #          #         print('key', k__)\n    #          #         input(f'values {v__[:10]}')\n    #          #     input('DONE')\n    #          pass\n\n    provided_dataname, *_ = data.keys()\n    if _:\n        raise ValueError(\n            f\"Found multiple values for the cif 'provided_dataname'={_}\")\n\n    # Extract the data of interest as saved by the provided_dataname\n    data = data[provided_dataname]\n\n    # def format_mmcif_dict(data: dict[str, Any], name: str = None, **kwargs) -&gt; dict:\n    atom_data = data.get('_atom_site')\n    atom_numbers = atom_data.get('id')\n    number_of_atoms = len(atom_numbers)\n    atom_types = atom_data.get('label_atom_id')\n    alt_locations = atom_data.get('label_alt_id', repeat(' ', number_of_atoms))\n    residue_types = atom_data.get('label_comp_id')\n    chains = atom_data.get('label_asym_id')\n    residue_numbers = atom_data.get('label_seq_id')\n    occupancies = atom_data.get('occupancy')\n    code_for_insertion = atom_data.get('', repeat(' ', number_of_atoms))\n    b_factors = atom_data.get('B_iso_or_equiv')\n    element = atom_data.get('type_symbol', repeat(None, number_of_atoms))\n    charge = atom_data.get('pdbx_formal_charge', repeat(None, number_of_atoms))\n    alt_locations = ''.join(alt_locations).replace('.', ' ')\n    code_for_insertion = ''.join(code_for_insertion).replace('.', ' ')\n    charge = ''.join(charge).replace('?', ' ')\n\n    atoms = [Atom.without_coordinates(idx, *info) for idx, info in enumerate(\n        zip(atom_numbers, atom_types, alt_locations, residue_types, chains, residue_numbers, code_for_insertion,\n            occupancies, b_factors, element, charge))]\n\n    coords = np.array([atom_data.get('Cartn_x'),\n                       atom_data.get('Cartn_y'),\n                       atom_data.get('Cartn_z')], dtype=float).T\n    cell_data = data.get('_cell')\n    if cell_data:\n        # 'length_a': '124.910', 'length_b': '189.250', 'length_c': '376.830',\n        # 'angle_alpha': '90.00', 'angle_beta': '90.02', 'angle_gamma': '90.00'\n        # Formatted in Hermann-Mauguin notation\n        space_group = data.get('_symmetry', {}).get('space_group_name_H-M')\n        cryst_record = utils.symmetry.generate_cryst1_record(\n            list(map(float, (cell_data['length_a'], cell_data['length_b'], cell_data['length_c'],\n                             cell_data['angle_alpha'], cell_data['angle_beta'], cell_data['angle_gamma']))),\n            space_group)\n    else:\n        cryst_record = None\n\n    reflections_data = data.get('_reflns')\n    if reflections_data:\n        resolution = reflections_data['d_resolution_high']\n    else:\n        resolution = None\n\n    # _struct_ref\n    # Get the cannonical sequence\n    entity_data = data.get('_entity_poly')\n    db_data = data.get('_struct_ref')\n    if db_data:\n        # 'db_name': ['UNP', 'UNP'], 'db_code': ['B0BGB0_9BACT', 'Q4Q413_LEIMA'],\n        # 'pdbx_db_accession': ['B0BGB0', 'Q4Q413'], 'entity_id': ['1', '2'],\n        # 'pdbx_seq_one_letter_code': ['MESVNTSFLSPSLVTIRDFDNGQFAVLRIGRTGFPADKGDIDLCLDKMKGVRDAQQSIGDDTEFGFKGPHIRIRCVDIDD\\nKHTYNAMVYVDLIVGTGASEVERETAEELAKEKLRAALQVDIADEHSCVTQFEMKLREELLSSDSFHPDKDEYYKDFL', 'MPVIQTFVSTPLDHHKRENLAQVYRAVTRDVLGKPEDLVMMTFHDSTPMHFFGSTDPVACVRVEALGGYGPSEPEKVTSI\\nVTAAITKECGIVADRIFVLYFSPLHCGWNGTNF']\n        entity_info = {f'{name}_{entity_id}': {'chains': [],\n                                               'reference_sequence': reference_sequence.replace('\\n', ''),\n                                               'dbref': {'db': db_name, 'accession': db_accession}}\n                       for entity_id, reference_sequence, db_name, db_accession in zip(\n                db_data.get('entity_id'),\n                entity_data.get('pdbx_seq_one_letter_code_can'),  # db_data.get('pdbx_seq_one_letter_code'),\n                db_data.get('db_name'), db_data.get('pdbx_db_accession'))\n                       }\n    else:\n        entity_info = {}\n    operations = data.get('_pdbx_struct_oper_list')\n    if operations:\n        biomt = np.array(\n            [operations['matrix[1][1]'], operations['matrix[1][2]'], operations['matrix[1][3]'],\n             operations['vector[1]'],\n             operations['matrix[2][1]'], operations['matrix[2][2]'], operations['matrix[2][3]'],\n             operations['vector[2]'],\n             operations['matrix[3][1]'], operations['matrix[3][2]'], operations['matrix[3][3]'],\n             operations['vector[3]']], dtype=float)\n        # print('biomt', biomt)\n\n        if isinstance(operations['id'], list):\n            biomt = biomt.T\n        biomt = biomt.reshape(-1, 3, 4).tolist()\n        # print('biomt', biomt)\n    else:\n        biomt = None\n\n    # # Separated...\n    # rotations = np.array(\n    #     [operations['matrix[1][1]'], operations['matrix[1][2]'], operations['matrix[1][3]'],\n    #      operations['matrix[2][1]'], operations['matrix[2][2]'], operations['matrix[2][3]'],\n    #      operations['matrix[3][1]'], operations['matrix[3][2]'], operations['matrix[3][3]']], dtype=float)\n    # translations = np.array([operations['vector[1]'], operations['vector[2]'], operations['vector[3]']],\n    #                         dtype=float)\n    # print('rotations', rotations)\n    # print('translations', translations)\n    # if isinstance(operations['id'], list):\n    #     # number_of_operations = len(operations['id'])\n    #     rotations = rotations.T  # .reshape(-1, 3, 3)\n    #     # rotations = rotations.reshape(-1, number_of_operations)\n    #     translations = translations.T\n    # # else:\n    # #     pass\n    # translations = translations.reshape(-1, 3)\n    # rotations = rotations.reshape(-1, 3, 3)\n    #\n    # print('rotations', rotations)\n    # print('translations', translations)\n    # # biomt = rotations, translations\n\n    # reference_sequence = {chain: reference_sequence}\n    formatted_info = dict(\n        atoms=atoms,\n        biological_assembly=assembly,\n        biomt=biomt,\n        coords=coords,\n        # coords=coords if separate_coords else None,\n        cryst_record=cryst_record,\n        entity_info=entity_info,\n        name=name,\n        resolution=resolution,\n        # reference_sequence=reference_sequence,\n    )\n    # Explicitly overwrite any parsing if argument was passed to caller\n    formatted_info.update(**kwargs)\n\n    return formatted_info\n</code></pre>"},{"location":"reference/structure/coordinates/","title":"coordinates","text":""},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates","title":"Coordinates","text":"<pre><code>Coordinates(coords: ndarray | list[list[float]] = None)\n</code></pre> <p>Responsible for handling StructureBase coordinates by storing in a numpy.ndarray with shape (n, 3) where n is the number of atoms in the structure and the 3 dimensions represent x, y, and z coordinates</p> <p>Parameters:</p> <ul> <li> <code>coords</code>             (<code>ndarray | list[list[float]]</code>, default:                 <code>None</code> )         \u2013          <p>The coordinates to store with shape (N, 3). If none are passed an empty container will be generated</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def __init__(self, coords: np.ndarray | list[list[float]] = None):\n    \"\"\"Construct the instance\n\n    Args:\n        coords: The coordinates to store with shape (N, 3). If none are passed an empty container will be generated\n    \"\"\"\n    if coords is None:\n        self.coords = np.array([])\n    elif not isinstance(coords, (np.ndarray, list)):\n        raise TypeError(f\"Can't initialize {self.__class__.__name__} with {type(coords).__name__}. Type must be a \"\n                        f'numpy.ndarray of float with shape (n, 3) or list[list[float]]')\n    else:\n        self.coords = np.array(coords, np.float_)\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates.delete","title":"delete","text":"<pre><code>delete(indices: Sequence[int])\n</code></pre> <p>Delete coordinates from the instance</p> <p>Parameters:</p> <ul> <li> <code>indices</code>             (<code>Sequence[int]</code>)         \u2013          <p>The indices to delete from the Coords array</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def delete(self, indices: Sequence[int]):\n    \"\"\"Delete coordinates from the instance\n\n    Args:\n        indices: The indices to delete from the Coords array\n    \"\"\"\n    self.coords = np.delete(self.coords, indices, axis=0)\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates.insert","title":"insert","text":"<pre><code>insert(at: int, new_coords: ndarray | list[list[float]])\n</code></pre> <p>Insert additional coordinates into the instance</p> <p>Parameters:</p> <ul> <li> <code>at</code>             (<code>int</code>)         \u2013          <p>The index to perform the insert at</p> </li> <li> <code>new_coords</code>             (<code>ndarray | list[list[float]]</code>)         \u2013          <p>The coordinate values to insert into Coords</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def insert(self, at: int, new_coords: np.ndarray | list[list[float]]):\n    \"\"\"Insert additional coordinates into the instance\n\n    Args:\n        at: The index to perform the insert at\n        new_coords: The coordinate values to insert into Coords\n    \"\"\"\n    self.coords = np.concatenate((self.coords[:at], new_coords, self.coords[at:]))\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates.append","title":"append","text":"<pre><code>append(new_coords: ndarray | list[list[float]])\n</code></pre> <p>Append additional coordinates onto the instance</p> <p>Parameters:</p> <ul> <li> <code>new_coords</code>             (<code>ndarray | list[list[float]]</code>)         \u2013          <p>The coordinate values to append to Coords</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def append(self, new_coords: np.ndarray | list[list[float]]):\n    \"\"\"Append additional coordinates onto the instance\n\n    Args:\n        new_coords: The coordinate values to append to Coords\n    \"\"\"\n    self.coords = np.concatenate((self.coords, new_coords))\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates.replace","title":"replace","text":"<pre><code>replace(indices: Sequence[int], new_coords: ndarray | list[list[float]])\n</code></pre> <p>Replace existing coordinates in the instance with new coordinates</p> <p>Parameters:</p> <ul> <li> <code>indices</code>             (<code>Sequence[int]</code>)         \u2013          <p>The indices to replace in the Coords array</p> </li> <li> <code>new_coords</code>             (<code>ndarray | list[list[float]]</code>)         \u2013          <p>The coordinate values to replace in Coords</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def replace(self, indices: Sequence[int], new_coords: np.ndarray | list[list[float]]):\n    \"\"\"Replace existing coordinates in the instance with new coordinates\n\n    Args:\n        indices: The indices to replace in the Coords array\n        new_coords: The coordinate values to replace in Coords\n    \"\"\"\n    try:\n        self.coords[indices] = new_coords\n    except ValueError as error:\n        # They are probably different lengths or another numpy indexing/setting issue\n        if len(self.coords) == 0:  # There are no coords. Use the .set() mechanism\n            self.set(new_coords)\n        else:\n            raise ValueError(\n                f\"The selected indices aren't the same shape as the 'new_coords': {error}\")\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.Coordinates.set","title":"set","text":"<pre><code>set(coords: ndarray | list[list[float]])\n</code></pre> <p>Set self.coords to the provided coordinates</p> <p>Parameters:</p> <ul> <li> <code>coords</code>             (<code>ndarray | list[list[float]]</code>)         \u2013          <p>The coordinate values to set</p> </li> </ul> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def set(self, coords: np.ndarray | list[list[float]]):\n    \"\"\"Set self.coords to the provided coordinates\n\n    Args:\n        coords: The coordinate values to set\n    \"\"\"\n    self.coords = coords\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.guide_superposition","title":"guide_superposition","text":"<pre><code>guide_superposition(fixed_coords: ndarray, moving_coords: ndarray, number_of_points: int = 4) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>TTakes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords</p> <p>This function implements a more general variant of the method from: R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216 This version has been augmented slightly. The version in the original paper only considers rotation and translation and does not allow the coordinates of either object to be rescaled (multiplication by a scalar). (Additional documentation can be found at https://pypi.org/project/superpose3d/ )</p> <p>The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation. Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</p> <p>MIT License. Copyright (c) 2016, Andrew Jewett Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Parameters:</p> <ul> <li> <code>fixed_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'frozen' object</p> </li> <li> <code>moving_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'mobile' object</p> </li> <li> <code>number_of_points</code>             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>The number of points included in the coordinate sets</p> </li> </ul> <p>Returns:     rotation, translation_vector</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef guide_superposition(fixed_coords: np.ndarray, moving_coords: np.ndarray, number_of_points: int = 4) \\\n        -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"TTakes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation\n    operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations\n    should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords\n\n    This function implements a more general variant of the method from:\n    R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216\n    This version has been augmented slightly. The version in the original paper only considers rotation and translation\n    and does not allow the coordinates of either object to be rescaled (multiplication by a scalar).\n    (Additional documentation can be found at https://pypi.org/project/superpose3d/ )\n\n    The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries\n    form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation.\n    Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n\n    MIT License. Copyright (c) 2016, Andrew Jewett\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n    documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n    permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n    Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n    WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS\n    OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n    OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    Args:\n        fixed_coords: The coordinates for the 'frozen' object\n        moving_coords: The coordinates for the 'mobile' object\n        number_of_points: The number of points included in the coordinate sets\n    Returns:\n        rotation, translation_vector\n    \"\"\"\n    # number_of_points = fixed_coords.shape[0]\n    # if number_of_points != moving_coords.shape[0]:\n    #     raise ValueError(f'{guide_superposition.__name__}: Inputs should have the same size. '\n    #                      f'Input 1={number_of_points}, 2={moving_coords.shape[0]}')\n\n    # Find the center of mass of each object:\n    # center_of_mass_fixed = fixed_coords.mean(axis=0)\n    # center_of_mass_moving = moving_coords.mean(axis=0)\n    center_of_mass_fixed = fixed_coords.sum(axis=0)\n    center_of_mass_moving = moving_coords.sum(axis=0)\n    center_of_mass_fixed /= number_of_points\n    center_of_mass_moving /= number_of_points\n\n    # Subtract the centers-of-mass from the original coordinates for each object\n    # Translate the center of mass to the origin\n    fixed_coords_at_origin = fixed_coords - center_of_mass_fixed\n    moving_coords_at_origin = moving_coords - center_of_mass_moving\n\n    # Calculate the \"m\" array from the Diamond paper (equation 16)\n    m = moving_coords_at_origin.T @ fixed_coords_at_origin\n\n    # Calculate \"v\" (equation 18)\n    # v = np.empty(3)\n    # v[0] = m[1, 2] - m[2, 1]\n    # v[1] = m[2, 0] - m[0, 2]\n    # v[2] = m[0, 1] - m[1, 0]\n    v = [m[1, 2] - m[2, 1], m[2, 0] - m[0, 2], m[0, 1] - m[1, 0]]\n\n    # Calculate \"P\" (equation 22)\n    matrix_p = np.zeros((4, 4))\n    # Calculate \"q\" (equation 17)\n    # q = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[:3, :3] = m + m.T - 2*identity_matrix*np.trace(m)\n    # matrix_p[:3, :3] = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[3, :3] = v\n    matrix_p[:3, 3] = v\n    # [[ q[0, 0] q[0, 1] q[0, 2] v[0] ]\n    #  [ q[1, 0] q[1, 1] q[1, 2] v[1] ]\n    #  [ q[2, 0] q[2, 1] q[2, 2] v[2] ]\n    #  [ v[0]    v[1]    v[2]    0    ]]\n\n    # Calculate \"p\" - optimal_quat\n    # \"p\" contains the optimal rotation (in backwards-quaternion format)\n    # (Note: A discussion of various quaternion conventions is included below)\n    # try:\n    # The eigenvalues/eigenvector are returned as 1D array in ascending order; largest is last\n    a_eigenvals, aa_eigenvects = np.linalg.eigh(matrix_p)\n    # except np.linalg.LinAlgError:\n    #     singular = True  # I have never seen this happen\n    # Pull out the largest magnitude\n    optimal_quat = aa_eigenvects[:, -1]\n    # normalize the vector\n    # (It should be normalized already, but just in case it is not, do it again)\n    # optimal_quat /= np.linalg.norm(optimal_quat)\n\n    # Calculate the rotation matrix corresponding to \"optimal_quat\" which is in scipy quaternion format\n    # \"\"\"\n    rotation_matrix = np.empty((3, 3))\n    quat0, quat1, quat2, quat3 = optimal_quat\n    quat2_0, quat2_1, quat2_2, quat2_3 = optimal_quat**2\n    rotation_matrix[0, 1] = quat0*quat1 - quat2*quat3  # 2*( )\n    rotation_matrix[1, 0] = quat0*quat1 + quat2*quat3  # 2*( )\n    rotation_matrix[1, 2] = quat1*quat2 - quat0*quat3  # 2*( )\n    rotation_matrix[2, 1] = quat1*quat2 + quat0*quat3  # 2*( )\n    rotation_matrix[0, 2] = quat0*quat2 + quat1*quat3  # 2*( )\n    rotation_matrix[2, 0] = quat0*quat2 - quat1*quat3  # 2*( )\n    rotation_matrix *= 2\n    rotation_matrix[0, 0] = quat2_0 - quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[1, 1] = -quat2_0 + quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[2, 2] = -quat2_0 - quat2_1 + quat2_2 + quat2_3\n    # rotation_matrix[0, 0] = quat0*quat0 - quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[1, 1] = -quat0*quat0 + quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[2, 2] = -quat0*quat0 - quat1*quat1 + quat2*quat2 + quat3*quat3\n    \"\"\"\n    # Alternatively, in modern python versions, this code also works:\n    rotation_matrix = Rotation.from_quat(optimal_quat).as_matrix()\n    \"\"\"\n    # input(f'{rotation_matrix_} {rotation_matrix}')\n    # input(rotation_matrix_ == rotation_matrix)\n    # \"\"\"\n    # Finally compute the RMSD between the two coordinate sets:\n    # First compute E0 from equation 24 of the paper\n    # e0 = np.sum((fixed_coords_at_origin - moving_coords_at_origin) ** 2)\n    # sum_sqr_dist = max(0, ((fixed_coords_at_origin-moving_coords_at_origin) ** 2).sum() - 2.*pPp)\n\n    # Lastly, calculate the translational offset:\n    # Recall that:\n    # RMSD=sqrt((\u03a3_i  w_i * |X_i - (\u03a3_j c*R_ij*x_j + T_i))|^2) / (\u03a3_j w_j))\n    #    =sqrt((\u03a3_i  w_i * |X_i - x_i'|^2) / (\u03a3_j w_j))\n    #  where\n    # x_i' = \u03a3_j c*R_ij*x_j + T_i\n    #      = Xcm_i + c*R_ij*(x_j - xcm_j)\n    #  and Xcm and xcm = center_of_mass for the frozen and mobile point clouds\n    #                  = center_of_mass_fixed[]       and       center_of_mass_moving[],  respectively\n    # Hence:\n    #  T_i = Xcm_i - \u03a3_j c*R_ij*xcm_j  =  a_translate[i]\n\n    # a_translate = center_of_mass_fixed - np.matmul(c * rotation_matrix, center_of_mass_moving).T.reshape(3,)\n\n    # Calculate the translation\n    translation = center_of_mass_fixed - rotation_matrix@center_of_mass_moving\n\n    return rotation_matrix, translation\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.superposition3d","title":"superposition3d","text":"<pre><code>superposition3d(fixed_coords: ndarray, moving_coords: ndarray) -&gt; tuple[float, ndarray, ndarray]\n</code></pre> <p>Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords</p> <p>This function implements a more general variant of the method from: R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216 This version has been augmented slightly. The version in the original paper only considers rotation and translation and does not allow the coordinates of either object to be rescaled (multiplication by a scalar). (Additional documentation can be found at https://pypi.org/project/superpose3d/ )</p> <p>The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation. Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</p> <p>MIT License. Copyright (c) 2016, Andrew Jewett Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Parameters:</p> <ul> <li> <code>fixed_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'frozen' object</p> </li> <li> <code>moving_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'mobile' object</p> </li> </ul> <p>Raises:     ValueError: If coordinates are not the same length Returns:     rmsd, rotation, translation_vector</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef superposition3d(fixed_coords: np.ndarray, moving_coords: np.ndarray) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation\n    operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations\n    should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords\n\n    This function implements a more general variant of the method from:\n    R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216\n    This version has been augmented slightly. The version in the original paper only considers rotation and translation\n    and does not allow the coordinates of either object to be rescaled (multiplication by a scalar).\n    (Additional documentation can be found at https://pypi.org/project/superpose3d/ )\n\n    The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries\n    form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation.\n    Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n\n    MIT License. Copyright (c) 2016, Andrew Jewett\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n    documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n    permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n    Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n    WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS\n    OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n    OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    Args:\n        fixed_coords: The coordinates for the 'frozen' object\n        moving_coords: The coordinates for the 'mobile' object\n    Raises:\n        ValueError: If coordinates are not the same length\n    Returns:\n        rmsd, rotation, translation_vector\n    \"\"\"\n    number_of_points = fixed_coords.shape[0]\n    if number_of_points != moving_coords.shape[0]:\n        raise ValueError('superposition3d: Inputs should have the same size')  # . '\n        #                  f'Input 1={number_of_points}, 2={moving_coords.shape[0]}')\n\n    # convert weights into array\n    # if a_weights is None or len(a_weights) == 0:\n    # a_weights = np.full((number_of_points, 1), 1.)\n    # sum_weights = float(number_of_points)\n    # else:  # reshape a_eights so multiplications are done column-wise\n    #     a_weights = np.array(a_weights).reshape(number_of_points, 1)\n    #     sum_weights = np.sum(a_weights, axis=0)\n\n    # Find the center of mass of each object:\n    center_of_mass_fixed = fixed_coords.sum(axis=0)\n    center_of_mass_moving = moving_coords.sum(axis=0)\n\n    # Subtract the centers-of-mass from the original coordinates for each object\n    # if sum_weights != 0:\n    # try:\n    center_of_mass_fixed /= number_of_points\n    center_of_mass_moving /= number_of_points\n    # except ZeroDivisionError:\n    #     pass  # The weights are a total of zero which is allowed algorithmically, but not possible\n\n    # Translate the center of mass to the origin\n    fixed_coords_at_origin = fixed_coords - center_of_mass_fixed\n    moving_coords_at_origin = moving_coords - center_of_mass_moving\n\n    # Calculate the \"m\" array from the Diamond paper (equation 16)\n    m = moving_coords_at_origin.T @ fixed_coords_at_origin\n\n    # Calculate \"v\" (equation 18)\n    # v = np.empty(3)\n    # v[0] = m[1, 2] - m[2, 1]\n    # v[1] = m[2, 0] - m[0, 2]\n    # v[2] = m[0, 1] - m[1, 0]\n    v = [m[1, 2] - m[2, 1], m[2, 0] - m[0, 2], m[0, 1] - m[1, 0]]\n\n    # Calculate \"P\" (equation 22)\n    matrix_p = np.zeros((4, 4))\n    # Calculate \"q\" (equation 17)\n    # q = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[:3, :3] = m + m.T - 2*identity_matrix*np.trace(m)\n    # matrix_p[:3, :3] = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[3, :3] = v\n    matrix_p[:3, 3] = v\n    # [[ q[0, 0] q[0, 1] q[0, 2] v[0] ]\n    #  [ q[1, 0] q[1, 1] q[1, 2] v[1] ]\n    #  [ q[2, 0] q[2, 1] q[2, 2] v[2] ]\n    #  [ v[0]    v[1]    v[2]    0    ]]\n\n    # Calculate \"p\" - optimal_quat\n    # \"p\" contains the optimal rotation (in backwards-quaternion format)\n    # (Note: A discussion of various quaternion conventions is included below)\n    if number_of_points &lt; 2:\n        # Specify the default values for p, pPp\n        optimal_quat = np.array([0., 0., 0., 1.])  # p = [0,0,0,1]    default value\n        pPp = 0.  # = p^T * P * p    (zero by default)\n    else:\n        # try:\n        # The eigenvalues/eigenvector are returned as 1D array in ascending order; largest is last\n        a_eigenvals, aa_eigenvects = np.linalg.eigh(matrix_p)\n        # except np.linalg.LinAlgError:\n        #     singular = True  # I have never seen this happen\n        # Pull out the largest magnitude\n        pPp = a_eigenvals[-1]\n        optimal_quat = aa_eigenvects[:, -1]\n        # normalize the vector\n        # (It should be normalized already, but just in case it is not, do it again)\n        # optimal_quat /= np.linalg.norm(optimal_quat)\n\n    # Calculate the rotation matrix corresponding to \"optimal_quat\" which is in scipy quaternion format\n    # \"\"\"\n    rotation_matrix = np.empty((3, 3))\n    quat0, quat1, quat2, quat3 = optimal_quat\n    quat2_0, quat2_1, quat2_2, quat2_3 = optimal_quat**2\n    rotation_matrix[0, 1] = quat0*quat1 - quat2*quat3  # 2*( )\n    rotation_matrix[1, 0] = quat0*quat1 + quat2*quat3  # 2*( )\n    rotation_matrix[1, 2] = quat1*quat2 - quat0*quat3  # 2*( )\n    rotation_matrix[2, 1] = quat1*quat2 + quat0*quat3  # 2*( )\n    rotation_matrix[0, 2] = quat0*quat2 + quat1*quat3  # 2*( )\n    rotation_matrix[2, 0] = quat0*quat2 - quat1*quat3  # 2*( )\n    rotation_matrix *= 2\n    rotation_matrix[0, 0] = quat2_0 - quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[1, 1] = -quat2_0 + quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[2, 2] = -quat2_0 - quat2_1 + quat2_2 + quat2_3\n    # rotation_matrix[0, 0] = quat0*quat0 - quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[1, 1] = -quat0*quat0 + quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[2, 2] = -quat0*quat0 - quat1*quat1 + quat2*quat2 + quat3*quat3\n    \"\"\"\n    # Alternatively, in modern python versions, this code also works:\n    rotation_matrix = Rotation.from_quat(optimal_quat).as_matrix()\n    \"\"\n    input(f'{rotation_matrix_.as_matrix()} {rotation_matrix}')\n    input(np.allclose(rotation_matrix_.as_matrix(), rotation_matrix))\n    # \"\"\"\n    # Finally compute the RMSD between the two coordinate sets:\n    # First compute E0 from equation 24 of the paper\n    # e0 = np.sum((fixed_coords_at_origin - moving_coords_at_origin) ** 2)\n    # sum_sqr_dist = max(0, ((fixed_coords_at_origin-moving_coords_at_origin) ** 2).sum() - 2.*pPp)\n\n    # if sum_weights != 0.:\n    # try:\n    rmsd = np.sqrt(max(0, ((fixed_coords_at_origin-moving_coords_at_origin)**2).sum() - 2.*pPp) / number_of_points)\n    # except ZeroDivisionError:\n    #     rmsd = 0.  # The weights are a total of zero which is allowed algorithmically, but not possible\n\n    # Lastly, calculate the translational offset:\n    # Recall that:\n    # RMSD=sqrt((\u03a3_i  w_i * |X_i - (\u03a3_j c*R_ij*x_j + T_i))|^2) / (\u03a3_j w_j))\n    #    =sqrt((\u03a3_i  w_i * |X_i - x_i'|^2) / (\u03a3_j w_j))\n    #  where\n    # x_i' = \u03a3_j c*R_ij*x_j + T_i\n    #      = Xcm_i + c*R_ij*(x_j - xcm_j)\n    #  and Xcm and xcm = center_of_mass for the frozen and mobile point clouds\n    #                  = center_of_mass_fixed[]       and       center_of_mass_moving[],  respectively\n    # Hence:\n    #  T_i = Xcm_i - \u03a3_j c*R_ij*xcm_j  =  a_translate[i]\n\n    # a_translate = center_of_mass_fixed - np.matmul(c * rotation_matrix, center_of_mass_moving).T.reshape(3,)\n\n    # Calculate the translation\n    translation = center_of_mass_fixed - rotation_matrix@center_of_mass_moving\n\n    return rmsd, rotation_matrix, translation\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.superposition3d_quat","title":"superposition3d_quat","text":"<pre><code>superposition3d_quat(fixed_coords: ndarray, moving_coords: ndarray) -&gt; tuple[float, ndarray, ndarray]\n</code></pre> <p>Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords</p> <p>This function implements a more general variant of the method from: R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216 This version has been augmented slightly. The version in the original paper only considers rotation and translation and does not allow the coordinates of either object to be rescaled (multiplication by a scalar). (Additional documentation can be found at https://pypi.org/project/superpose3d/ )</p> <p>The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation. Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</p> <p>MIT License. Copyright (c) 2016, Andrew Jewett Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Parameters:</p> <ul> <li> <code>fixed_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'frozen' object</p> </li> <li> <code>moving_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'mobile' object</p> </li> </ul> <p>Raises:     ValueError: If coordinates are not the same length Returns:     rmsd, quaternion, translation_vector</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef superposition3d_quat(fixed_coords: np.ndarray, moving_coords: np.ndarray) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotation and translation\n    operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations\n    should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords\n\n    This function implements a more general variant of the method from:\n    R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216\n    This version has been augmented slightly. The version in the original paper only considers rotation and translation\n    and does not allow the coordinates of either object to be rescaled (multiplication by a scalar).\n    (Additional documentation can be found at https://pypi.org/project/superpose3d/ )\n\n    The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries\n    form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation.\n    Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n\n    MIT License. Copyright (c) 2016, Andrew Jewett\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n    documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n    permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n    Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n    WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS\n    OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n    OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    Args:\n        fixed_coords: The coordinates for the 'frozen' object\n        moving_coords: The coordinates for the 'mobile' object\n    Raises:\n        ValueError: If coordinates are not the same length\n    Returns:\n        rmsd, quaternion, translation_vector\n    \"\"\"\n    number_of_points = fixed_coords.shape[0]\n    if number_of_points != moving_coords.shape[0]:\n        raise ValueError('superposition3d: Inputs should have the same size')  # . '\n        #                  f'Input 1={number_of_points}, 2={moving_coords.shape[0]}')\n\n    # convert weights into array\n    # if a_weights is None or len(a_weights) == 0:\n    # a_weights = np.full((number_of_points, 1), 1.)\n    # sum_weights = float(number_of_points)\n    # else:  # reshape a_eights so multiplications are done column-wise\n    #     a_weights = np.array(a_weights).reshape(number_of_points, 1)\n    #     sum_weights = np.sum(a_weights, axis=0)\n\n    # Find the center of mass of each object:\n    center_of_mass_fixed = fixed_coords.sum(axis=0)\n    center_of_mass_moving = moving_coords.sum(axis=0)\n\n    # Subtract the centers-of-mass from the original coordinates for each object\n    # if sum_weights != 0:\n    # try:\n    center_of_mass_fixed /= number_of_points\n    center_of_mass_moving /= number_of_points\n    # except ZeroDivisionError:\n    #     pass  # The weights are a total of zero which is allowed algorithmically, but not possible\n\n    # Translate the center of mass to the origin\n    fixed_coords_at_origin = fixed_coords - center_of_mass_fixed\n    moving_coords_at_origin = moving_coords - center_of_mass_moving\n\n    # Calculate the \"m\" array from the Diamond paper (equation 16)\n    m = moving_coords_at_origin.T @ fixed_coords_at_origin\n\n    # Calculate \"v\" (equation 18)\n    # v = np.empty(3)\n    # v[0] = m[1, 2] - m[2, 1]\n    # v[1] = m[2, 0] - m[0, 2]\n    # v[2] = m[0, 1] - m[1, 0]\n    v = [m[1, 2] - m[2, 1], m[2, 0] - m[0, 2], m[0, 1] - m[1, 0]]\n\n    # Calculate \"P\" (equation 22)\n    matrix_p = np.zeros((4, 4))\n    # Calculate \"q\" (equation 17)\n    # q = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[:3, :3] = m + m.T - 2*identity_matrix*np.trace(m)\n    # matrix_p[:3, :3] = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[3, :3] = v\n    matrix_p[:3, 3] = v\n    # [[ q[0, 0] q[0, 1] q[0, 2] v[0] ]\n    #  [ q[1, 0] q[1, 1] q[1, 2] v[1] ]\n    #  [ q[2, 0] q[2, 1] q[2, 2] v[2] ]\n    #  [ v[0]    v[1]    v[2]    0    ]]\n\n    # Calculate \"p\" - optimal_quat\n    # \"p\" contains the optimal rotation (in backwards-quaternion format)\n    # (Note: A discussion of various quaternion conventions is included below)\n    if number_of_points &lt; 2:\n        # Specify the default values for p, pPp\n        optimal_quat = np.array([0., 0., 0., 1.])  # p = [0,0,0,1]    default value\n        pPp = 0.  # = p^T * P * p    (zero by default)\n    else:\n        # try:\n        # The eigenvalues/eigenvector are returned as 1D array in ascending order; largest is last\n        a_eigenvals, aa_eigenvects = np.linalg.eigh(matrix_p)\n        # except np.linalg.LinAlgError:\n        #     singular = True  # I have never seen this happen\n        # Pull out the largest magnitude\n        pPp = a_eigenvals[-1]\n        optimal_quat = aa_eigenvects[:, -1]\n        # normalize the vector\n        # (It should be normalized already, but just in case it is not, do it again)\n        # optimal_quat /= np.linalg.norm(optimal_quat)\n\n    # Calculate the rotation matrix corresponding to \"optimal_quat\" which is in scipy quaternion format\n    # \"\"\"\n    rotation_matrix = np.empty((3, 3))\n    quat0, quat1, quat2, quat3 = optimal_quat\n    quat2_0, quat2_1, quat2_2, quat2_3 = optimal_quat**2\n    rotation_matrix[0, 1] = quat0*quat1 - quat2*quat3  # 2*( )\n    rotation_matrix[1, 0] = quat0*quat1 + quat2*quat3  # 2*( )\n    rotation_matrix[1, 2] = quat1*quat2 - quat0*quat3  # 2*( )\n    rotation_matrix[2, 1] = quat1*quat2 + quat0*quat3  # 2*( )\n    rotation_matrix[0, 2] = quat0*quat2 + quat1*quat3  # 2*( )\n    rotation_matrix[2, 0] = quat0*quat2 - quat1*quat3  # 2*( )\n    rotation_matrix *= 2\n    rotation_matrix[0, 0] = quat2_0 - quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[1, 1] = -quat2_0 + quat2_1 - quat2_2 + quat2_3\n    rotation_matrix[2, 2] = -quat2_0 - quat2_1 + quat2_2 + quat2_3\n    # rotation_matrix[0, 0] = quat0*quat0 - quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[1, 1] = -quat0*quat0 + quat1*quat1 - quat2*quat2 + quat3*quat3\n    # rotation_matrix[2, 2] = -quat0*quat0 - quat1*quat1 + quat2*quat2 + quat3*quat3\n    \"\"\"\n    # Alternatively, in modern python versions, this code also works:\n    rotation_matrix = Rotation.from_quat(optimal_quat).as_matrix()\n    \"\"\n    input(f'{rotation_matrix_.as_matrix()} {rotation_matrix}')\n    input(np.allclose(rotation_matrix_.as_matrix(), rotation_matrix))\n    # \"\"\"\n    # Finally compute the RMSD between the two coordinate sets:\n    # First compute E0 from equation 24 of the paper\n    # e0 = np.sum((fixed_coords_at_origin - moving_coords_at_origin) ** 2)\n    # sum_sqr_dist = max(0, ((fixed_coords_at_origin-moving_coords_at_origin) ** 2).sum() - 2.*pPp)\n\n    # if sum_weights != 0.:\n    # try:\n    rmsd = np.sqrt(max(0, ((fixed_coords_at_origin-moving_coords_at_origin)**2).sum() - 2.*pPp) / number_of_points)\n    # except ZeroDivisionError:\n    #     rmsd = 0.  # The weights are a total of zero which is allowed algorithmically, but not possible\n\n    # Lastly, calculate the translational offset:\n    # Recall that:\n    # RMSD=sqrt((\u03a3_i  w_i * |X_i - (\u03a3_j c*R_ij*x_j + T_i))|^2) / (\u03a3_j w_j))\n    #    =sqrt((\u03a3_i  w_i * |X_i - x_i'|^2) / (\u03a3_j w_j))\n    #  where\n    # x_i' = \u03a3_j c*R_ij*x_j + T_i\n    #      = Xcm_i + c*R_ij*(x_j - xcm_j)\n    #  and Xcm and xcm = center_of_mass for the frozen and mobile point clouds\n    #                  = center_of_mass_fixed[]       and       center_of_mass_moving[],  respectively\n    # Hence:\n    #  T_i = Xcm_i - \u03a3_j c*R_ij*xcm_j  =  a_translate[i]\n\n    # a_translate = center_of_mass_fixed - np.matmul(c * rotation_matrix, center_of_mass_moving).T.reshape(3,)\n\n    # Calculate the translation\n    translation = center_of_mass_fixed - rotation_matrix@center_of_mass_moving\n    # The p array is a quaternion that uses this convention:\n    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_quat.html\n    # However it seems that the following convention is much more popular:\n    # https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n    # https://mathworld.wolfram.com/Quaternion.html\n    # So I return \"q\" (a version of \"p\" using the more popular convention).\n    # rotation_matrix = np.array([p[3], p[0], p[1], p[2]])\n    # KM: Disregard above, I am using the scipy version for python continuity which returns X, Y, Z, W\n    return rmsd, optimal_quat, translation\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.superposition3d_weighted","title":"superposition3d_weighted","text":"<pre><code>superposition3d_weighted(fixed_coords: ndarray, moving_coords: ndarray, a_weights: ndarray = None, quaternion: bool = False) -&gt; tuple[float, ndarray, ndarray]\n</code></pre> <p>Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotations, translations, and (optionally) rescale operations to minimize the root mean squared distance (RMSD) between them. The found transformation operations should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords</p> <p>This function implements a more general variant of the method from: R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216 This version has been augmented slightly. The version in the original paper only considers rotation and translation and does not allow the coordinates of either object to be rescaled (multiplication by a scalar). (Additional documentation can be found at https://pypi.org/project/superpose3d/ )</p> <p>The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation. Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation</p> <p>MIT License. Copyright (c) 2016, Andrew Jewett Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Parameters:</p> <ul> <li> <code>fixed_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'frozen' object</p> </li> <li> <code>moving_coords</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates for the 'mobile' object</p> </li> <li> <code>a_weights</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>Weights for the calculation of RMSD</p> </li> <li> <code>quaternion</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to report the rotation angle and axis in Scipy.Rotation quaternion format</p> </li> </ul> <p>Raises:     ValueError: If coordinates are not the same length Returns:     rmsd, rotation/quaternion_matrix, translation_vector</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def superposition3d_weighted(fixed_coords: np.ndarray, moving_coords: np.ndarray, a_weights: np.ndarray = None,\n                             quaternion: bool = False) -&gt; tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"Takes two xyz coordinate sets (same length), and attempts to superimpose them using rotations, translations,\n    and (optionally) rescale operations to minimize the root mean squared distance (RMSD) between them. The found\n    transformation operations should be applied to the \"moving_coords\" to place them in the setting of the fixed_coords\n\n    This function implements a more general variant of the method from:\n    R. Diamond, (1988) \"A Note on the Rotational Superposition Problem\", Acta Cryst. A44, pp. 211-216\n    This version has been augmented slightly. The version in the original paper only considers rotation and translation\n    and does not allow the coordinates of either object to be rescaled (multiplication by a scalar).\n    (Additional documentation can be found at https://pypi.org/project/superpose3d/ )\n\n    The quaternion_matrix has the last entry storing cos(\u03b8/2) (where \u03b8 is the rotation angle). The first 3 entries\n    form a vector (of length sin(\u03b8/2)), pointing along the axis of rotation.\n    Details: https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n\n    MIT License. Copyright (c) 2016, Andrew Jewett\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n    documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the\n    rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n    permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the\n    Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n    WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS\n    OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n    OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    Args:\n        fixed_coords: The coordinates for the 'frozen' object\n        moving_coords: The coordinates for the 'mobile' object\n        a_weights: Weights for the calculation of RMSD\n        quaternion: Whether to report the rotation angle and axis in Scipy.Rotation quaternion format\n    Raises:\n        ValueError: If coordinates are not the same length\n    Returns:\n        rmsd, rotation/quaternion_matrix, translation_vector\n    \"\"\"\n    number_of_points = len(fixed_coords)\n    if number_of_points != len(moving_coords):\n        raise ValueError(f'{superposition3d.__name__}: Inputs should have the same size. '\n                         f'Input 1={number_of_points}, 2={moving_coords.shape[0]}')\n\n    # convert weights into array\n    if a_weights is None or len(a_weights) == 0:\n        a_weights = np.full((number_of_points, 1), 1.)\n        sum_weights = float(number_of_points)\n    else:  # reshape a_eights so multiplications are done column-wise\n        a_weights = np.array(a_weights).reshape(number_of_points, 1)\n        sum_weights = np.sum(a_weights, axis=0)\n\n    # Find the center of mass of each object:\n    center_of_mass_fixed = np.sum(fixed_coords * a_weights, axis=0)\n    center_of_mass_moving = np.sum(moving_coords * a_weights, axis=0)\n\n    # Subtract the centers-of-mass from the original coordinates for each object\n    # if sum_weights != 0:\n    try:\n        center_of_mass_fixed /= sum_weights\n        center_of_mass_moving /= sum_weights\n    except ZeroDivisionError:\n        pass  # the weights are a total of zero which is allowed algorithmically, but not possible\n\n    aa_xf = fixed_coords - center_of_mass_fixed\n    aa_xm = moving_coords - center_of_mass_moving\n\n    # Calculate the \"m\" array from the Diamond paper (equation 16)\n    m = np.matmul(aa_xm.T, (aa_xf * a_weights))\n\n    # Calculate \"v\" (equation 18)\n    # v = np.empty(3)\n    # v[0] = m[1, 2] - m[2, 1]\n    # v[1] = m[2, 0] - m[0, 2]\n    # v[2] = m[0, 1] - m[1, 0]\n    v = [m[1, 2] - m[2, 1], m[2, 0] - m[0, 2], m[0, 1] - m[1, 0]]\n\n    # Calculate \"P\" (equation 22)\n    matrix_p = np.zeros((4, 4))\n    # Calculate \"q\" (equation 17)\n    # q = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[:3, :3] = m + m.T - 2*identity_matrix*np.trace(m)\n    # matrix_p[:3, :3] = m + m.T - 2*utils.symmetry.identity_matrix*np.trace(m)\n    matrix_p[3, :3] = v\n    matrix_p[:3, 3] = v\n    # [[ q[0, 0] q[0, 1] q[0, 2] v[0] ]\n    #  [ q[1, 0] q[1, 1] q[1, 2] v[1] ]\n    #  [ q[2, 0] q[2, 1] q[2, 2] v[2] ]\n    #  [ v[0]    v[1]    v[2]    0    ]]\n\n    # Calculate \"p\" - optimal_quat\n    # \"p\" contains the optimal rotation (in backwards-quaternion format)\n    # (Note: A discussion of various quaternion conventions is included below)\n    if number_of_points &lt; 2:\n        # Specify the default values for p, pPp\n        optimal_quat = np.array([0., 0., 0., 1.])  # p = [0,0,0,1]    default value\n        pPp = 0.  # = p^T * P * p    (zero by default)\n    else:\n        # try:\n        a_eigenvals, aa_eigenvects = np.linalg.eigh(matrix_p)\n        # except np.linalg.LinAlgError:\n        #     singular = True  # I have never seen this happen\n        pPp = np.max(a_eigenvals)\n        optimal_quat = aa_eigenvects[:, np.argmax(a_eigenvals)]  # pull out the largest magnitude eigenvector\n        # normalize the vector\n        # (It should be normalized already, but just in case it is not, do it again)\n        optimal_quat /= np.linalg.norm(optimal_quat)\n\n    # Calculate the rotation matrix corresponding to \"optimal_quat\" which is in scipy quaternion format\n    \"\"\"\n    rotation_matrix = np.empty((3, 3))\n    rotation_matrix[0][0] = (quat0*quat0)-(quat1*quat1)\n                     -(quat2*quat2)+(quat3*quat3)\n    rotation_matrix[1][1] = -(quat0*quat0)+(quat1*quat1)\n                      -(quat2*quat2)+(quat3*quat3)\n    rotation_matrix[2][2] = -(quat0*quat0)-(quat1*quat1)\n                      +(quat2*quat2)+(quat3*quat3)\n    rotation_matrix[0][1] = 2*(quat0*quat1 - quat2*quat3)\n    rotation_matrix[1][0] = 2*(quat0*quat1 + quat2*quat3)\n    rotation_matrix[1][2] = 2*(quat1*quat2 - quat0*quat3)\n    rotation_matrix[2][1] = 2*(quat1*quat2 + quat0*quat3)\n    rotation_matrix[0][2] = 2*(quat0*quat2 + quat1*quat3)\n    rotation_matrix[2][0] = 2*(quat0*quat2 - quat1*quat3)\n    \"\"\"\n    # Alternatively, in modern python versions, this code also works:\n    rotation_matrix = Rotation.from_quat(optimal_quat).as_matrix()\n\n    # Finally compute the RMSD between the two coordinate sets:\n    # First compute E0 from equation 24 of the paper\n    # e0 = np.sum((aa_xf - aa_xm) ** 2)\n    # sum_sqr_dist = max(0, ((aa_xf-aa_xm) ** 2).sum() - 2.*pPp)\n\n    # if sum_weights != 0.:\n    try:\n        rmsd = np.sqrt(max(0, ((aa_xf-aa_xm) ** 2).sum() - 2.*pPp) / sum_weights)\n    except ZeroDivisionError:\n        rmsd = 0.  # the weights are a total of zero which is allowed algorithmically, but not possible\n\n    # Lastly, calculate the translational offset:\n    # Recall that:\n    # RMSD=sqrt((\u03a3_i  w_i * |X_i - (\u03a3_j c*R_ij*x_j + T_i))|^2) / (\u03a3_j w_j))\n    #    =sqrt((\u03a3_i  w_i * |X_i - x_i'|^2) / (\u03a3_j w_j))\n    #  where\n    # x_i' = \u03a3_j c*R_ij*x_j + T_i\n    #      = Xcm_i + c*R_ij*(x_j - xcm_j)\n    #  and Xcm and xcm = center_of_mass for the frozen and mobile point clouds\n    #                  = center_of_mass_fixed[]       and       center_of_mass_moving[],  respectively\n    # Hence:\n    #  T_i = Xcm_i - \u03a3_j c*R_ij*xcm_j  =  a_translate[i]\n\n    # a_translate = center_of_mass_fixed - np.matmul(c * aa_rotate, center_of_mass_moving).T.reshape(3,)\n\n    # return rmsd, aa_rotate, center_of_mass_fixed - np.matmul(aa_rotate, center_of_mass_moving).T.reshape(3,)\n    # Calculate the translation\n    translation = center_of_mass_fixed - np.matmul(rotation_matrix, center_of_mass_moving)\n    if quaternion:  # does the caller want the quaternion?\n        # The p array is a quaternion that uses this convention:\n        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.Rotation.from_quat.html\n        # However it seems that the following convention is much more popular:\n        # https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation\n        # https://mathworld.wolfram.com/Quaternion.html\n        # So I return \"q\" (a version of \"p\" using the more popular convention).\n        # rotation_matrix = np.array([p[3], p[0], p[1], p[2]])\n        # KM: Disregard above, I am using the scipy version for python continuity\n        return rmsd, optimal_quat, translation\n    else:\n        return rmsd, rotation_matrix, translation\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.transform_coordinates","title":"transform_coordinates","text":"<pre><code>transform_coordinates(coords: ndarray | Iterable, rotation: ndarray | Iterable = None, translation: ndarray | Iterable | int | float = None, rotation2: ndarray | Iterable = None, translation2: ndarray | Iterable | int | float = None) -&gt; ndarray\n</code></pre> <p>Take a set of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the order of operations as: rotation, translation, rotation2, translation2</p> <p>Parameters:</p> <ul> <li> <code>coords</code>             (<code>ndarray | Iterable</code>)         \u2013          <p>The coordinates to transform, can be shape (number of coordinates, 3)</p> </li> <li> <code>rotation</code>             (<code>ndarray | Iterable</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected general rotation matrix shape (3, 3)</p> </li> <li> <code>translation</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected shape (3)</p> </li> <li> <code>rotation2</code>             (<code>ndarray | Iterable</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected general rotation matrix shape (3, 3)</p> </li> <li> <code>translation2</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected shape (3)</p> </li> </ul> <p>Returns:     The transformed coordinate set with the same shape as the original</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def transform_coordinates(coords: np.ndarray | Iterable, rotation: np.ndarray | Iterable = None,\n                          translation: np.ndarray | Iterable | int | float = None,\n                          rotation2: np.ndarray | Iterable = None,\n                          translation2: np.ndarray | Iterable | int | float = None) -&gt; np.ndarray:\n    \"\"\"Take a set of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the order of\n    operations as: rotation, translation, rotation2, translation2\n\n    Args:\n        coords: The coordinates to transform, can be shape (number of coordinates, 3)\n        rotation: The first rotation to apply, expected general rotation matrix shape (3, 3)\n        translation: The first translation to apply, expected shape (3)\n        rotation2: The second rotation to apply, expected general rotation matrix shape (3, 3)\n        translation2: The second translation to apply, expected shape (3)\n    Returns:\n        The transformed coordinate set with the same shape as the original\n    \"\"\"\n    new_coords = coords.copy()\n\n    if rotation is not None:\n        np.matmul(new_coords, np.transpose(rotation), out=new_coords)\n\n    if translation is not None:\n        new_coords += translation  # No array allocation, sets in place\n\n    if rotation2 is not None:\n        np.matmul(new_coords, np.transpose(rotation2), out=new_coords)\n\n    if translation2 is not None:\n        new_coords += translation2\n\n    return coords\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.transform_coordinate_sets_with_broadcast","title":"transform_coordinate_sets_with_broadcast","text":"<pre><code>transform_coordinate_sets_with_broadcast(coord_sets: ndarray, rotation: ndarray = None, translation: ndarray | Iterable | int | float = None, rotation2: ndarray = None, translation2: ndarray | Iterable | int | float = None) -&gt; ndarray\n</code></pre> <p>Take stacked sets of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the order of operations as: rotation, translation, rotation2, translation2. Non-efficient memory use</p> <p>Parameters:</p> <ul> <li> <code>coord_sets</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates to transform, can be shape (number of sets, number of coordinates, 3)</p> </li> <li> <code>rotation</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)</p> </li> <li> <code>translation</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected shape (number of sets, 3)</p> </li> <li> <code>rotation2</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)</p> </li> <li> <code>translation2</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected shape (number of sets, 3)</p> </li> </ul> <p>Returns:     The transformed coordinate set with the same shape as the original</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def transform_coordinate_sets_with_broadcast(coord_sets: np.ndarray,\n                                             rotation: np.ndarray = None,\n                                             translation: np.ndarray | Iterable | int | float = None,\n                                             rotation2: np.ndarray = None,\n                                             translation2: np.ndarray | Iterable | int | float = None) \\\n        -&gt; np.ndarray:\n    \"\"\"Take stacked sets of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the\n    order of operations as: rotation, translation, rotation2, translation2. Non-efficient memory use\n\n    Args:\n        coord_sets: The coordinates to transform, can be shape (number of sets, number of coordinates, 3)\n        rotation: The first rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)\n        translation: The first translation to apply, expected shape (number of sets, 3)\n        rotation2: The second rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)\n        translation2: The second translation to apply, expected shape (number of sets, 3)\n    Returns:\n        The transformed coordinate set with the same shape as the original\n    \"\"\"\n    # in general, the np.tensordot module accomplishes this coordinate set multiplication without stacking\n    # np.tensordot(a, b, axes=1)  &lt;-- axes=1 performs the correct multiplication with a 3d (3,3,N) by 2d (3,3) matrix\n    # np.matmul solves as well due to broadcasting\n    set_shape = getattr(coord_sets, 'shape', None)\n    if set_shape is None or set_shape[0] &lt; 1:\n        return coord_sets\n    # else:  # Create a new array for the result\n    #     new_coord_sets = coord_sets.copy()\n\n    if rotation is not None:\n        coord_sets = np.matmul(coord_sets, rotation.swapaxes(-2, -1))\n\n    if translation is not None:\n        coord_sets += translation  # No array allocation, sets in place\n\n    if rotation2 is not None:\n        coord_sets = np.matmul(coord_sets, rotation2.swapaxes(-2, -1))\n\n    if translation2 is not None:\n        coord_sets += translation2\n\n    return coord_sets\n</code></pre>"},{"location":"reference/structure/coordinates/#structure.coordinates.transform_coordinate_sets","title":"transform_coordinate_sets","text":"<pre><code>transform_coordinate_sets(coord_sets: ndarray, rotation: ndarray = None, translation: ndarray | Iterable | int | float = None, rotation2: ndarray = None, translation2: ndarray | Iterable | int | float = None) -&gt; ndarray\n</code></pre> <p>Take stacked sets of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the order of operations as: rotation, translation, rotation2, translation2. If transformation uses broadcasting, for efficient memory use, the returned array will be the size of the coord_sets multiplied by rotation. Additional broadcasting is not allowed. If that behavior is desired, use \"transform_coordinate_sets_with_broadcast()\" instead</p> <p>Parameters:</p> <ul> <li> <code>coord_sets</code>             (<code>ndarray</code>)         \u2013          <p>The coordinates to transform, can be shape (number of sets, number of coordinates, 3)</p> </li> <li> <code>rotation</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)</p> </li> <li> <code>translation</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected shape (number of sets, 3)</p> </li> <li> <code>rotation2</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)</p> </li> <li> <code>translation2</code>             (<code>ndarray | Iterable | int | float</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected shape (number of sets, 3)</p> </li> </ul> <p>Returns:     The transformed coordinate set with the same shape as the original</p> Source code in <code>symdesign/structure/coordinates.py</code> <pre><code>def transform_coordinate_sets(coord_sets: np.ndarray,\n                              rotation: np.ndarray = None, translation: np.ndarray | Iterable | int | float = None,\n                              rotation2: np.ndarray = None, translation2: np.ndarray | Iterable | int | float = None) \\\n        -&gt; np.ndarray:\n    \"\"\"Take stacked sets of x,y,z coordinates and transform. Transformation proceeds by matrix multiplication with the\n    order of operations as: rotation, translation, rotation2, translation2. If transformation uses broadcasting, for\n    efficient memory use, the returned array will be the size of the coord_sets multiplied by rotation. Additional\n    broadcasting is not allowed. If that behavior is desired, use \"transform_coordinate_sets_with_broadcast()\" instead\n\n    Args:\n        coord_sets: The coordinates to transform, can be shape (number of sets, number of coordinates, 3)\n        rotation: The first rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)\n        translation: The first translation to apply, expected shape (number of sets, 3)\n        rotation2: The second rotation to apply, expected general rotation matrix shape (number of sets, 3, 3)\n        translation2: The second translation to apply, expected shape (number of sets, 3)\n    Returns:\n        The transformed coordinate set with the same shape as the original\n    \"\"\"\n    # in general, the np.tensordot module accomplishes this coordinate set multiplication without stacking\n    # np.tensordot(a, b, axes=1)  &lt;-- axes=1 performs the correct multiplication with a 3d (3,3,N) by 2d (3,3) matrix\n    # np.matmul solves as well due to broadcasting\n    set_shape = getattr(coord_sets, 'shape', None)\n    if set_shape is None or set_shape[0] &lt; 1:\n        return coord_sets\n\n    if rotation is not None:\n        new_coord_sets = np.matmul(coord_sets, rotation.swapaxes(-2, -1))\n    else:  # Create a new array for the result\n        new_coord_sets = coord_sets.copy()\n\n    if translation is not None:\n        new_coord_sets += translation  # No array allocation, sets in place\n\n    if rotation2 is not None:\n        np.matmul(new_coord_sets, rotation2.swapaxes(-2, -1), out=new_coord_sets)\n        # new_coord_sets[:] = np.matmul(new_coord_sets, rotation2.swapaxes(-2, -1))\n        # new_coord_sets = np.matmul(new_coord_sets, rotation2.swapaxes(-2, -1))\n\n    if translation2 is not None:\n        new_coord_sets += translation2\n\n    return new_coord_sets\n</code></pre>"},{"location":"reference/structure/model/","title":"model","text":""},{"location":"reference/structure/model/#structure.model.MetricsMixin","title":"MetricsMixin","text":"<p>             Bases: <code>ABC</code></p> <p>Perform Metric evaluation for derived classes</p> <p>Subclasses of Metrics must implement _metrics_table property and calculate_metrics() method</p>"},{"location":"reference/structure/model/#structure.model.MetricsMixin.metrics","title":"metrics  <code>property</code>","text":"<pre><code>metrics: _metrics_table\n</code></pre> <p>Metrics as sqlalchemy Mapped class. init: Retrieves all metrics, loads sqlalchemy Mapped class</p>"},{"location":"reference/structure/model/#structure.model.MetricsMixin.df","title":"df  <code>property</code>","text":"<pre><code>df: Series\n</code></pre> <p>Metrics as a Series. init: Retrieves all metrics, loads pd.Series</p>"},{"location":"reference/structure/model/#structure.model.MetricsMixin.calculate_metrics","title":"calculate_metrics  <code>abstractmethod</code>","text":"<pre><code>calculate_metrics(**kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Perform Metric calculation for the Entity in question</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@abc.abstractmethod\ndef calculate_metrics(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Perform Metric calculation for the Entity in question\"\"\"\n</code></pre>"},{"location":"reference/structure/model/#structure.model.MetricsMixin.clear_metrics","title":"clear_metrics","text":"<pre><code>clear_metrics() -&gt; None\n</code></pre> <p>Clear all Metrics.state_attributes for the Entity in question</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def clear_metrics(self) -&gt; None:\n    \"\"\"Clear all Metrics.state_attributes for the Entity in question\"\"\"\n    for attr in MetricsMixin.state_attributes:\n        try:\n            self.__delattr__(attr)\n        except AttributeError:\n            continue\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ParseStructureMixin","title":"ParseStructureMixin","text":"<p>             Bases: <code>ABC</code></p>"},{"location":"reference/structure/model/#structure.model.ParseStructureMixin.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(file: AnyStr, **kwargs)\n</code></pre> <p>Create a new Structure from a file with Atom records</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_file(cls, file: AnyStr, **kwargs):\n    \"\"\"Create a new Structure from a file with Atom records\"\"\"\n    if '.pdb' in file:\n        return cls.from_pdb(file, **kwargs)\n    elif '.cif' in file:\n        return cls.from_mmcif(file, **kwargs)\n    else:\n        raise NotImplementedError(\n            f\"{cls.__name__}: The file type {os.path.splitext(file)[-1]} isn't supported for parsing. Please use \"\n            f\"the supported types '.pdb' or '.cif'. Alternatively use those constructors instead (ex: from_pdb(), \"\n            'from_mmcif()) if the file extension is nonsense, but the file format is respected.')\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ParseStructureMixin.from_pdb","title":"from_pdb  <code>classmethod</code>","text":"<pre><code>from_pdb(file: AnyStr, **kwargs)\n</code></pre> <p>Create a new Structure from a .pdb formatted file</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_pdb(cls, file: AnyStr, **kwargs):\n    \"\"\"Create a new Structure from a .pdb formatted file\"\"\"\n    data = read_pdb_file(file, **kwargs)\n    return cls._finish(cls(file_path=file, **data))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ParseStructureMixin.from_pdb_lines","title":"from_pdb_lines  <code>classmethod</code>","text":"<pre><code>from_pdb_lines(pdb_lines: Iterable[str], **kwargs)\n</code></pre> <p>Create a new Structure from already parsed .pdb file lines</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_pdb_lines(cls, pdb_lines: Iterable[str], **kwargs):\n    \"\"\"Create a new Structure from already parsed .pdb file lines\"\"\"\n    data = read_pdb_file(pdb_lines=pdb_lines, **kwargs)\n    return cls._finish(cls(**data))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ParseStructureMixin.from_mmcif","title":"from_mmcif  <code>classmethod</code>","text":"<pre><code>from_mmcif(file: AnyStr, **kwargs)\n</code></pre> <p>Create a new Structure from a .cif formatted file</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_mmcif(cls, file: AnyStr, **kwargs):\n    \"\"\"Create a new Structure from a .cif formatted file\"\"\"\n    data = read_mmcif_file(file, **kwargs)\n    return cls._finish(cls(file_path=file, **data))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity","title":"StructuredGeneEntity","text":"<pre><code>StructuredGeneEntity(metadata: ProteinMetadata = None, uniprot_ids: tuple[str, ...] = None, thermophilicity: bool = None, reference_sequence: str = None, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsResidues</code>, <code>GeneEntity</code></p> <p>Implements methods to map a Structure to a GeneEntity</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>             (<code>ProteinMetadata</code>, default:                 <code>None</code> )         \u2013          <p>Unique database references</p> </li> <li> <code>uniprot_ids</code>             (<code>tuple[str, ...]</code>, default:                 <code>None</code> )         \u2013          <p>The UniProtID(s) that describe this protein sequence</p> </li> <li> <code>thermophilicity</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>The extent to which the sequence is deemed thermophilic</p> </li> <li> <code>reference_sequence</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The reference sequence (according to expression sequence or reference database)</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, metadata: sql.ProteinMetadata = None, uniprot_ids: tuple[str, ...] = None,\n             thermophilicity: bool = None, reference_sequence: str = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        metadata: Unique database references\n        uniprot_ids: The UniProtID(s) that describe this protein sequence\n        thermophilicity: The extent to which the sequence is deemed thermophilic\n        reference_sequence: The reference sequence (according to expression sequence or reference database)\n    \"\"\"\n    super().__init__(**kwargs)  # StructuredGeneEntity\n    self._alpha = default_fragment_contribution\n    self.alpha = []\n    self.fragment_map = None\n    self.fragment_profile = None  # fragment specific scoring matrix\n\n    self._api_data = None  # {chain: {'accession': 'Q96DC8', 'db': 'UniProt'}, ...}\n\n    if metadata is None:\n        if reference_sequence is not None:\n            self._reference_sequence = reference_sequence\n\n        self.thermophilicity = thermophilicity\n        if uniprot_ids is not None:\n            self.uniprot_ids = uniprot_ids\n    else:\n        if metadata.reference_sequence is not None:\n            self._reference_sequence = metadata.reference_sequence\n\n        self.thermophilicity = metadata.thermophilicity\n        if metadata.uniprot_entities is not None:\n            self.uniprot_ids = metadata.uniprot_ids\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.fragment_map","title":"fragment_map  <code>instance-attribute</code>","text":"<pre><code>fragment_map: list[dict[int, set[FragmentObservation]]] | None = None\n</code></pre> <p>{1: {-2: {FragObservation(), ...},          -1: {}, ...},     2: {}, ...} Where the outer list indices match Residue.index, and each dictionary holds the various fragment indices     (with fragment_length length) for that residue, where each index in the inner set can have multiple     observations</p>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.uniprot_ids","title":"uniprot_ids  <code>property</code> <code>writable</code>","text":"<pre><code>uniprot_ids: tuple[str | None, ...]\n</code></pre> <p>The UniProtID(s) used for accessing external protein level features</p>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.reference_sequence","title":"reference_sequence  <code>property</code>","text":"<pre><code>reference_sequence: str\n</code></pre> <p>Return the entire sequence, constituting all described residues, not just structurally modeled ones</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The sequence according to the Entity reference, or the Structure sequence if no reference available</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.offset_index","title":"offset_index  <code>property</code>","text":"<pre><code>offset_index: int\n</code></pre> <p>The starting Residue index for the instance. Zero-indexed</p>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.disorder","title":"disorder  <code>property</code>","text":"<pre><code>disorder: dict[int, dict[str, str]]\n</code></pre> <p>Return the Residue number keys where disordered residues are found by comparison of the reference sequence with the structure sequence</p> <p>Returns:</p> <ul> <li> <code>dict[int, dict[str, str]]</code>         \u2013          <p>Mutation index to mutations in the format of {1: {'from': 'A', 'to': 'K'}, ...}</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.clear_api_data","title":"clear_api_data","text":"<pre><code>clear_api_data()\n</code></pre> <p>Removes any state information from the PDB API</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def clear_api_data(self):\n    \"\"\"Removes any state information from the PDB API\"\"\"\n    del self._reference_sequence\n    self.uniprot_ids = (None,)\n    self.thermophilicity = None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.retrieve_api_metadata","title":"retrieve_api_metadata","text":"<pre><code>retrieve_api_metadata()\n</code></pre> <p>Try to set attributes from PDB API</p> Sets <p>self._api_data: dict[str, Any]     {'chains': [],      'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},      'reference_sequence': 'MSLEHHHHHH...',      'thermophilicity': True self._uniprot_id: str | None self._reference_sequence: str self.thermophilicity: bool</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def retrieve_api_metadata(self):\n    \"\"\"Try to set attributes from PDB API\n\n    Sets:\n        self._api_data: dict[str, Any]\n            {'chains': [],\n             'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},\n             'reference_sequence': 'MSLEHHHHHH...',\n             'thermophilicity': True\n        self._uniprot_id: str | None\n        self._reference_sequence: str\n        self.thermophilicity: bool\n    \"\"\"\n    entity_id = self.entity_id\n    try:\n        retrieve_api_info = resources.wrapapi.api_database_factory().pdb.retrieve_data\n    except AttributeError:\n        retrieve_api_info = query.pdb.query_pdb_by\n    api_return = retrieve_api_info(entity_id=entity_id)\n    \"\"\"Get the data on it's own since retrieve_api_info returns\n    {'EntityID':\n       {'chains': ['A', 'B', ...],\n        'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'},\n        'reference_sequence': 'MSLEHHHHHH...',\n        'thermophilicity': 1.0},\n    ...}\n    \"\"\"\n    if api_return:\n        if entity_id.lower() in api_return:\n            self.name = name = entity_id.lower()\n\n        self._api_data = api_return.get(name, {})\n    else:\n        self._api_data = {}\n\n    if self._api_data is not None:\n        for data_type, data in self._api_data.items():\n            # self.log.debug('Retrieving UNP ID for {self.name}\\nAPI DATA for chain {chain}:\\n{api_data}')\n            if data_type == 'reference_sequence':\n                self._reference_sequence = data\n            elif data_type == 'thermophilicity':\n                self.thermophilicity = data\n            elif data_type == 'dbref':\n                if data.get('db') == query.pdb.UKB:\n                    self.uniprot_ids = data.get('accession')\n    else:\n        self.log.warning(f'{repr(self)}: No information found from PDB API')\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.add_fragments_to_profile","title":"add_fragments_to_profile","text":"<pre><code>add_fragments_to_profile(fragments: Iterable[FragmentInfo], alignment_type: alignment_types_literal, **kwargs)\n</code></pre> <p>Distribute fragment information to self.fragment_map. Zero-indexed residue array</p> <p>Parameters:</p> <ul> <li> <code>fragments</code>             (<code>Iterable[FragmentInfo]</code>)         \u2013          <p>The fragment list to assign to the sequence profile with format [{'mapped': residue_index1 (int), 'paired': residue_index2 (int), 'cluster': tuple(int, int, int),   'match': match_score (float)}]</p> </li> <li> <code>alignment_type</code>             (<code>alignment_types_literal</code>)         \u2013          <p>Either 'mapped' or 'paired' indicating how the fragment observation was generated relative to this GeneEntity. Are the fragments mapped to the ContainsResidues or was it paired to it?</p> </li> </ul> Sets <p>self.fragment_map (list[list[dict[str, str | float]]]):     [{-2: {FragObservation(), ...},       -1: {}, ...},      {}, ...]     Where the outer list indices match Residue.index, and each dictionary holds the various fragment indices     (with fragment_length length) for that residue, where each index in the inner set can have multiple     observations</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def add_fragments_to_profile(self, fragments: Iterable[FragmentInfo],\n                             alignment_type: alignment_types_literal, **kwargs):\n    \"\"\"Distribute fragment information to self.fragment_map. Zero-indexed residue array\n\n    Args:\n        fragments: The fragment list to assign to the sequence profile with format\n            [{'mapped': residue_index1 (int), 'paired': residue_index2 (int), 'cluster': tuple(int, int, int),\n              'match': match_score (float)}]\n        alignment_type: Either 'mapped' or 'paired' indicating how the fragment observation was generated relative\n            to this GeneEntity. Are the fragments mapped to the ContainsResidues or was it paired to it?\n\n    Sets:\n        self.fragment_map (list[list[dict[str, str | float]]]):\n            [{-2: {FragObservation(), ...},\n              -1: {}, ...},\n             {}, ...]\n            Where the outer list indices match Residue.index, and each dictionary holds the various fragment indices\n            (with fragment_length length) for that residue, where each index in the inner set can have multiple\n            observations\n    \"\"\"\n    if alignment_type not in alignment_types:\n        raise ValueError(\n            f\"Argument 'alignment_type' must be one of '{', '.join(alignment_types)}' not {alignment_type}\")\n\n    fragment_db = self.fragment_db\n    fragment_map = self.fragment_map\n    if fragment_map is None:\n        # Create empty fragment_map to store information about each fragment observation in the profile\n        self.fragment_map = fragment_map = [defaultdict(set) for _ in range(self.number_of_residues)]\n\n    # Add frequency information to the fragment profile using parsed cluster information. Frequency information is\n    # added in a fragment index dependent manner. If multiple fragment indices are present in a single residue, a\n    # new observation is created for that fragment index.\n    for fragment in fragments:\n        # Offset the specified fragment index to the overall index in the ContainsStructures\n        fragment_index = getattr(fragment, alignment_type)\n        cluster = fragment.cluster\n        match = fragment.match\n        residue_index = fragment_index - self.offset_index\n        # Retrieve the amino acid frequencies for this fragment cluster, for this alignment side\n        aa_freq = getattr(fragment_db.info[cluster], alignment_type)\n        for frag_idx, frequencies in aa_freq.items():  # (lower_bound - upper_bound), [freqs]\n            _frequencies = frequencies.copy()\n            _frag_info = FragmentObservation(source=alignment_type, cluster=cluster, match=match,\n                                             weight=_frequencies.pop('weight'), frequencies=_frequencies)\n            fragment_map[residue_index + frag_idx][frag_idx].add(_frag_info)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.simplify_fragment_profile","title":"simplify_fragment_profile","text":"<pre><code>simplify_fragment_profile(evo_fill: bool = False, **kwargs)\n</code></pre> <p>Take a multi-indexed, a multi-observation fragment_profile and flatten to single frequency for each residue.</p> <p>Weight the frequency of each observation by the fragment indexed, average observation weight, proportionally scaled by the match score between the fragment database and the observed fragment overlap</p> <p>From the self.fragment_map data, create a fragment profile and add to the GeneEntity</p> <p>Parameters:</p> <ul> <li> <code>evo_fill</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to fill missing positions with evolutionary profile values</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>alpha</code>         \u2013          <p>float = 0.5 - The maximum contribution of the fragment profile to use, bounded between (0, 1]. 0 means no use of fragments in the .profile, while 1 means only use fragments</p> </li> </ul> Sets <p>self.fragment_profile (Profile)     [{'A': 0.23, 'C': 0.01, ..., stats': (1, 0.37)}, {...}, ...]     list of profile_entry that combines all fragment information at a single residue using a weighted     average. 'count' is number of fragment observations at each residue, and 'weight' is the total     fragment weight over the entire residue</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def simplify_fragment_profile(self, evo_fill: bool = False, **kwargs):\n    \"\"\"Take a multi-indexed, a multi-observation fragment_profile and flatten to single frequency for each residue.\n\n    Weight the frequency of each observation by the fragment indexed, average observation weight, proportionally\n    scaled by the match score between the fragment database and the observed fragment overlap\n\n    From the self.fragment_map data, create a fragment profile and add to the GeneEntity\n\n    Args:\n        evo_fill: Whether to fill missing positions with evolutionary profile values\n\n    Keyword Args:\n        alpha: float = 0.5 - The maximum contribution of the fragment profile to use, bounded between (0, 1].\n            0 means no use of fragments in the .profile, while 1 means only use fragments\n\n    Sets:\n        self.fragment_profile (Profile)\n            [{'A': 0.23, 'C': 0.01, ..., stats': (1, 0.37)}, {...}, ...]\n            list of profile_entry that combines all fragment information at a single residue using a weighted\n            average. 'count' is number of fragment observations at each residue, and 'weight' is the total\n            fragment weight over the entire residue\n    \"\"\"\n    # keep_extras: Whether to keep values for all positions that are missing data\n    fragment_db = self.fragment_db\n    fragment_map = self.fragment_map\n    if fragment_map is None:  # Need this for _calculate_alpha()\n        raise RuntimeError(\n            f'Must {self.add_fragments_to_profile.__name__}() before '\n            f'{self.simplify_fragment_profile.__name__}(). No fragments were set')\n    elif not fragment_db:\n        raise AttributeError(\n            f\"{self.simplify_fragment_profile.__name__}: No '.fragment_db'. Can't calculate \"\n            'fragment contribution without one')\n\n    database_bkgnd_aa_freq = fragment_db.aa_frequencies\n    # Fragment profile is correct size for indexing all STRUCTURAL residues\n    #  self.reference_sequence is not used for this. Instead, self.sequence is used in place since the use\n    #  of a disorder indicator that removes any disordered residues from input evolutionary profiles is calculated\n    #  on the full reference sequence. This ensures that the profile is the right length of the structure and\n    #  captures disorder specific evolutionary signals that could be important in the calculation of profiles\n    sequence = self.sequence\n    no_design = []\n    fragment_profile = [[{} for _ in range(fragment_db.fragment_length)]\n                        for _ in range(self.number_of_residues)]\n    indexed_observations: dict[int, set[FragmentObservation]]\n    for residue_index, indexed_observations in enumerate(fragment_map):\n        total_fragment_observations = total_fragment_weight_x_match = total_fragment_weight = 0\n\n        # Sum the weight for each fragment observation\n        for index, observations in indexed_observations.items():\n            for observation in observations:\n                total_fragment_observations += 1\n                observation_weight = observation.weight\n                total_fragment_weight += observation_weight\n                total_fragment_weight_x_match += observation_weight * observation.match\n\n        # New style, consolidated\n        residue_frequencies = {'count': total_fragment_observations,\n                               'weight': total_fragment_weight,\n                               'info': 0.,\n                               'type': sequence[residue_index],\n                               }\n        if total_fragment_weight_x_match &gt; 0:\n            # Combine all amino acid frequency distributions for all observations at each index\n            residue_frequencies.update(**aa_counts_alph3)  # {'A': 0, 'R': 0, ...}\n            for index, observations in indexed_observations.items():\n                for observation in observations:\n                    # Multiply weight associated with observations by the match of the observation, then\n                    # scale the observation weight by the total. If no weight, side chain isn't significant.\n                    scaled_frag_weight = observation.weight * observation.match / total_fragment_weight_x_match\n                    # Add all occurrences to summed frequencies list\n                    for aa, frequency in observation.frequencies.items():\n                        residue_frequencies[aa] += frequency * scaled_frag_weight\n\n            residue_frequencies['lod'] = get_lod(residue_frequencies, database_bkgnd_aa_freq)\n        else:  # Add to list for removal from the profile\n            no_design.append(residue_index)\n            # {'A': 0, 'R': 0, ...}\n            residue_frequencies.update(lod=aa_nan_counts_alph3.copy(), **aa_nan_counts_alph3)\n\n        # Add results to final fragment_profile residue position\n        fragment_profile[residue_index] = residue_frequencies\n        # Since self.evolutionary_profile is copied or removed, an empty dictionary is fine here\n        # If this changes, maybe the == 0 condition needs an aa_counts_alph3.copy() instead of {}\n\n    if evo_fill and self.evolutionary_profile:\n        # If not an empty dictionary, add the corresponding value from evolution\n        # For Rosetta, the packer palette is subtractive so the use of an overlapping evolution and\n        # null fragment would result in nothing allowed during design...\n        evolutionary_profile = self.evolutionary_profile\n        for residue_index in no_design:\n            fragment_profile[residue_index] = evolutionary_profile.get(residue_index + ZERO_OFFSET)\n\n    # Format into fragment_profile Profile object\n    self.fragment_profile = Profile(fragment_profile, dtype='fragment')\n\n    self._calculate_alpha(**kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.calculate_profile","title":"calculate_profile","text":"<pre><code>calculate_profile(favor_fragments: bool = False, boltzmann: bool = True, **kwargs)\n</code></pre> <p>Combine weights for profile PSSM and fragment SSM using fragment significance value to determine overlap</p> <p>Using self.evolutionary_profile     (ProfileDict): HHblits - {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...},                                          'type': 'W', 'info': 0.00, 'weight': 0.00}, {...}}                    PSIBLAST - {1: {'A': 0.13, 'R': 0.12, ..., 'lod': {'A': -5, 'R': 2, ...},                                    'type': 'W', 'info': 3.20, 'weight': 0.73}, {...}} self.fragment_profile     (dict[int, dict[str, float | list[float]]]):         {48: {'A': 0.167, 'D': 0.028, 'E': 0.056, ..., 'count': 4, 'weight': 0.274}, 50: {...}, ...} self.alpha     (list[float]): [0., 0., 0., 0.5, 0.321, ...]</p> <p>Parameters:</p> <ul> <li> <code>favor_fragments</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to favor fragment profile in the lod score of the resulting profile Currently this routine is only used for Rosetta designs where the fragments should be favored by a particular weighting scheme. By default, the boltzmann weighting scheme is applied</p> </li> <li> <code>boltzmann</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to weight the fragment profile by a Boltzmann probability scaling using the formula lods = exp(lods[i]/kT)/Z, where Z = sum(exp(lods[i]/kT)), and kT is 1 by default. If False, residues are weighted by the residue local maximum lod score in a linear fashion All lods are scaled to a maximum provided in the Rosetta REF2015 per residue reference weight.</p> </li> </ul> Sets <p>self.profile: (ProfileDict)     {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...},          'type': 'W', 'info': 0.00, 'weight': 0.00}, ...}, ...}</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_profile(self, favor_fragments: bool = False, boltzmann: bool = True, **kwargs):\n    \"\"\"Combine weights for profile PSSM and fragment SSM using fragment significance value to determine overlap\n\n    Using self.evolutionary_profile\n        (ProfileDict): HHblits - {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...},\n                                             'type': 'W', 'info': 0.00, 'weight': 0.00}, {...}}\n                       PSIBLAST - {1: {'A': 0.13, 'R': 0.12, ..., 'lod': {'A': -5, 'R': 2, ...},\n                                       'type': 'W', 'info': 3.20, 'weight': 0.73}, {...}}\n    self.fragment_profile\n        (dict[int, dict[str, float | list[float]]]):\n            {48: {'A': 0.167, 'D': 0.028, 'E': 0.056, ..., 'count': 4, 'weight': 0.274}, 50: {...}, ...}\n    self.alpha\n        (list[float]): [0., 0., 0., 0.5, 0.321, ...]\n\n    Args:\n        favor_fragments: Whether to favor fragment profile in the lod score of the resulting profile\n            Currently this routine is only used for Rosetta designs where the fragments should be favored by a\n            particular weighting scheme. By default, the boltzmann weighting scheme is applied\n        boltzmann: Whether to weight the fragment profile by a Boltzmann probability scaling using the formula\n            lods = exp(lods[i]/kT)/Z, where Z = sum(exp(lods[i]/kT)), and kT is 1 by default.\n            If False, residues are weighted by the residue local maximum lod score in a linear fashion\n            All lods are scaled to a maximum provided in the Rosetta REF2015 per residue reference weight.\n\n    Sets:\n        self.profile: (ProfileDict)\n            {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...},\n                 'type': 'W', 'info': 0.00, 'weight': 0.00}, ...}, ...}\n    \"\"\"\n    if self._alpha == 0:\n        # Round up to avoid division error\n        self.log.warning(f'{self.calculate_profile.__name__}: _alpha set with 1e-5 tolerance due to 0 value')\n        self._alpha = 0.000001\n\n    # Copy the evolutionary profile to self.profile (structure specific scoring matrix)\n    self.profile = profile = deepcopy(self.evolutionary_profile)\n    if sum(self.alpha) == 0:  # No fragments to combine\n        return\n\n    # Combine fragment and evolutionary probability profile according to alpha parameter\n    fragment_profile = self.fragment_profile\n    # log_string = []\n    for entry_idx, weight in enumerate(self.alpha):\n        # Weight will be 0 if the fragment_profile is empty\n        if weight:\n            # log_string.append(f'Residue {entry + 1:5d}: {weight * 100:.0f}% fragment weight')\n            frag_profile_entry = fragment_profile[entry_idx]\n            inverse_weight = 1 - weight\n            _profile_entry = profile[entry_idx + ZERO_OFFSET]\n            _profile_entry.update({aa: weight * frag_profile_entry[aa] + inverse_weight * _profile_entry[aa]\n                                   for aa in protein_letters_alph3})\n    # if log_string:\n    #     # self.log.info(f'At {self.name}, combined evolutionary and fragment profiles into Design Profile with:'\n    #     #               f'\\n\\t%s' % '\\n\\t'.join(log_string))\n    #     pass\n\n    if favor_fragments:\n        fragment_db = self.fragment_db\n        boltzman_energy = 1\n        favor_seqprofile_score_modifier = 0.2 * utils.rosetta.reference_average_residue_weight\n        if not fragment_db:\n            raise AttributeError(\n                f\"{self.calculate_profile.__name__}: No fragment database connected. Can't 'favor_fragments' \"\n                'without one')\n        database_bkgnd_aa_freq = fragment_db.aa_frequencies\n\n        null_residue = get_lod(database_bkgnd_aa_freq, database_bkgnd_aa_freq, as_int=False)\n        # This was needed in the case of domain errors with lod\n        # null_residue = {aa: float(frequency) for aa, frequency in null_residue.items()}\n\n        # Set all profile entries to a null entry first\n        for entry, data in self.profile.items():\n            data['lod'] = null_residue  # Caution, all reference same object\n\n        alpha = self.alpha\n        for entry, data in self.profile.items():\n            data['lod'] = get_lod(fragment_profile[entry - ZERO_OFFSET], database_bkgnd_aa_freq, as_int=False)\n            # Adjust scores with particular weighting scheme\n            partition = 0.\n            for aa, value in data['lod'].items():\n                if boltzmann:  # Boltzmann scaling, sum for the partition function\n                    value = math.exp(value / boltzman_energy)\n                    partition += value\n                else:  # if value &lt; 0:\n                    # With linear scaling, remove any lod penalty\n                    value = max(0, value)\n\n                data['lod'][aa] = value\n\n            # Find the maximum/residue (local) lod score\n            max_lod = max(data['lod'].values())\n            # Takes the percent of max alpha for each entry multiplied by the standard residue scaling factor\n            modified_entry_alpha = (alpha[entry - ZERO_OFFSET] / self._alpha) * favor_seqprofile_score_modifier\n            if boltzmann:\n                # lods = e ** odds[i]/Z, Z = sum(exp(odds[i]/kT))\n                modifier = partition\n                modified_entry_alpha /= (max_lod / partition)\n            else:\n                modifier = max_lod\n\n            # Weight the final lod score by the modifier and the scaling factor for the chosen method\n            data['lod'] = {aa: value / modifier * modified_entry_alpha for aa, value in data['lod'].items()}\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.format_missing_loops_for_design","title":"format_missing_loops_for_design","text":"<pre><code>format_missing_loops_for_design(max_loop_length: int = 12, exclude_n_term: bool = True, ignore_termini: bool = False, **kwargs) -&gt; tuple[list[tuple], dict[int, int], int]\n</code></pre> <p>Process missing residue information to prepare for loop modeling files. Assumes residues in pose numbering!</p> <p>Parameters:</p> <ul> <li> <code>max_loop_length</code>             (<code>int</code>, default:                 <code>12</code> )         \u2013          <p>The max length for loop modeling. 12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014</p> </li> <li> <code>exclude_n_term</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to exclude the N-termini from modeling due to Remodel Bug</p> </li> <li> <code>ignore_termini</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to ignore terminal loops in the loop file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[tuple], dict[int, int], int]</code>         \u2013          <p>Pairs of indices where each loop starts and ends, adjacent indices (not all indices are disordered) mapped to their disordered residue indices, and the n-terminal residue index</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_missing_loops_for_design(\n    self, max_loop_length: int = 12, exclude_n_term: bool = True, ignore_termini: bool = False, **kwargs\n) -&gt; tuple[list[tuple], dict[int, int], int]:\n    \"\"\"Process missing residue information to prepare for loop modeling files. Assumes residues in pose numbering!\n\n    Args:\n        max_loop_length: The max length for loop modeling.\n            12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014\n        exclude_n_term: Whether to exclude the N-termini from modeling due to Remodel Bug\n        ignore_termini: Whether to ignore terminal loops in the loop file\n\n    Returns:\n        Pairs of indices where each loop starts and ends, adjacent indices (not all indices are disordered) mapped\n            to their disordered residue indices, and the n-terminal residue index\n    \"\"\"\n    disordered_residues = self.disorder\n    # Formatted as {residue_number: {'from': aa, 'to': aa}, ...}\n    reference_sequence_length = len(self.reference_sequence)\n    loop_indices = []\n    loop_to_disorder_indices = {}  # Holds the indices that should be inserted into the total residues to be modeled\n    n_terminal_idx = 0  # Initialize as an impossible value\n    excluded_disorder_len = 0  # Total residues excluded from loop modeling. Needed for pose numbering translation\n    segment_length = 0  # Iterate each missing residue\n    n_term = False\n    loop_start = loop_end = None\n    for idx, residue_number in enumerate(disordered_residues.keys(), 1):\n        segment_length += 1\n        if residue_number - 1 not in disordered_residues:  # indicate that this residue_number starts disorder\n            # print('Residue number -1 not in loops', residue_number)\n            loop_start = residue_number - 1 - excluded_disorder_len  # - 1 as loop modeling needs existing residue\n            if loop_start &lt; 1:\n                n_term = True\n\n        if residue_number + 1 not in disordered_residues:\n            # The segment has ended\n            if residue_number != reference_sequence_length:\n                # Not the c-terminus\n                # logger.debug('f{residue_number=} +1 not in loops. Adding loop with {segment_length=}')\n                if segment_length &lt;= max_loop_length:\n                    # Modeling useful, add to loop_indices\n                    if n_term and (ignore_termini or exclude_n_term):\n                        # The n-terminus should be included\n                        excluded_disorder_len += segment_length\n                        n_term = False  # No more n_term considerations\n                    else:  # Include the segment in the disorder_indices\n                        loop_end = residue_number + 1 - excluded_disorder_len\n                        loop_indices.append((loop_start, loop_end))\n                        for it, residue_index in enumerate(range(loop_start + 1, loop_end), 1):\n                            loop_to_disorder_indices[residue_index] = residue_number - (segment_length - it)\n                        # Set the start and end indices as out of bounds numbers\n                        loop_to_disorder_indices[loop_start], loop_to_disorder_indices[loop_end] = -1, -1\n                        if n_term and idx != 1:  # If this is the n-termini and not start Met\n                            n_terminal_idx = loop_end  # Save idx of last n-term insertion\n                else:  # Modeling not useful, sum the exclusion length\n                    excluded_disorder_len += segment_length\n\n                # After handling disordered segment, reset increment and loop indices\n                segment_length = 0\n                loop_start = loop_end = None\n            # Residue number is the c-terminal residue\n            elif ignore_termini:\n                if segment_length &lt;= max_loop_length:\n                    # loop_end = loop_start + 1 + segment_length  # - excluded_disorder\n                    loop_end = residue_number - excluded_disorder_len\n                    loop_indices.append((loop_start, loop_end))\n                    for it, residue_index in enumerate(range(loop_start + 1, loop_end), 1):\n                        loop_to_disorder_indices[residue_index] = residue_number - (segment_length - it)\n                    # Don't include start index in the loop_to_disorder map since c-terminal doesn't have attachment\n                    loop_to_disorder_indices[loop_end] = -1\n\n    return loop_indices, loop_to_disorder_indices, n_terminal_idx\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.make_loop_file","title":"make_loop_file","text":"<pre><code>make_loop_file(out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Format a loops file according to Rosetta specifications. Assumes residues in pose numbering!</p> <p>The loop file format consists of one line for each specified loop with the format:</p> <p>LOOP 779 784 0 0 1</p> <p>Where LOOP specifies a loop line, start idx, end idx, cut site (0 lets Rosetta choose), skip rate, and extended</p> <p>All indices should refer to existing locations in the structure file so if a loop should be inserted into missing density, the density needs to be modeled first before the loop file would work to be modeled. You can't therefore specify that a loop should be between 779 and 780 if the loop is 12 residues long since there is  no specification about how to insert those residues. This type of task requires a blueprint file.</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location the file should be written</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>max_loop_length=12</code>             (<code>int</code>)         \u2013          <p>The max length for loop modeling. 12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014</p> </li> <li> <code>exclude_n_term=True</code>             (<code>bool</code>)         \u2013          <p>Whether to exclude the N-termini from modeling due to Remodel Bug</p> </li> <li> <code>ignore_termini=False</code>             (<code>bool</code>)         \u2013          <p>Whether to ignore terminal loops in the loop file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The path of the file if one was written</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_loop_file(self, out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr | None:\n    \"\"\"Format a loops file according to Rosetta specifications. Assumes residues in pose numbering!\n\n    The loop file format consists of one line for each specified loop with the format:\n\n    LOOP 779 784 0 0 1\n\n    Where LOOP specifies a loop line, start idx, end idx, cut site (0 lets Rosetta choose), skip rate, and extended\n\n    All indices should refer to existing locations in the structure file so if a loop should be inserted into\n    missing density, the density needs to be modeled first before the loop file would work to be modeled. You\n    can't therefore specify that a loop should be between 779 and 780 if the loop is 12 residues long since there is\n     no specification about how to insert those residues. This type of task requires a blueprint file.\n\n    Args:\n        out_path: The location the file should be written\n\n    Keyword Args:\n        max_loop_length=12 (int): The max length for loop modeling.\n            12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014\n        exclude_n_term=True (bool): Whether to exclude the N-termini from modeling due to Remodel Bug\n        ignore_termini=False (bool): Whether to ignore terminal loops in the loop file\n\n    Returns:\n        The path of the file if one was written\n    \"\"\"\n    loop_indices, _, _ = self.format_missing_loops_for_design(**kwargs)\n    if not loop_indices:\n        return None\n\n    loop_file = os.path.join(out_path, f'{self.name}.loops')\n    with open(loop_file, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(f'LOOP {start} {stop} 0 0 1' for start, stop in loop_indices))\n\n    return loop_file\n</code></pre>"},{"location":"reference/structure/model/#structure.model.StructuredGeneEntity.make_blueprint_file","title":"make_blueprint_file","text":"<pre><code>make_blueprint_file(out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Format a blueprint file according to Rosetta specifications. Assumes residues in pose numbering!</p> The blueprint file format is described nicely here <p>https://www.rosettacommons.org/docs/latest/application_documentation/design/rosettaremodel</p> <p>In a gist, a blueprint file consists of entries describing the type of design available at each position.</p> Ex <p>1 x L PIKAA M   &lt;- Extension</p> <p>1 x L PIKAA V   &lt;- Extension</p> <p>1 V L PIKAA V   &lt;- Attachment point</p> <p>2 D .</p> <p>3 K .</p> <p>4 I .</p> <p>5 L N PIKAA N   &lt;- Attachment point</p> <p>0 x I NATAA     &lt;- Insertion</p> <p>0 x I NATAA     &lt;- Insertion</p> <p>6 N A PIKAA A   &lt;- Attachment point</p> <p>7 G .</p> <p>0 X L PIKAA Y   &lt;- Extension</p> <p>0 X L PIKAA P   &lt;- Extension</p> <p>All structural indices must be specified in \"pose numbering\", i.e. starting with 1 ending with the last residue. If you have missing density in the middle, you should not specify those residues that are missing, but keep continuous numbering. You can specify an inclusion by specifying the entry index as 0 followed by the blueprint directive. For missing density at the n- or c-termini, the file should still start 1, however, the n-termini should be extended by prepending extra entries to the structurally defined n-termini entry 1. These blueprint entries should also have 1 as the residue index. For c-termini, extra entries should be appended with the indices as 0 like in insertions. For all unmodeled entries for which design should be performed, there should be flanking attachment points that are also capable of design. Designable entries are seen above with the PIKAA directive. Other directives are available. The only location this isn't required is at the c-terminal attachment point</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location the file should be written</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>max_loop_length=12</code>             (<code>int</code>)         \u2013          <p>The max length for loop modeling. 12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014</p> </li> <li> <code>exclude_n_term=True</code>             (<code>bool</code>)         \u2013          <p>Whether to exclude the N-termini from modeling due to Remodel Bug</p> </li> <li> <code>ignore_termini=False</code>             (<code>bool</code>)         \u2013          <p>Whether to ignore terminal loops in the loop file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The path of the file if one was written</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_blueprint_file(self, out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr | None:\n    \"\"\"Format a blueprint file according to Rosetta specifications. Assumes residues in pose numbering!\n\n    The blueprint file format is described nicely here:\n        https://www.rosettacommons.org/docs/latest/application_documentation/design/rosettaremodel\n\n    In a gist, a blueprint file consists of entries describing the type of design available at each position.\n\n    Ex:\n        1 x L PIKAA M   &lt;- Extension\n\n        1 x L PIKAA V   &lt;- Extension\n\n        1 V L PIKAA V   &lt;- Attachment point\n\n        2 D .\n\n        3 K .\n\n        4 I .\n\n        5 L N PIKAA N   &lt;- Attachment point\n\n        0 x I NATAA     &lt;- Insertion\n\n        0 x I NATAA     &lt;- Insertion\n\n        6 N A PIKAA A   &lt;- Attachment point\n\n        7 G .\n\n        0 X L PIKAA Y   &lt;- Extension\n\n        0 X L PIKAA P   &lt;- Extension\n\n    All structural indices must be specified in \"pose numbering\", i.e. starting with 1 ending with the last residue.\n    If you have missing density in the middle, you should not specify those residues that are missing, but keep\n    continuous numbering. You can specify an inclusion by specifying the entry index as 0 followed by the blueprint\n    directive. For missing density at the n- or c-termini, the file should still start 1, however, the n-termini\n    should be extended by prepending extra entries to the structurally defined n-termini entry 1. These blueprint\n    entries should also have 1 as the residue index. For c-termini, extra entries should be appended with the\n    indices as 0 like in insertions. For all unmodeled entries for which design should be performed, there should\n    be flanking attachment points that are also capable of design. Designable entries are seen above with the PIKAA\n    directive. Other directives are available. The only location this isn't required is at the c-terminal attachment\n    point\n\n    Args:\n        out_path: The location the file should be written\n\n    Keyword Args:\n        max_loop_length=12 (int): The max length for loop modeling.\n            12 is the max for accurate KIC as of benchmarks from T. Kortemme, 2014\n        exclude_n_term=True (bool): Whether to exclude the N-termini from modeling due to Remodel Bug\n        ignore_termini=False (bool): Whether to ignore terminal loops in the loop file\n\n    Returns:\n        The path of the file if one was written\n    \"\"\"\n    disordered_residues = self.disorder\n    # Formatted as {residue_number: {'from': aa, 'to': aa}, ...}\n    # trying to remove tags at this stage runs into a serious indexing problem where tags need to be deleted from\n    # disordered_residues and then all subsequent indices adjusted.\n\n    # # look for existing tag to remove from sequence and save identity\n    # available_tags = find_expression_tags(self.reference_sequence)\n    # if available_tags:\n    #     loop_sequences = ''.join(mutation['from'] for mutation in disordered_residues)\n    #     remove_loop_pairs = []\n    #     for tag in available_tags:\n    #         tag_location = loop_sequences.find(tag['sequences'])\n    #         if tag_location != -1:\n    #             remove_loop_pairs.append((tag_location, len(tag['sequences'])))\n    #     for tag_start, tag_length in remove_loop_pairs:\n    #         for\n    #\n    #     # untagged_seq = remove_terminal_tags(loop_sequences, [tag['sequence'] for tag in available_tags])\n\n    _, disorder_indices, start_idx = self.format_missing_loops_for_design(**kwargs)\n    if not disorder_indices:\n        return\n\n    residues = self.residues\n    # for residue_number in sorted(disorder_indices):  # ensure ascending order, insert dependent on prior inserts\n    for residue_index, disordered_residue in disorder_indices.items():\n        mutation = disordered_residues.get(disordered_residue)\n        if mutation:  # add disordered residue to residues list if they exist\n            residues.insert(residue_index - 1, mutation['from'])  # offset to match residues zero-index\n\n    #                 index AA SS Choice AA\n    # structure_str   = '%d %s %s'\n    # loop_str        = '%d X %s PIKAA %s'\n    blueprint_lines = []\n    for idx, residue in enumerate(residues, 1):\n        if isinstance(residue, Residue):  # use structure_str template\n            residue_type = protein_letters_3to1_extended.get(residue.type)\n            blueprint_lines.append(f'{residue.number} {residue_type} '\n                                   f'{f\"L PIKAA {residue_type}\" if idx in disorder_indices else \".\"}')\n        else:  # residue is the residue type from above insertion, use loop_str template\n            blueprint_lines.append(f'{1 if idx &lt; start_idx else 0} X {\"L\"} PIKAA {residue}')\n\n    blueprint_file = os.path.join(out_path, f'{self.name}.blueprint')\n    with open(blueprint_file, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(blueprint_lines))\n    return blueprint_file\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Structure","title":"Structure","text":"<pre><code>Structure(metadata: ProteinMetadata = None, uniprot_ids: tuple[str, ...] = None, thermophilicity: bool = None, reference_sequence: str = None, **kwargs)\n</code></pre> <p>             Bases: <code>StructuredGeneEntity</code>, <code>ParseStructureMixin</code></p> <p>The base class to handle structural manipulation of groups of Residue instances</p> <p>Parameters:</p> <ul> <li> <code>metadata</code>             (<code>ProteinMetadata</code>, default:                 <code>None</code> )         \u2013          <p>Unique database references</p> </li> <li> <code>uniprot_ids</code>             (<code>tuple[str, ...]</code>, default:                 <code>None</code> )         \u2013          <p>The UniProtID(s) that describe this protein sequence</p> </li> <li> <code>thermophilicity</code>             (<code>bool</code>, default:                 <code>None</code> )         \u2013          <p>The extent to which the sequence is deemed thermophilic</p> </li> <li> <code>reference_sequence</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The reference sequence (according to expression sequence or reference database)</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, metadata: sql.ProteinMetadata = None, uniprot_ids: tuple[str, ...] = None,\n             thermophilicity: bool = None, reference_sequence: str = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        metadata: Unique database references\n        uniprot_ids: The UniProtID(s) that describe this protein sequence\n        thermophilicity: The extent to which the sequence is deemed thermophilic\n        reference_sequence: The reference sequence (according to expression sequence or reference database)\n    \"\"\"\n    super().__init__(**kwargs)  # StructuredGeneEntity\n    self._alpha = default_fragment_contribution\n    self.alpha = []\n    self.fragment_map = None\n    self.fragment_profile = None  # fragment specific scoring matrix\n\n    self._api_data = None  # {chain: {'accession': 'Q96DC8', 'db': 'UniProt'}, ...}\n\n    if metadata is None:\n        if reference_sequence is not None:\n            self._reference_sequence = reference_sequence\n\n        self.thermophilicity = thermophilicity\n        if uniprot_ids is not None:\n            self.uniprot_ids = uniprot_ids\n    else:\n        if metadata.reference_sequence is not None:\n            self._reference_sequence = metadata.reference_sequence\n\n        self.thermophilicity = metadata.thermophilicity\n        if metadata.uniprot_entities is not None:\n            self.uniprot_ids = metadata.uniprot_ids\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures","title":"ContainsStructures","text":"<pre><code>ContainsStructures(**kwargs)\n</code></pre> <p>             Bases: <code>Structure</code></p> <p>Implements methods to interact with a Structure which contains other Structure instances</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # ContainsStructures\n    self.structure_containers = []\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.biological_assembly","title":"biological_assembly  <code>property</code>","text":"<pre><code>biological_assembly: str | None\n</code></pre> <p>The integer which maps the structure to an assembly state from the PDB</p>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.file_path","title":"file_path  <code>property</code>","text":"<pre><code>file_path: str | None\n</code></pre> <p>The integer which maps the structure to an assembly state from the PDB</p>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.resolution","title":"resolution  <code>property</code>","text":"<pre><code>resolution: float | None\n</code></pre> <p>The integer which maps the structure to an assembly state from the PDB</p>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.reset_and_reindex_structures","title":"reset_and_reindex_structures  <code>staticmethod</code>","text":"<pre><code>reset_and_reindex_structures(structs: Sequence[ContainsResidues] | Structures)\n</code></pre> <p>Given ContainsResidues instances, reset the states and renumber indices in the order passed</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@staticmethod\ndef reset_and_reindex_structures(structs: Sequence[ContainsResidues] | Structures):\n    \"\"\"Given ContainsResidues instances, reset the states and renumber indices in the order passed\"\"\"\n    struct: ContainsResidues\n    other_structs: tuple[ContainsResidues]\n\n    struct, *other_structs = structs\n    struct.reset_state()\n    struct._start_indices(at=0, dtype='atom')\n    struct._start_indices(at=0, dtype='residue')\n    prior_struct = struct\n    for struct in other_structs:\n        struct.reset_state()\n        struct._start_indices(at=prior_struct.end_index + 1, dtype='atom')\n        struct._start_indices(at=prior_struct.residue_indices[-1] + 1, dtype='residue')\n        prior_struct = struct\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.fragment_db","title":"fragment_db","text":"<pre><code>fragment_db(fragment_db: FragmentDatabase)\n</code></pre> <p>Set the Structure FragmentDatabase to assist with Fragment creation, manipulation, and profiles. Sets .fragment_db for each dependent Structure in 'structure_containers'</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@ContainsResidues.fragment_db.setter\ndef fragment_db(self, fragment_db: FragmentDatabase):\n    \"\"\"Set the Structure FragmentDatabase to assist with Fragment creation, manipulation, and profiles.\n    Sets .fragment_db for each dependent Structure in 'structure_containers'\n    \"\"\"\n    # Set this instance then set all dependents\n    super(Structure, Structure).fragment_db.fset(self, fragment_db)\n    _fragment_db = self._fragment_db\n    if _fragment_db is not None:\n        for structure_type in self.structure_containers:\n            for structure in self.__getattribute__(structure_type):\n                structure.fragment_db = _fragment_db\n    else:  # This is likely the RELOAD_DB token. Just return.\n        return\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.format_header","title":"format_header","text":"<pre><code>format_header(**kwargs) -&gt; str\n</code></pre> <p>Returns any super().format_header() along with the SEQRES records</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The .pdb file header string</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_header(self, **kwargs) -&gt; str:\n    \"\"\"Returns any super().format_header() along with the SEQRES records\n\n    Returns:\n        The .pdb file header string\n    \"\"\"\n    if self.is_parent() and isinstance(self.metadata.cryst_record, str):\n        _header = self.metadata.cryst_record\n    else:\n        _header = ''\n\n    return super().format_header(**kwargs) + self._format_seqres(**kwargs) + _header\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.mutate_residue","title":"mutate_residue","text":"<pre><code>mutate_residue(residue: Residue = None, index: int = None, number: int = None, to: str = 'A', **kwargs) -&gt; list[int] | list\n</code></pre> <p>Mutate a specific Residue to a new residue type. Type can be 1 or 3 letter format</p> <p>Parameters:</p> <ul> <li> <code>residue</code>             (<code>Residue</code>, default:                 <code>None</code> )         \u2013          <p>A Residue instance to mutate</p> </li> <li> <code>index</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>A Residue index to select the Residue instance of interest</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>A Residue number to select the Residue instance of interest</p> </li> <li> <code>to</code>             (<code>str</code>, default:                 <code>'A'</code> )         \u2013          <p>The type of amino acid to mutate to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int] | list</code>         \u2013          <p>The indices of the Atoms being removed from the Structure</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def mutate_residue(\n    self, residue: Residue = None, index: int = None, number: int = None, to: str = 'A', **kwargs\n) -&gt; list[int] | list:\n    \"\"\"Mutate a specific Residue to a new residue type. Type can be 1 or 3 letter format\n\n    Args:\n        residue: A Residue instance to mutate\n        index: A Residue index to select the Residue instance of interest\n        number: A Residue number to select the Residue instance of interest\n        to: The type of amino acid to mutate to\n\n    Returns:\n        The indices of the Atoms being removed from the Structure\n    \"\"\"\n    delete_indices = super().mutate_residue(residue=residue, index=index, number=number, to=to)\n    if self.is_dependent() or not delete_indices:  # Probably an empty list, there are no indices to delete\n        return delete_indices\n    structure: Structure\n\n    # Remove delete_indices from each Structure _atom_indices\n    # If subsequent structures, update their _atom_indices accordingly\n    delete_length = len(delete_indices)\n    for structure_type in self.structure_containers:\n        residue_found = False\n        # Iterate over each Structure in each structure_container\n        for structure in self.__getattribute__(structure_type):\n            if residue_found:  # The Structure the Residue belongs to is already accounted for, just offset\n                structure._offset_indices(start_at=0, offset=-delete_length, dtype='atom')\n            else:\n                try:\n                    structure_atom_indices = structure.atom_indices\n                    atom_delete_index = structure_atom_indices.index(delete_indices[0])\n                except ValueError:  # When delete_indices[0] isn't in structure_atom_indices\n                    continue  # Haven't reached the correct Structure yet\n                else:\n                    try:\n                        for idx in iter(delete_indices):\n                            structure_atom_indices.pop(atom_delete_index)\n                    except IndexError:  # When atom_delete_index isn't in structure_atom_indices\n                        raise IndexError(\n                            f\"{self.mutate_residue.__name__}: The index {idx} isn't in the {repr(self)}\")\n                        # structure._offset_indices(start_at=0, offset=-delete_indices.index(idx), dtype='atom')\n                    else:\n                        structure._offset_indices(start_at=atom_delete_index, offset=-delete_length, dtype='atom')\n                        residue_found = True\n\n            structure.reset_state()\n\n    return delete_indices\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.delete_residues","title":"delete_residues","text":"<pre><code>delete_residues(residues: Iterable[Residue] | None = None, indices: Iterable[int] | None = None, numbers: Container[int] | None = None, **kwargs) -&gt; list[Residue] | list\n</code></pre> <p>Deletes Residue instances from the Structure</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>Iterable[Residue] | None</code>, default:                 <code>None</code> )         \u2013          <p>Residue instances to delete</p> </li> <li> <code>indices</code>             (<code>Iterable[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Residue indices to select the Residue instances of interest</p> </li> <li> <code>numbers</code>             (<code>Container[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Residue numbers to select the Residue instances of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue] | list</code>         \u2013          <p>Each deleted Residue</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def delete_residues(self, residues: Iterable[Residue] | None = None, indices: Iterable[int] | None = None,\n                    numbers: Container[int] | None = None, **kwargs) -&gt; list[Residue] | list:\n    \"\"\"Deletes Residue instances from the Structure\n\n    Args:\n        residues: Residue instances to delete\n        indices: Residue indices to select the Residue instances of interest\n        numbers: Residue numbers to select the Residue instances of interest\n\n    Returns:\n        Each deleted Residue\n    \"\"\"\n    residues = super().delete_residues(residues=residues, numbers=numbers, indices=indices)\n    if self.is_dependent() or not residues:  # There are no Residue instances to delete\n        return residues\n    structure: Structure\n\n    # The routine below assumes the Residue instances are sorted in ascending order\n    atom_index_offset_amount = 0\n    for residue_idx, residue in enumerate(residues):\n        # Find the Residue, Atom indices to delete\n        # Offset these indices if prior indices have already been removed\n        atom_delete_indices = [idx - atom_index_offset_amount for idx in residue.atom_indices]\n        delete_length = len(atom_delete_indices)\n        residue_index = residue.index - residue_idx\n        # Offset the next Residue Atom indices by the incrementing amount\n        atom_index_offset_amount += delete_length\n        for structure_type in self.structure_containers:\n            residue_found = False\n            # Iterate over each Structure in each structure_container\n            for structure in self.__getattribute__(structure_type):\n                if residue_found:\n                    # The Structure the Residue belongs to is already accounted for, just offset the indices\n                    structure._offset_indices(start_at=0, offset=-delete_length, dtype='atom')\n                    structure._offset_indices(start_at=0, offset=-1, dtype='residue')\n                # try:  # Remove atom_delete_indices, residue_indices from Structure\n                elif residue_index not in structure._residue_indices:\n                    pass  # This structure is not the one of interest\n                else:  # Remove atom_delete_indices, residue_index from Structure\n                    structure._delete_indices(atom_delete_indices, dtype='atom')\n                    structure._delete_indices([residue_index], dtype='residue')\n                    residue_found = True\n\n                structure.reset_state()\n\n    return residues\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.insert_residue_type","title":"insert_residue_type","text":"<pre><code>insert_residue_type(index: int, residue_type: str, chain_id: str = None) -&gt; Residue\n</code></pre> <p>Insert a standard Residue type into the Structure based on Pose numbering (1 to N) at the origin. No structural alignment is performed.</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>)         \u2013          <p>The pose numbered location which a new Residue should be inserted into the Structure</p> </li> <li> <code>residue_type</code>             (<code>str</code>)         \u2013          <p>Either the 1 or 3 letter amino acid code for the residue in question</p> </li> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain identifier to associate the new Residue with</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def insert_residue_type(self, index: int, residue_type: str, chain_id: str = None) -&gt; Residue:\n    \"\"\"Insert a standard Residue type into the Structure based on Pose numbering (1 to N) at the origin.\n    No structural alignment is performed.\n\n    Args:\n        index: The pose numbered location which a new Residue should be inserted into the Structure\n        residue_type: Either the 1 or 3 letter amino acid code for the residue in question\n        chain_id: The chain identifier to associate the new Residue with\n    \"\"\"\n    new_residue = super().insert_residue_type(index, residue_type, chain_id=chain_id)\n    if self.is_dependent():\n        return new_residue\n\n    new_residue_atom_indices = new_residue.atom_indices\n    structure: Structure\n\n    # Must update other Structures indices\n    for structure_type in self.structure_containers:\n        structures = self.__getattribute__(structure_type)\n        idx = 0\n        # Iterate over Structures in each structure_container\n        for idx, structure in enumerate(structures, idx):\n            try:  # Update each Structure _residue_indices and _atom_indices with additional indices\n                structure._insert_indices(\n                    structure.residue_indices.index(index), [index], dtype='residue')\n                structure._insert_indices(\n                    structure.atom_indices.index(new_residue.start_index), new_residue_atom_indices, dtype='atom')\n            except (ValueError, IndexError):\n                # This should happen if the index isn't in the StructureBase.*_indices of interest\n                # Edge case where the index is being appended to the c-terminus\n                if index - 1 == structure.residue_indices[-1] and new_residue.chain_id == structure.chain_id:\n                    structure._insert_indices(structure.number_of_residues, [index], dtype='residue')\n                    structure._insert_indices(structure.number_of_atoms, new_residue_atom_indices, dtype='atom')\n                else:\n                    continue\n\n            # This was the Structure with insertion and insert_indices proceeded successfully\n            structure.reset_state()\n            break\n        else:  # No matching structure found. The structure_type container should be empty...\n            continue\n        # For each subsequent structure in the structure container, update the indices with the last index from\n        # the prior structure\n        prior_structure = structure\n        for structure in structures[idx + 1:]:\n            structure._start_indices(at=prior_structure.atom_indices[-1] + 1, dtype='atom')\n            structure._start_indices(at=prior_structure.residue_indices[-1] + 1, dtype='residue')\n            structure.reset_state()\n            prior_structure = structure\n\n    return new_residue\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsStructures.insert_residues","title":"insert_residues","text":"<pre><code>insert_residues(index: int, new_residues: Iterable[Residue], chain_id: str = None) -&gt; list[Residue]\n</code></pre> <p>Insert Residue instances into the Structure at the origin. No structural alignment is performed!</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int</code>)         \u2013          <p>The index to perform the insertion at</p> </li> <li> <code>new_residues</code>             (<code>Iterable[Residue]</code>)         \u2013          <p>The Residue instances to insert</p> </li> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The chain identifier to associate the new Residue instances with</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The newly inserted Residue instances</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def insert_residues(self, index: int, new_residues: Iterable[Residue], chain_id: str = None) -&gt; list[Residue]:\n    \"\"\"Insert Residue instances into the Structure at the origin. No structural alignment is performed!\n\n    Args:\n        index: The index to perform the insertion at\n        new_residues: The Residue instances to insert\n        chain_id: The chain identifier to associate the new Residue instances with\n\n    Returns:\n        The newly inserted Residue instances\n    \"\"\"\n    new_residues = super().insert_residues(index, new_residues, chain_id=chain_id)\n    if self.is_dependent():\n        return new_residues\n\n    number_new_residues = len(new_residues)\n    first_new_residue = new_residues[0]\n    new_residues_chain_id = first_new_residue.chain_id\n    atom_start_index = first_new_residue.start_index\n    new_residue_atom_indices = list(range(atom_start_index, new_residues[-1].end_index))\n    new_residue_indices = list(range(index, index + number_new_residues))\n    structure: Structure\n    structures: Iterable[Structure]\n\n    # Must update other Structures indices\n    for structure_type in self.structure_containers:\n        structures = self.__getattribute__(structure_type)\n        idx = 0\n        # Iterate over Structures in each structure_container\n        for idx, structure in enumerate(structures, idx):\n            try:  # Update each Structure _residue_indices and _atom_indices with additional indices\n                structure._insert_indices(\n                    structure.residue_indices.index(index), [index], dtype='residue')\n                structure._insert_indices(\n                    structure.atom_indices.index(atom_start_index), new_residue_atom_indices, dtype='atom')\n                break  # Move to the next container to update the indices by a set increment\n            except (ValueError, IndexError):\n                # This should happen if the index isn't in the StructureBase.*_indices of interest\n                # Edge case where the index is being appended to the c-terminus\n                if index - 1 == structure.residue_indices[-1] and new_residues_chain_id == structure.chain_id:\n                    structure._insert_indices(structure.number_of_residues, new_residue_indices, dtype='residue')\n                    structure._insert_indices(structure.number_of_atoms, new_residue_atom_indices, dtype='atom')\n                else:\n                    continue\n\n            # This was the Structure with insertion and insert_indices proceeded successfully\n            structure.reset_state()\n            break\n        else:  # The target structure wasn't found\n            raise DesignError(\n                f\"{self.insert_residues.__name__}: Couldn't locate the Structure to be modified by the inserted \"\n                f\"residues\")\n        # For each subsequent structure in the structure container, update the indices with the last index from\n        # the prior structure\n        prior_structure = structure\n        for structure in structures[idx + 1:]:\n            structure._start_indices(at=prior_structure.atom_indices[-1] + 1, dtype='atom')\n            structure._start_indices(at=prior_structure.residue_indices[-1] + 1, dtype='residue')\n            structure.reset_state()\n            prior_structure = structure\n\n    # self.log.debug(f'Deleted {number_new_residues} Residue instances')\n\n    return new_residues\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains","title":"ContainsChains","text":"<pre><code>ContainsChains(chains: bool | Sequence[Chain] = True, chain_ids: Iterable[str] = None, rename_chains: bool = False, as_mates: bool = False, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsStructures</code></p> <p>Implements methods to interact with a Structure which contains Chain instances</p> <p>Parameters:</p> <ul> <li> <code>chain_ids</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>A list of identifiers to assign to each Chain instance</p> </li> <li> <code>chains</code>             (<code>bool | Sequence[Chain]</code>, default:                 <code>True</code> )         \u2013          <p>Whether to create Chain instances from passed Structure container instances, or existing Chain instances to create the Model with</p> </li> <li> <code>rename_chains</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to name each chain an incrementally new Alphabetical character</p> </li> <li> <code>as_mates</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether Chain instances should be controlled by a captain (True), or be dependents</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, chains: bool | Sequence[Chain] = True, chain_ids: Iterable[str] = None,\n             rename_chains: bool = False, as_mates: bool = False, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        chain_ids: A list of identifiers to assign to each Chain instance\n        chains: Whether to create Chain instances from passed Structure container instances, or existing Chain\n            instances to create the Model with\n        rename_chains: Whether to name each chain an incrementally new Alphabetical character\n        as_mates: Whether Chain instances should be controlled by a captain (True), or be dependents\n    \"\"\"\n    super().__init__(**kwargs)  # ContainsChains\n    # Use the same list as default to save parsed chain ids\n    self.original_chain_ids = self.chain_ids = []\n    if chains:  # Populate chains\n        self.structure_containers.append('_chains')\n        if isinstance(chains, Sequence):\n            # Set the chains accordingly, copying them to remove prior relationships\n            self._chains = list(chains)\n            self._copy_structure_containers()  # Copy each Chain in chains\n            if as_mates:\n                if self.residues is None:\n                    raise DesignError(\n                        f\"Couldn't initialize {self.__class__.__name__}.chains as it is missing '.residues' while \"\n                        f\"{as_mates=}\"\n                    )\n            else:  # Create the instance from existing chains\n                self.assign_residues_from_structures(chains)\n                # Reindex all residue and atom indices\n                self.reset_and_reindex_structures(self._chains)\n                # Set the parent attribute for all containers\n                self._update_structure_container_attributes(_parent=self)\n\n            if chain_ids:\n                for chain, id_ in zip(self.chains, chain_ids):\n                    chain.chain_id = id_\n            # By using extend, self.original_chain_ids are set as well\n            self.chain_ids.extend([chain.chain_id for chain in self.chains])\n        else:  # Create Chain instances from Residues\n            self._chains = []\n            self._create_chains(chain_ids=chain_ids)\n            if as_mates:\n                for chain in self.chains:\n                    chain.make_parent()\n\n        if rename_chains or not self.are_chain_ids_pdb_compatible():\n            self.rename_chains()\n\n        self.log.debug(f'Original chain_ids: {\",\".join(self.original_chain_ids)} | '\n                       f'Loaded chain_ids: {\",\".join(self.chain_ids)}')\n    else:\n        self._chains = []\n\n    if self.is_parent():\n        reference_sequence = self.metadata.reference_sequence\n        if isinstance(reference_sequence, dict):  # Was parsed from file\n            self.set_reference_sequence_from_seqres(reference_sequence)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.chains","title":"chains  <code>property</code>","text":"<pre><code>chains: list[Chain]\n</code></pre> <p>Returns the Chain instances which are contained in the instance</p>"},{"location":"reference/structure/model/#structure.model.ContainsChains.number_of_chains","title":"number_of_chains  <code>property</code>","text":"<pre><code>number_of_chains: int\n</code></pre> <p>Return the number of Chain instances in the Structure</p>"},{"location":"reference/structure/model/#structure.model.ContainsChains.chain_breaks","title":"chain_breaks  <code>property</code>","text":"<pre><code>chain_breaks: list[int]\n</code></pre> <p>Return the index where each of the Chain instances ends, i.e. at the c-terminal Residue</p>"},{"location":"reference/structure/model/#structure.model.ContainsChains.from_chains","title":"from_chains  <code>classmethod</code>","text":"<pre><code>from_chains(chains: Sequence[Chain], **kwargs)\n</code></pre> <p>Create an instance from a Sequence of Chain objects. Automatically renames all chains</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_chains(cls, chains: Sequence[Chain], **kwargs):\n    \"\"\"Create an instance from a Sequence of Chain objects. Automatically renames all chains\"\"\"\n    return cls(chains=chains, rename_chains=True, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.has_dependent_chains","title":"has_dependent_chains","text":"<pre><code>has_dependent_chains() -&gt; bool\n</code></pre> <p>Returns True if the .chains are dependents, otherwise Returns False if .chains are symmetry mates</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def has_dependent_chains(self) -&gt; bool:\n    \"\"\"Returns True if the .chains are dependents, otherwise Returns False if .chains are symmetry mates\"\"\"\n    return '_chains' in self.structure_containers\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.is_parsed_multimodel","title":"is_parsed_multimodel","text":"<pre><code>is_parsed_multimodel() -&gt; bool\n</code></pre> <p>Returns True if parsing located multiple MODEL records, aka a 'multimodel' or multistate Structure</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_parsed_multimodel(self) -&gt; bool:\n    \"\"\"Returns True if parsing located multiple MODEL records, aka a 'multimodel' or multistate Structure\"\"\"\n    return self.chain_ids != self.original_chain_ids\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.are_chain_ids_pdb_compatible","title":"are_chain_ids_pdb_compatible","text":"<pre><code>are_chain_ids_pdb_compatible() -&gt; bool\n</code></pre> <p>Returns True if the chain_ids are compatible with legacy PDB format</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def are_chain_ids_pdb_compatible(self) -&gt; bool:\n    \"\"\"Returns True if the chain_ids are compatible with legacy PDB format\"\"\"\n    for chain_id in self.chain_ids:\n        if len(chain_id) &gt; 1:\n            return False\n\n    return True\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.rename_chains","title":"rename_chains","text":"<pre><code>rename_chains(exclude_chains: Sequence = None)\n</code></pre> <p>Renames each chain an incrementally new Alphabetical character using Structure.available_letters</p> <p>Parameters:</p> <ul> <li> <code>exclude_chains</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>The chains which shouldn't be modified</p> </li> </ul> Sets <p>self.chain_ids (list[str])</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def rename_chains(self, exclude_chains: Sequence = None):\n    \"\"\"Renames each chain an incrementally new Alphabetical character using Structure.available_letters\n\n    Args:\n        exclude_chains: The chains which shouldn't be modified\n\n    Sets:\n        self.chain_ids (list[str])\n    \"\"\"\n    if exclude_chains is None:\n        exclude_chains = []\n\n    # Update chain_ids, then each chain\n    available_chain_ids = chain_id_generator()\n    self.chain_ids = []\n    for chain in self.chains:\n        chain_id = next(available_chain_ids)\n        while chain_id in exclude_chains:\n            chain_id = next(available_chain_ids)\n        chain.chain_id = chain_id\n        self.chain_ids.append(chain_id)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.renumber_residues_by_chain","title":"renumber_residues_by_chain","text":"<pre><code>renumber_residues_by_chain()\n</code></pre> <p>For each Chain instance, renumber Residue objects sequentially starting with 1</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def renumber_residues_by_chain(self):\n    \"\"\"For each Chain instance, renumber Residue objects sequentially starting with 1\"\"\"\n    for chain in self.chains:\n        chain.renumber_residues()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.get_chain","title":"get_chain","text":"<pre><code>get_chain(chain_id: str) -&gt; Chain | None\n</code></pre> <p>Return the Chain object specified by the passed ChainID from the Structure</p> <p>Parameters:</p> <ul> <li> <code>chain_id</code>             (<code>str</code>)         \u2013          <p>The name of the Chain to query</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Chain | None</code>         \u2013          <p>The Chain if one was found</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_chain(self, chain_id: str) -&gt; Chain | None:\n    \"\"\"Return the Chain object specified by the passed ChainID from the Structure\n\n    Args:\n        chain_id: The name of the Chain to query\n\n    Returns:\n        The Chain if one was found\n    \"\"\"\n    for idx, id_ in enumerate(self.chain_ids):\n        if id_ == chain_id:\n            try:\n                return self.chains[idx]\n            except IndexError:\n                raise IndexError(\n                    f'The number of chains in {repr(self)}, {self.number_of_chains} != {len(self.chain_ids)}, '\n                    'the number of .chain_ids')\n    return None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.set_reference_sequence_from_seqres","title":"set_reference_sequence_from_seqres","text":"<pre><code>set_reference_sequence_from_seqres(reference_sequence: dict[str, str])\n</code></pre> <p>If SEQRES was parsed, set the reference_sequence attribute from each parsed chain_id. Ensure that this is called after self._create_chains()</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def set_reference_sequence_from_seqres(self, reference_sequence: dict[str, str]):\n    \"\"\"If SEQRES was parsed, set the reference_sequence attribute from each parsed chain_id. Ensure that this is\n    called after self._create_chains()\n    \"\"\"\n    for original_chain, chain in zip(self.original_chain_ids, self.chains):\n        try:\n            chain._reference_sequence = reference_sequence[original_chain]\n        except KeyError:  # original_chain not parsed in SEQRES\n            pass\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.chain_id_generator","title":"chain_id_generator  <code>staticmethod</code>","text":"<pre><code>chain_id_generator() -&gt; Generator[str, None, None]\n</code></pre> <p>Provide a generator which produces all combinations of chain ID strings</p> <p>Returns     The generator producing a maximum 2 character string where single characters are exhausted,         first in uppercase, then in lowercase</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@staticmethod\ndef chain_id_generator() -&gt; Generator[str, None, None]:\n    \"\"\"Provide a generator which produces all combinations of chain ID strings\n\n    Returns\n        The generator producing a maximum 2 character string where single characters are exhausted,\n            first in uppercase, then in lowercase\n    \"\"\"\n    return chain_id_generator()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsChains.orient","title":"orient","text":"<pre><code>orient(symmetry: str = None)\n</code></pre> <p>Orient a symmetric Structure at the origin with symmetry axis set on canonical axes defined by symmetry file</p> <p>Sets the Structure with coordinates as described by a canonical orientation</p> <p>Parameters:</p> <ul> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The symmetry of the Structure</p> </li> </ul> <p>Raises:     SymmetryError: When the specified symmetry is incompatible with the Structure     StructureException: When the orient program fails</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def orient(self, symmetry: str = None):\n    \"\"\"Orient a symmetric Structure at the origin with symmetry axis set on canonical axes defined by symmetry file\n\n    Sets the Structure with coordinates as described by a canonical orientation\n\n    Args:\n        symmetry: The symmetry of the Structure\n    Raises:\n        SymmetryError: When the specified symmetry is incompatible with the Structure\n        StructureException: When the orient program fails\n    \"\"\"\n    # These notes are obviated by the use of the below protocol with from_file() constructor\n    # orient_oligomer.f program notes\n    # C\t\tWill not work in any of the infinite situations where a PDB file is f***ed up,\n    # C\t\tin ways such as but not limited to:\n    # C     equivalent residues in different chains don't have the same numbering; different subunits\n    # C\t\tare all listed with the same chain ID (e.g. with incremental residue numbering) instead\n    # C\t\tof separate IDs; multiple conformations are written out for the same subunit structure\n    # C\t\t(as in an NMR ensemble), negative residue numbers, etc. etc.\n    try:\n        subunit_number = utils.symmetry.valid_subunit_number[symmetry]\n    except KeyError:\n        raise SymmetryError(\n            f\"{self.orient.__name__}: Symmetry {symmetry} isn't a valid symmetry. Please try one of: \"\n            f'{\", \".join(utils.symmetry.valid_symmetries)}')\n\n    number_of_subunits = self.number_of_chains\n    multicomponent = False\n    if symmetry == 'C1':\n        self.log.debug(\"C1 symmetry doesn't have a canonical orientation. Translating to the origin\")\n        self.translate(-self.center_of_mass)\n        return\n    elif number_of_subunits &gt; 1:\n        if number_of_subunits != subunit_number:\n            if number_of_subunits in utils.symmetry.multicomponent_valid_subunit_number.get(symmetry):\n                multicomponent = True\n            else:\n                raise SymmetryError(\n                    f\"{self.name} couldn't be oriented: It has {number_of_subunits} subunits while a multiple of \"\n                    f'{subunit_number} are expected for symmetry={symmetry}')\n    else:\n        raise SymmetryError(\n            f\"{self.name}: Can't orient a Structure with only a single chain. No symmetry present\")\n\n    orient_input = Path(putils.orient_exe_dir, 'input.pdb')\n    orient_output = Path(putils.orient_exe_dir, 'output.pdb')\n\n    def clean_orient_input_output():\n        orient_input.unlink(missing_ok=True)\n        orient_output.unlink(missing_ok=True)\n\n    clean_orient_input_output()\n    orient_kwargs = {'out_path': str(orient_input)}\n    if multicomponent:\n        if not isinstance(self, ContainsEntities):\n            raise SymmetryError(\n                f\"Couldn't {repr(self)}.{self.orient.__name__} as the symmetry is {multicomponent=}, however, there\"\n                f\" are no .entities in the class {self.__class__.__name__}\"\n            )\n        self.entities[0].write(assembly=True, **orient_kwargs)\n    else:\n        self.write(**orient_kwargs)\n\n    name = self.name\n    p = subprocess.Popen([putils.orient_exe_path], stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE, cwd=putils.orient_exe_dir)\n    in_symm_file = os.path.join(putils.orient_exe_dir, 'symm_files', symmetry)\n    stdout, stderr = p.communicate(input=in_symm_file.encode('utf-8'))\n    self.log.debug(name + stdout.decode()[28:])\n    self.log.debug(stderr.decode()) if stderr else None\n    if not orient_output.exists() or orient_output.stat().st_size == 0:\n        try:\n            log_file = getattr(self.log.handlers[0], 'baseFilename', None)\n        except IndexError:  # No handlers attached\n            log_file = None\n        log_message = f'. Check {log_file} for more information' if log_file else \\\n            f': {stderr.decode()}' if stderr else ''\n        clean_orient_input_output()\n        raise StructureException(\n            f\"{putils.orient_exe_path} couldn't orient {name}{log_message}\")\n\n    oriented_pdb = Model.from_file(str(orient_output), name=self.name, log=self.log)\n    orient_fixed_struct = oriented_pdb.chains[0]\n    if multicomponent:\n        moving_struct = self.entities[0]\n    else:\n        moving_struct = self.chains[0]\n\n    orient_fixed_seq = orient_fixed_struct.sequence\n    moving_seq = moving_struct.sequence\n\n    fixed_coords = orient_fixed_struct.ca_coords\n    moving_coords = moving_struct.ca_coords\n    if orient_fixed_seq != moving_seq:\n        # Do an alignment, get selective indices, then follow with superposition\n        self.log.debug(f'{self.orient.__name__}(): existing Chain {moving_struct.chain_id} and '\n                       f'oriented Chain {orient_fixed_struct.chain_id} are being aligned for superposition')\n        fixed_indices, moving_indices = get_equivalent_indices(orient_fixed_seq, moving_seq)\n        fixed_coords = fixed_coords[fixed_indices]\n        moving_coords = moving_coords[moving_indices]\n\n    _, rot, tx = superposition3d(fixed_coords, moving_coords)\n\n    self.transform(rotation=rot, translation=tx)\n    clean_orient_input_output()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities","title":"ContainsEntities","text":"<pre><code>ContainsEntities(entities: bool | Sequence[Entity] = True, entity_info: dict[str, dict[dict | list | str]] = None, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsChains</code></p> <p>Implements methods to interact with a Structure which contains Entity instances</p> <p>Parameters:</p> <ul> <li> <code>entities</code>             (<code>bool | Sequence[Entity]</code>, default:                 <code>True</code> )         \u2013          <p>Existing Entity instances used to construct the Structure, or evaluates False to skip creating Entity instances from the existing '.chains' Chain instances</p> </li> <li> <code>entity_info</code>             (<code>dict[str, dict[dict | list | str]]</code>, default:                 <code>None</code> )         \u2013          <p>Metadata describing the Entity instances</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, entities: bool | Sequence[Entity] = True, entity_info: dict[str, dict[dict | list | str]] = None,\n             **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        entities: Existing Entity instances used to construct the Structure, or evaluates False to skip creating\n            Entity instances from the existing '.chains' Chain instances\n        entity_info: Metadata describing the Entity instances\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # ContainsEntities\n\n    self.entity_info = {} if entity_info is None else entity_info\n\n    if entities:\n        self.structure_containers.append('_entities')\n        if isinstance(entities, Sequence):\n            # Create the instance from existing entities\n            self.assign_residues_from_structures(entities)\n            # Set the entities accordingly, first copying, then resetting, and finally updating the parent\n            self._entities = entities\n            self._copy_structure_containers()\n            self.reset_and_reindex_structures(self._entities)\n            self._update_structure_container_attributes(_parent=self)\n            rename_chains = kwargs.get('rename_chains')\n            if rename_chains:  # Set each successive Entity to have an incrementally higher chain id\n                available_chain_ids = chain_id_generator()\n                for entity in self.entities:\n                    entity.chain_id = next(available_chain_ids)\n                    # If the full structure wanted contiguous chain_ids, this should be used\n                    # for _ in range(entity.number_of_symmetry_mates):\n                    #     # Discard ids\n                    #     next(available_chain_ids)\n                    # self.log.debug(f'Entity {entity.name} new chain identifier {entity.chain_id}')\n                # self.chain_ids.extend([entity.chain_id for entity in self.entities])\n        else:  # Provided as True\n            self._entities = []\n            self._create_entities(**kwargs)\n\n        if not self.chain_ids:\n            # Set chain_ids according to self.entities as it wasn't set by self.chains (probably False)\n            self.chain_ids.extend([entity.chain_id for entity in self.entities])\n    else:\n        self._entities = []\n\n    # If any of the entities are symmetric, ensure the new Model is aware they are\n    for entity in self.entities:\n        if entity.is_symmetric():\n            self.symmetric_dependents = '_entities'\n            break\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.entity_info","title":"entity_info  <code>instance-attribute</code> <code>property</code> <code>writable</code>","text":"<pre><code>entity_info: dict[str, dict[dict | list | str]] | dict = {} if entity_info is None else entity_info\n</code></pre> <p>Mapping of the Entity name to Metadata describing the Entity instance</p>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.entities","title":"entities  <code>property</code>","text":"<pre><code>entities: list[Entity]\n</code></pre> <p>Returns each of the Entity instances in the Structure</p>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.number_of_entities","title":"number_of_entities  <code>property</code>","text":"<pre><code>number_of_entities: int\n</code></pre> <p>Return the number of Entity instances in the Structure</p>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.entity_breaks","title":"entity_breaks  <code>property</code>","text":"<pre><code>entity_breaks: list[int]\n</code></pre> <p>Return the index where each of the Entity instances ends, i.e. at the c-terminal Residue</p>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.sequence","title":"sequence  <code>property</code>","text":"<pre><code>sequence: str\n</code></pre> <p>Return the sequence of structurally modeled residues for every Entity instance</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The concatenated sequence for all Entity instances combined</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.reference_sequence","title":"reference_sequence  <code>property</code>","text":"<pre><code>reference_sequence: str\n</code></pre> <p>Return the sequence for every Entity instance, constituting all Residues, not just structurally modeled ones</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The concatenated reference sequences for all Entity instances combined</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.atom_indices_per_entity","title":"atom_indices_per_entity  <code>property</code>","text":"<pre><code>atom_indices_per_entity: list[list[int]]\n</code></pre> <p>Return the atom indices for each Entity</p>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.from_entities","title":"from_entities  <code>classmethod</code>","text":"<pre><code>from_entities(entities: list[Entity] | Structures, rename_chains: bool = True, **kwargs)\n</code></pre> <p>Construct a Structure instance from a container of Entity objects</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_entities(cls, entities: list[Entity] | Structures, rename_chains: bool = True, **kwargs):\n    \"\"\"Construct a Structure instance from a container of Entity objects\"\"\"\n    if not isinstance(entities, (list, Structures)):\n        raise ValueError(\n            f\"{cls.__name__}.{cls.from_entities.__name__}() constructor received \"\n            f\"'entities'={type(entities).__name__}. Expected list[Entity]\"  # or Structures\n        )\n    return cls(entities=entities, chains=False, rename_chains=rename_chains, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.format_header","title":"format_header","text":"<pre><code>format_header(**kwargs) -&gt; str\n</code></pre> <p>Return any super().format_header()</p> <p>Other Parameters:</p> <ul> <li> <code>assembly</code>         \u2013          <p>bool = False - Whether to write header details for the assembly</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The .pdb file header string</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_header(self, **kwargs) -&gt; str:\n    \"\"\"Return any super().format_header()\n\n    Keyword Args:\n        assembly: bool = False - Whether to write header details for the assembly\n\n    Returns:\n        The .pdb file header string\n    \"\"\"\n    return super().format_header(**kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.retrieve_metadata_from_pdb","title":"retrieve_metadata_from_pdb","text":"<pre><code>retrieve_metadata_from_pdb(biological_assembly: int = None) -&gt; dict[str, Any] | dict\n</code></pre> <p>Query the PDB API for information on the PDB code found at the Model.name attribute</p> <p>For each new instance, makes one call to the PDB API, plus an additional call for each Entity, and one more if biological_assembly is passed. If this has been loaded before, it uses the persistent wrapapi.APIDatabase</p> <p>Parameters:</p> <ul> <li> <code>biological_assembly</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The number of the biological assembly that is associated with this structural state</p> </li> </ul> Sets <p>self.api_entry (dict[str, dict[Any] | float] | dict):     {'assembly': [['A', 'B'], ...],      'entity': {'EntityID':                     {'chains': ['A', 'B', ...],                      'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'}                      'reference_sequence': 'MSLEHHHHHH...',                      'thermophilicity': 1.0},                 ...},      'res': resolution,      'struct': {'space': space_group, 'a_b_c': (a, b, c),                 'ang_a_b_c': (ang_a, ang_b, ang_c)}     }</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def retrieve_metadata_from_pdb(self, biological_assembly: int = None) -&gt; dict[str, Any] | dict:\n    \"\"\"Query the PDB API for information on the PDB code found at the Model.name attribute\n\n    For each new instance, makes one call to the PDB API, plus an additional call for each Entity, and one more\n    if biological_assembly is passed. If this has been loaded before, it uses the persistent wrapapi.APIDatabase\n\n    Args:\n        biological_assembly: The number of the biological assembly that is associated with this structural state\n\n    Sets:\n        self.api_entry (dict[str, dict[Any] | float] | dict):\n            {'assembly': [['A', 'B'], ...],\n             'entity': {'EntityID':\n                            {'chains': ['A', 'B', ...],\n                             'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'}\n                             'reference_sequence': 'MSLEHHHHHH...',\n                             'thermophilicity': 1.0},\n                        ...},\n             'res': resolution,\n             'struct': {'space': space_group, 'a_b_c': (a, b, c),\n                        'ang_a_b_c': (ang_a, ang_b, ang_c)}\n            }\n    \"\"\"\n    # api_entry = self.api_entry\n    # if api_entry is not None:  # Already tried to solve this\n    #     return\n\n    # if self.api_db:\n    try:\n        # retrieve_api_info = self.api_db.pdb.retrieve_data\n        retrieve_api_info = resources.wrapapi.api_database_factory().pdb.retrieve_data\n    except AttributeError:\n        retrieve_api_info = query.pdb.query_pdb_by\n\n    # if self.name:  # Try to solve API details from name\n    parsed_name = self.name\n    splitter_iter = iter('_-')  # 'entity, assembly'\n    idx = count(-1)\n    extra = None\n    while len(parsed_name) != 4:\n        try:  # To parse the name using standard PDB API entry ID's\n            parsed_name, *extra = parsed_name.split(next(splitter_iter))\n        except StopIteration:\n            # We didn't find an EntryID in parsed_name from splitting typical PDB formatted strings\n            self.log.debug(f\"The name '{self.name}' can't be coerced to PDB API format\")\n            # api_entry = {}\n            return {}\n        else:\n            next(idx)\n    # Set the index to the index that was stopped at\n    idx = next(idx)\n\n    # At some point, len(parsed_name) == 4\n    if biological_assembly is not None:\n        # query_args.update(assembly_integer=self.assembly)\n        # # self.api_entry.update(_get_assembly_info(self.name))\n        api_entry = retrieve_api_info(entry=parsed_name) or {}\n        api_entry['assembly'] = retrieve_api_info(entry=parsed_name, assembly_integer=biological_assembly)\n        # ^ returns [['A', 'A', 'A', ...], ...]\n    elif extra:  # Extra not None or []\n        # Try to parse any found extra to an integer denoting entity or assembly ID\n        integer, *non_sense = extra\n        if integer.isdigit() and not non_sense:\n            integer = int(integer)\n            if idx == 0:  # Entity integer, such as 1ABC_1.pdb\n                api_entry = dict(entity=retrieve_api_info(entry=parsed_name, entity_integer=integer))\n                # retrieve_api_info returns\n                # {'EntityID': {'chains': ['A', 'B', ...],\n                #               'dbref': {'accession': ('Q96DC8',), 'db': 'UniProt'}\n                #               'reference_sequence': 'MSLEHHHHHH...',\n                #               'thermophilicity': 1.0},\n                #  ...}\n                parsed_name = f'{parsed_name}_{integer}'\n            else:  # Get entry alone. This is an assembly or unknown conjugation. Either way entry info is needed\n                api_entry = retrieve_api_info(entry=parsed_name) or {}\n\n                if idx == 1:  # This is an assembly integer, such as 1ABC-1.pdb\n                    api_entry['assembly'] = retrieve_api_info(entry=parsed_name, assembly_integer=integer)\n        else:  # This isn't an integer or there are extra characters\n            # It's likely they are extra characters that won't be of help\n            # Tod0, try to collect anyway?\n            self.log.debug(\n                f\"The name '{self.name}' contains extra info that can't be coerced to PDB API format\")\n            api_entry = {}\n    elif extra is None:  # Nothing extra as it was correct length to begin with, just query entry\n        api_entry = retrieve_api_info(entry=parsed_name)\n    else:\n        raise RuntimeError(\n            f\"This logic wasn't expected and shouldn't be allowed to persist: \"\n            f'self.name={self.name}, parse_name={parsed_name}, extra={extra}, idx={idx}')\n    if api_entry:\n        self.log.debug(f'Found PDB API information: '\n                       f'{\", \".join(f\"{k}={v}\" for k, v in api_entry.items())}')\n        # Set the identified name to lowercase\n        self.name = parsed_name.lower()\n        for entity in self.entities:\n            entity.name = entity.name.lower()\n\n    return api_entry\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.get_entity","title":"get_entity","text":"<pre><code>get_entity(entity_id: str) -&gt; Entity | None\n</code></pre> <p>Retrieve an Entity by name</p> <p>Parameters:</p> <ul> <li> <code>entity_id</code>             (<code>str</code>)         \u2013          <p>The name of the Entity to query</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Entity | None</code>         \u2013          <p>The Entity if one was found</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_entity(self, entity_id: str) -&gt; Entity | None:\n    \"\"\"Retrieve an Entity by name\n\n    Args:\n        entity_id: The name of the Entity to query\n\n    Returns:\n        The Entity if one was found\n    \"\"\"\n    for entity in self.entities:\n        if entity_id == entity.name:\n            return entity\n    return None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.entity_from_chain","title":"entity_from_chain","text":"<pre><code>entity_from_chain(chain_id: str) -&gt; Entity | None\n</code></pre> <p>Returns the entity associated with a particular chain id</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def entity_from_chain(self, chain_id: str) -&gt; Entity | None:\n    \"\"\"Returns the entity associated with a particular chain id\"\"\"\n    for entity in self.entities:\n        if chain_id == entity.chain_id:\n            return entity\n    return None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.ContainsEntities.match_entity_by_seq","title":"match_entity_by_seq","text":"<pre><code>match_entity_by_seq(other_seq: str = None, force_closest: bool = True, tolerance: float = 0.7) -&gt; Entity | None\n</code></pre> <p>From another sequence, returns the first matching chain from the corresponding Entity</p> <p>Uses a local alignment to produce the match score</p> <p>Parameters:</p> <ul> <li> <code>other_seq</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The sequence to query</p> </li> <li> <code>force_closest</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to force the search if a perfect match isn't identified</p> </li> <li> <code>tolerance</code>             (<code>float</code>, default:                 <code>0.7</code> )         \u2013          <p>The acceptable difference between sequences to consider them the same Entity. Tuning this parameter is necessary if you have sequences which should be considered different entities, but are fairly similar</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Entity | None</code>         \u2013          <p>The matching Entity if one was found</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def match_entity_by_seq(\n    self, other_seq: str = None, force_closest: bool = True, tolerance: float = 0.7\n) -&gt; Entity | None:\n    \"\"\"From another sequence, returns the first matching chain from the corresponding Entity\n\n    Uses a local alignment to produce the match score\n\n    Args:\n        other_seq: The sequence to query\n        force_closest: Whether to force the search if a perfect match isn't identified\n        tolerance: The acceptable difference between sequences to consider them the same Entity.\n            Tuning this parameter is necessary if you have sequences which should be considered different entities,\n            but are fairly similar\n\n    Returns:\n        The matching Entity if one was found\n    \"\"\"\n    for entity in self.entities:\n        if other_seq == entity.sequence:\n            return entity\n\n    # We didn't find an ideal match\n    if force_closest:\n        entity_alignment_scores = {}\n        for entity in self.entities:\n            alignment = generate_alignment(other_seq, entity.sequence, local=True)\n            entity_alignment_scores[entity] = alignment.score\n\n        max_score, max_score_entity = 0, None\n        for entity, score in entity_alignment_scores.items():\n            normalized_score = score / len(entity.sequence)\n            if normalized_score &gt; max_score:\n                max_score = normalized_score  # alignment_score_d[entity]\n                max_score_entity = entity\n\n        if max_score &gt; tolerance:\n            return max_score_entity\n\n    return None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin","title":"SymmetryOpsMixin","text":"<pre><code>SymmetryOpsMixin(sym_entry: SymEntry | int = None, symmetry: str = None, transformations: list[TransformationMapping] = None, uc_dimensions: list[float] = None, symmetry_operators: ndarray | list = None, rotation_matrices: ndarray | list = None, translation_matrices: ndarray | list = None, surrounding_uc: bool = True, **kwargs)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Implements methods to interact with symmetric Structure instances</p> <p>Parameters:</p> <ul> <li> <code>sym_entry</code>             (<code>SymEntry | int</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry which specifies all symmetry parameters</p> </li> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a symmetry to be searched against compatible symmetries</p> </li> <li> <code>transformations</code>             (<code>list[TransformationMapping]</code>, default:                 <code>None</code> )         \u2013          <p>Transformation operations that reproduce the oligomeric/assembly for each Entity</p> </li> <li> <code>rotation_matrices</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>Rotation operations that create the symmetric state</p> </li> <li> <code>translation_matrices</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>Translation operations that create the symmetric state</p> </li> <li> <code>uc_dimensions</code>             (<code>list[float]</code>, default:                 <code>None</code> )         \u2013          <p>The unit cell dimensions for the crystalline symmetry</p> </li> <li> <code>symmetry_operators</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>A set of custom expansion matrices</p> </li> <li> <code>surrounding_uc</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the 3x3 layer group, or 3x3x3 space group should be generated</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, sym_entry: utils.SymEntry.SymEntry | int = None, symmetry: str = None,\n             transformations: list[types.TransformationMapping] = None, uc_dimensions: list[float] = None,\n             symmetry_operators: np.ndarray | list = None, rotation_matrices: np.ndarray | list = None,\n             translation_matrices: np.ndarray | list = None, surrounding_uc: bool = True, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        sym_entry: The SymEntry which specifies all symmetry parameters\n        symmetry: The name of a symmetry to be searched against compatible symmetries\n        transformations: Transformation operations that reproduce the oligomeric/assembly for each Entity\n        rotation_matrices: Rotation operations that create the symmetric state\n        translation_matrices: Translation operations that create the symmetric state\n        uc_dimensions: The unit cell dimensions for the crystalline symmetry\n        symmetry_operators: A set of custom expansion matrices\n        surrounding_uc: Whether the 3x3 layer group, or 3x3x3 space group should be generated\n    \"\"\"\n    super().__init__(**kwargs)  # SymmetryOpsMixin\n    self._expand_matrices = self._expand_translations = None\n    self.set_symmetry(sym_entry=sym_entry, symmetry=symmetry, uc_dimensions=uc_dimensions,\n                      operators=symmetry_operators, rotations=rotation_matrices, translations=translation_matrices,\n                      transformations=transformations, surrounding_uc=surrounding_uc)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.sym_entry","title":"sym_entry  <code>property</code> <code>writable</code>","text":"<pre><code>sym_entry: SymEntry | None\n</code></pre> <p>The SymEntry specifies the symmetric parameters for the utilized symmetry</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.point_group_symmetry","title":"point_group_symmetry  <code>property</code>","text":"<pre><code>point_group_symmetry: str | None\n</code></pre> <p>The point group underlying the resulting SymEntry</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.dimension","title":"dimension  <code>property</code>","text":"<pre><code>dimension: int | None\n</code></pre> <p>The dimension of the symmetry from None, 0, 2, or 3</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.uc_dimensions","title":"uc_dimensions  <code>property</code>","text":"<pre><code>uc_dimensions: tuple[float, float, float, float, float, float] | None\n</code></pre> <p>The unit cell dimensions for the lattice specified by lengths a, b, c and angles alpha, beta, gamma</p> <p>Returns:</p> <ul> <li> <code>tuple[float, float, float, float, float, float] | None</code>         \u2013          <p>length a, length b, length c, angle alpha, angle beta, angle gamma</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.cryst_record","title":"cryst_record  <code>property</code> <code>writable</code>","text":"<pre><code>cryst_record: str | None\n</code></pre> <p>Return the symmetry parameters as a CRYST1 entry</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.expand_matrices","title":"expand_matrices  <code>property</code>","text":"<pre><code>expand_matrices: ndarray\n</code></pre> <p>The symmetry rotations to generate each of the symmetry mates</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.expand_translations","title":"expand_translations  <code>property</code>","text":"<pre><code>expand_translations: ndarray\n</code></pre> <p>The symmetry translations to generate each of the symmetry mates</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.chain_transforms","title":"chain_transforms  <code>property</code>","text":"<pre><code>chain_transforms: list[TransformationMapping]\n</code></pre> <p>Returns the transformation operations for each of the symmetry mates (excluding the ASU)</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.number_of_symmetric_residues","title":"number_of_symmetric_residues  <code>property</code>","text":"<pre><code>number_of_symmetric_residues: int\n</code></pre> <p>Describes the number of Residues when accounting for symmetry mates</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.number_of_symmetry_mates","title":"number_of_symmetry_mates  <code>property</code>","text":"<pre><code>number_of_symmetry_mates: int\n</code></pre> <p>Describes the number of symmetric copies present in the coordinates</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.number_of_uc_symmetry_mates","title":"number_of_uc_symmetry_mates  <code>property</code>","text":"<pre><code>number_of_uc_symmetry_mates: int\n</code></pre> <p>Describes the number of symmetry mates present in the unit cell</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.asu_model_index","title":"asu_model_index  <code>property</code>","text":"<pre><code>asu_model_index: int\n</code></pre> <p>The asu equivalent model in the SymmetricModel. Zero-indexed</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.asu_indices","title":"asu_indices  <code>property</code>","text":"<pre><code>asu_indices: slice\n</code></pre> <p>Return the ASU indices</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.symmetric_coords","title":"symmetric_coords  <code>property</code>","text":"<pre><code>symmetric_coords: ndarray | None\n</code></pre> <p>Return a view of the symmetric Coords</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.symmetric_coords_split","title":"symmetric_coords_split  <code>property</code>","text":"<pre><code>symmetric_coords_split: list[ndarray]\n</code></pre> <p>A view of the symmetric coords split at different symmetric models</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.center_of_mass_symmetric","title":"center_of_mass_symmetric  <code>property</code>","text":"<pre><code>center_of_mass_symmetric: ndarray\n</code></pre> <p>The center of mass for the symmetric system with shape (3,)</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.center_of_mass_symmetric_models","title":"center_of_mass_symmetric_models  <code>property</code>","text":"<pre><code>center_of_mass_symmetric_models: ndarray\n</code></pre> <p>The center of mass points for each symmetry mate in the symmetric system with shape (number_of_symmetry_mates, 3)</p>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.format_biomt","title":"format_biomt","text":"<pre><code>format_biomt(**kwargs) -&gt; str\n</code></pre> <p>Return the SymmetricModel expand_matrices as a BIOMT record</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The BIOMT REMARK 350 with PDB file formatting</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_biomt(self, **kwargs) -&gt; str:\n    \"\"\"Return the SymmetricModel expand_matrices as a BIOMT record\n\n    Returns:\n        The BIOMT REMARK 350 with PDB file formatting\n    \"\"\"\n    if self.is_symmetric():\n        if self.dimension &lt; 2:\n            return '%s\\n' % '\\n'.join(\n                'REMARK 350   BIOMT{:1d}{:4d}{:10.6f}{:10.6f}{:10.6f}{:15.5f}            '.format(\n                    v_idx, m_idx, *vec, point)\n                for m_idx, (mat, tx) in enumerate(\n                    zip(self.expand_matrices.tolist(), self.expand_translations.tolist()), 1)\n                for v_idx, (vec, point) in enumerate(zip(mat, tx), 1)\n            )\n    return ''\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.format_header","title":"format_header","text":"<pre><code>format_header(**kwargs) -&gt; str\n</code></pre> <p>Returns any super().format_header() along with the BIOMT record</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The .pdb file header string</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_header(self, **kwargs) -&gt; str:\n    \"\"\"Returns any super().format_header() along with the BIOMT record\n\n    Returns:\n        The .pdb file header string\n    \"\"\"\n    return super().format_header(**kwargs) + self.format_biomt()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.set_symmetry","title":"set_symmetry","text":"<pre><code>set_symmetry(sym_entry: SymEntry | int = None, symmetry: str = None, crystal: bool = False, cryst_record: str = None, uc_dimensions: list[float] = None, **kwargs)\n</code></pre> <p>Set the model symmetry using the CRYST1 record, or the unit cell dimensions and the Hermann-Mauguin symmetry notation (in CRYST1 format, ex P432) for the Model assembly. If the assembly is a point group, only the symmetry notation is required</p> <p>Parameters:</p> <ul> <li> <code>sym_entry</code>             (<code>SymEntry | int</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry which specifies all symmetry parameters</p> </li> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a symmetry to be searched against compatible symmetries</p> </li> <li> <code>crystal</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether crystalline symmetry should be used</p> </li> <li> <code>cryst_record</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a CRYST1 record is known and should be used</p> </li> <li> <code>uc_dimensions</code>             (<code>list[float]</code>, default:                 <code>None</code> )         \u2013          <p>The unit cell dimensions for the crystalline symmetry</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def set_symmetry(self, sym_entry: utils.SymEntry.SymEntry | int = None, symmetry: str = None,\n                 crystal: bool = False, cryst_record: str = None, uc_dimensions: list[float] = None,\n                 **kwargs):\n    \"\"\"Set the model symmetry using the CRYST1 record, or the unit cell dimensions and the Hermann-Mauguin symmetry\n    notation (in CRYST1 format, ex P432) for the Model assembly. If the assembly is a point group, only the symmetry\n    notation is required\n\n    Args:\n        sym_entry: The SymEntry which specifies all symmetry parameters\n        symmetry: The name of a symmetry to be searched against compatible symmetries\n        crystal: Whether crystalline symmetry should be used\n        cryst_record: If a CRYST1 record is known and should be used\n        uc_dimensions: The unit cell dimensions for the crystalline symmetry\n    \"\"\"\n    # Try to solve for symmetry as uc_dimensions are needed for cryst ops, if available\n    crystal_symmetry = None\n    if symmetry is not None:\n        # Ensure conversion to Hermann\u2013Mauguin notation. ex: P23 not P 2 3\n        symmetry = ''.join(symmetry.split())\n\n    if cryst_record:\n        self.cryst_record = cryst_record\n        crystal = True\n    else:\n        cryst_record = self.cryst_record  # Populated above or from file parsing\n        if uc_dimensions and symmetry:\n            crystal_symmetry = symmetry\n            crystal = True\n\n    if cryst_record:  # Populated above or from file parsing\n        self.log.debug(f'Parsed record: {cryst_record.strip()}')\n        if uc_dimensions is None and symmetry is None:  # Only if didn't provide either\n            uc_dimensions, crystal_symmetry = parse_cryst_record(cryst_record)\n            crystal_symmetry = ''.join(crystal_symmetry)\n        self.log.debug(f'Found uc_dimensions={uc_dimensions}, symmetry={crystal_symmetry}')\n\n    if crystal:  # CRYST in symmetry.upper():\n        if sym_entry is None:\n            sym_entry = utils.SymEntry.CrystRecord\n\n    number_of_entities = self.number_of_entities\n    if sym_entry is not None:\n        if isinstance(sym_entry, utils.SymEntry.SymEntry):\n            if sym_entry.needs_cryst_record():  # Replace with relevant info from the CRYST1 record\n                if sym_entry.is_token():\n                    # Create a new SymEntry\n                    sym_entry = utils.SymEntry.CrystSymEntry(\n                        space_group=crystal_symmetry,\n                        sym_map=[crystal_symmetry] + ['C1' for _ in range(number_of_entities)])\n                    # Set the uc_dimensions as they must be parsed or provided\n                    self.log.critical(f'Setting {self}.sym_entry to new crystalline symmetry {sym_entry}')\n                elif sym_entry.resulting_symmetry == crystal_symmetry:\n                    # This is already the specified SymEntry, use the CRYST record to set cryst_record\n                    self.log.critical(f'Setting {self}.sym_entry to {sym_entry}')\n                else:\n                    raise SymmetryError(\n                        f\"The parsed CRYST record with symmetry '{crystal_symmetry}' doesn't match the symmetry \"\n                        f\"'{sym_entry.resulting_symmetry}' specified by the provided {type(sym_entry).__name__}\"\n                    )\n                sym_entry.uc_dimensions = uc_dimensions\n                sym_entry.cryst_record = self.cryst_record\n            # else:  # SymEntry is set up properly\n            #     self.sym_entry = sym_entry\n        else:  # Try to solve using sym_entry as integer and any info in symmetry.\n            sym_entry = utils.SymEntry.parse_symmetry_to_sym_entry(\n                sym_entry_number=sym_entry, symmetry=symmetry)\n    elif symmetry:  # Provided without uc_dimensions, crystal=True, or cryst_record. Assuming point group\n        sym_entry = utils.SymEntry.parse_symmetry_to_sym_entry(\n            symmetry=symmetry,\n            # The below fails as most of the time entity.symmetry isn't set up at this point\n            # sym_map=[symmetry] + [entity.symmetry for entity in self.entities]\n        )\n    else:  # No symmetry was provided\n        # self.sym_entry/self.symmetry can be None\n        return\n\n    # Ensure the number of Entity instances matches the SymEntry groups\n    n_groups = sym_entry.number_of_groups\n    if number_of_entities != n_groups:\n        if n_groups == 1:\n            verb = 'was'\n        else:\n            verb = 'were'\n\n        raise SymmetryError(\n            f'The {self.__class__.__name__} has {number_of_entities} entities. '\n            f'{n_groups} {verb} expected based on the {repr(sym_entry)} specified'\n        )\n    self.sym_entry = sym_entry\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.reset_mates","title":"reset_mates","text":"<pre><code>reset_mates()\n</code></pre> <p>Remove oligomeric chains. They should be generated fresh</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def reset_mates(self):\n    \"\"\"Remove oligomeric chains. They should be generated fresh\"\"\"\n    self._chains.clear()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.is_surrounding_uc","title":"is_surrounding_uc","text":"<pre><code>is_surrounding_uc() -&gt; bool\n</code></pre> <p>Returns True if the current coordinates contains symmetry mates from the surrounding unit cells</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_surrounding_uc(self) -&gt; bool:\n    \"\"\"Returns True if the current coordinates contains symmetry mates from the surrounding unit cells\"\"\"\n    if self.dimension &gt; 0:\n        # This is True if self.number_of_symmetry_mates was set to a larger value\n        return self.number_of_symmetry_mates &gt; self.number_of_uc_symmetry_mates\n    else:\n        return False\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.make_indices_symmetric","title":"make_indices_symmetric","text":"<pre><code>make_indices_symmetric(indices: Iterable[int], dtype: atom_or_residue_literal = 'atom') -&gt; list[int]\n</code></pre> <p>Extend asymmetric indices using the symmetry state across atom or residue indices</p> <p>Parameters:</p> <ul> <li> <code>indices</code>             (<code>Iterable[int]</code>)         \u2013          <p>The asymmetric indices to symmetrize</p> </li> <li> <code>dtype</code>             (<code>atom_or_residue_literal</code>, default:                 <code>'atom'</code> )         \u2013          <p>The type of indices to perform symmetrization with</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The symmetric indices of the asymmetric input</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_indices_symmetric(self, indices: Iterable[int], dtype: atom_or_residue_literal = 'atom') -&gt; list[int]:\n    \"\"\"Extend asymmetric indices using the symmetry state across atom or residue indices\n\n    Args:\n        indices: The asymmetric indices to symmetrize\n        dtype: The type of indices to perform symmetrization with\n\n    Returns:\n        The symmetric indices of the asymmetric input\n    \"\"\"\n    try:\n        jump_size = getattr(self, f'number_of_{dtype}s')\n    except AttributeError:\n        raise AttributeError(\n            f\"The dtype 'number_of_{dtype}' wasn't found in the {self.__class__.__name__} object. \"\n            \"'Possible values of dtype are 'atom' or 'residue'\")\n\n    model_jumps = [jump_size * model_num for model_num in range(self.number_of_symmetry_mates)]\n    return [idx + model_jump for model_jump in model_jumps for idx in indices]\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.return_symmetric_copies","title":"return_symmetric_copies","text":"<pre><code>return_symmetric_copies(structure: StructureBase, **kwargs) -&gt; list[StructureBase]\n</code></pre> <p>Expand the provided Structure using self.symmetry for the symmetry specification</p> <p>Parameters:</p> <ul> <li> <code>structure</code>             (<code>StructureBase</code>)         \u2013          <p>A StructureBase instance containing .coords method/attribute</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[StructureBase]</code>         \u2013          <p>The symmetric copies of the input structure</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def return_symmetric_copies(self, structure: StructureBase, **kwargs) -&gt; list[StructureBase]:\n    \"\"\"Expand the provided Structure using self.symmetry for the symmetry specification\n\n    Args:\n        structure: A StructureBase instance containing .coords method/attribute\n\n    Returns:\n        The symmetric copies of the input structure\n    \"\"\"\n    number_of_symmetry_mates = self.number_of_symmetry_mates\n    sym_coords = self.return_symmetric_coords(structure.coords)\n\n    sym_mates = []\n    for coord_set in np.split(sym_coords, number_of_symmetry_mates):\n        symmetry_mate = structure.copy()\n        symmetry_mate.coords = coord_set\n        sym_mates.append(symmetry_mate)\n\n    return sym_mates\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.generate_symmetric_coords","title":"generate_symmetric_coords","text":"<pre><code>generate_symmetric_coords(surrounding_uc: bool = True)\n</code></pre> <p>Expand the asu using self.symmetry for the symmetry specification, and optional unit cell dimensions if self.dimension &gt; 0. Expands assembly to complete point group, unit cell, or surrounding unit cells</p> <p>Parameters:</p> <ul> <li> <code>surrounding_uc</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the 3x3 layer group, or 3x3x3 space group should be generated</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def generate_symmetric_coords(self, surrounding_uc: bool = True):\n    \"\"\"Expand the asu using self.symmetry for the symmetry specification, and optional unit cell dimensions if\n    self.dimension &gt; 0. Expands assembly to complete point group, unit cell, or surrounding unit cells\n\n    Args:\n        surrounding_uc: Whether the 3x3 layer group, or 3x3x3 space group should be generated\n    \"\"\"\n    self.log.debug('Generating symmetric coords')\n    if surrounding_uc:\n        if self.dimension &gt; 0:\n            if self.dimension == 3:\n                uc_number = 27\n            elif self.dimension == 2:\n                uc_number = 9\n            else:\n                assert_never(self.dimension)\n            # Set the number_of_symmetry_mates to account for the unit cell number\n            # This results in is_surrounding_uc() being True during return_symmetric_coords()\n            self._number_of_symmetry_mates = self.number_of_uc_symmetry_mates * uc_number\n\n    # Set the self.symmetric_coords property\n    self._symmetric_coords = Coordinates(self.return_symmetric_coords(self.coords))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.cart_to_frac","title":"cart_to_frac","text":"<pre><code>cart_to_frac(cart_coords: ndarray | Iterable | int | float) -&gt; ndarray\n</code></pre> <p>Return fractional coordinates from cartesian coordinates From http://www.ruppweb.org/Xray/tutorial/Coordinate%20system%20transformation.htm</p> <p>Parameters:</p> <ul> <li> <code>cart_coords</code>             (<code>ndarray | Iterable | int | float</code>)         \u2013          <p>The cartesian coordinates of a unit cell</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The fractional coordinates of a unit cell</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def cart_to_frac(self, cart_coords: np.ndarray | Iterable | int | float) -&gt; np.ndarray:\n    \"\"\"Return fractional coordinates from cartesian coordinates\n    From http://www.ruppweb.org/Xray/tutorial/Coordinate%20system%20transformation.htm\n\n    Args:\n        cart_coords: The cartesian coordinates of a unit cell\n\n    Returns:\n        The fractional coordinates of a unit cell\n    \"\"\"\n    if self.uc_dimensions is None:\n        raise ValueError(\n            \"Can't manipulate the unit cell. No unit cell dimensions were passed\")\n\n    return np.matmul(cart_coords, np.transpose(self.sym_entry.deorthogonalization_matrix))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.frac_to_cart","title":"frac_to_cart","text":"<pre><code>frac_to_cart(frac_coords: ndarray | Iterable | int | float) -&gt; ndarray\n</code></pre> <p>Return cartesian coordinates from fractional coordinates From http://www.ruppweb.org/Xray/tutorial/Coordinate%20system%20transformation.htm</p> <p>Parameters:</p> <ul> <li> <code>frac_coords</code>             (<code>ndarray | Iterable | int | float</code>)         \u2013          <p>The fractional coordinates of a unit cell</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The cartesian coordinates of a unit cell</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def frac_to_cart(self, frac_coords: np.ndarray | Iterable | int | float) -&gt; np.ndarray:\n    \"\"\"Return cartesian coordinates from fractional coordinates\n    From http://www.ruppweb.org/Xray/tutorial/Coordinate%20system%20transformation.htm\n\n    Args:\n        frac_coords: The fractional coordinates of a unit cell\n\n    Returns:\n        The cartesian coordinates of a unit cell\n    \"\"\"\n    if self.uc_dimensions is None:\n        raise ValueError(\n            \"Can't manipulate the unit cell. No unit cell dimensions were passed\")\n\n    return np.matmul(frac_coords, np.transpose(self.sym_entry.orthogonalization_matrix))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.return_symmetric_coords","title":"return_symmetric_coords","text":"<pre><code>return_symmetric_coords(coords: list | ndarray) -&gt; ndarray\n</code></pre> <p>Provided an input set of coordinates, return the symmetrized coordinates corresponding to the SymmetricModel</p> <p>Parameters:</p> <ul> <li> <code>coords</code>             (<code>list | ndarray</code>)         \u2013          <p>The coordinates to symmetrize</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The symmetrized coordinates</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def return_symmetric_coords(self, coords: list | np.ndarray) -&gt; np.ndarray:\n    \"\"\"Provided an input set of coordinates, return the symmetrized coordinates corresponding to the SymmetricModel\n\n    Args:\n        coords: The coordinates to symmetrize\n\n    Returns:\n        The symmetrized coordinates\n    \"\"\"\n    # surrounding_uc: bool = True\n    #   surrounding_uc: Whether the 3x3 layer group, or 3x3x3 space group should be generated\n    if self.dimension &gt; 0:\n        if self.is_surrounding_uc():\n            shift_3d = [0., 1., -1.]\n            if self.dimension == 3:\n                z_shifts = shift_3d\n            elif self.dimension == 2:\n                z_shifts = [0.]\n            else:\n                assert_never(self.dimension)\n\n            uc_frac_coords = self.return_unit_cell_coords(coords, fractional=True)\n            surrounding_frac_coords = \\\n                np.concatenate([uc_frac_coords + [x, y, z] for x in shift_3d for y in shift_3d for z in z_shifts])\n            return self.frac_to_cart(surrounding_frac_coords)\n        else:\n            return self.return_unit_cell_coords(coords)\n    else:  # self.dimension = 0 or None\n        return (np.matmul(np.tile(coords, (self.number_of_symmetry_mates, 1, 1)),\n                          self._expand_matrices) + self._expand_translations).reshape(-1, 3)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.return_unit_cell_coords","title":"return_unit_cell_coords","text":"<pre><code>return_unit_cell_coords(coords: ndarray, fractional: bool = False) -&gt; ndarray\n</code></pre> <p>Return the unit cell coordinates from a set of coordinates for the specified SymmetricModel</p> <p>Parameters:</p> <ul> <li> <code>coords</code>             (<code>ndarray</code>)         \u2013          <p>The cartesian coordinates to expand to the unit cell</p> </li> <li> <code>fractional</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return coordinates in fractional or cartesian (False) unit cell frame</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>All unit cell coordinates</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def return_unit_cell_coords(self, coords: np.ndarray, fractional: bool = False) -&gt; np.ndarray:\n    \"\"\"Return the unit cell coordinates from a set of coordinates for the specified SymmetricModel\n\n    Args:\n        coords: The cartesian coordinates to expand to the unit cell\n        fractional: Whether to return coordinates in fractional or cartesian (False) unit cell frame\n\n    Returns:\n        All unit cell coordinates\n    \"\"\"\n    model_coords = (np.matmul(np.tile(self.cart_to_frac(coords), (self.number_of_uc_symmetry_mates, 1, 1)),\n                              self._expand_matrices) + self._expand_translations).reshape(-1, 3)\n    if fractional:\n        return model_coords\n    else:\n        return self.frac_to_cart(model_coords)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.find_contacting_asu","title":"find_contacting_asu","text":"<pre><code>find_contacting_asu(distance: float = 8.0, **kwargs) -&gt; list[Entity]\n</code></pre> <p>Find the maximally contacting symmetry mate for each Entity and return the corresponding Entity instances</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>The distance to check for contacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Entity]</code>         \u2013          <p>The minimal set of Entities containing the maximally touching configuration</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def find_contacting_asu(self, distance: float = 8., **kwargs) -&gt; list[Entity]:\n    \"\"\"Find the maximally contacting symmetry mate for each Entity and return the corresponding Entity instances\n\n    Args:\n        distance: The distance to check for contacts\n\n    Returns:\n        The minimal set of Entities containing the maximally touching configuration\n    \"\"\"\n    entities = self.entities\n    if not entities:\n        # The SymmetricModel was probably set without them. Create them, then try to find the asu\n        self._create_entities()\n        entities = self.entities\n\n    number_of_entities = len(entities)\n    if number_of_entities != 1:\n        idx = count()\n        chain_combinations: list[tuple[Entity, Entity]] = []\n        entity_combinations: list[tuple[Entity, Entity]] = []\n        contact_count = \\\n            np.zeros(sum(map(math.prod, combinations((entity.number_of_symmetry_mates for entity in entities), 2))))\n        for entity1, entity2 in combinations(entities, 2):\n            for chain1 in entity1.chains:\n                chain_cb_coord_tree = BallTree(chain1.cb_coords)\n                for chain2 in entity2.chains:\n                    entity_combinations.append((entity1, entity2))\n                    chain_combinations.append((chain1, chain2))\n                    contact_count[next(idx)] = \\\n                        chain_cb_coord_tree.two_point_correlation(chain2.cb_coords, [distance])[0]\n\n        max_contact_idx = contact_count.argmax()\n        additional_chains = []\n        max_chains = list(chain_combinations[max_contact_idx])\n        if len(max_chains) != number_of_entities:  # We found 2 entities at this point\n            # find the indices where either of the maximally contacting chains are utilized\n            selected_chain_indices = {idx for idx, chain_pair in enumerate(chain_combinations)\n                                      if max_chains[0] in chain_pair or max_chains[1] in chain_pair}\n            remaining_entities = set(entities).difference(entity_combinations[max_contact_idx])\n            for entity in remaining_entities:  # get the max contacts and the associated entity and chain indices\n                # find the indices where the missing entity is utilized\n                remaining_indices = \\\n                    {idx for idx, entity_pair in enumerate(entity_combinations) if entity in entity_pair}\n                # pair_position = [0 if entity_pair[0] == entity else 1\n                #                  for idx, entity_pair in enumerate(entity_combinations) if entity in entity_pair]\n                # only use those where found asu chains already occur\n                viable_remaining_indices = list(remaining_indices.intersection(selected_chain_indices))\n                # out of the viable indices where the selected chains are matched with the missing entity,\n                # find the highest contact\n                max_idx = contact_count[viable_remaining_indices].argmax()\n                for entity_idx, entity_in_combo in enumerate(\n                        entity_combinations[viable_remaining_indices[max_idx]]):\n                    if entity == entity_in_combo:\n                        additional_chains.append(chain_combinations[viable_remaining_indices[max_idx]][entity_idx])\n\n        new_entities = max_chains + additional_chains\n        # Rearrange the entities to have the same order as provided\n        entities = [new_entity for entity in entities for new_entity in new_entities if entity == new_entity]\n\n    return entities\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.get_contacting_asu","title":"get_contacting_asu","text":"<pre><code>get_contacting_asu(distance: float = 8.0, **kwargs) -&gt; SymmetricModel\n</code></pre> <p>Find the maximally contacting symmetry mate for each Entity and return the corresponding Entity instances as  a new Pose</p> <p>If the chain IDs of the asu are the same, then chain IDs will automatically be renamed</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>The distance to check for contacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SymmetricModel</code>         \u2013          <p>A new Model with the minimal set of Entity instances. Will also be symmetric</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_contacting_asu(self, distance: float = 8., **kwargs) -&gt; SymmetricModel:  # Todo -&gt; Self python 3.11\n    \"\"\"Find the maximally contacting symmetry mate for each Entity and return the corresponding Entity instances as\n     a new Pose\n\n    If the chain IDs of the asu are the same, then chain IDs will automatically be renamed\n\n    Args:\n        distance: The distance to check for contacts\n\n    Returns:\n        A new Model with the minimal set of Entity instances. Will also be symmetric\n    \"\"\"\n    if self.number_of_entities == 1:\n        return self.copy()\n\n    entities = self.find_contacting_asu(distance=distance, **kwargs)\n\n    if len({entity.chain_id for entity in entities}) != len(entities):\n        rename = True\n    else:\n        rename = False\n\n    cls = type(self)\n    # assert cls is Pose, f\"Can't {self.get_contacting_asu.__name__} for the class={cls}. Only for Pose\"\n    return cls.from_entities(\n        entities, name=f'{self.name}-asu', log=self.log, sym_entry=self.sym_entry, rename_chains=rename,\n        cryst_record=self.cryst_record, **kwargs)  # , biomt_header=self.format_biomt(),\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetryOpsMixin.set_contacting_asu","title":"set_contacting_asu","text":"<pre><code>set_contacting_asu(from_assembly: bool = False, **kwargs)\n</code></pre> <p>Find the maximally contacting symmetry mate for each Entity, then set the Pose with this info</p> <p>Parameters:</p> <ul> <li> <code>from_assembly</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the ASU should be set fresh from the entire assembly instances</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 8.0 - The distance to check for contacts</p> </li> </ul> Sets <p>self: To a SymmetricModel with the minimal set of Entities containing the maximally touching configuration</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def set_contacting_asu(self, from_assembly: bool = False, **kwargs):\n    \"\"\"Find the maximally contacting symmetry mate for each Entity, then set the Pose with this info\n\n    Args:\n        from_assembly: Whether the ASU should be set fresh from the entire assembly instances\n\n    Keyword Args:\n        distance: float = 8.0 - The distance to check for contacts\n\n    Sets:\n        self: To a SymmetricModel with the minimal set of Entities containing the maximally touching configuration\n    \"\"\"\n    number_of_entities = self.number_of_entities\n    if not self.is_symmetric():\n        raise SymmetryError(\n            f\"Couldn't {self.set_contacting_asu.__name__}() with the asymmetric {repr(self)}\"\n        )\n    elif number_of_entities == 1:\n        return  # This can't be set any better\n\n    # Check to see if the parsed Model is already represented symmetrically\n    if from_assembly or self.has_dependent_chains():  # and self.is_asymmetric_mates() and not preserve_asymmetry:\n        # If .from_chains() or .from_file(), ensure the SymmetricModel is an asu\n        self.log.debug(f'Setting the {repr(self)} to an ASU from a symmetric representation. '\n                       \"This method hasn't been thoroughly debugged\")\n        # Essentially performs,\n        # self.assign_residues_from_structures(self.entities)\n        # however, without _assign_residues(), the containers are not updated.\n        # Set base Structure attributes\n        new_coords = []\n        new_atoms = []\n        new_residues = []\n        for entity in enumerate(self.entities):\n            new_coords.append(entity.coords)\n            new_atoms.extend(entity.atoms)\n            new_residues.extend(entity.residues)\n\n        self._coords = Coordinates(np.concatenate(new_coords))\n        self._atom_indices = list(range(len(new_atoms)))\n        self._atoms.set(new_atoms)\n        self._residue_indices = list(range(len(new_residues)))\n        self._residues.set(new_residues)\n\n        # Remove extra chains by creating fresh\n        self._create_chains()\n        # Update entities to reflect new indices\n        self.reset_structures_states(self.entities)\n        # Recurse this call to ensure that the entities are contacting\n        self.set_contacting_asu(**kwargs)\n        # else:\n        #     raise SymmetryError(\n        #         f\"Couldn't {self.set_contacting_asu.__name__}() with the number of parsed chains, \"\n        #         f\"{self.number_of_chains}. When symmetry={self.symmetry} and the number of entities is \"\n        #         f\"{self.number_of_entities}, the number of symmetric chains should be \"\n        #         f'{number_of_entities * self.number_of_symmetry_mates}'\n        #     )\n    else:  # number_of_entities == number_of_chains:\n        self.log.debug(f'Finding the ASU with the most contacting interface')\n        entities = self.find_contacting_asu(**kwargs)\n\n        # With perfect symmetry, v this is sufficient\n        self._no_reset = True\n        self.coords = np.concatenate([entity.coords for entity in entities])\n        del self._no_reset\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Chain","title":"Chain","text":"<pre><code>Chain(chain_id: str = None, name: str = None, **kwargs)\n</code></pre> <p>             Bases: <code>Structure</code>, <code>MetricsMixin</code></p> <p>A grouping of Atom, Coordinates, and Residue instances, typically from a connected polymer</p> <p>Parameters:</p> <ul> <li> <code>chain_id</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the Chain identifier to use for this instance</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the Chain identifier to use for this instance. Typically used by Entity subclasses.</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, chain_id: str = None, name: str = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        chain_id: The name of the Chain identifier to use for this instance\n        name: The name of the Chain identifier to use for this instance. Typically used by Entity subclasses.\n    \"\"\"\n    kwargs['name'] = name = name if name else chain_id\n    super().__init__(**kwargs)  # Chain\n    if type(self) is Chain and name is not None:\n        # Only if this instance is a Chain, not Entity, set the chain_id\n        self.chain_id = name\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Chain.chain_id","title":"chain_id  <code>property</code> <code>writable</code>","text":"<pre><code>chain_id: str\n</code></pre> <p>The Chain ID for the instance</p>"},{"location":"reference/structure/model/#structure.model.Chain.entity","title":"entity  <code>property</code>","text":"<pre><code>entity: Entity | None\n</code></pre> <p>The Entity associated with the instance</p>"},{"location":"reference/structure/model/#structure.model.Chain.entity_id","title":"entity_id  <code>property</code>","text":"<pre><code>entity_id: str\n</code></pre> <p>The Entity ID associated with the instance</p>"},{"location":"reference/structure/model/#structure.model.Chain.reference_sequence","title":"reference_sequence  <code>property</code>","text":"<pre><code>reference_sequence: str\n</code></pre> <p>Return the entire sequence, constituting all described residues, not just structurally modeled ones</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The sequence according to the Entity reference, or the Structure sequence if no reference available</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.Chain.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(**kwargs) -&gt; dict[str, Any]\n</code></pre> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_metrics(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\"\"\"\n    self.log.warning(f\"{self.calculate_metrics.__name__} doesn't calculate anything yet...\")\n    return {\n        # 'name': self.name,\n        # 'n_terminal_helix': self.is_termini_helical(),\n        # 'c_terminal_helix': self.is_termini_helical(termini='c'),\n        # 'thermophile': thermophile\n        # 'number_of_residues': self.number_of_residues,\n    }\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity","title":"Entity","text":"<pre><code>Entity(operators: tuple[ndarray | list[list[float]], ndarray | list[float]] | ndarray = None, rotations: ndarray | list[list[float]] = None, translations: ndarray | list[float] = None, **kwargs)\n</code></pre> <p>             Bases: <code>SymmetryOpsMixin</code>, <code>ContainsChains</code>, <code>Chain</code></p> <p>Maps a biological instance of a Structure which ContainsChains(1-N) and is a Complex, to a single GeneProduct</p> <p>Parameters:</p> <ul> <li> <code>operators</code>             (<code>tuple[ndarray | list[list[float]], ndarray | list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>A set of symmetry operations to designate how to apply symmetry</p> </li> <li> <code>rotations</code>             (<code>ndarray | list[list[float]]</code>, default:                 <code>None</code> )         \u2013          <p>A set of rotation matrices used to recapitulate the SymmetricModel from the asymmetric unit</p> </li> <li> <code>translations</code>             (<code>ndarray | list[float]</code>, default:                 <code>None</code> )         \u2013          <p>A set of translation vectors used to recapitulate the SymmetricModel from the asymmetric unit</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self,\n             operators: tuple[np.ndarray | list[list[float]], np.ndarray | list[float]] | np.ndarray = None,\n             rotations: np.ndarray | list[list[float]] = None, translations: np.ndarray | list[float] = None,\n             # transformations: list[types.TransformationMapping] = None, surrounding_uc: bool = True,\n             **kwargs):\n    \"\"\"When init occurs chain_ids are set if chains were passed. If not, then they are auto generated\n\n    Args:\n        operators: A set of symmetry operations to designate how to apply symmetry\n        rotations: A set of rotation matrices used to recapitulate the SymmetricModel from the asymmetric unit\n        translations: A set of translation vectors used to recapitulate the SymmetricModel from the asymmetric unit\n    \"\"\"\n    self._is_captain = True\n    super().__init__(**kwargs,  # Entity\n                     as_mates=True)  # &lt;- needed when .from_file() w/ self.is_parent()\n    self._captain = None\n    self.dihedral_chain = None\n    self.mate_rotation_axes = []\n\n    chains = self.chains\n    if not chains:\n        raise DesignError(\n            f\"Can't construct {self.__class__.__name__} instance without 'chains'. \"\n            \"Ensure that you didn't construct with chains=False | None\"\n        )\n\n    representative, *additional_chains = chains\n    if self.is_parent():\n        # When this instance is the parent (.from_file(), .from_chains(parent=None))\n        # Set attributes from representative now that _chains is parsed\n        self._coords.set(representative.coords)\n        self._assign_residues(representative.residues, atoms=representative.atoms)\n\n    # Set up the chain copies\n    number_of_symmetry_mates = len(chains)\n    symmetry = None\n    if number_of_symmetry_mates &gt; 1:\n        if self.is_dihedral():\n            symmetry = f'D{int(number_of_symmetry_mates / 2)}'\n        elif self.is_cyclic():\n            symmetry = f'C{number_of_symmetry_mates}'\n        else:  # Higher than D, probably T, O, I, or asymmetric\n            try:\n                symmetry = utils.symmetry.subunit_number_to_symmetry[number_of_symmetry_mates]\n            except KeyError:\n                self.log.warning(f\"Couldn't find a compatible symmetry for the Entity with \"\n                                 f\"{number_of_symmetry_mates} chain copies\")\n                # symmetry = None\n                # self.symmetry = \"Unknown-symmetry\"\n    self.set_symmetry(symmetry=symmetry)\n\n    if not self.is_symmetric():\n        # No symmetry keyword args were passed\n        if operators is not None or rotations is not None or translations is not None:\n            passed_args = []\n            if operators:\n                passed_args.append('operators')\n            if rotations:\n                passed_args.append('rotations')\n            if translations:\n                passed_args.append('translations')\n            raise ConstructionError(\n                f\"Couldn't set_symmetry() using {', '.join(passed_args)} without explicitly passing \"\n                \"'symmetry' or 'sym_entry'\"\n            )\n        return\n\n    # Set rotations and translations to the correct symmetry operations\n    # where operators, rotations, and translations are user provided from some sort of BIOMT (fiber, other)\n    if operators is not None:\n        symmetry_source_arg = \"'operators' \"\n\n        num_operators = len(operators)\n        if isinstance(operators, tuple) and num_operators == 2:\n            self.log.warning(\"Providing custom symmetry 'operators' may result in improper symmetric \"\n                             'configuration. Proceed with caution')\n            rotations, translations = operators\n        elif isinstance(operators, Sequence) and num_operators == number_of_symmetry_mates:\n            rotations = []\n            translations = []\n            try:\n                for rot, tx in operators:\n                    rotations.append(rot)\n                    translations.append(tx)\n            except TypeError:  # Unpack failed\n                raise ValueError(\n                    f\"Couldn't parse the 'operators'={repr(operators)}.\\n\\n\"\n                    \"Expected a Sequence[rotation shape=(3,3). translation shape=(3,)] pairs.\"\n                )\n        elif isinstance(operators, np.ndarray):\n            if operators.shape[1:] == (3, 4):\n                # Parse from a single input of 3 row by 4 column style, like BIOMT\n                rotations = operators[:, :, :3]\n                translations = operators[:, :, 3:].squeeze()\n            elif operators.shape[1:] == 3:  # Assume just rotations\n                rotations = operators\n                translations = np.tile(utils.symmetry.origin, len(rotations))\n            else:\n                raise ConstructionError(\n                    f\"The 'operators' form {repr(operators)} isn't supported.\")\n        else:\n            raise ConstructionError(\n                f\"The 'operators' form {repr(operators)} isn't supported. Must provide a tuple of \"\n                'array-like objects with the order (rotation matrices, translation vectors) or use the '\n                \"'rotations' and 'translations' keyword args\")\n    else:\n        symmetry_source_arg = ''\n\n    # Now that symmetry is set, check if the Structure parsed all symmetric chains\n    if len(chains) == self.number_of_entities * number_of_symmetry_mates:\n        parsed_assembly = True\n    else:\n        parsed_assembly = False\n\n    # Set the symmetry operations\n    if rotations is not None and translations is not None:\n        if not isinstance(rotations, np.ndarray):\n            rotations = np.ndarray(rotations)\n        if rotations.ndim == 3:\n            # Assume operators were provided in a standard orientation and transpose for subsequent efficiency\n            # Using .swapaxes(-2, -1) call here instead of .transpose() for safety\n            self._expand_matrices = rotations.swapaxes(-2, -1)\n        else:\n            raise SymmetryError(\n                f\"Expected {symmetry_source_arg}rotation matrices with 3 dimensions, not {rotations.ndim} \"\n                \"dimensions. Ensure the passed rotation matrices have a shape of (N symmetry operations, 3, 3)\"\n            )\n\n        if not isinstance(translations, np.ndarray):\n            translations = np.ndarray(translations)\n        if translations.ndim == 2:\n            # Assume operators were provided in a standard orientation each vector needs to be in own array on dim=2\n            self._expand_translations = translations[:, None, :]\n        else:\n            raise SymmetryError(\n                f\"Expected {symmetry_source_arg}translation vectors with 2 dimensions, not {translations.ndim} \"\n                \"dimensions. Ensure the passed translations have a shape of (N symmetry operations, 3)\"\n            )\n    else:\n        symmetry_source_arg = \"'chains' \"\n        if self.dimension == 0:\n            # The _expand_matrices rotation matrices are pre-transposed to avoid repetitive operations\n            _expand_matrices = utils.symmetry.point_group_symmetry_operatorsT[self.symmetry]\n            # The _expand_translations vectors are pre-sliced to enable numpy operations\n            _expand_translations = \\\n                np.tile(utils.symmetry.origin, (number_of_symmetry_mates, 1))[:, None, :]\n\n            if parsed_assembly:\n                # The Structure should have symmetric chains\n                # This routine is essentially orient(). However, with one Entity, no extra need for orient\n                _expand_matrices = [utils.symmetry.identity_matrix]\n                _expand_translations = [utils.symmetry.origin]\n                self_seq = self.sequence\n                ca_coords = self.ca_coords\n                # Todo match this mechanism with the symmetric chain index\n                for chain in additional_chains:\n                    chain_seq = chain.sequence\n                    additional_chain_coords = chain.ca_coords\n                    first_chain_coords = ca_coords\n                    if chain_seq != self_seq:\n                        # Get aligned indices, then follow with superposition\n                        self.log.debug(f'{repr(chain)} and {repr(self)} require alignment to symmetrize')\n                        fixed_indices, moving_indices = get_equivalent_indices(chain_seq, self_seq)\n                        additional_chain_coords = additional_chain_coords[fixed_indices]\n                        first_chain_coords = first_chain_coords[moving_indices]\n\n                    _, rot, tx = superposition3d(additional_chain_coords, first_chain_coords)\n                    _expand_matrices.append(rot)\n                    _expand_translations.append(tx)\n\n                self._expand_matrices = np.array(_expand_matrices).swapaxes(-2, -1)\n                self._expand_translations = np.array(_expand_translations)[:, None, :]\n            else:\n                self._expand_matrices = _expand_matrices\n                self._expand_translations = _expand_translations\n        else:\n            self._expand_matrices, self._expand_translations = \\\n                utils.symmetry.space_group_symmetry_operatorsT[self.symmetry]\n\n    # Removed parsed chain information\n    self.reset_mates()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.mate_rotation_axes","title":"mate_rotation_axes  <code>instance-attribute</code>","text":"<pre><code>mate_rotation_axes: list[dict[str, int | ndarray]] | list = []\n</code></pre> <p>Maps mate entities to their rotation matrix</p>"},{"location":"reference/structure/model/#structure.model.Entity.entity_id","title":"entity_id  <code>property</code> <code>writable</code>","text":"<pre><code>entity_id: str\n</code></pre> <p>The Entity ID associated with the instance</p>"},{"location":"reference/structure/model/#structure.model.Entity.number_of_entities","title":"number_of_entities  <code>property</code>","text":"<pre><code>number_of_entities: int\n</code></pre> <p>Return the number of distinct entities (Gene/Protein products) found in the PoseMetadata</p>"},{"location":"reference/structure/model/#structure.model.Entity.entities","title":"entities  <code>property</code>","text":"<pre><code>entities: list[Entity]\n</code></pre> <p>Returns the Entity instance as a list</p>"},{"location":"reference/structure/model/#structure.model.Entity.chains","title":"chains  <code>property</code>","text":"<pre><code>chains: list[Entity]\n</code></pre> <p>The mate Chain instances of the instance. If not created, returns transformed copies of the instance</p>"},{"location":"reference/structure/model/#structure.model.Entity.assembly","title":"assembly  <code>property</code>","text":"<pre><code>assembly: Model\n</code></pre> <p>Access the oligomeric Structure which is a copy of the Entity plus any additional symmetric mate chains</p> <p>Returns:</p> <ul> <li> <code>Model</code>         \u2013          <p>Structures object with the underlying chains in the oligomer</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.Entity.max_symmetry_chain_idx","title":"max_symmetry_chain_idx  <code>property</code>","text":"<pre><code>max_symmetry_chain_idx: int\n</code></pre> <p>The maximum symmetry order present</p>"},{"location":"reference/structure/model/#structure.model.Entity.max_symmetry","title":"max_symmetry  <code>property</code>","text":"<pre><code>max_symmetry: int\n</code></pre> <p>The maximum symmetry order present</p>"},{"location":"reference/structure/model/#structure.model.Entity.from_chains","title":"from_chains  <code>classmethod</code>","text":"<pre><code>from_chains(chains: list[Chain] | Structures, residue_indices: list[int] = None, **kwargs)\n</code></pre> <p>Initialize an Entity from Chain instances</p> <p>Parameters:</p> <ul> <li> <code>chains</code>             (<code>list[Chain] | Structures</code>)         \u2013          <p>A list of Chain instances that match the Entity</p> </li> <li> <code>residue_indices</code>             (<code>list[int]</code>, default:                 <code>None</code> )         \u2013          <p>The indices which the new Entity should contain</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_chains(cls, chains: list[Chain] | Structures, residue_indices: list[int] = None, **kwargs):\n    \"\"\"Initialize an Entity from Chain instances\n\n    Args:\n        chains: A list of Chain instances that match the Entity\n        residue_indices: The indices which the new Entity should contain\n    \"\"\"\n    operators = kwargs.get('operators')\n    if residue_indices is None:\n        asymmetry = False\n        if asymmetry:\n            pass\n        # No check\n        else:\n            representative, *additional_chains = chains\n\n        residue_indices = representative.residue_indices\n\n    return cls(chains=chains, residue_indices=residue_indices, operators=operators, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.coords","title":"coords","text":"<pre><code>coords(coords: ndarray | list[list[float]])\n</code></pre> <p>Set the Coords object while propagating changes to symmetric \"mate\" chains</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@StructureBase.coords.setter\ndef coords(self, coords: np.ndarray | list[list[float]]):\n    \"\"\"Set the Coords object while propagating changes to symmetric \"mate\" chains\"\"\"\n    if self.is_symmetric() and self._is_captain:\n        # **This routine handles imperfect symmetry**\n        self.log.debug('Entity captain is updating coords')\n        # Must do these before super().coords.fset()\n        # Populate .chains (if not already) with current coords and transformation\n        self_, *mate_chains = self.chains\n        # Set current .ca_coords as prior_ca_coords\n        prior_ca_coords = self.ca_coords.copy()\n\n        # Set coords with new coords\n        super(ContainsAtoms, ContainsAtoms).coords.fset(self, coords)\n        if self.is_dependent():\n            _parent = self.parent\n            if _parent.is_symmetric() and not self._parent_is_updating:\n                _parent._dependent_is_updating = True\n                # Update the parent which propagates symmetric updates\n                _parent.coords = _parent.coords\n                _parent._dependent_is_updating = False\n\n        # Find the transformation from the old coordinates to the new\n        new_ca_coords = self.ca_coords\n        _, new_rot, new_tx = superposition3d(new_ca_coords, prior_ca_coords)\n\n        new_rot_t = np.transpose(new_rot)\n        # Remove prior transforms by setting a fresh container\n        _expand_matrices = [utils.symmetry.identity_matrix]\n        _expand_translations = [utils.symmetry.origin]\n        # Find the transform between the new coords and the current mate chain coords\n        # for chain, transform in zip(mate_chains, current_chain_transforms):\n        for chain in mate_chains:\n            # self.log.debug(f'Updated transform of mate {chain.chain_id}')\n            # In liu of using chain.coords as lengths might be different\n            # Transform prior_coords to chain.coords position, then transform using new_rot and new_tx\n            # new_chain_ca_coords = \\\n            #     np.matmul(np.matmul(prior_ca_coords,\n            #                         np.transpose(transform['rotation'])) + transform['translation'],\n            #               np.transpose(new_rot)) + new_tx\n            new_chain_ca_coords = np.matmul(chain.ca_coords, new_rot_t) + new_tx\n            # Find the transform from current coords and the new mate chain coords\n            _, rot, tx = superposition3d(new_chain_ca_coords, new_ca_coords)\n            # Save transform\n            # self._chain_transforms.append(dict(rotation=rot, translation=tx))\n            rot_t = np.transpose(rot)\n            _expand_matrices.append(rot_t)\n            _expand_translations.append(tx)\n            # Transform existing mate chain\n            chain.coords = np.matmul(coords, rot_t) + tx\n            # self.log.debug(f'Setting coords on mate chain {chain.chain_id}')\n\n        self._expand_matrices = np.array(_expand_matrices)\n        self._expand_translations = np.array(_expand_translations)[:, None, :]\n    else:  # Accept the new coords\n        super(ContainsAtoms, ContainsAtoms).coords.fset(self, coords)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.is_captain","title":"is_captain","text":"<pre><code>is_captain() -&gt; bool\n</code></pre> <p>Is the Entity instance the captain?</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_captain(self) -&gt; bool:\n    \"\"\"Is the Entity instance the captain?\"\"\"\n    return self._is_captain\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.is_mate","title":"is_mate","text":"<pre><code>is_mate() -&gt; bool\n</code></pre> <p>Is the Entity instance a mate?</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_mate(self) -&gt; bool:\n    \"\"\"Is the Entity instance a mate?\"\"\"\n    return not self._is_captain\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.has_mates","title":"has_mates","text":"<pre><code>has_mates() -&gt; bool\n</code></pre> <p>Returns True if this Entity is a captain and has mates</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def has_mates(self) -&gt; bool:\n    \"\"\"Returns True if this Entity is a captain and has mates\"\"\"\n    return len(self._chains) &gt; 1\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.remove_mate_chains","title":"remove_mate_chains","text":"<pre><code>remove_mate_chains()\n</code></pre> <p>Clear the Entity of all Chain and Oligomer information</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def remove_mate_chains(self):\n    \"\"\"Clear the Entity of all Chain and Oligomer information\"\"\"\n    self._expand_matrices = self._expand_translations = []\n    self.reset_mates()\n    self._is_captain = False\n    self.chain_ids = [self.chain_id]\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.make_oligomer","title":"make_oligomer","text":"<pre><code>make_oligomer(symmetry: str = None, rotation: list[list[float]] | ndarray = None, translation: list[float] | ndarray = None, rotation2: list[list[float]] | ndarray = None, translation2: list[float] | ndarray = None, **kwargs)\n</code></pre> <p>Given a symmetry and transformational mapping, generate oligomeric copies of the Entity</p> <p>Assumes that the symmetric system treats the canonical symmetric axis as the Z-axis, and if the Entity is not at the origin, that a transformation describing its current position relative to the origin is passed so that it can be moved to the origin. At the origin, makes the required oligomeric rotations, to generate an oligomer where symmetric copies are stored in the .chains attribute then reverses the operations back to original reference frame if any was provided</p> <p>Parameters:</p> <ul> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The symmetry to set the Entity to</p> </li> <li> <code>rotation</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected array shape (3,)</p> </li> <li> <code>rotation2</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation2</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected array shape (3,)</p> </li> </ul> Sets <p>self.symmetry (str) self.sym_entry (SymEntry) self.number_of_symmetry_mates (int) self._expand_matrices self._expand_translations</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_oligomer(self, symmetry: str = None, rotation: list[list[float]] | np.ndarray = None,\n                  translation: list[float] | np.ndarray = None, rotation2: list[list[float]] | np.ndarray = None,\n                  translation2: list[float] | np.ndarray = None, **kwargs):\n    \"\"\"Given a symmetry and transformational mapping, generate oligomeric copies of the Entity\n\n    Assumes that the symmetric system treats the canonical symmetric axis as the Z-axis, and if the Entity is not at\n    the origin, that a transformation describing its current position relative to the origin is passed so that it\n    can be moved to the origin. At the origin, makes the required oligomeric rotations, to generate an oligomer\n    where symmetric copies are stored in the .chains attribute then reverses the operations back to original\n    reference frame if any was provided\n\n    Args:\n        symmetry: The symmetry to set the Entity to\n        rotation: The first rotation to apply, expected array shape (3, 3)\n        translation: The first translation to apply, expected array shape (3,)\n        rotation2: The second rotation to apply, expected array shape (3, 3)\n        translation2: The second translation to apply, expected array shape (3,)\n\n    Sets:\n        self.symmetry (str)\n        self.sym_entry (SymEntry)\n        self.number_of_symmetry_mates (int)\n        self._expand_matrices\n        self._expand_translations\n    \"\"\"\n    self.set_symmetry(symmetry=symmetry)\n    if not self.is_symmetric():\n        return\n\n    symmetry = self.symmetry\n    degeneracy_matrices = None\n    if symmetry in utils.symmetry.cubic_point_groups:\n        rotation_matrices = utils.symmetry.point_group_symmetry_operators[symmetry]\n    elif 'D' in symmetry:  # Provide a 180-degree rotation along x (all D orient symmetries have axis here)\n        rotation_matrices = \\\n            utils.SymEntry.get_rot_matrices(\n                utils.symmetry.rotation_range[symmetry.replace('D', 'C')],\n                'z', 360\n            )\n        degeneracy_matrices = [utils.symmetry.identity_matrix, utils.symmetry.flip_x_matrix]\n    else:  # Symmetry is cyclic\n        rotation_matrices = utils.SymEntry.get_rot_matrices(utils.symmetry.rotation_range[symmetry], 'z')\n\n    degeneracy_rotation_matrices = utils.SymEntry.make_rotations_degenerate(\n        rotation_matrices, degeneracy_matrices)\n\n    assert self.number_of_symmetry_mates == len(degeneracy_rotation_matrices), \\\n        (f\"The number of symmetry mates, {self.number_of_symmetry_mates} != {len(degeneracy_rotation_matrices)}, \"\n         \"the number of operations\")\n\n    if rotation is None:\n        rotation = inv_rotation = utils.symmetry.identity_matrix\n    else:\n        inv_rotation = np.linalg.inv(rotation)\n    if translation is None:\n        translation = utils.symmetry.origin\n\n    if rotation2 is None:\n        rotation2 = inv_rotation2 = utils.symmetry.identity_matrix\n    else:\n        inv_rotation2 = np.linalg.inv(rotation2)\n    if translation2 is None:\n        translation2 = utils.symmetry.origin\n    # this is helpful for dihedral symmetry as entity must be transformed to origin to get canonical dihedral\n    # entity_inv = entity.get_transformed_copy(rotation=inv_expand_matrix, rotation2=inv_set_matrix[group])\n    # need to reverse any external transformation to the entity coords so rotation occurs at the origin...\n    # and undo symmetry expansion matrices\n    # centered_coords = transform_coordinate_sets(self.coords, translation=-translation2,\n    # centered_coords = transform_coordinate_sets(self._coords.coords, translation=-translation2)\n    cb_coords = self.cb_coords\n    centered_coords = transform_coordinate_sets(cb_coords, translation=-translation2)\n\n    centered_coords_inv = transform_coordinate_sets(centered_coords, rotation=inv_rotation2,\n                                                    translation=-translation, rotation2=inv_rotation)\n    _expand_matrices = [utils.symmetry.identity_matrix]\n    _expand_translations = [utils.symmetry.origin]\n    subunit_count = count()\n    for rotation_matrix in degeneracy_rotation_matrices:\n        if next(subunit_count) == 0 and np.all(rotation_matrix == utils.symmetry.identity_matrix):\n            self.log.debug(f'Skipping {self.make_oligomer.__name__} transformation 1 as it is identity')\n            continue\n        rot_centered_coords = transform_coordinate_sets(centered_coords_inv, rotation=rotation_matrix)\n        new_coords = transform_coordinate_sets(rot_centered_coords, rotation=rotation, translation=translation,\n                                               rotation2=rotation2, translation2=translation2)\n        _, rot, tx = superposition3d(new_coords, cb_coords)\n        _expand_matrices.append(rot)\n        _expand_translations.append(tx)\n\n    self._expand_matrices = np.array(_expand_matrices).swapaxes(-2, -1)\n    self._expand_translations = np.array(_expand_translations)[:, None, :]\n\n    # Set the new properties\n    self.reset_mates()\n    self._set_chain_ids()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.get_transformed_mate","title":"get_transformed_mate","text":"<pre><code>get_transformed_mate(rotation: list[list[float]] | ndarray = None, translation: list[float] | ndarray = None, rotation2: list[list[float]] | ndarray = None, translation2: list[float] | ndarray = None) -&gt; Entity\n</code></pre> <p>Make a semi-deep copy of the Entity, stripping any captain attributes, transforming the coordinates</p> <p>Transformation proceeds by matrix multiplication and vector addition with the order of operations as: rotation, translation, rotation2, translation2</p> <p>Parameters:</p> <ul> <li> <code>rotation</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first translation to apply, expected array shape (3,)</p> </li> <li> <code>rotation2</code>             (<code>list[list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second rotation to apply, expected array shape (3, 3)</p> </li> <li> <code>translation2</code>             (<code>list[float] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second translation to apply, expected array shape (3,)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Entity</code>         \u2013          <p>A transformed copy of the original object</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_transformed_mate(self, rotation: list[list[float]] | np.ndarray = None,\n                         translation: list[float] | np.ndarray = None,\n                         rotation2: list[list[float]] | np.ndarray = None,\n                         translation2: list[float] | np.ndarray = None) -&gt; Entity:\n    \"\"\"Make a semi-deep copy of the Entity, stripping any captain attributes, transforming the coordinates\n\n    Transformation proceeds by matrix multiplication and vector addition with the order of operations as:\n    rotation, translation, rotation2, translation2\n\n    Args:\n        rotation: The first rotation to apply, expected array shape (3, 3)\n        translation: The first translation to apply, expected array shape (3,)\n        rotation2: The second rotation to apply, expected array shape (3, 3)\n        translation2: The second translation to apply, expected array shape (3,)\n\n    Returns:\n        A transformed copy of the original object\n    \"\"\"\n    if rotation is not None:  # required for np.ndarray or None checks\n        new_coords = np.matmul(self.coords, np.transpose(rotation))\n    else:\n        new_coords = self.coords\n\n    if translation is not None:  # required for np.ndarray or None checks\n        new_coords += np.array(translation)\n\n    if rotation2 is not None:  # required for np.ndarray or None checks\n        np.matmul(new_coords, np.transpose(rotation2), out=new_coords)\n\n    if translation2 is not None:  # required for np.ndarray or None checks\n        new_coords += np.array(translation2)\n\n    new_structure = self.copy()\n    new_structure._make_mate(self)\n    new_structure.coords = new_coords\n\n    return new_structure\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.write","title":"write","text":"<pre><code>write(out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, assembly: bool = False, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Write Entity Structure to a file specified by out_path or with a passed file_handle</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>bytes | str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the Structure object should be written to disk</p> </li> <li> <code>file_handle</code>             (<code>IO</code>, default:                 <code>None</code> )         \u2013          <p>Used to write Structure details to an open FileObject</p> </li> <li> <code>header</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string that is desired at the top of the file</p> </li> <li> <code>assembly</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to write the oligomeric form of the Entity</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>increment_chains</code>         \u2013          <p>bool = False - Whether to write each Structure with a new chain name, otherwise write as a new Model</p> </li> <li> <code>chain_id</code>         \u2013          <p>str = None - The chain ID to use</p> </li> <li> <code>atom_offset</code>         \u2013          <p>int = 0 - How much to offset the atom number by. Default returns one-indexed. Not used if assembly=True</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The name of the written file if out_path is used</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def write(self, out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None,\n          assembly: bool = False, **kwargs) -&gt; AnyStr | None:\n    \"\"\"Write Entity Structure to a file specified by out_path or with a passed file_handle\n\n    Args:\n        out_path: The location where the Structure object should be written to disk\n        file_handle: Used to write Structure details to an open FileObject\n        header: A string that is desired at the top of the file\n        assembly: Whether to write the oligomeric form of the Entity\n\n    Keyword Args:\n        increment_chains: bool = False - Whether to write each Structure with a new chain name, otherwise write as\n            a new Model\n        chain_id: str = None - The chain ID to use\n        atom_offset: int = 0 - How much to offset the atom number by. Default returns one-indexed.\n            Not used if assembly=True\n\n    Returns:\n        The name of the written file if out_path is used\n    \"\"\"\n    self.log.debug(f'{Entity.__name__} is writing {repr(self)}')\n\n    def _write(handle) -&gt; None:\n        if assembly:\n            kwargs.pop('atom_offset', None)\n            # if 'increment_chains' not in kwargs:\n            #     kwargs['increment_chains'] = True\n            # assembly_models = self._generate_assembly_models(**kwargs)\n            assembly_models = Models(self.chains)\n            assembly_models.write(file_handle=handle, multimodel=False, **kwargs)\n        else:\n            super(Structure, Structure).write(self, file_handle=handle, **kwargs)\n\n    if file_handle:\n        return _write(file_handle)\n    else:  # out_path always has default argument current working directory\n        # assembly=True implies all chains will be written, so asu=False to write each SEQRES record\n        _header = self.format_header(assembly=assembly, **kwargs)\n        if header is not None:\n            if not isinstance(header, str):\n                header = str(header)\n            _header += (header if header[-2:] == '\\n' else f'{header}\\n')\n\n        with open(out_path, 'w') as outfile:\n            outfile.write(_header)\n            _write(outfile)\n\n        return out_path\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(**kwargs) -&gt; dict[str, Any]\n</code></pre> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_metrics(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"\"\"\"\n    self.log.debug(f\"{self.calculate_spatial_orientation_metrics.__name__} missing argument 'reference'\")\n    return self.calculate_spatial_orientation_metrics()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.calculate_spatial_orientation_metrics","title":"calculate_spatial_orientation_metrics","text":"<pre><code>calculate_spatial_orientation_metrics(reference: ndarray = utils.symmetry.origin) -&gt; dict[str, Any]\n</code></pre> <p>Calculate metrics for the instance</p> <p>Parameters:</p> <ul> <li> <code>reference</code>             (<code>ndarray</code>, default:                 <code>origin</code> )         \u2013          <p>The reference where the point should be measured from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>{'radius' 'min_radius' 'max_radius' 'n_terminal_orientation' 'c_terminal_orientation'</p> </li> <li> <code>dict[str, Any]</code>         \u2013          <p>}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_spatial_orientation_metrics(self, reference: np.ndarray = utils.symmetry.origin) -&gt; dict[str, Any]:\n    \"\"\"Calculate metrics for the instance\n\n    Args:\n        reference: The reference where the point should be measured from\n\n    Returns:\n        {'radius'\n         'min_radius'\n         'max_radius'\n         'n_terminal_orientation'\n         'c_terminal_orientation'\n        }\n    \"\"\"\n    return {\n        'radius': self.assembly.distance_from_reference(reference=reference),\n        'min_radius': self.assembly.distance_from_reference(measure='min', reference=reference),\n        'max_radius': self.assembly.distance_from_reference(measure='max', reference=reference),\n        'n_terminal_orientation': self.termini_proximity_from_reference(reference=reference),\n        'c_terminal_orientation': self.termini_proximity_from_reference(termini='c', reference=reference),\n    }\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.get_alphafold_features","title":"get_alphafold_features","text":"<pre><code>get_alphafold_features(symmetric: bool = False, heteromer: bool = False, msas: Sequence = tuple(), no_msa: bool = False, templates: bool = False, **kwargs) -&gt; FeatureDict\n</code></pre> <p>Retrieve the required feature dictionary for this instance to use in Alphafold inference</p> <p>Parameters:</p> <ul> <li> <code>symmetric</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the symmetric Entity should be used for feature production. If True, this function will fully process the FeatureDict in the symmetric form compatible with Alphafold multimer</p> </li> <li> <code>heteromer</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether Alphafold should be run as a heteromer. Features directly used in Alphafold from this instance should never be used with heteromer=True</p> </li> <li> <code>msas</code>             (<code>Sequence</code>, default:                 <code>tuple()</code> )         \u2013          <p>A sequence of multiple sequence alignments if they should be included in the features</p> </li> <li> <code>no_msa</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether multiple sequence alignments should be included in the features</p> </li> <li> <code>templates</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the Entity should be returned with it's template features</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDict</code>         \u2013          <p>The Alphafold FeatureDict which is essentially a dictionary with dict[str, np.ndarray]</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_alphafold_features(self, symmetric: bool = False, heteromer: bool = False, msas: Sequence = tuple(),\n                           no_msa: bool = False, templates: bool = False, **kwargs) -&gt; FeatureDict:\n    # multimer: bool = False,\n    \"\"\"Retrieve the required feature dictionary for this instance to use in Alphafold inference\n\n    Args:\n        symmetric: Whether the symmetric Entity should be used for feature production. If True, this function will\n            fully process the FeatureDict in the symmetric form compatible with Alphafold multimer\n        heteromer: Whether Alphafold should be run as a heteromer. Features directly used in\n            Alphafold from this instance should never be used with heteromer=True\n        msas: A sequence of multiple sequence alignments if they should be included in the features\n        no_msa: Whether multiple sequence alignments should be included in the features\n        templates: Whether the Entity should be returned with it's template features\n\n    Returns:\n        The Alphafold FeatureDict which is essentially a dictionary with dict[str, np.ndarray]\n    \"\"\"\n    if heteromer:\n        if symmetric:\n            raise ValueError(\n                f\"Couldn't {self.get_alphafold_features.__name__} with both 'symmetric' and \"\n                f\"'heteromer' True. Only run with symmetric True if this {self.__class__.__name__} \"\n                \"instance alone should be predicted as a multimer\")\n    # if templates:\n    #     if symmetric:\n    #         # raise ValueError(f\"Couldn't {self.get_alphafold_features.__name__} with both 'symmetric' and \"\n    #         logger.warning(f\"Couldn't {self.get_alphafold_features.__name__} with both 'symmetric' and \"\n    #                        f\"'templates' True. Templates not set up for multimer\")\n    # elif symmetric:\n    #     # Set multimer True as we need to make_msa_features_multimeric\n    #     multimer = True\n\n    # # IS THIS NECESSARY. DON'T THINK SO IF I HAVE MSA\n    # chain_features = alphafold.alphafold.data.pipeline.DataPipeline.process(input_fasta_path=P, msa_output_dir=P)\n    # This ^ runs\n    number_of_residues = self.number_of_residues\n    sequence = self.sequence\n    sequence_features = af_pipeline.make_sequence_features(\n        sequence=sequence, description=self.name, num_res=number_of_residues)\n    # sequence_features = {\n    #     'aatype': ,  # MAKE ONE HOT with X i.e.unknown are X\n    #     'between_segment_residues': np.zeros((number_of_residues,), dtype=np.int32),\n    #     'domain_name': np.array([input_description.encode('utf-8')], dtype=np.object_),\n    #     'residue_index': np.arange(number_of_residues, dtype=np.int32),\n    #     'seq_length': np.full(number_of_residues, number_of_residues, dtype=np.int32),\n    #     'sequence': np.array([sequence.encode('utf-8')], dtype=np.object_)\n    # }\n\n    def make_msa_features_multimeric(msa_feats: FeatureDict) -&gt; FeatureDict:\n        \"\"\"Create the feature names for Alphafold heteromeric inputs run in multimer mode\"\"\"\n        valid_feats = af_msa_pairing.MSA_FEATURES + ('msa_species_identifiers',)\n        return {f'{k}_all_seq': v for k, v in msa_feats.items() if k in valid_feats}\n\n    # Multiple sequence alignment processing\n    if msas:\n        msa_features = af_pipeline.make_msa_features(msas)\n        # Can use a single one...\n        # Other types from AlphaFold include: (uniref90_msa, bfd_msa, mgnify_msa)\n        if heteromer:\n            # Todo ensure that uniref90 runner was used...\n            #  OR equivalent so that each sequence in a multimeric msa is paired\n            # Stockholm format looks like\n            # #=GF DE                          path/to/profiles/entity_id\n            # #=GC RF                          AY--R...\n            # 2gtr_1                           AY--R...\n            # UniRef100_A0A455ABB#2            NC--R...\n            # UniRef100_UPI00106966C#3         CI--L...\n            raise NotImplementedError('No uniprot90 database hooked up...')\n            # with open(self.msa_file, 'r') as f:\n            #     uniref90_lines = f.read()\n\n            uniref90_msa = af_data_parsers.parse_stockholm(uniref90_lines)\n            msa_features = af_pipeline.make_msa_features((uniref90_msa,))\n            msa_features.update(make_msa_features_multimeric(msa_features))\n    else:\n        msa = self.msa\n        if no_msa or msa is None:  # or self.msa_file is None:\n            # When no msa_used, construct our own\n            num_sequences = 1\n            deletion_matrix = np.zeros((num_sequences, number_of_residues), dtype=np.int32)\n            species_ids = ['']  # Must include an empty '' as the first \"reference\" sequence\n            msa_numeric = sequences_to_numeric(\n                [sequence], translation_table=numerical_translation_alph1_unknown_gaped_bytes\n            ).astype(dtype=np.int32)\n        elif msa:\n            deletion_matrix = msa.deletion_matrix.astype(np.int32)  # [:, msa.query_indices]\n            num_sequences = msa.length\n            species_ids = msa.sequence_identifiers\n            # Set the msa.alphabet_type to ensure the numerical_alignment is embedded correctly\n            msa.alphabet_type = protein_letters_alph1_unknown_gaped\n            msa_numeric = msa.numerical_alignment[:, msa.query_indices]\n            # self.log.critical(f'982 Found {len(np.flatnonzero(msa.query_indices))} indices utilized in design')\n        # Todo\n        #  move to additional AlphaFold set up function...\n        #  elif os.path.exists(self.msa_file):\n        #      with open(self.msa_file, 'r') as f:\n        #          uniclust_lines = f.read()\n        #      file, extension = os.path.splitext(self.msa_file)\n        #      if extension == '.sto':\n        #          uniclust30_msa = af_data_parsers.parse_stockholm(uniclust_lines)\n        #      else:\n        #          raise ValueError(\n        #              f\"Currently, the multiple sequence alignment file type '{extension}' isn't supported\\n\"\n        #              f\"\\tOffending file located at: {self.msa_file}\")\n        #      msas = (uniclust30_msa,)\n        else:\n            raise ValueError(\"Couldn't acquire AlphaFold msa features\")\n\n        self.log.debug(f\"Found the first 5 species_ids: {species_ids[:5]}\")\n        msa_features = {\n            'deletion_matrix_int': deletion_matrix,\n            # When not single sequence, GET THIS FROM THE MATRIX PROBABLY USING CODE IN COLLAPSE PROFILE cumcount...\n            # 'msa': sequences_to_numeric([sequence], translation_table=HHBLITS_AA_TO_ID).astype(dtype=np.int32),\n            'msa': msa_numeric,\n            'num_alignments': np.full(number_of_residues, num_sequences, dtype=np.int32),\n            # Fill by the number of residues how many sequences are in the MSA\n            'msa_species_identifiers': np.array([id_.encode('utf-8') for id_ in species_ids], dtype=np.object_)\n        }\n        # Debug features\n        for feat, values in msa_features.items():\n            self.log.debug(f'For feature {feat}, found shape {values.shape}')\n\n        if heteromer:\n            # Make a deepcopy just incase this screws up something\n            msa_features.update(make_msa_features_multimeric(deepcopy(msa_features)))\n\n    # Template processing\n    if templates:\n        template_features = self.get_alphafold_template_features()  # symmetric=symmetric, heteromer=heteromer)\n    else:\n        template_features = empty_placeholder_template_features(num_templates=0, num_res=number_of_residues)\n    # Debug template features\n    for feat, values in template_features.items():\n        self.log.debug(f'For feature {feat}, found shape {values.shape}')\n\n    entity_features = {\n        **msa_features,\n        **sequence_features,\n        **template_features\n    }\n    if symmetric and self.is_symmetric():\n        # Hard code in chain_id as we are using a multimeric predict on the oligomeric version\n        chain_id = 'A'\n        entity_features = af_pipeline_multimer.convert_monomer_features(entity_features, chain_id=chain_id)\n\n        chain_count = count(1)\n        entity_integer = 1\n        entity_id = af_pipeline_multimer.int_id_to_str_id(entity_integer)\n        all_chain_features = {}\n        for sym_idx in range(1, 1 + self.number_of_symmetry_mates):\n            # chain_id = next(available_chain_ids_iter)  # The mmCIF formatted chainID with 'AB' type notation\n            this_entity_features = deepcopy(entity_features)\n            # Where chain_id increments for each new chain instance i.e. A_1 is 1, A_2 is 2, ...\n            # Where entity_id increments for each new Entity instance i.e. A_1 is 1, A_2 is 1, ...\n            # Where sym_id increments for each new Entity instance regardless of chain i.e. A_1 is 1, A_2 is 2, ...,\n            # B_1 is 1, B2 is 2\n            this_entity_features.update({'asym_id': next(chain_count) * np.ones(number_of_residues),\n                                         'sym_id': sym_idx * np.ones(number_of_residues),\n                                         'entity_id': entity_integer * np.ones(number_of_residues)})\n            chain_name = f'{entity_id}_{sym_idx}'\n            all_chain_features[chain_name] = this_entity_features\n\n        # Alternative to pair_and_merge using hhblits a3m output\n        # See PMID:36224222 \"Structural predictions of dimeric and trimeric subcomponents\" methods section\n        # The first of the two MSAs is constructed by extracting the organism identifiers (OX) from the resulting\n        # a3m file and pairing sequences using the top hit from each OX. The second is constructed by block\n        # diagonalizing the resulting a3m file.\n        np_example = af_feature_processing.pair_and_merge(all_chain_features=all_chain_features)\n        # Pad MSA to avoid zero-sized extra_msa.\n        np_example = af_pipeline_multimer.pad_msa(np_example, 512)\n\n        return np_example  # This is still a FeatureDict and could be named entity_features\n    else:\n        return entity_features\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.find_chain_symmetry","title":"find_chain_symmetry","text":"<pre><code>find_chain_symmetry()\n</code></pre> <p>Search for the chain symmetry by using quaternion geometry to solve the symmetric order of the rotations  which superimpose chains on the Entity. Translates the Entity to the origin using center of mass, then the axis of rotation only needs to be translated to the center of mass to recapitulate the specific symmetry operation</p> <p>Requirements - all chains are the same length</p> Sets <p>self.mate_rotation_axes (list[dict[str, int | np.ndarray]]) self._max_symmetry (int) self.max_symmetry_chain_idx (int)</p> <p>Returns:</p> <ul> <li>         \u2013          <p>The name of the file written for symmetry definition file creation</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def find_chain_symmetry(self):\n    \"\"\"Search for the chain symmetry by using quaternion geometry to solve the symmetric order of the rotations\n     which superimpose chains on the Entity. Translates the Entity to the origin using center of mass, then the axis\n    of rotation only needs to be translated to the center of mass to recapitulate the specific symmetry operation\n\n    Requirements - all chains are the same length\n\n    Sets:\n        self.mate_rotation_axes (list[dict[str, int | np.ndarray]])\n        self._max_symmetry (int)\n        self.max_symmetry_chain_idx (int)\n\n    Returns:\n        The name of the file written for symmetry definition file creation\n    \"\"\"\n    # Find the superposition from the Entity to every mate chain\n    # center_of_mass = self.center_of_mass\n    # symmetric_center_of_mass = self.center_of_mass_symmetric\n    # self.log.debug(f'symmetric_center_of_mass={symmetric_center_of_mass}')\n    self.mate_rotation_axes.clear()\n    self.mate_rotation_axes.append({'sym': 1, 'axis': utils.symmetry.origin})\n    self.log.debug(f'Reference chain is {self.chain_id}')\n    if self.is_symmetric():\n\n        def _get_equivalent_coords(\n            self_ca_coords: np.ndarray, self_seq: str, chain_: Chain\n        ) -&gt; tuple[np.ndarray, np.ndarray]:\n            return self_ca_coords, chain_.ca_coords\n    else:\n\n        def _get_equivalent_coords(\n            self_ca_coords: np.ndarray, self_seq: str, chain_: Chain\n        ) -&gt; tuple[np.ndarray, np.ndarray]:\n            chain_seq = chain_.sequence\n            additional_chain_coords = chain_.ca_coords\n            if chain_seq != self_seq:\n                # Get aligned indices, then follow with superposition\n                self.log.debug(f'{repr(chain_)} and {repr(self)} require alignment to symmetrize')\n                fixed_indices, moving_indices = get_equivalent_indices(chain_seq, self_seq)\n                additional_chain_coords = additional_chain_coords[fixed_indices]\n                self_ca_coords = self_ca_coords[moving_indices]\n\n            return self_ca_coords, additional_chain_coords\n\n    ca_coords = self.ca_coords\n    sequence = self.sequence\n    for chain in self.chains[1:]:\n        self_coords, chain_coords = _get_equivalent_coords(ca_coords, sequence, chain)\n        rmsd, quat, tx = superposition3d_quat(self_coords, chain_coords)\n        # rmsd, quat, tx = superposition3d_quat(cb_coords-center_of_mass, chain.cb_coords-center_of_mass)\n        self.log.debug(f'rmsd={rmsd} quaternion={quat} translation={tx}')\n        w = abs(quat[3])\n        omega = math.acos(w)\n        try:\n            symmetry_order = int(math.pi/omega + .5)  # Round to the nearest integer\n        except ZeroDivisionError:  # w is 1, omega is 0\n            # No axis of symmetry here\n            symmetry_order = 1\n            self.log.warning(f\"Couldn't find any symmetry order for {self.name} mate Chain {chain.chain_id}. \"\n                             f'Setting symmetry_order={symmetry_order}')\n        self.log.debug(f'{chain.chain_id}:{symmetry_order}-fold axis')\n        self.mate_rotation_axes.append({'sym': symmetry_order, 'axis': quat[:3]})\n\n    # Find the highest order symmetry in the Structure\n    max_sym = 0\n    max_chain_idx = None\n    for chain_idx, data in enumerate(self.mate_rotation_axes):\n        if data['sym'] &gt; max_sym:\n            max_sym = data['sym']\n            max_chain_idx = chain_idx\n\n    self._max_symmetry = max_sym\n    self._max_symmetry_chain_idx = max_chain_idx\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.is_cyclic","title":"is_cyclic","text":"<pre><code>is_cyclic() -&gt; bool\n</code></pre> <p>Report whether the symmetry is cyclic</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if the Structure is cyclic, False if not</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_cyclic(self) -&gt; bool:\n    \"\"\"Report whether the symmetry is cyclic\n\n    Returns:\n        True if the Structure is cyclic, False if not\n    \"\"\"\n    return self.number_of_symmetry_mates == self.max_symmetry\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.is_dihedral","title":"is_dihedral","text":"<pre><code>is_dihedral() -&gt; bool\n</code></pre> <p>Report whether the symmetry is dihedral</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if the Structure is dihedral, False if not</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def is_dihedral(self) -&gt; bool:\n    \"\"\"Report whether the symmetry is dihedral\n\n    Returns:\n        True if the Structure is dihedral, False if not\n    \"\"\"\n    return self.number_of_symmetry_mates / self.max_symmetry == 2\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.find_dihedral_chain","title":"find_dihedral_chain","text":"<pre><code>find_dihedral_chain() -&gt; Entity | None\n</code></pre> <p>From the symmetric system, find a dihedral chain and return the instance</p> <p>Returns:</p> <ul> <li> <code>Entity | None</code>         \u2013          <p>The dihedral mate Chain</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def find_dihedral_chain(self) -&gt; Entity | None:  # Todo python 3.11 self\n    \"\"\"From the symmetric system, find a dihedral chain and return the instance\n\n    Returns:\n        The dihedral mate Chain\n    \"\"\"\n    if not self.is_dihedral():\n        return None\n\n    # Ensure if the structure is dihedral a selected dihedral_chain is orthogonal to the maximum symmetry axis\n    max_symmetry_axis = self.mate_rotation_axes[self.max_symmetry_chain_idx]['axis']\n    for chain_idx, data in enumerate(self.mate_rotation_axes):\n        this_chain_axis = data['axis']\n        if data['sym'] == 2:\n            axis_dot_product = np.dot(max_symmetry_axis, this_chain_axis)\n            if axis_dot_product &lt; 0.01:\n                if np.allclose(this_chain_axis, [1, 0, 0]):\n                    self.log.debug(f'The relation between {self.max_symmetry_chain_idx} and {chain_idx} would '\n                                   'result in a malformed .sdf file')\n                    pass  # This won't work in the make_symmdef.pl script, should choose orthogonal y-axis\n                else:\n                    return self.chains[chain_idx]\n    return None\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.make_sdf","title":"make_sdf","text":"<pre><code>make_sdf(struct_file: AnyStr = None, out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr\n</code></pre> <p>Use the make_symmdef_file.pl script from Rosetta to make a symmetry definition file on the Structure</p> <p>perl $ROSETTA/source/src/apps/public/symmetry/make_symmdef_file.pl -p filepath/to/pdb.pdb -i B -q</p> <p>Parameters:</p> <ul> <li> <code>struct_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The location of the input .pdb file</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location the symmetry definition file should be written</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>modify_sym_energy_for_cryst</code>         \u2013          <p>bool = False - Whether the symmetric energy in the file should be modified</p> </li> <li> <code>energy</code>         \u2013          <p>int = 2 - Scalar to modify the Rosetta energy by</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>Symmetry definition filename</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_sdf(self, struct_file: AnyStr = None, out_path: AnyStr = os.getcwd(), **kwargs) -&gt; AnyStr:\n    \"\"\"Use the make_symmdef_file.pl script from Rosetta to make a symmetry definition file on the Structure\n\n    perl $ROSETTA/source/src/apps/public/symmetry/make_symmdef_file.pl -p filepath/to/pdb.pdb -i B -q\n\n    Args:\n        struct_file: The location of the input .pdb file\n        out_path: The location the symmetry definition file should be written\n\n    Keyword Args:\n        modify_sym_energy_for_cryst: bool = False - Whether the symmetric energy in the file should be modified\n        energy: int = 2 - Scalar to modify the Rosetta energy by\n\n    Returns:\n        Symmetry definition filename\n    \"\"\"\n    out_file = os.path.join(out_path, f'{self.name}.sdf')\n    if os.path.exists(out_file):\n        return out_file\n\n    if self.symmetry in utils.symmetry.cubic_point_groups:\n        sdf_mode = 'PSEUDO'\n        self.log.warning('Using experimental symmetry definition file generation, proceed with caution as Rosetta '\n                         'runs may fail due to improper set up')\n    else:\n        sdf_mode = 'NCS'\n\n    if not struct_file:\n        struct_file = self.write(assembly=True, out_path=f'make_sdf_input-{self.name}-{random() * 100000:.0f}.pdb',\n                                 increment_chains=True)\n\n    # As increment_chains is used, get the chain name corresponding to the same index as incremental chain\n    available_chain_ids = chain_id_generator()\n    for _ in range(self.max_symmetry_chain_idx):\n        next(available_chain_ids)\n    chains = [next(available_chain_ids)]\n    if self.is_dihedral():\n        chains.append(self.find_dihedral_chain().chain_id)\n\n    sdf_cmd = [\n        'perl', putils.make_symmdef, '-m', sdf_mode, '-q', '-p', struct_file, '-a', self.chain_ids[0], '-i'\n    ] + chains\n    self.log.info(f'Creating symmetry definition file: {subprocess.list2cmdline(sdf_cmd)}')\n    # with open(out_file, 'w') as file:\n    p = subprocess.Popen(sdf_cmd, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)\n    out, err = p.communicate()\n\n    if os.path.exists(struct_file):\n        os.system(f'rm {struct_file}')\n    if p.returncode != 0:\n        raise DesignError(\n            f'Symmetry definition file creation failed for {self.name}')\n\n    self.format_sdf(out.decode('utf-8').split('\\n')[:-1], to_file=out_file, **kwargs)\n    #                 modify_sym_energy_for_cryst=False, energy=2)\n\n    return out_file\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.format_sdf","title":"format_sdf","text":"<pre><code>format_sdf(lines: list, to_file: AnyStr = None, out_path: AnyStr = os.getcwd(), modify_sym_energy_for_cryst: bool = False, energy: int = None) -&gt; AnyStr\n</code></pre> <p>Ensure proper sdf formatting before proceeding</p> <p>Parameters:</p> <ul> <li> <code>lines</code>             (<code>list</code>)         \u2013          <p>The symmetry definition file lines</p> </li> <li> <code>to_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The name of the symmetry definition file</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location the symmetry definition file should be written</p> </li> <li> <code>modify_sym_energy_for_cryst</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the symmetric energy should match crystallographic systems</p> </li> <li> <code>energy</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>Scalar to modify the Rosetta energy by</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The location the symmetry definition file was written</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def format_sdf(self, lines: list, to_file: AnyStr = None, out_path: AnyStr = os.getcwd(),\n               modify_sym_energy_for_cryst: bool = False, energy: int = None) -&gt; AnyStr:\n    \"\"\"Ensure proper sdf formatting before proceeding\n\n    Args:\n        lines: The symmetry definition file lines\n        to_file: The name of the symmetry definition file\n        out_path: The location the symmetry definition file should be written\n        modify_sym_energy_for_cryst: Whether the symmetric energy should match crystallographic systems\n        energy: Scalar to modify the Rosetta energy by\n\n    Returns:\n        The location the symmetry definition file was written\n    \"\"\"\n    subunits, virtuals, jumps_com, jumps_subunit, trunk = [], [], [], [], []\n    for idx, line in enumerate(lines, 1):\n        if line.startswith('xyz'):\n            virtual = line.split()[1]\n            if virtual.endswith('_base'):\n                subunits.append(virtual)\n            else:\n                virtuals.append(virtual.lstrip('VRT'))\n            # last_vrt = line + 1\n        elif line.startswith('connect_virtual'):\n            jump = line.split()[1].lstrip('JUMP')\n            if jump.endswith('_to_com'):\n                jumps_com.append(jump[:-7])\n            elif jump.endswith('_to_subunit'):\n                jumps_subunit.append(jump[:-11])\n            else:\n                trunk.append(jump)\n            last_jump = idx  # index where the VRTs and connect_virtuals end. The \"last jump\"\n\n    if set(trunk).difference(virtuals):\n        raise SymmetryError(\n            f\"Symmetry Definition File VRTS are malformed. See '{to_file}'\")\n    if len(subunits) != self.number_of_symmetry_mates:\n        raise SymmetryError(\n            f\"Symmetry Definition File VRTX_base are malformed. See '{to_file}'\")\n\n    if self.is_dihedral():  # Remove dihedral connecting (trunk) virtuals: VRT, VRT0, VRT1\n        virtuals = [virtual for virtual in virtuals if len(virtual) &gt; 1]  # subunit_\n    else:\n        try:\n            virtuals.remove('')\n        except ValueError:  # '' not present\n            pass\n\n    jumps_com_to_add = set(virtuals).difference(jumps_com)\n    count_ = 0\n    if jumps_com_to_add:\n        for count_, jump_com in enumerate(jumps_com_to_add, count_):\n            lines.insert(last_jump + count_,\n                         f'connect_virtual JUMP{jump_com}_to_com VRT{jump_com} VRT{jump_com}_base')\n        lines[-2] = lines[-2].strip() + (' JUMP%s_to_subunit' * len(jumps_com_to_add)) % tuple(jumps_com_to_add)\n\n    jumps_subunit_to_add = set(virtuals).difference(jumps_subunit)\n    if jumps_subunit_to_add:\n        for count_, jump_subunit in enumerate(jumps_subunit_to_add, count_):\n            lines.insert(last_jump + count_,\n                         f'connect_virtual JUMP{jump_subunit}_to_subunit VRT{jump_subunit}_base SUBUNIT')\n        lines[-1] = \\\n            lines[-1].strip() + (' JUMP%s_to_subunit' * len(jumps_subunit_to_add)) % tuple(jumps_subunit_to_add)\n\n    if modify_sym_energy_for_cryst:\n        # new energy should equal the energy multiplier times the scoring subunit plus additional complex subunits\n        # where complex subunits = num_subunits - 1\n        # new_energy = 'E = %d*%s + ' % (energy, subunits[0])  # assumes subunits are read in alphanumerical order\n        # new_energy += ' + '.join('1*(%s:%s)' % t for t in zip(repeat(subunits[0]), subunits[1:]))\n        lines[1] = f'E = 2*{subunits[0]}+{\"+\".join(f\"1*({subunits[0]}:{pair})\" for pair in subunits[1:])}'\n    else:\n        if not energy:\n            energy = len(subunits)\n        lines[1] = \\\n            f'E = {energy}*{subunits[0]}+{\"+\".join(f\"{energy}*({subunits[0]}:{pair})\" for pair in subunits[1:])}'\n\n    if not to_file:\n        to_file = os.path.join(out_path, f'{self.name}.sdf')\n\n    with open(to_file, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(lines))\n    if count_ != 0:\n        self.log.info(f\"Symmetry Definition File '{to_file}' was missing {count_} lines. A fix was attempted and \"\n                      'modeling may be affected')\n    return to_file\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.reset_mates","title":"reset_mates","text":"<pre><code>reset_mates()\n</code></pre> <p>Remove oligomeric chains. They should be generated fresh</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def reset_mates(self):\n    \"\"\"Remove oligomeric chains. They should be generated fresh\"\"\"\n    self._chains.clear()\n    self._chains.append(self)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Entity.fragment_db","title":"fragment_db","text":"<pre><code>fragment_db(fragment_db: FragmentDatabase)\n</code></pre> <p>Set the Structure FragmentDatabase to assist with Fragment creation, manipulation, and profiles. Sets .fragment_db for each dependent Structure in 'structure_containers'</p> <p>Entity specific implementation to prevent recursion with [1:]</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@ContainsResidues.fragment_db.setter\ndef fragment_db(self, fragment_db: FragmentDatabase):\n    \"\"\"Set the Structure FragmentDatabase to assist with Fragment creation, manipulation, and profiles.\n    Sets .fragment_db for each dependent Structure in 'structure_containers'\n\n    Entity specific implementation to prevent recursion with [1:]\n    \"\"\"\n    # Set this instance then set all dependents\n    super(Structure, Structure).fragment_db.fset(self, fragment_db)\n    _fragment_db = self._fragment_db\n    if _fragment_db is not None:\n        for structure_type in self.structure_containers:\n            for structure in self.__getattribute__(structure_type)[1:]:\n                structure.fragment_db = _fragment_db\n    else:  # This is likely the RELOAD_DB token. Just return.\n        return\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Model","title":"Model","text":"<pre><code>Model(chains: bool | Sequence[Chain] = True, chain_ids: Iterable[str] = None, rename_chains: bool = False, as_mates: bool = False, **kwargs)\n</code></pre> <p>             Bases: <code>ContainsChains</code></p> <p>The main class for simple Structure manipulation, particularly containing multiple Chain instances</p> <p>Can initialize by passing a file, or passing Atom/Residue/Chain instances. If your Structure is symmetric, a SymmetricModel should be used instead. If you have multiple Model instances, use the MultiModel class.</p> <p>Parameters:</p> <ul> <li> <code>chain_ids</code>             (<code>Iterable[str]</code>, default:                 <code>None</code> )         \u2013          <p>A list of identifiers to assign to each Chain instance</p> </li> <li> <code>chains</code>             (<code>bool | Sequence[Chain]</code>, default:                 <code>True</code> )         \u2013          <p>Whether to create Chain instances from passed Structure container instances, or existing Chain instances to create the Model with</p> </li> <li> <code>rename_chains</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to name each chain an incrementally new Alphabetical character</p> </li> <li> <code>as_mates</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether Chain instances should be controlled by a captain (True), or be dependents</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, chains: bool | Sequence[Chain] = True, chain_ids: Iterable[str] = None,\n             rename_chains: bool = False, as_mates: bool = False, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        chain_ids: A list of identifiers to assign to each Chain instance\n        chains: Whether to create Chain instances from passed Structure container instances, or existing Chain\n            instances to create the Model with\n        rename_chains: Whether to name each chain an incrementally new Alphabetical character\n        as_mates: Whether Chain instances should be controlled by a captain (True), or be dependents\n    \"\"\"\n    super().__init__(**kwargs)  # ContainsChains\n    # Use the same list as default to save parsed chain ids\n    self.original_chain_ids = self.chain_ids = []\n    if chains:  # Populate chains\n        self.structure_containers.append('_chains')\n        if isinstance(chains, Sequence):\n            # Set the chains accordingly, copying them to remove prior relationships\n            self._chains = list(chains)\n            self._copy_structure_containers()  # Copy each Chain in chains\n            if as_mates:\n                if self.residues is None:\n                    raise DesignError(\n                        f\"Couldn't initialize {self.__class__.__name__}.chains as it is missing '.residues' while \"\n                        f\"{as_mates=}\"\n                    )\n            else:  # Create the instance from existing chains\n                self.assign_residues_from_structures(chains)\n                # Reindex all residue and atom indices\n                self.reset_and_reindex_structures(self._chains)\n                # Set the parent attribute for all containers\n                self._update_structure_container_attributes(_parent=self)\n\n            if chain_ids:\n                for chain, id_ in zip(self.chains, chain_ids):\n                    chain.chain_id = id_\n            # By using extend, self.original_chain_ids are set as well\n            self.chain_ids.extend([chain.chain_id for chain in self.chains])\n        else:  # Create Chain instances from Residues\n            self._chains = []\n            self._create_chains(chain_ids=chain_ids)\n            if as_mates:\n                for chain in self.chains:\n                    chain.make_parent()\n\n        if rename_chains or not self.are_chain_ids_pdb_compatible():\n            self.rename_chains()\n\n        self.log.debug(f'Original chain_ids: {\",\".join(self.original_chain_ids)} | '\n                       f'Loaded chain_ids: {\",\".join(self.chain_ids)}')\n    else:\n        self._chains = []\n\n    if self.is_parent():\n        reference_sequence = self.metadata.reference_sequence\n        if isinstance(reference_sequence, dict):  # Was parsed from file\n            self.set_reference_sequence_from_seqres(reference_sequence)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Models","title":"Models","text":"<pre><code>Models(models: Iterable[ContainsEntities | Entity], name: str = None, **kwargs)\n</code></pre> <p>             Bases: <code>UserList</code></p> <p>Container for Model instances. Primarily used for writing [symmetric] multimodel-like Structure instances</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, models: Iterable[ContainsEntities | Entity], name: str = None, **kwargs):\n    super().__init__(initlist=models)  # Sets UserList.data to models\n\n    for model in self:\n        if not isinstance(model, (ContainsEntities, Entity)):\n            raise TypeError(\n                f\"Can't initialize {self.__class__.__name__} with a {type(model).__name__}. Must be an Iterable\"\n                f' of {Model.__name__}')\n\n    self.name = name if name else f'Nameless-{Models.__name__}'\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Models.number_of_models","title":"number_of_models  <code>property</code>","text":"<pre><code>number_of_models: int\n</code></pre> <p>The number of unique models that are found in the Models object</p>"},{"location":"reference/structure/model/#structure.model.Models.from_models","title":"from_models  <code>classmethod</code>","text":"<pre><code>from_models(models: Iterable[Model], **kwargs)\n</code></pre> <p>Initialize from an iterable of Model instances</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_models(cls, models: Iterable[Model], **kwargs):\n    \"\"\"Initialize from an iterable of Model instances\"\"\"\n    return cls(models=models, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Models.write","title":"write","text":"<pre><code>write(out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, multimodel: bool = False, increment_chains: bool = False, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Write Model Atoms to a file specified by out_path or with a passed file_handle</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>bytes | str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the Structure object should be written to disk</p> </li> <li> <code>file_handle</code>             (<code>IO</code>, default:                 <code>None</code> )         \u2013          <p>Used to write Structure details to an open FileObject</p> </li> <li> <code>header</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string that is desired at the top of the file</p> </li> <li> <code>multimodel</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether MODEL and ENDMDL records should be added at the end of each Model</p> </li> <li> <code>increment_chains</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to write each Chain with an incrementing chain ID, otherwise use the chain IDs present, repeating for each Model</p> </li> </ul> <p>Keyword Args     assembly: bool = False - Whether to write an assembly representation of each Model instance</p> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The name of the written file if out_path is used</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def write(self, out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None,\n          multimodel: bool = False, increment_chains: bool = False, **kwargs) -&gt; AnyStr | None:\n    \"\"\"Write Model Atoms to a file specified by out_path or with a passed file_handle\n\n    Args:\n        out_path: The location where the Structure object should be written to disk\n        file_handle: Used to write Structure details to an open FileObject\n        header: A string that is desired at the top of the file\n        multimodel: Whether MODEL and ENDMDL records should be added at the end of each Model\n        increment_chains: Whether to write each Chain with an incrementing chain ID,\n            otherwise use the chain IDs present, repeating for each Model\n\n    Keyword Args\n        assembly: bool = False - Whether to write an assembly representation of each Model instance\n\n    Returns:\n        The name of the written file if out_path is used\n    \"\"\"\n    logger.debug(f'{Models.__name__} is writing {repr(self)}')\n\n    def _write(handle) -&gt; None:\n        if increment_chains:\n            available_chain_ids = chain_id_generator()\n\n            def _get_chain_id(struct: Chain) -&gt; str:\n                return next(available_chain_ids)\n\n        else:\n\n            def _get_chain_id(struct: Chain) -&gt; str:\n                return struct.chain_id\n\n        chain: Chain\n        offset = 0\n\n        def _write_model(_model):\n            nonlocal offset\n            for chain in _model.entities:\n                chain_id = _get_chain_id(chain)\n                chain.write(file_handle=handle, chain_id=chain_id, atom_offset=offset, **kwargs)\n                c_term_residue: Residue = chain.c_terminal_residue\n                offset += chain.number_of_atoms\n                handle.write(f'TER   {offset + 1:&gt;5d}      {c_term_residue.type:3s} '\n                             f'{chain_id:1s}{c_term_residue.number:&gt;4d}\\n')\n\n        if multimodel:\n            for model_number, model in enumerate(self, 1):\n                handle.write('{:9s}{:&gt;4d}\\n'.format('MODEL', model_number))\n                _write_model(model)\n                handle.write('ENDMDL\\n')\n        else:\n            for model in self:\n                _write_model(model)\n\n    if file_handle:\n        return _write(file_handle)\n    else:  # out_path always has default argument current working directory\n        _header = ''  # self.format_header(**kwargs)\n        if header is not None:\n            if not isinstance(header, str):\n                header = str(header)\n            _header += (header if header[-2:] == '\\n' else f'{header}\\n')\n\n        with open(out_path, 'w') as outfile:\n            outfile.write(_header)\n            _write(outfile)\n        return out_path\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel","title":"SymmetricModel","text":"<pre><code>SymmetricModel(sym_entry: SymEntry | int = None, symmetry: str = None, transformations: list[TransformationMapping] = None, uc_dimensions: list[float] = None, symmetry_operators: ndarray | list = None, rotation_matrices: ndarray | list = None, translation_matrices: ndarray | list = None, surrounding_uc: bool = True, **kwargs)\n</code></pre> <p>             Bases: <code>SymmetryOpsMixin</code>, <code>ContainsEntities</code></p> <p>Parameters:</p> <ul> <li> <code>sym_entry</code>             (<code>SymEntry | int</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry which specifies all symmetry parameters</p> </li> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a symmetry to be searched against compatible symmetries</p> </li> <li> <code>transformations</code>             (<code>list[TransformationMapping]</code>, default:                 <code>None</code> )         \u2013          <p>Transformation operations that reproduce the oligomeric/assembly for each Entity</p> </li> <li> <code>rotation_matrices</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>Rotation operations that create the symmetric state</p> </li> <li> <code>translation_matrices</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>Translation operations that create the symmetric state</p> </li> <li> <code>uc_dimensions</code>             (<code>list[float]</code>, default:                 <code>None</code> )         \u2013          <p>The unit cell dimensions for the crystalline symmetry</p> </li> <li> <code>symmetry_operators</code>             (<code>ndarray | list</code>, default:                 <code>None</code> )         \u2013          <p>A set of custom expansion matrices</p> </li> <li> <code>surrounding_uc</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the 3x3 layer group, or 3x3x3 space group should be generated</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, sym_entry: utils.SymEntry.SymEntry | int = None, symmetry: str = None,\n             transformations: list[types.TransformationMapping] = None, uc_dimensions: list[float] = None,\n             symmetry_operators: np.ndarray | list = None, rotation_matrices: np.ndarray | list = None,\n             translation_matrices: np.ndarray | list = None, surrounding_uc: bool = True, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        sym_entry: The SymEntry which specifies all symmetry parameters\n        symmetry: The name of a symmetry to be searched against compatible symmetries\n        transformations: Transformation operations that reproduce the oligomeric/assembly for each Entity\n        rotation_matrices: Rotation operations that create the symmetric state\n        translation_matrices: Translation operations that create the symmetric state\n        uc_dimensions: The unit cell dimensions for the crystalline symmetry\n        symmetry_operators: A set of custom expansion matrices\n        surrounding_uc: Whether the 3x3 layer group, or 3x3x3 space group should be generated\n    \"\"\"\n    super().__init__(**kwargs)  # SymmetryOpsMixin\n    self._expand_matrices = self._expand_translations = None\n    self.set_symmetry(sym_entry=sym_entry, symmetry=symmetry, uc_dimensions=uc_dimensions,\n                      operators=symmetry_operators, rotations=rotation_matrices, translations=translation_matrices,\n                      transformations=transformations, surrounding_uc=surrounding_uc)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.symmetric_coords_split_by_entity","title":"symmetric_coords_split_by_entity  <code>property</code>","text":"<pre><code>symmetric_coords_split_by_entity: list[list[ndarray]]\n</code></pre> <p>A view of the symmetric coords split for each symmetric model by the Pose Entity indices</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.symmetric_coords_by_entity","title":"symmetric_coords_by_entity  <code>property</code>","text":"<pre><code>symmetric_coords_by_entity: list[ndarray]\n</code></pre> <p>A view of the symmetric coords for each Entity in order of the Pose Entity indices</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.center_of_mass_symmetric_entities","title":"center_of_mass_symmetric_entities  <code>property</code>","text":"<pre><code>center_of_mass_symmetric_entities: list[list[ndarray]]\n</code></pre> <p>The center of mass position for each Entity instance in the symmetric system for each symmetry mate with shape [(number_of_symmetry_mates, 3), ... number_of_entities]</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.assembly","title":"assembly  <code>property</code>","text":"<pre><code>assembly: Model\n</code></pre> <p>Provides the Structure object containing all symmetric chains in the assembly unless the design is 2- or 3-D then the assembly only contains the contacting models</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.assembly_minimally_contacting","title":"assembly_minimally_contacting  <code>property</code>","text":"<pre><code>assembly_minimally_contacting: Model\n</code></pre> <p>Provides the Structure object only containing the SymmetricModel instances contacting the ASU</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.entity_transformations","title":"entity_transformations  <code>property</code>","text":"<pre><code>entity_transformations: list[TransformationMapping] | list\n</code></pre> <p>The transformation parameters for each Entity in the SymmetricModel. Each entry has the TransformationMapping type</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.assembly_tree","title":"assembly_tree  <code>property</code>","text":"<pre><code>assembly_tree: BinaryTreeType\n</code></pre> <p>Holds the tree structure of the backbone and cb symmetric_coords not including the asu coords</p>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.from_assembly","title":"from_assembly  <code>classmethod</code>","text":"<pre><code>from_assembly(assembly: Model, sym_entry: SymEntry | int = None, symmetry: str = None, **kwargs)\n</code></pre> <p>Initialize from a symmetric assembly</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>@classmethod\ndef from_assembly(cls, assembly: Model, sym_entry: utils.SymEntry.SymEntry | int = None,\n                  symmetry: str = None, **kwargs):\n    \"\"\"Initialize from a symmetric assembly\"\"\"\n    if symmetry is None and sym_entry is None:\n        raise ValueError(\n            \"Can't initialize without symmetry. Pass 'symmetry' or 'sym_entry' to \"\n            f'{cls.__name__}.{cls.from_assembly.__name__}() constructor')\n    return cls(structure=assembly, sym_entry=sym_entry, symmetry=symmetry, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.set_symmetry","title":"set_symmetry","text":"<pre><code>set_symmetry(sym_entry: SymEntry | int = None, symmetry: str = None, crystal: bool = False, cryst_record: str = None, uc_dimensions: list[float] = None, operators: tuple[ndarray | list[list[float]], ndarray | list[float]] | ndarray = None, rotations: ndarray | list[list[float]] = None, translations: ndarray | list[float] = None, transformations: list[TransformationMapping] = None, surrounding_uc: bool = True, **kwargs)\n</code></pre> <p>Set the model symmetry using the CRYST1 record, or the unit cell dimensions and the Hermann-Mauguin symmetry notation (in CRYST1 format, ex P432) for the Model assembly. If the assembly is a point group, only the symmetry notation is required</p> <p>Parameters:</p> <ul> <li> <code>sym_entry</code>             (<code>SymEntry | int</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry which specifies all symmetry parameters</p> </li> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of a symmetry to be searched against compatible symmetries</p> </li> <li> <code>crystal</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether crystalline symmetry should be used</p> </li> <li> <code>cryst_record</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a CRYST1 record is known and should be used</p> </li> <li> <code>uc_dimensions</code>             (<code>list[float]</code>, default:                 <code>None</code> )         \u2013          <p>The unit cell dimensions for the crystalline symmetry</p> </li> <li> <code>operators</code>             (<code>tuple[ndarray | list[list[float]], ndarray | list[float]] | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>A set of custom expansion matrices</p> </li> <li> <code>rotations</code>             (<code>ndarray | list[list[float]]</code>, default:                 <code>None</code> )         \u2013          <p>A set of rotation matrices used to recapitulate the SymmetricModel from the asymmetric unit</p> </li> <li> <code>translations</code>             (<code>ndarray | list[float]</code>, default:                 <code>None</code> )         \u2013          <p>A set of translation vectors used to recapitulate the SymmetricModel from the asymmetric unit</p> </li> <li> <code>transformations</code>             (<code>list[TransformationMapping]</code>, default:                 <code>None</code> )         \u2013          <p>Transformation operations that reproduce the oligomeric state for each Entity</p> </li> <li> <code>surrounding_uc</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the 3x3 layer group, or 3x3x3 space group should be generated</p> </li> <li> <code>crystal</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether crystalline symmetry should be used</p> </li> <li> <code>cryst_record</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a CRYST1 record is known and should be used</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def set_symmetry(self, sym_entry: utils.SymEntry.SymEntry | int = None, symmetry: str = None,\n                 crystal: bool = False, cryst_record: str = None, uc_dimensions: list[float] = None,\n                 operators: tuple[np.ndarray | list[list[float]], np.ndarray | list[float]] | np.ndarray = None,\n                 rotations: np.ndarray | list[list[float]] = None, translations: np.ndarray | list[float] = None,\n                 transformations: list[types.TransformationMapping] = None, surrounding_uc: bool = True,\n                 **kwargs):\n    \"\"\"Set the model symmetry using the CRYST1 record, or the unit cell dimensions and the Hermann-Mauguin symmetry\n    notation (in CRYST1 format, ex P432) for the Model assembly. If the assembly is a point group, only the symmetry\n    notation is required\n\n    Args:\n        sym_entry: The SymEntry which specifies all symmetry parameters\n        symmetry: The name of a symmetry to be searched against compatible symmetries\n        crystal: Whether crystalline symmetry should be used\n        cryst_record: If a CRYST1 record is known and should be used\n        uc_dimensions: The unit cell dimensions for the crystalline symmetry\n        operators: A set of custom expansion matrices\n        rotations: A set of rotation matrices used to recapitulate the SymmetricModel from the asymmetric unit\n        translations: A set of translation vectors used to recapitulate the SymmetricModel from the asymmetric unit\n        transformations: Transformation operations that reproduce the oligomeric state for each Entity\n        surrounding_uc: Whether the 3x3 layer group, or 3x3x3 space group should be generated\n        crystal: Whether crystalline symmetry should be used\n        cryst_record: If a CRYST1 record is known and should be used\n    \"\"\"\n    chains = self._chains\n    super().set_symmetry(\n        sym_entry=sym_entry, symmetry=symmetry,\n        crystal=crystal, cryst_record=cryst_record, uc_dimensions=uc_dimensions,\n    )\n    number_of_symmetry_mates = self.number_of_symmetry_mates\n\n    if not self.is_symmetric():\n        # No symmetry keyword args were passed\n        if operators is not None or rotations is not None or translations is not None:\n            passed_args = []\n            if operators:\n                passed_args.append('operators')\n            if rotations:\n                passed_args.append('rotations')\n            if translations:\n                passed_args.append('translations')\n            raise ConstructionError(\n                f\"Couldn't set_symmetry() using {', '.join(passed_args)} without explicitly passing \"\n                \"'symmetry' or 'sym_entry'\"\n            )\n        return\n\n    # Set rotations and translations to the correct symmetry operations\n    # where operators, rotations, and translations are user provided from some sort of BIOMT (fiber, other)\n    if operators is not None:\n        symmetry_source_arg = \"'operators' \"\n\n        num_operators = len(operators)\n        if isinstance(operators, tuple) and num_operators == 2:\n            self.log.warning(\"Providing custom symmetry 'operators' may result in improper symmetric \"\n                             'configuration. Proceed with caution')\n            rotations, translations = operators\n        elif isinstance(operators, Sequence) and num_operators == number_of_symmetry_mates:\n            rotations = []\n            translations = []\n            try:\n                for rot, tx in operators:\n                    rotations.append(rot)\n                    translations.append(tx)\n            except TypeError:  # Unpack failed\n                raise ValueError(\n                    f\"Couldn't parse the 'operators'={repr(operators)}.\\n\\n\"\n                    \"Expected a Sequence[rotation shape=(3,3). translation shape=(3,)] pairs.\"\n                )\n        elif isinstance(operators, np.ndarray):\n            if operators.shape[1:] == (3, 4):\n                # Parse from a single input of 3 row by 4 column style, like BIOMT\n                rotations = operators[:, :, :3]\n                translations = operators[:, :, 3:].squeeze()\n            elif operators.shape[1:] == 3:  # Assume just rotations\n                rotations = operators\n                translations = np.tile(utils.symmetry.origin, len(rotations))\n            else:\n                raise ConstructionError(\n                    f\"The 'operators' form {repr(operators)} isn't supported.\")\n        else:\n            raise ConstructionError(\n                f\"The 'operators' form {repr(operators)} isn't supported. Must provide a tuple of \"\n                'array-like objects with the order (rotation matrices, translation vectors) or use the '\n                \"'rotations' and 'translations' keyword args\")\n    else:\n        symmetry_source_arg = ''\n\n    # Now that symmetry is set, check if the Structure parsed all symmetric chains\n    if len(chains) == self.number_of_entities * number_of_symmetry_mates:\n        parsed_assembly = True\n    else:\n        parsed_assembly = False\n\n    # Set the symmetry operations\n    if rotations is not None and translations is not None:\n        if not isinstance(rotations, np.ndarray):\n            rotations = np.ndarray(rotations)\n        if rotations.ndim == 3:\n            # Assume operators were provided in a standard orientation and transpose for subsequent efficiency\n            # Using .swapaxes(-2, -1) call here instead of .transpose() for safety\n            self._expand_matrices = rotations.swapaxes(-2, -1)\n        else:\n            raise SymmetryError(\n                f\"Expected {symmetry_source_arg}rotation matrices with 3 dimensions, not {rotations.ndim} \"\n                \"dimensions. Ensure the passed rotation matrices have a shape of (N symmetry operations, 3, 3)\"\n            )\n\n        if not isinstance(translations, np.ndarray):\n            translations = np.ndarray(translations)\n        if translations.ndim == 2:\n            # Assume operators were provided in a standard orientation each vector needs to be in own array on dim=2\n            self._expand_translations = translations[:, None, :]\n        else:\n            raise SymmetryError(\n                f\"Expected {symmetry_source_arg}translation vectors with 2 dimensions, not {translations.ndim} \"\n                \"dimensions. Ensure the passed translations have a shape of (N symmetry operations, 3)\"\n            )\n    else:  # The symmetry operators must be possible to find or canonical.\n        symmetry_source_arg = \"'chains' \"\n        # Get canonical operators\n        if self.dimension == 0:\n            # The _expand_matrices rotation matrices are pre-transposed to avoid repetitive operations\n            _expand_matrices = utils.symmetry.point_group_symmetry_operatorsT[self.symmetry]\n            # The _expand_translations vectors are pre-sliced to enable numpy operations\n            _expand_translations = \\\n                np.tile(utils.symmetry.origin, (self.number_of_symmetry_mates, 1))[:, None, :]\n\n            if parsed_assembly:\n                # The Structure should have symmetric chains\n                # Set up symmetry operations using orient\n\n                # Save the original position for subsequent reversion\n                ca_coords = self.ca_coords.copy()\n                # Transform to canonical orientation.\n                self.orient()\n                # Set the symmetry operations again as they are incorrect after orient()\n                self._expand_matrices = _expand_matrices\n                self._expand_translations = _expand_translations\n                # Next, transform back to original and carry the correctly situated symmetry operations along.\n                _, rot, tx = superposition3d(ca_coords, self.ca_coords)\n                self.transform(rotation=rot, translation=tx)\n            else:\n                self._expand_matrices = _expand_matrices\n                self._expand_translations = _expand_translations\n        else:\n            self._expand_matrices, self._expand_translations = \\\n                utils.symmetry.space_group_symmetry_operatorsT[self.symmetry]\n\n    # Removed parsed chain information\n    self.reset_mates()\n\n    try:\n        self._symmetric_coords.coords\n    except AttributeError:\n        self.generate_symmetric_coords(surrounding_uc=surrounding_uc)\n\n    # Check if the oligomer is constructed for each entity\n    for entity, subunit_number in zip(self.entities, self.sym_entry.group_subunit_numbers):\n        if entity.number_of_symmetry_mates != subunit_number:\n            # Generate oligomers for each entity\n            self.make_oligomers(transformations=transformations)\n            break\n\n    # Once oligomers are specified the ASU can be set properly\n    self.set_contacting_asu(from_assembly=parsed_assembly)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.get_asu_interaction_model_indices","title":"get_asu_interaction_model_indices","text":"<pre><code>get_asu_interaction_model_indices(calculate_contacts: bool = True, distance: float = 8.0, **kwargs) -&gt; list[int]\n</code></pre> <p>From an ASU, find the symmetric models that immediately surround the ASU</p> <p>Parameters:</p> <ul> <li> <code>calculate_contacts</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to calculate interacting models by atomic contacts</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>When calculate_contacts is True, the CB distance which nearby symmetric models should be found When calculate_contacts is False, uses the ASU radius plus the maximum Entity radius</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The indices of the models that contact the asu</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_asu_interaction_model_indices(self, calculate_contacts: bool = True, distance: float = 8., **kwargs) -&gt; \\\n        list[int]:\n    \"\"\"From an ASU, find the symmetric models that immediately surround the ASU\n\n    Args:\n        calculate_contacts: Whether to calculate interacting models by atomic contacts\n        distance: When calculate_contacts is True, the CB distance which nearby symmetric models should be found\n            When calculate_contacts is False, uses the ASU radius plus the maximum Entity radius\n\n    Returns:\n        The indices of the models that contact the asu\n    \"\"\"\n    if calculate_contacts:\n        # DEBUG self.report_symmetric_coords(self.get_asu_interaction_model_indices.__name__)\n        asu_query = self.assembly_tree.query_radius(self.coords[self.backbone_and_cb_indices], distance)\n        # Combine each subarray of the asu_query and divide by the assembly_tree interval length -&gt; len(asu_query)\n        interacting_models = (\n                                     np.array(list({asu_idx for asu_contacts in asu_query.tolist()\n                                                    for asu_idx in asu_contacts.tolist()})\n                                              )\n                                     // len(asu_query)\n                             ) + 1\n        # The asu is missing from assembly_tree so add 1 to get the\n        # correct model indices ^\n        interacting_models = np.unique(interacting_models).tolist()\n    else:\n        # The furthest point from the asu COM + the max individual Entity radius\n        distance = self.radius + max([entity.radius for entity in self.entities])\n        self.log.debug(f'For ASU neighbor query, using the distance={distance}')\n        center_of_mass = self.center_of_mass\n        interacting_models = [idx for idx, sym_model_com in enumerate(self.center_of_mass_symmetric_models)\n                              if np.linalg.norm(center_of_mass - sym_model_com) &lt;= distance]\n        # Remove the first index from the interacting_models due to exclusion of asu from convention above\n        interacting_models.pop(0)\n\n    return interacting_models\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.get_asu_atom_indices","title":"get_asu_atom_indices","text":"<pre><code>get_asu_atom_indices(as_slice: bool = False) -&gt; list[int] | slice\n</code></pre> <p>Find the coordinate indices of the asu equivalent model in the SymmetricModel. Zero-indexed</p> <p>Returns:</p> <ul> <li> <code>list[int] | slice</code>         \u2013          <p>The indices in the SymmetricModel where the ASU is also located</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_asu_atom_indices(self, as_slice: bool = False) -&gt; list[int] | slice:\n    \"\"\"Find the coordinate indices of the asu equivalent model in the SymmetricModel. Zero-indexed\n\n    Returns:\n        The indices in the SymmetricModel where the ASU is also located\n    \"\"\"\n    asu_model_idx = self.asu_model_index\n    number_of_atoms = self.number_of_atoms\n    start_idx = number_of_atoms * asu_model_idx\n    end_idx = number_of_atoms * (asu_model_idx + 1)\n\n    if as_slice:\n        return slice(start_idx, end_idx)\n    else:\n        return list(range(start_idx, end_idx))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.get_oligomeric_atom_indices","title":"get_oligomeric_atom_indices","text":"<pre><code>get_oligomeric_atom_indices(entity: Entity) -&gt; list[int]\n</code></pre> <p>Find the coordinate indices of the intra-oligomeric equivalent models in the SymmetricModel. Zero-indexed</p> <p>Parameters:</p> <ul> <li> <code>entity</code>             (<code>Entity</code>)         \u2013          <p>The Entity with oligomeric chains to query for corresponding symmetry mates</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The indices in the SymmetricModel where the intra-oligomeric contacts are located</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_oligomeric_atom_indices(self, entity: Entity) -&gt; list[int]:\n    \"\"\"Find the coordinate indices of the intra-oligomeric equivalent models in the SymmetricModel. Zero-indexed\n\n    Args:\n        entity: The Entity with oligomeric chains to query for corresponding symmetry mates\n\n    Returns:\n        The indices in the SymmetricModel where the intra-oligomeric contacts are located\n    \"\"\"\n    number_of_atoms = self.number_of_atoms\n    oligomeric_atom_indices = []\n    for model_number in self.oligomeric_model_indices.get(entity):\n        oligomeric_atom_indices.extend(range(number_of_atoms * model_number,\n                                             number_of_atoms * (model_number + 1)))\n    return oligomeric_atom_indices\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.get_asu_interaction_indices","title":"get_asu_interaction_indices","text":"<pre><code>get_asu_interaction_indices(**kwargs) -&gt; list[int]\n</code></pre> <p>Find the coordinate indices for the models in the SymmetricModel interacting with the asu. Zero-indexed</p> <p>Other Parameters:</p> <ul> <li> <code>calculate_contacts</code>         \u2013          <p>bool = True - Whether to calculate interacting models by atomic contacts</p> </li> <li> <code>distance</code>         \u2013          <p>float = 8.0 - When calculate_contacts is True, the CB distance which nearby symmetric models  should be found. When calculate_contacts is False, uses the ASU radius plus the maximum Entity radius</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[int]</code>         \u2013          <p>The indices in the SymmetricModel where the asu contacts other models</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_asu_interaction_indices(self, **kwargs) -&gt; list[int]:\n    \"\"\"Find the coordinate indices for the models in the SymmetricModel interacting with the asu. Zero-indexed\n\n    Keyword Args:\n        calculate_contacts: bool = True - Whether to calculate interacting models by atomic contacts\n        distance: float = 8.0 - When calculate_contacts is True, the CB distance which nearby symmetric models\n             should be found. When calculate_contacts is False, uses the ASU radius plus the maximum Entity radius\n\n    Returns:\n        The indices in the SymmetricModel where the asu contacts other models\n    \"\"\"\n    number_of_atoms = self.number_of_atoms\n    interacting_indices = []\n    for model_number in self.get_asu_interaction_model_indices(**kwargs):\n        interacting_indices.extend(range(number_of_atoms * model_number,\n                                         number_of_atoms * (model_number+1)))\n\n    return interacting_indices\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.make_oligomers","title":"make_oligomers","text":"<pre><code>make_oligomers(transformations: list[TransformationMapping] = None)\n</code></pre> <p>Generate oligomers for each Entity in the SymmetricModel</p> <p>Parameters:</p> <ul> <li> <code>transformations</code>             (<code>list[TransformationMapping]</code>, default:                 <code>None</code> )         \u2013          <p>The entity_transformations operations that reproduce the individual oligomers</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def make_oligomers(self, transformations: list[types.TransformationMapping] = None):\n    \"\"\"Generate oligomers for each Entity in the SymmetricModel\n\n    Args:\n        transformations: The entity_transformations operations that reproduce the individual oligomers\n    \"\"\"\n    self.log.debug(f'Initializing oligomeric symmetry')\n    if transformations is None or not all(transformations):\n        # If this fails then the symmetry is failed... It should never return an empty list as\n        # .entity_transformations -&gt; ._assign_pose_transformation() will raise SymmetryError\n        transformations = self.entity_transformations\n\n    for entity, subunit_number, symmetry, transformation in zip(\n            self.entities, self.sym_entry.group_subunit_numbers, self.sym_entry.groups, transformations):\n        if entity.number_of_symmetry_mates != subunit_number:\n            entity.make_oligomer(symmetry=symmetry, **transformation)\n        else:\n            self.log.debug(f'{repr(entity)} is already the correct oligomer, skipping make_oligomer()')\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.symmetric_assembly_is_clash","title":"symmetric_assembly_is_clash","text":"<pre><code>symmetric_assembly_is_clash(measure: coords_type_literal = default_clash_criteria, distance: float = default_clash_distance, warn: bool = False) -&gt; bool\n</code></pre> <p>Returns True if the SymmetricModel presents any clashes at the specified distance</p> <p>Parameters:</p> <ul> <li> <code>measure</code>             (<code>coords_type_literal</code>, default:                 <code>default_clash_criteria</code> )         \u2013          <p>The atom type to measure clashing by</p> </li> <li> <code>distance</code>             (<code>float</code>, default:                 <code>default_clash_distance</code> )         \u2013          <p>The distance which clashes should be checked</p> </li> <li> <code>warn</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to emit warnings about identified clashes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>True if the symmetric assembly clashes with the asu, False otherwise</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def symmetric_assembly_is_clash(self, measure: coords_type_literal = default_clash_criteria,\n                                distance: float = default_clash_distance, warn: bool = False) -&gt; bool:\n    \"\"\"Returns True if the SymmetricModel presents any clashes at the specified distance\n\n    Args:\n        measure: The atom type to measure clashing by\n        distance: The distance which clashes should be checked\n        warn: Whether to emit warnings about identified clashes\n\n    Returns:\n        True if the symmetric assembly clashes with the asu, False otherwise\n    \"\"\"\n    if not self.is_symmetric():\n        self.log.warning(\"Can't check if the assembly is clashing as it has no symmetry\")\n        return False\n    indices = self.__getattribute__(f'{measure}_indices')\n    clashes = self.assembly_tree.two_point_correlation(self.coords[indices], [distance])\n    if clashes[0] &gt; 0:\n        if warn:\n            self.log.warning(\n                f\"{self.name}: Found {clashes[0]} clashing sites. Pose isn't a viable symmetric assembly\")\n        return True  # Clash\n    else:\n        return False  # No clash\n</code></pre>"},{"location":"reference/structure/model/#structure.model.SymmetricModel.write","title":"write","text":"<pre><code>write(out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, assembly: bool = False, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Write SymmetricModel Atoms to a file specified by out_path or with a passed file_handle</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>bytes | str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the Structure object should be written to disk</p> </li> <li> <code>file_handle</code>             (<code>IO</code>, default:                 <code>None</code> )         \u2013          <p>Used to write Structure details to an open FileObject</p> </li> <li> <code>header</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string that is desired at the top of the file</p> </li> <li> <code>assembly</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to write the full assembly. Default writes only the ASU</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>increment_chains</code>         \u2013          <p>bool = False - Whether to write each Structure with a new chain name, otherwise write as a new Model</p> </li> <li> <code>surrounding_uc</code>         \u2013          <p>bool = False - Whether the 3x3 layer group, or 3x3x3 space group should be written when assembly is True and self.dimension &gt; 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The name of the written file if out_path is used</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def write(self, out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None,\n          assembly: bool = False, **kwargs) -&gt; AnyStr | None:\n    \"\"\"Write SymmetricModel Atoms to a file specified by out_path or with a passed file_handle\n\n    Args:\n        out_path: The location where the Structure object should be written to disk\n        file_handle: Used to write Structure details to an open FileObject\n        header: A string that is desired at the top of the file\n        assembly: Whether to write the full assembly. Default writes only the ASU\n\n    Keyword Args:\n        increment_chains: bool = False - Whether to write each Structure with a new chain name, otherwise write as\n            a new Model\n        surrounding_uc: bool = False - Whether the 3x3 layer group, or 3x3x3 space group should be written when\n            assembly is True and self.dimension &gt; 1\n\n    Returns:\n        The name of the written file if out_path is used\n    \"\"\"\n    self.log.debug(f'{SymmetricModel.__name__} is writing {repr(self)}')\n    is_symmetric = self.is_symmetric()\n\n    def _write(handle) -&gt; None:\n        if is_symmetric:\n            if assembly:\n                models = self._generate_assembly_models(**kwargs)\n                models.write(file_handle=handle, **kwargs)\n            else:  # Skip all models, write asu. Use biomt_record/cryst_record for symmetry\n                for entity in self.entities:\n                    entity.write(file_handle=handle, **kwargs)\n        else:  # Finish with a standard write\n            super(Structure, Structure).write(self, file_handle=handle, **kwargs)\n\n    if file_handle:\n        return _write(file_handle)\n    else:  # out_path default argument is current working directory\n        # Write the header as an asu if no assembly requested or not symmetric\n        assembly_header = (is_symmetric and assembly) or not is_symmetric\n        _header = self.format_header(assembly=assembly_header, **kwargs)\n        if header is not None:\n            if not isinstance(header, str):\n                header = str(header)\n            _header += (header if header[-2:] == '\\n' else f'{header}\\n')\n\n        with open(out_path, 'w') as outfile:\n            outfile.write(_header)\n            _write(outfile)\n        return out_path\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose","title":"Pose","text":"<pre><code>Pose(**kwargs)\n</code></pre> <p>             Bases: <code>SymmetricModel</code>, <code>MetricsMixin</code></p> <p>A Pose is made of single or multiple Structure objects such as Entities, Chains, or other structures. All objects share a common feature such as the same symmetric system or the same general atom configuration in separate models across the Structure or sequence.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # Pose\n    self._design_selection_entity_names = {entity.name for entity in self.entities}\n    self._design_selector_atom_indices = set(self._atom_indices)\n    self._required_atom_indices = []\n    self._interface_residue_indices_by_entity_name_pair = {}\n    self._interface_residue_indices_by_interface = {}\n    self._interface_residue_indices_by_interface_unique = {}\n    self._fragment_info_by_entity_pair = {}\n    self.split_interface_ss_elements = {}\n    self.ss_sequence_indices = []\n    self.ss_type_sequence = []\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.split_interface_ss_elements","title":"split_interface_ss_elements  <code>instance-attribute</code>","text":"<pre><code>split_interface_ss_elements: dict[int, list[int]] = {}\n</code></pre> <p>Stores the interface number mapped to an index corresponding to the secondary structure type  Ex: {1: [0, 0, 1, 2, ...] , 2: [9, 9, 9, 13, ...]]}</p>"},{"location":"reference/structure/model/#structure.model.Pose.ss_sequence_indices","title":"ss_sequence_indices  <code>instance-attribute</code>","text":"<pre><code>ss_sequence_indices: list[int] = []\n</code></pre> <p>Index which indicates the Residue membership to the secondary structure type element sequence</p>"},{"location":"reference/structure/model/#structure.model.Pose.ss_type_sequence","title":"ss_type_sequence  <code>instance-attribute</code>","text":"<pre><code>ss_type_sequence: list[str] = []\n</code></pre> <p>The ordered secondary structure type sequence which contains one character/secondary structure element</p>"},{"location":"reference/structure/model/#structure.model.Pose.active_entities","title":"active_entities  <code>property</code>","text":"<pre><code>active_entities: list[Entity]\n</code></pre> <p>The Entity instances that are available for design calculations given a design selector</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_residues","title":"interface_residues  <code>property</code>","text":"<pre><code>interface_residues: list[Residue]\n</code></pre> <p>The Residue instances identified in interfaces in the Pose sorted based on index. Residue instances may be completely buried depending on interface distance</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_residues_by_entity_pair","title":"interface_residues_by_entity_pair  <code>property</code>","text":"<pre><code>interface_residues_by_entity_pair: dict[tuple[Entity, Entity], tuple[list[Residue], list[Residue]]]\n</code></pre> <p>The Residue instances identified between pairs of Entity instances</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_neighbor_residues","title":"interface_neighbor_residues  <code>property</code>","text":"<pre><code>interface_neighbor_residues: list[Residue]\n</code></pre> <p>The Residue instances identified as neighbors to interfaces in the Pose. Assumes default distance of 8 A</p>"},{"location":"reference/structure/model/#structure.model.Pose.design_residues","title":"design_residues  <code>property</code> <code>writable</code>","text":"<pre><code>design_residues: list[Residue]\n</code></pre> <p>The Residue instances identified for design in the Pose. Includes interface_residues</p>"},{"location":"reference/structure/model/#structure.model.Pose.required_residues","title":"required_residues  <code>property</code>","text":"<pre><code>required_residues: list[Residue]\n</code></pre> <p>Returns the Residue instances that are required according to DesignSelector</p>"},{"location":"reference/structure/model/#structure.model.Pose.core_residues","title":"core_residues  <code>property</code>","text":"<pre><code>core_residues: list[Residue]\n</code></pre> <p>Get the Residue instances that reside in the core of the interfaces</p> <p>Parameters:</p> <ul> <li> <code>relative_sasa_thresh</code>         \u2013          <p>The relative area threshold that the Residue should fall below before it is considered 'core'. Default cutoff percent is based on Levy, E. 2010</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The core Residue instances</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.Pose.rim_residues","title":"rim_residues  <code>property</code>","text":"<pre><code>rim_residues: list[Residue]\n</code></pre> <p>Get the Residue instances that reside in the rim of the interface</p> <p>Parameters:</p> <ul> <li> <code>relative_sasa_thresh</code>         \u2013          <p>The relative area threshold that the Residue should fall below before it is considered 'rim'. Default cutoff percent is based on Levy, E. 2010</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The rim Residue instances</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.Pose.support_residues","title":"support_residues  <code>property</code>","text":"<pre><code>support_residues: list[Residue]\n</code></pre> <p>Get the Residue instances that support the interface</p> <p>Parameters:</p> <ul> <li> <code>relative_sasa_thresh</code>         \u2013          <p>The relative area threshold that the Residue should fall below before it is considered 'support'. Default cutoff percent is based on Levy, E. 2010</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>atom</code>         \u2013          <p>bool = True - Whether the output should be generated for each atom. If False, will be generated for each Residue</p> </li> <li> <code>probe_radius</code>         \u2013          <p>float = 1.4 - The radius which surface area should be generated</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Residue]</code>         \u2013          <p>The support Residue instances</p> </li> </ul>"},{"location":"reference/structure/model/#structure.model.Pose.fragment_info_by_entity_pair","title":"fragment_info_by_entity_pair  <code>property</code>","text":"<pre><code>fragment_info_by_entity_pair: dict[tuple[Entity, Entity], list[FragmentInfo]]\n</code></pre> <p>Returns the FragmentInfo present as the result of structural overlap between pairs of Entity instances</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_fragment_residue_indices","title":"interface_fragment_residue_indices  <code>property</code> <code>writable</code>","text":"<pre><code>interface_fragment_residue_indices: list[int]\n</code></pre> <p>The Residue indices where Fragment occurrences are observed</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_residues_by_interface_unique","title":"interface_residues_by_interface_unique  <code>property</code>","text":"<pre><code>interface_residues_by_interface_unique: dict[int, list[Residue]]\n</code></pre> <p>Keeps the Residue instances grouped by membership to each side of the interface. Residues are unique to one side of the interface     Ex: {1: [Residue, ...], 2: [Residue, ...]}</p>"},{"location":"reference/structure/model/#structure.model.Pose.interface_residues_by_interface","title":"interface_residues_by_interface  <code>property</code>","text":"<pre><code>interface_residues_by_interface: dict[int, list[Residue]]\n</code></pre> <p>Keeps the Residue instances grouped by membership to each side of the interface. Residues can be duplicated on each side when interface contains a 2-fold axis of symmetry     Ex: {1: [Residue, ...], 2: [Residue, ...]}</p>"},{"location":"reference/structure/model/#structure.model.Pose.fragment_metrics_by_entity_pair","title":"fragment_metrics_by_entity_pair  <code>property</code>","text":"<pre><code>fragment_metrics_by_entity_pair: dict[tuple[Entity, Entity], dict[str, Any]]\n</code></pre> <p>Returns the metrics from structural overlapping Fragment observations between pairs of Entity instances</p>"},{"location":"reference/structure/model/#structure.model.Pose.calculate_metrics","title":"calculate_metrics","text":"<pre><code>calculate_metrics(**kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Calculate metrics for the instance</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>{ 'entity_max_radius_average_deviation', 'entity_min_radius_average_deviation', 'entity_radius_average_deviation', 'interface_b_factor', 'interface1_secondary_structure_fragment_topology', 'interface1_secondary_structure_fragment_count', 'interface1_secondary_structure_topology', 'interface1_secondary_structure_count', 'interface2_secondary_structure_fragment_topology', 'interface2_secondary_structure_fragment_count', 'interface2_secondary_structure_topology', 'interface2_secondary_structure_count', 'maximum_radius', 'minimum_radius', 'multiple_fragment_ratio', 'nanohedra_score_normalized', 'nanohedra_score_center_normalized', 'nanohedra_score', 'nanohedra_score_center', 'number_residues_interface_fragment_total', 'number_residues_interface_fragment_center', 'number_fragments_interface', 'number_residues_interface', 'number_residues_interface_non_fragment', 'percent_fragment_helix', 'percent_fragment_strand', 'percent_fragment_coil', 'percent_residues_fragment_interface_total', 'percent_residues_fragment_interface_center', 'percent_residues_non_fragment_interface', 'pose_length', 'symmetric_interface'</p> </li> <li> <code>dict[str, Any]</code>         \u2013          <p>}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_metrics(self, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Calculate metrics for the instance\n\n    Returns:\n        {\n            'entity_max_radius_average_deviation',\n            'entity_min_radius_average_deviation',\n            'entity_radius_average_deviation',\n            'interface_b_factor',\n            'interface1_secondary_structure_fragment_topology',\n            'interface1_secondary_structure_fragment_count',\n            'interface1_secondary_structure_topology',\n            'interface1_secondary_structure_count',\n            'interface2_secondary_structure_fragment_topology',\n            'interface2_secondary_structure_fragment_count',\n            'interface2_secondary_structure_topology',\n            'interface2_secondary_structure_count',\n            'maximum_radius',\n            'minimum_radius',\n            'multiple_fragment_ratio',\n            'nanohedra_score_normalized',\n            'nanohedra_score_center_normalized',\n            'nanohedra_score',\n            'nanohedra_score_center',\n            'number_residues_interface_fragment_total',\n            'number_residues_interface_fragment_center',\n            'number_fragments_interface',\n            'number_residues_interface',\n            'number_residues_interface_non_fragment',\n            'percent_fragment_helix',\n            'percent_fragment_strand',\n            'percent_fragment_coil',\n            'percent_residues_fragment_interface_total',\n            'percent_residues_fragment_interface_center',\n            'percent_residues_non_fragment_interface',\n            'pose_length',\n            'symmetric_interface'\n        }\n    \"\"\"\n    minimum_radius, maximum_radius = float('inf'), 0.\n    entity_metrics = []\n    reference_com = self.center_of_mass_symmetric\n    for entity in self.entities:\n        # _entity_metrics = entity.calculate_metrics()\n        _entity_metrics = entity.calculate_spatial_orientation_metrics(reference=reference_com)\n\n        if _entity_metrics['min_radius'] &lt; minimum_radius:\n            minimum_radius = _entity_metrics['min_radius']\n        if _entity_metrics['max_radius'] &gt; maximum_radius:\n            maximum_radius = _entity_metrics['max_radius']\n        entity_metrics.append(_entity_metrics)\n\n    pose_metrics = {'minimum_radius': minimum_radius,\n                    'maximum_radius': maximum_radius,\n                    'pose_length': self.number_of_residues\n                    # 'sequence': self.sequence\n                    }\n    radius_ratio_sum = min_ratio_sum = max_ratio_sum = 0.  # residue_ratio_sum\n    counter = 1\n    # index_combinations = combinations(range(1, 1 + len(entity_metrics)), 2)\n    for counter, (metrics1, metrics2) in enumerate(combinations(entity_metrics, 2), counter):\n        if metrics1['radius'] &gt; metrics2['radius']:\n            radius_ratio = metrics2['radius'] / metrics1['radius']\n        else:\n            radius_ratio = metrics1['radius'] / metrics2['radius']\n\n        if metrics1['min_radius'] &gt; metrics2['min_radius']:\n            min_ratio = metrics2['min_radius'] / metrics1['min_radius']\n        else:\n            min_ratio = metrics1['min_radius'] / metrics2['min_radius']\n\n        if metrics1['max_radius'] &gt; metrics2['max_radius']:\n            max_ratio = metrics2['max_radius'] / metrics1['max_radius']\n        else:\n            max_ratio = metrics1['max_radius'] / metrics2['max_radius']\n\n        radius_ratio_sum += 1 - radius_ratio\n        min_ratio_sum += 1 - min_ratio\n        max_ratio_sum += 1 - max_ratio\n        # residue_ratio_sum += abs(1 - residue_ratio)\n        # entity_idx1, entity_idx2 = next(index_combinations)\n        # pose_metrics.update({f'entity_radius_ratio_{entity_idx1}v{entity_idx2}': radius_ratio,\n        #                      f'entity_min_radius_ratio_{entity_idx1}v{entity_idx2}': min_ratio,\n        #                      f'entity_max_radius_ratio_{entity_idx1}v{entity_idx2}': max_ratio,\n        #                      f'entity_number_of_residues_ratio_{entity_idx1}v{entity_idx2}': residue_ratio})\n\n    pose_metrics.update({'entity_radius_average_deviation': radius_ratio_sum / counter,\n                         'entity_min_radius_average_deviation': min_ratio_sum / counter,\n                         'entity_max_radius_average_deviation': max_ratio_sum / counter,\n                         # 'entity_number_of_residues_average_deviation': residue_ratio_sum / counter\n                         })\n    pose_metrics.update(**self.interface_metrics())\n\n    return pose_metrics\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.apply_design_selector","title":"apply_design_selector","text":"<pre><code>apply_design_selector(selection: StructureSpecification = None, mask: StructureSpecification = None, required: StructureSpecification = None)\n</code></pre> <p>Set up a design selector for the Pose including selections, masks, and required Entities and Atoms</p> Sets <p>self._design_selection_entity_names set[str] self._design_selector_atom_indices set[int] self._required_atom_indices Sequence[int]</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def apply_design_selector(self, selection: StructureSpecification = None, mask: StructureSpecification = None,\n                          required: StructureSpecification = None):\n    \"\"\"Set up a design selector for the Pose including selections, masks, and required Entities and Atoms\n\n    Sets:\n        self._design_selection_entity_names set[str]\n        self._design_selector_atom_indices set[int]\n        self._required_atom_indices Sequence[int]\n    \"\"\"\n\n    def grab_indices(entities: set[str] = None, chains: set[str] = None, residues: set[int] = None) \\\n            -&gt; tuple[set[str], set[int]]:\n        # atoms: set[int] = None\n        \"\"\"Parses the residue selector to a set of entities and a set of atom indices\n\n        Args:\n            entities: The Entity identifiers to include in selection schemes\n            chains: The Chain identifiers to include in selection schemes\n            residues: The Residue identifiers to include in selection schemes\n\n        Returns:\n            A tuple with the names of Entity instances and the indices of the Atom/Coord instances that are parsed\n        \"\"\"\n        # if start_with_none:\n        #     set_function = set.union\n        # else:  # Start with all indices and include those of interest\n        #     set_function = set.intersection\n\n        entity_atom_indices = []\n        entities_of_interest = []\n        # All selectors could be a set() or None.\n        if entities:\n            for entity_name in entities:\n                entity = self.get_entity(entity_name)\n                if entity is None:\n                    raise NameError(\n                        f\"No entity named '{entity_name}'\")\n                entities_of_interest.append(entity)\n                entity_atom_indices.extend(entity.atom_indices)\n\n        if chains:\n            for chain_id in chains:\n                chain = self.get_chain(chain_id)\n                if chain is None:\n                    raise NameError(\n                        f\"No chain named '{chain_id}'\")\n                entities_of_interest.append(chain.entity)\n                entity_atom_indices.extend(chain.atom_indices)\n\n        # vv This is for the additive model\n        # atom_indices.union(iter_chain.from_iterable(self.chain(chain_id).get_residue_atom_indices(numbers=residues)\n        #                                     for chain_id in chains))\n        # vv This is for the intersectional model\n        # atom_indices = set_function(atom_indices, entity_atom_indices)\n        # entity_set = set_function(entity_set, [ent.name for ent in entities_of_interest])\n        atom_indices = set(entity_atom_indices)\n        entity_set = {ent.name for ent in entities_of_interest}\n\n        if residues:\n            atom_indices = atom_indices.union(self.get_residue_atom_indices(numbers=residues))\n        # if pdb_residues:\n        #     atom_indices = set_function(atom_indices, self.get_residue_atom_indices(numbers=residues, pdb=True))\n        # if atoms:\n        #     atom_indices = set_function(atom_indices, [idx for idx in self._atom_indices if idx in atoms])\n\n        return entity_set, atom_indices\n\n    if selection:\n        self.log.debug(f\"The 'design_selector' {selection=}\")\n        entity_selection, atom_selection = grab_indices(**selection)\n    else:  # Use existing entities and indices\n        entity_selection = self._design_selection_entity_names\n        atom_selection = self._design_selector_atom_indices\n\n    if mask:\n        self.log.debug(f\"The 'design_selector' {mask=}\")\n        entity_mask, atom_mask = grab_indices(**mask)  # , start_with_none=True)\n    else:\n        entity_mask = set()\n        atom_mask = set()\n\n    entity_selection = entity_selection.difference(entity_mask)\n    atom_selection = atom_selection.difference(atom_mask)\n\n    if required:\n        self.log.debug(f\"The 'design_selector' {required=}\")\n        entity_required, required_atom_indices = grab_indices(**required)  # , start_with_none=True)\n        self._required_atom_indices = list(required_atom_indices)\n    else:\n        entity_required = set()\n\n    self._design_selection_entity_names = entity_selection.union(entity_required)\n    self._design_selector_atom_indices = atom_selection.union(self._required_atom_indices)\n\n    self.log.debug(f'Entities: {\", \".join(entity.name for entity in self.entities)}')\n    self.log.debug(f'Active Entities: {\", \".join(name for name in self._design_selection_entity_names)}')\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_alphafold_features","title":"get_alphafold_features","text":"<pre><code>get_alphafold_features(symmetric: bool = False, multimer: bool = False, **kwargs) -&gt; FeatureDict\n</code></pre> <p>Retrieve the required feature dictionary for this instance to use in Alphafold inference</p> <p>Parameters:</p> <ul> <li> <code>symmetric</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the symmetric version of the Pose should be used for feature production</p> </li> <li> <code>multimer</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run as a multimer. If multimer is True while symmetric is False, the Pose will be processed according to the ASU</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>msas</code>         \u2013          <p>Sequence - A sequence of multiple sequence alignments if they should be included in the features</p> </li> <li> <code>no_msa</code>         \u2013          <p>bool = False - Whether multiple sequence alignments should be included in the features</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FeatureDict</code>         \u2013          <p>The alphafold FeatureDict which is essentially a dictionary with dict[str, np.ndarray]</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_alphafold_features(\n        self, symmetric: bool = False, multimer: bool = False, **kwargs\n) -&gt; FeatureDict:\n    \"\"\"Retrieve the required feature dictionary for this instance to use in Alphafold inference\n\n    Args:\n        symmetric: Whether the symmetric version of the Pose should be used for feature production\n        multimer: Whether to run as a multimer. If multimer is True while symmetric is False, the Pose will\n            be processed according to the ASU\n\n    Keyword Args:\n        msas: Sequence - A sequence of multiple sequence alignments if they should be included in the features\n        no_msa: bool = False - Whether multiple sequence alignments should be included in the features\n\n    Returns:\n        The alphafold FeatureDict which is essentially a dictionary with dict[str, np.ndarray]\n    \"\"\"\n    # heteromer = heteromer or self.number_of_entities &gt; 1\n    # symmetric = symmetric or heteromer or self.number_of_chains &gt; 1\n    # if multimer:\n\n    # Set up the ASU unless otherwise specified\n    number_of_symmetry_mates = 1\n    if self.is_symmetric():\n        if symmetric:\n            number_of_symmetry_mates = self.number_of_symmetry_mates\n            multimer = True\n    # else:\n    #     self.log.warning(\"Can't run as symmetric since Pose isn't symmetric\")\n\n    if self.number_of_entities &gt; 1:\n        # self.log.warning(f\"Can't run with symmetric=True while multimer=False on a {self.__class__.__name__}. \"\n        #                  f\"Setting heteromer=True\")\n        heteromer = multimer = True\n    else:\n        heteromer = False\n        # if self.number_of_chains &gt; 1:\n        #     multimer = True\n        # raise ValueError(f\"Can't run monomer when {self.number_of_entities} entities are present in the \"\n        #                  f\"{self.__class__.__name__}. Use Entity.{Entity.get_alphafold_features.__name__} on \"\n        #                  f\"each entity individually instead\")\n\n    chain_count = count(1)\n    all_chain_features = {}\n    available_chain_ids = list(chain_id_generator())[:self.number_of_entities * self.number_of_symmetry_mates]\n    available_chain_ids_iter = iter(available_chain_ids)\n    for entity_idx, entity in enumerate(self.entities):\n        entity_features = entity.get_alphafold_features(heteromer=heteromer, **kwargs)  # no_msa=no_msa)\n        # The above function creates most of the work for the adaptation\n        # particular importance needs to be given to the MSA used.\n        # Should fragments be utilized in the MSA? If so, naming them in some way to pair is required!\n        # Follow the example in:\n        #    af_pipeline.make_msa_features(msas: Sequence[af_data_parsers.Msa]) -&gt; FeatureDict\n        # to featurize\n\n        if multimer:  # symmetric:\n            # The chain_id passed to this function is used by the entity_features to (maybe) tie different chains\n            # to this chain. In alphafold implementation, the chain_id passed should be oriented so that each\n            # additional entity has the chain_id of the chain number within the entire system.\n            # For example, for an A4B4 heteromer with C4 symmetry, the chain_id for entity idx 0 would be A and for\n            # entity idx 1 would be E. This may not be important, but this is how symmetric is prepared\n            chain_id = available_chain_ids[self.number_of_symmetry_mates * entity_idx]\n            entity_features = af_pipeline_multimer.convert_monomer_features(entity_features, chain_id=chain_id)\n\n            entity_integer = entity_idx + 1\n            entity_id = af_pipeline_multimer.int_id_to_str_id(entity_integer)\n\n        entity_length = entity.number_of_residues\n        # for _ in range(self.number_of_symmetry_mates):\n        for sym_idx in range(1, 1 + number_of_symmetry_mates):\n            # chain_id = next(available_chain_ids_iter)  # The mmCIF formatted chainID with 'AB' type notation\n            this_entity_features = deepcopy(entity_features)\n            # Where chain_id increments for each new chain instance i.e. A_1 is 1, A_2 is 2, ...\n            # Where entity_id increments for each new Entity instance i.e. A_1 is 1, A_2 is 1, ...\n            # Where sym_id increments for each new Entity instance regardless of chain i.e. A_1 is 1, A_2 is 2, ...,\n            # B_1 is 1, B2 is 2\n            if multimer:  # symmetric:\n                this_entity_features.update({'asym_id': next(chain_count) * np.ones(entity_length),\n                                             'sym_id': sym_idx * np.ones(entity_length),\n                                             'entity_id': entity_integer * np.ones(entity_length)})\n                chain_name = f'{entity_id}_{sym_idx}'\n            else:\n                chain_name = next(available_chain_ids_iter)\n            # Make the key '&lt;seq_id&gt;_&lt;sym_id&gt;' where seq_id is the chain name assigned to the Entity where\n            # chain names increment according to reverse spreadsheet style i.e. A,B,...AA,BA,...\n            # and sym_id increments from 1 to number_of_symmetry_mates\n            all_chain_features[chain_name] = this_entity_features\n            # all_chain_features[next(available_chain_ids_iter)] = this_entity_features\n            # all_chain_features[next(available_chain_ids_iter)] = this_entity_features\n            # NOT HERE chain_features = convert_monomer_features(copy.deepcopy(entity_features), chain_id=chain_id)\n            # all_chain_features[chain_id] = chain_features\n\n    # This v performed above during all_chain_features creation\n    # all_chain_features = \\\n    #     af_pipeline_multimer.add_assembly_features(all_chain_features)\n\n    if multimer:  # symmetric:\n        all_chain_features = af_feature_processing.pair_and_merge(all_chain_features=all_chain_features)\n        # Pad MSA to avoid zero-sized extra_msa.\n        all_chain_features = af_pipeline_multimer.pad_msa(all_chain_features, 512)\n\n    return all_chain_features\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_proteinmpnn_params","title":"get_proteinmpnn_params","text":"<pre><code>get_proteinmpnn_params(ca_only: bool = False, pssm_bias_flag: bool = False, pssm_multi: float = 0.0, bias_pssm_by_probabilities: bool = False, pssm_log_odds_flag: bool = False, interface: bool = False, neighbors: bool = False, **kwargs) -&gt; dict[str, ndarray]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>ca_only</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a minimal CA variant of the protein should be used for design calculations</p> </li> <li> <code>pssm_bias_flag</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use bias to modulate the residue probabilities designed</p> </li> <li> <code>pssm_multi</code>             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>How much to skew the design probabilities towards the sequence profile. Bounded between [0, 1] where 0 is no sequence profile probability. Only used with pssm_bias_flag and modifies each coefficient in pssm_coef by the fractional amount</p> </li> <li> <code>bias_pssm_by_probabilities</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to produce bias by profile probabilities as opposed to lods</p> </li> <li> <code>pssm_log_odds_flag</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use log_odds threshold (&gt;0) to limit amino acid types of designed residues Creates pssm_log_odds_mask based on the threshold</p> </li> <li> <code>interface</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to design the interface</p> </li> <li> <code>neighbors</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to design interface neighbors</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>A mapping of the ProteinMPNN parameter names to their data, typically arrays</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_proteinmpnn_params(self, ca_only: bool = False, pssm_bias_flag: bool = False, pssm_multi: float = 0.,\n                           bias_pssm_by_probabilities: bool = False, pssm_log_odds_flag: bool = False,\n                           interface: bool = False, neighbors: bool = False, **kwargs) -&gt; dict[str, np.ndarray]:\n    # decode_core_first: bool = False\n    \"\"\"\n\n    Args:\n        ca_only: Whether a minimal CA variant of the protein should be used for design calculations\n        pssm_bias_flag: Whether to use bias to modulate the residue probabilities designed\n        pssm_multi: How much to skew the design probabilities towards the sequence profile.\n            Bounded between [0, 1] where 0 is no sequence profile probability.\n            Only used with pssm_bias_flag and modifies each coefficient in pssm_coef by the fractional amount\n        bias_pssm_by_probabilities: Whether to produce bias by profile probabilities as opposed to lods\n        pssm_log_odds_flag: Whether to use log_odds threshold (&gt;0) to limit amino acid types of designed residues\n            Creates pssm_log_odds_mask based on the threshold\n        interface: Whether to design the interface\n        neighbors: Whether to design interface neighbors\n\n    Keyword Args:\n        distance: float = 8. - The distance to measure Residues across an interface\n\n    Returns:\n        A mapping of the ProteinMPNN parameter names to their data, typically arrays\n    \"\"\"\n    #   decode_core_first: Whether to decode the interface core first\n    # Initialize pose data structures for design\n    number_of_residues = self.number_of_residues\n    if interface:\n        self.find_and_split_interface(**kwargs)\n\n        # Add all interface + required residues\n        self.design_residues = self.required_residues + self.interface_residues\n        if neighbors:\n            self.design_residues += self.interface_neighbor_residues\n\n        design_indices = [residue.index for residue in self.design_residues]\n    else:\n        self.design_residues = self.residues\n        design_indices = list(range(number_of_residues))\n\n    if ca_only:  # self.job.design.ca_only:\n        coords_type = 'ca_coords'\n        num_model_residues = 1\n    else:\n        coords_type = 'backbone_coords'\n        num_model_residues = 4\n\n    # Make masks for the sequence design task\n    # Residue position mask denotes which residues should be designed. 1 - designed, 0 - known\n    residue_mask = np.zeros(number_of_residues, dtype=np.int32)  # (number_of_residues,)\n    residue_mask[design_indices] = 1\n\n    omit_AAs_np = np.zeros(ml.mpnn_alphabet_length, dtype=np.int32)  # (alphabet_length,)\n    bias_AAs_np = np.zeros_like(omit_AAs_np)  # (alphabet_length,)\n    omit_AA_mask = np.zeros((number_of_residues, ml.mpnn_alphabet_length),\n                            dtype=np.int32)  # (number_of_residues, alphabet_length)\n    bias_by_res = np.zeros(omit_AA_mask.shape, dtype=np.float32)  # (number_of_residues, alphabet_length)\n\n    # Get sequence profile to include for design bias\n    pssm_threshold = 0.  # Must be a greater probability than wild-type\n    pssm_log_odds = pssm_as_array(self.profile, lod=True)  # (number_of_residues, 20)\n    pssm_log_odds_mask = np.where(pssm_log_odds &gt;= pssm_threshold, 1., 0.)  # (number_of_residues, 20)\n    pssm_coef = np.ones(residue_mask.shape, dtype=np.float32)  # (number_of_residues,)\n    # shape (1, 21) where last index (20) is 1\n\n    # Make the pssm_bias between 0 and 1 specifying how important position is where 1 is more important\n    if bias_pssm_by_probabilities:\n        pssm_probability = pssm_as_array(self.profile)\n        pssm_bias = softmax(pssm_probability)  # (number_of_residues, 20)\n    else:\n        pssm_bias = softmax(pssm_log_odds)  # (number_of_residues, 20)\n\n    if self.is_symmetric():\n        number_of_symmetry_mates = self.number_of_symmetry_mates\n        number_of_sym_residues = number_of_residues * number_of_symmetry_mates\n        X = self.return_symmetric_coords(getattr(self, coords_type))\n        # Should be N, CA, C, O for each residue\n        #  v - Residue\n        # [[[N  [x, y, z],\n        #   [CA [x, y, z],\n        #   [C  [x, y, z],\n        #   [O  [x, y, z]],\n        #  [[], ...      ]]\n        # split the coordinates into those grouped by residues\n        # X = np.array(.split(X, self.number_of_residues))\n        X = X.reshape((number_of_sym_residues, num_model_residues, 3))  # (number_of_sym_residues, 4, 3)\n\n        S = np.tile(self.sequence_numeric, number_of_symmetry_mates)  # (number_of_sym_residues,)\n        # self.log.info(f'self.sequence_numeric: {self.sequence_numeric}')\n        # self.log.info(f'Tiled sequence_numeric.shape: {S.shape}')\n        # self.log.info(f'Tiled sequence_numeric start: {S[:5]}')\n        # self.log.info(f'Tiled sequence_numeric chain_break: '\n        #               f'{S[number_of_residues-5: number_of_residues+5]}')\n\n        # Make masks for the sequence design task\n        residue_mask = np.tile(residue_mask, number_of_symmetry_mates)  # (number_of_sym_residues,)\n        mask = np.ones_like(residue_mask)  # (number_of_sym_residues,)\n        # Chain mask denotes which chains should be designed. 1 - designed, 0 - known\n        # For symmetric systems, treat each chain as designed as the logits are averaged during model.tied_sample()\n        chain_mask = np.ones_like(residue_mask)  # (number_of_sym_residues,)\n        # Set up a simple array where each residue index has the index of the chain starting with the index of 1\n        chain_encoding = np.zeros_like(residue_mask)  # (number_of_residues,)\n        # Set up an array where each residue index is incremented, however each chain break has an increment of 100\n        residue_idx = np.arange(number_of_sym_residues, dtype=np.int32)  # (number_of_residues,)\n        number_of_entities = self.number_of_entities\n        for model_idx in range(number_of_symmetry_mates):\n            model_offset = model_idx * number_of_residues\n            model_entity_number = model_idx * number_of_entities\n            for idx, entity in enumerate(self.entities, 1):\n                entity_number_of_residues = entity.number_of_residues\n                entity_start = entity.offset_index + model_offset\n                chain_encoding[entity_start:entity_start + entity_number_of_residues] = model_entity_number + idx\n                residue_idx[entity_start:entity_start + entity_number_of_residues] += \\\n                    (model_entity_number+idx) * 100\n\n        # self.log.debug(f'Tiled chain_encoding chain_break: '\n        #                f'{chain_encoding[number_of_residues-5: number_of_residues+5]}')\n        # self.log.debug(f'Tiled residue_idx chain_break: '\n        #                f'{residue_idx[number_of_residues-5: number_of_residues+5]}')\n\n        pssm_coef = np.tile(pssm_coef, number_of_symmetry_mates)  # (number_of_sym_residues,)\n        # Below have shape (number_of_sym_residues, alphabet_length)\n        pssm_bias = np.tile(pssm_bias, (number_of_symmetry_mates, 1))\n        pssm_log_odds_mask = np.tile(pssm_log_odds_mask, (number_of_symmetry_mates, 1))\n        omit_AA_mask = np.tile(omit_AA_mask, (number_of_symmetry_mates, 1))\n        bias_by_res = np.tile(bias_by_res, (number_of_symmetry_mates, 1))\n        self.log.debug(f'Tiled bias_by_res start: {bias_by_res[:5]}')\n        self.log.debug(f'Tiled bias_by_res: '\n                       f'{bias_by_res[number_of_residues-5: number_of_residues+5]}')\n        tied_beta = np.ones_like(residue_mask)  # (number_of_sym_residues,)\n        tied_pos = [self.make_indices_symmetric([idx], dtype='residue') for idx in design_indices]\n        # (design_residues, number_of_symmetry_mates)\n    else:\n        X = getattr(self, coords_type).reshape((number_of_residues, num_model_residues, 3))  # (residues, 4, 3)\n        S = self.sequence_numeric  # (number_of_residues,)\n        mask = np.ones_like(residue_mask)  # (number_of_residues,)\n        chain_mask = np.ones_like(residue_mask)  # (number_of_residues,)\n        # Set up a simple array where each residue index has the index of the chain starting with the index of 1\n        chain_encoding = np.zeros_like(residue_mask)  # (number_of_residues,)\n        # Set up an array where each residue index is incremented, however each chain break has an increment of 100\n        residue_idx = np.arange(number_of_residues, dtype=np.int32)  # (number_of_residues,)\n        for idx, entity in enumerate(self.entities):\n            entity_number_of_residues = entity.number_of_residues\n            entity_start = entity.offset_index\n            chain_encoding[entity_start: entity_start + entity_number_of_residues] = idx + 1\n            residue_idx[entity_start: entity_start + entity_number_of_residues] += idx * 100\n        tied_beta = None  # np.ones_like(residue_mask)  # (number_of_sym_residues,)\n        tied_pos = None  # [[]]\n\n    return dict(X=X,\n                S=S,\n                chain_mask=chain_mask,\n                chain_encoding=chain_encoding,\n                residue_idx=residue_idx,\n                mask=mask,\n                omit_AAs_np=omit_AAs_np,\n                bias_AAs_np=bias_AAs_np,\n                chain_M_pos=residue_mask,\n                omit_AA_mask=omit_AA_mask,\n                pssm_coef=pssm_coef,\n                pssm_bias=pssm_bias,\n                pssm_multi=pssm_multi,\n                pssm_log_odds_flag=pssm_log_odds_flag,\n                pssm_log_odds_mask=pssm_log_odds_mask,\n                pssm_bias_flag=pssm_bias_flag,\n                tied_pos=tied_pos,\n                tied_beta=tied_beta,\n                bias_by_res=bias_by_res\n                )\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.generate_proteinmpnn_decode_order","title":"generate_proteinmpnn_decode_order","text":"<pre><code>generate_proteinmpnn_decode_order(to_device: str = None, core_first: bool = False, **kwargs) -&gt; Tensor | ndarray\n</code></pre> <p>Return the decoding order for ProteinMPNN. Currently just returns an array of random floats</p> <p>For original ProteinMPNN GitHub release, the decoding order is only dependent on first entry in batch for model.tied_sample() while it is dependent on the entire batch for model.sample()</p> <p>Parameters:</p> <ul> <li> <code>to_device</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Whether the decoding order should be transferred to the device that a ProteinMPNN model is on</p> </li> <li> <code>core_first</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the core residues (identified as fragment pairs) should be decoded first</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor | ndarray</code>         \u2013          <p>The decoding order to be used in ProteinMPNN graph decoding</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def generate_proteinmpnn_decode_order(self, to_device: str = None, core_first: bool = False, **kwargs) -&gt; \\\n        torch.Tensor | np.ndarray:\n    \"\"\"Return the decoding order for ProteinMPNN. Currently just returns an array of random floats\n\n    For original ProteinMPNN GitHub release, the decoding order is only dependent on first entry in batch for\n    model.tied_sample() while it is dependent on the entire batch for model.sample()\n\n    Args:\n        to_device: Whether the decoding order should be transferred to the device that a ProteinMPNN model is on\n        core_first: Whether the core residues (identified as fragment pairs) should be decoded first\n\n    Returns:\n        The decoding order to be used in ProteinMPNN graph decoding\n    \"\"\"\n    pose_length = self.number_of_residues\n    if self.is_symmetric():\n        pose_length *= self.number_of_symmetry_mates\n\n    if core_first:\n        raise NotImplementedError(f'core_first is not available yet')\n    else:  # random decoding order\n        decode_order = np.random.rand(pose_length)\n\n    if to_device is None:\n        return decode_order\n    else:\n        return torch.from_numpy(decode_order).to(dtype=torch.float32, device=to_device)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_proteinmpnn_unbound_coords","title":"get_proteinmpnn_unbound_coords","text":"<pre><code>get_proteinmpnn_unbound_coords(ca_only: bool = False) -&gt; ndarray\n</code></pre> <p>Translate the coordinates along z in increments of 1000 to separate coordinates</p> <p>Parameters:</p> <ul> <li> <code>ca_only</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a minimal CA variant of the protein should be used for design calculations</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The Pose coords where each Entity has been translated away from other entities</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_proteinmpnn_unbound_coords(self, ca_only: bool = False) -&gt; np.ndarray:\n    \"\"\"Translate the coordinates along z in increments of 1000 to separate coordinates\n\n    Args:\n        ca_only: Whether a minimal CA variant of the protein should be used for design calculations\n\n    Returns:\n        The Pose coords where each Entity has been translated away from other entities\n    \"\"\"\n    if ca_only:\n        coords_type = 'ca_coords'\n        num_model_residues = 1\n    else:\n        coords_type = 'backbone_coords'\n        num_model_residues = 4\n\n    unbound_transform = np.array([0., 0., 1000.])\n    if self.is_symmetric():\n        number_of_residues = self.number_of_symmetric_residues\n        coord_func = self.return_symmetric_coords\n    else:\n        number_of_residues = self.number_of_residues\n        def coord_func(coords): return coords\n\n    # Caution this doesn't move the oligomeric unit, it moves the ASU entity.\n    # \"Unbound\" measurements shouldn't modify the oligomeric unit\n    entity_unbound_coords = []\n    for idx, entity in enumerate(self.entities, 1):\n        entity_unbound_coords.append(coord_func(getattr(entity, coords_type) + unbound_transform*idx))\n\n    return np.concatenate(entity_unbound_coords).reshape((number_of_residues, num_model_residues, 3))\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.score_sequences","title":"score_sequences","text":"<pre><code>score_sequences(sequences: Sequence[str] | Sequence[Sequence[str]] | array, method: design_programs_literal = putils.proteinmpnn, measure_unbound: bool = True, ca_only: bool = False, **kwargs) -&gt; dict[str, ndarray]\n</code></pre> <p>Analyze the output of sequence design</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>Sequence[str] | Sequence[Sequence[str]] | array</code>)         \u2013          <p>The sequences to score</p> </li> <li> <code>method</code>             (<code>design_programs_literal</code>, default:                 <code>proteinmpnn</code> )         \u2013          <p>Whether to score using ProteinMPNN or Rosetta</p> </li> <li> <code>measure_unbound</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the protein should be scored in the unbound state</p> </li> <li> <code>ca_only</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a minimal CA variant of the protein should be used for design calculations</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>model_name</code>         \u2013          <p>The name of the model to use from ProteinMPNN taking the format v_X_Y, where X is neighbor distance and Y is noise</p> </li> <li> <code>backbone_noise</code>         \u2013          <p>float = 0.0 - The amount of backbone noise to add to the pose during design</p> </li> <li> <code>pssm_multi</code>         \u2013          <p>float = 0.0 - How much to skew the design probabilities towards the sequence profile. Bounded between [0, 1] where 0 is no sequence profile probability. Only used with pssm_bias_flag</p> </li> <li> <code>pssm_log_odds_flag</code>         \u2013          <p>bool = False - Whether to use log_odds mask to limit the residues designed</p> </li> <li> <code>pssm_bias_flag</code>         \u2013          <p>bool = False - Whether to use bias to modulate the residue probabilites designed</p> </li> <li> <code>bias_pssm_by_probabilities</code>         \u2013          <p>Whether to produce bias by profile probabilities as opposed to profile lods</p> </li> <li> <code>#</code>             (<code>interface</code>)         \u2013          <p>Whether to design the interface</p> </li> <li> <code>#</code>             (<code>neighbors</code>)         \u2013          <p>Whether to design interface neighbors</p> </li> <li> <code>decode_core_first</code>         \u2013          <p>bool = False - Whether to decode identified fragments (constituting the protein core) first</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>A mapping of the design score type name to the per-residue output data which is a ndarray with shape</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>(number of sequences, pose_length).</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>For proteinmpnn,</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>these are the outputs: 'sequences', 'numeric_sequences', 'proteinmpnn_loss_complex', and</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>'proteinmpnn_loss_unbound' mapped to their corresponding arrays with data types as np.ndarray</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>For rosetta, this function isn't implemented</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>@torch.no_grad()  # Ensure no gradients are produced\ndef score_sequences(self, sequences: Sequence[str] | Sequence[Sequence[str]] | np.array,\n                    method: design_programs_literal = putils.proteinmpnn,\n                    measure_unbound: bool = True, ca_only: bool = False, **kwargs) -&gt; dict[str, np.ndarray]:\n    \"\"\"Analyze the output of sequence design\n\n    Args:\n        sequences: The sequences to score\n        method: Whether to score using ProteinMPNN or Rosetta\n        measure_unbound: Whether the protein should be scored in the unbound state\n        ca_only: Whether a minimal CA variant of the protein should be used for design calculations\n\n    Keyword Args:\n        model_name: The name of the model to use from ProteinMPNN taking the format v_X_Y,\n            where X is neighbor distance and Y is noise\n        backbone_noise: float = 0.0 - The amount of backbone noise to add to the pose during design\n        pssm_multi: float = 0.0 - How much to skew the design probabilities towards the sequence profile.\n            Bounded between [0, 1] where 0 is no sequence profile probability.\n            Only used with pssm_bias_flag\n        pssm_log_odds_flag: bool = False - Whether to use log_odds mask to limit the residues designed\n        pssm_bias_flag: bool = False - Whether to use bias to modulate the residue probabilites designed\n        bias_pssm_by_probabilities: Whether to produce bias by profile probabilities as opposed to profile lods\n        # interface: Whether to design the interface\n        # neighbors: Whether to design interface neighbors\n        decode_core_first: bool = False - Whether to decode identified fragments (constituting the protein core)\n            first\n\n    Returns:\n        A mapping of the design score type name to the per-residue output data which is a ndarray with shape\n        (number of sequences, pose_length).\n        For proteinmpnn,\n        these are the outputs: 'sequences', 'numeric_sequences', 'proteinmpnn_loss_complex', and\n        'proteinmpnn_loss_unbound' mapped to their corresponding arrays with data types as np.ndarray\n\n        For rosetta, this function isn't implemented\n    \"\"\"\n    if method == putils.rosetta:\n        sequences_and_scores = {}\n        raise NotImplementedError(f\"Can't score with Rosetta from this method yet...\")\n    elif method == putils.proteinmpnn:  # Design with vanilla version of ProteinMPNN\n        # Convert the sequences to correct format\n        # missing_alphabet = ''\n        warn_alphabet = 'With passed sequences type of {}, ensure that the order of ' \\\n                        f'integers is of the default ProteinMPNN alphabet \"{ml.mpnn_alphabet}\"'\n\n        def convert_and_check_sequence_type(sequences_) -&gt; Sequence[Sequence[str | int]]:\n            incorrect_input = ValueError(f'The passed sequences must be an Sequence[Sequence[Any]]')\n            nesting_level = count()\n            item = sequences_\n            # print(item)\n            while not isinstance(item, (int, str)):\n                next(nesting_level)\n                item = item[0]\n                # print(item)\n            else:\n                final_level = next(nesting_level)\n                item_type = type(item)\n                # print(final_level)\n                # print(item_type)\n                if final_level == 1:\n                    if item_type is str:\n                        sequences_ = sequences_to_numeric(\n                            sequences, translation_table=ml.proteinmpnn_default_translation_table)\n                    else:\n                        raise incorrect_input\n                elif final_level == 2:\n                    if item_type is str:\n                        for idx, sequence in enumerate(sequences_):\n                            sequences[idx] = ''.join(sequence)\n\n                        sequences_ = sequences_to_numeric(\n                            sequences, translation_table=ml.proteinmpnn_default_translation_table)\n                    else:\n                        self.log.warning(warn_alphabet.format('int'))\n                        sequences_ = np.array(sequences_)\n                else:\n                    raise incorrect_input\n                # print('Final', sequences)\n            return sequences_\n\n        if isinstance(sequences, (torch.Tensor, np.ndarray)):\n            if sequences.dtype in utils.np_torch_int_float_types:\n                # This is an integer sequence. An alphabet is required\n                self.log.warning(warn_alphabet.format(sequences.dtype))\n                numeric_sequences = sequences\n                sequences = numeric_to_sequence(sequences)\n                # raise ValueError(missing_alphabet)\n            else:  # This is an AnyStr type?\n                numeric_sequences = sequences_to_numeric(sequences)\n        else:  # Some sort of iterable\n            numeric_sequences = convert_and_check_sequence_type(sequences)\n\n        # pose_length = self.number_of_residues\n        size, pose_length, *_ = numeric_sequences.shape\n        batch_length = ml.PROTEINMPNN_SCORE_BATCH_LEN\n\n        # Set up parameters and model sampling type based on symmetry\n        if measure_unbound:\n            parameters = {'X_unbound': self.get_proteinmpnn_unbound_coords(ca_only=ca_only)}\n        else:\n            parameters = {}\n\n        # Set up return containers based on the asymmetric sequence\n        per_residue_complex_sequence_loss = np.empty_like(numeric_sequences, dtype=np.float32)\n        per_residue_unbound_sequence_loss = np.empty_like(per_residue_complex_sequence_loss)\n\n        # Set up parameters for the scoring task, including scoring all positions\n        parameters.update(**self.get_proteinmpnn_params(ca_only=ca_only, **kwargs))\n\n        # Remove the precalculated sequence array to add our own\n        parameters.pop('S')\n        # # Insert the designed sequences inplace of the pose sequence\n        # parameters['S'] = np.tile(numeric_sequences, (1, number_of_symmetry_mates))\n        # Set up for symmetry\n        numeric_sequences_ = np.tile(numeric_sequences, (1, self.number_of_symmetry_mates))\n        # Solve decoding order\n        # parameters['randn'] = self.generate_proteinmpnn_decode_order(**kwargs)  # to_device=device)\n        # decoding_order = self.generate_proteinmpnn_decode_order(**kwargs)  # to_device=device)\n        # Solve decoding order\n        parameters['decoding_order'] = self.generate_proteinmpnn_decode_order(**kwargs)  # to_device=device)\n\n        @ml.batch_calculation(size=size, batch_length=batch_length,\n                              setup=ml.setup_pose_batch_for_proteinmpnn,\n                              compute_failure_exceptions=(RuntimeError,\n                                                          np.core._exceptions._ArrayMemoryError))\n        def _proteinmpnn_batch_score(*args, **_kwargs):\n            return ml.proteinmpnn_batch_score(*args, **_kwargs)\n\n        # Set up the model with the desired weights\n        proteinmpnn_model = ml.proteinmpnn_factory(ca_only=ca_only, **kwargs)\n        device = proteinmpnn_model.device\n\n        # Send the numpy array to torch.tensor and the device\n        # Pass sequences as 'S' parameter to _proteinmpnn_batch_score instead of as setup_kwargs\n        # unique_parameters = ml.proteinmpnn_to_device(device, S=sequences, decoding_order=decoding_order)\n        unique_parameters = ml.proteinmpnn_to_device(device, S=numeric_sequences_)\n        # score_start = time.time()\n        scores = \\\n            _proteinmpnn_batch_score(proteinmpnn_model, **unique_parameters,  # S=sequences,\n                                     pose_length=pose_length,  # decoding_order=decoding_order,\n                                     setup_args=(device,),\n                                     setup_kwargs=parameters,\n                                     return_containers={\n                                         'proteinmpnn_loss_complex': per_residue_complex_sequence_loss,\n                                         'proteinmpnn_loss_unbound': per_residue_unbound_sequence_loss})\n        sequences_and_scores = {'sequences': sequences,\n                                'numeric_sequences': numeric_sequences,\n                                **scores}\n    else:\n        sequences_and_scores = {}\n        raise ValueError(\n            f\"The method '{method}' isn't a viable scoring protocol\")\n\n    return sequences_and_scores\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.design_sequences","title":"design_sequences","text":"<pre><code>design_sequences(method: design_programs_literal = putils.proteinmpnn, number: int = 10, temperatures: Sequence[float] = (0.1), interface: bool = False, neighbors: bool = False, measure_unbound: bool = True, ca_only: bool = False, **kwargs) -&gt; dict[str, ndarray]\n</code></pre> <p>Perform sequence design on the Pose</p> <p>Parameters:</p> <ul> <li> <code>method</code>             (<code>design_programs_literal</code>, default:                 <code>proteinmpnn</code> )         \u2013          <p>Whether to design using ProteinMPNN or Rosetta</p> </li> <li> <code>number</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>The number of sequences to design</p> </li> <li> <code>temperatures</code>             (<code>Sequence[float]</code>, default:                 <code>(0.1)</code> )         \u2013          <p>The temperatures to perform design at</p> </li> <li> <code>interface</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to design the interface</p> </li> <li> <code>neighbors</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to design interface neighbors</p> </li> <li> <code>measure_unbound</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the protein should be designed with concern for the unbound state</p> </li> <li> <code>ca_only</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a minimal CA variant of the protein should be used for design calculations</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>neighbors</code>             (<code>bool</code>)         \u2013          <p>bool = False - Whether to design interface neighbors</p> </li> <li> <code>model_name</code>         \u2013          <p>The name of the model to use from ProteinMPNN taking the format v_X_Y, where X is neighbor distance and Y is noise</p> </li> <li> <code>backbone_noise</code>         \u2013          <p>float = 0.0 - The amount of backbone noise to add to the pose during design</p> </li> <li> <code>pssm_multi</code>         \u2013          <p>float = 0.0 - How much to skew the design probabilities towards the sequence profile. Bounded between [0, 1] where 0 is no sequence profile probability. Only used with pssm_bias_flag</p> </li> <li> <code>pssm_log_odds_flag</code>         \u2013          <p>bool = False - Whether to use log_odds mask to limit the residues designed</p> </li> <li> <code>pssm_bias_flag</code>         \u2013          <p>bool = False - Whether to use bias to modulate the residue probabilites designed</p> </li> <li> <code>bias_pssm_by_probabilities</code>         \u2013          <p>Whether to produce bias by profile probabilities as opposed to profile lods</p> </li> <li> <code>decode_core_first</code>         \u2013          <p>bool = False - Whether to decode identified fragments (constituting the protein core) first</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>A mapping of the design score type to the per-residue output data which is a ndarray with shape</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>(number*temperatures, pose_length).</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>For proteinmpnn,</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>these are the outputs 'sequences', 'numeric_sequences', 'design_indices', 'proteinmpnn_loss_complex', and</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>'proteinmpnn_loss_unbound' mapped to their corresponding score types. For each return array, the return</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>varies such as: [temp1/number1, temp1/number2, ...,</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>tempN/number1, ...] where designs are sorted by temperature</p> </li> <li> <code>dict[str, ndarray]</code>         \u2013          <p>For rosetta, this function isn't implemented</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>@torch.no_grad()  # Ensure no gradients are produced\ndef design_sequences(self, method: design_programs_literal = putils.proteinmpnn, number: int = 10,\n                     temperatures: Sequence[float] = (0.1,), interface: bool = False, neighbors: bool = False,\n                     measure_unbound: bool = True, ca_only: bool = False, **kwargs) -&gt; dict[str, np.ndarray]:\n    \"\"\"Perform sequence design on the Pose\n\n    Args:\n        method: Whether to design using ProteinMPNN or Rosetta\n        number: The number of sequences to design\n        temperatures: The temperatures to perform design at\n        interface: Whether to design the interface\n        neighbors: Whether to design interface neighbors\n        measure_unbound: Whether the protein should be designed with concern for the unbound state\n        ca_only: Whether a minimal CA variant of the protein should be used for design calculations\n\n    Keyword Args:\n        neighbors: bool = False - Whether to design interface neighbors\n        model_name: The name of the model to use from ProteinMPNN taking the format v_X_Y,\n            where X is neighbor distance and Y is noise\n        backbone_noise: float = 0.0 - The amount of backbone noise to add to the pose during design\n        pssm_multi: float = 0.0 - How much to skew the design probabilities towards the sequence profile.\n            Bounded between [0, 1] where 0 is no sequence profile probability.\n            Only used with pssm_bias_flag\n        pssm_log_odds_flag: bool = False - Whether to use log_odds mask to limit the residues designed\n        pssm_bias_flag: bool = False - Whether to use bias to modulate the residue probabilites designed\n        bias_pssm_by_probabilities: Whether to produce bias by profile probabilities as opposed to profile lods\n        decode_core_first: bool = False - Whether to decode identified fragments (constituting the protein core)\n            first\n\n    Returns:\n        A mapping of the design score type to the per-residue output data which is a ndarray with shape\n        (number*temperatures, pose_length).\n        For proteinmpnn,\n        these are the outputs 'sequences', 'numeric_sequences', 'design_indices', 'proteinmpnn_loss_complex', and\n        'proteinmpnn_loss_unbound' mapped to their corresponding score types. For each return array, the return\n        varies such as: [temp1/number1, temp1/number2, ...,\n        tempN/number1, ...] where designs are sorted by temperature\n\n        For rosetta, this function isn't implemented\n    \"\"\"\n    # rosetta: Whether to design using Rosetta energy functions\n    if method == putils.rosetta:\n        sequences_and_scores = {}\n        raise NotImplementedError(f\"Can't design with Rosetta from this method yet...\")\n    elif method == putils.proteinmpnn:  # Design with vanilla version of ProteinMPNN\n        pose_length = self.number_of_residues\n        # Set up parameters and model sampling type based on symmetry\n        if self.is_symmetric():\n            # number_of_symmetry_mates = pose.number_of_symmetry_mates\n            # mpnn_sample = proteinmpnn_model.tied_sample\n            number_of_residues = pose_length * self.number_of_symmetry_mates\n        else:\n            # mpnn_sample = proteinmpnn_model.sample\n            number_of_residues = pose_length\n\n        if measure_unbound:\n            parameters = {'X_unbound': self.get_proteinmpnn_unbound_coords(ca_only=ca_only)}\n        else:\n            parameters = {}\n\n        # Set up the inference task\n        parameters.update(**self.get_proteinmpnn_params(interface=interface, neighbors=neighbors,\n                                                        ca_only=ca_only, **kwargs))\n        # Solve decoding order\n        parameters['randn'] = self.generate_proteinmpnn_decode_order(**kwargs)  # to_device=device)\n\n        # Set up the model with the desired weights\n        size = number\n        proteinmpnn_model = ml.proteinmpnn_factory(ca_only=ca_only, **kwargs)\n        device = proteinmpnn_model.device\n        batch_length = ml.calculate_proteinmpnn_batch_length(proteinmpnn_model, number_of_residues)\n        # batch_length = ml.PROTEINMPNN_DESIGN_BATCH_LEN\n        logger.info(f'Found ProteinMPNN batch_length={batch_length}')\n\n        generated_sequences = np.empty((size, len(temperatures), pose_length), dtype=np.int64)\n        per_residue_complex_sequence_loss = np.empty_like(generated_sequences, dtype=np.float32)\n        per_residue_unbound_sequence_loss = np.empty_like(per_residue_complex_sequence_loss)\n        design_indices = np.zeros((size, pose_length), dtype=bool)\n\n        @ml.batch_calculation(size=size, batch_length=batch_length,\n                              setup=ml.setup_pose_batch_for_proteinmpnn,\n                              compute_failure_exceptions=(RuntimeError,\n                                                          np.core._exceptions._ArrayMemoryError))\n        def _proteinmpnn_batch_design(*args, **_kwargs):\n            return ml.proteinmpnn_batch_design(*args, **_kwargs)\n\n        # Data has shape (batch_length, number_of_temperatures, pose_length)\n        number_of_temps = len(temperatures)\n        # design_start = time.time()\n        sequences_and_scores = \\\n            _proteinmpnn_batch_design(proteinmpnn_model, temperatures=temperatures, pose_length=pose_length,\n                                      setup_args=(device,),\n                                      setup_kwargs=parameters,\n                                      return_containers={\n                                          'sequences': generated_sequences,\n                                          'proteinmpnn_loss_complex': per_residue_complex_sequence_loss,\n                                          'proteinmpnn_loss_unbound': per_residue_unbound_sequence_loss,\n                                          'design_indices': design_indices})\n        # self.log.debug(f\"Took {time.time() - design_start:8f}s for _proteinmpnn_batch_design\")\n\n        sequences_and_scores['numeric_sequences'] = sequences_and_scores.pop('sequences')\n        sequences_and_scores['sequences'] = numeric_to_sequence(sequences_and_scores['numeric_sequences'])\n\n        # Format returns to have shape (temperatures*size, pose_length) where the temperatures vary slower\n        # Ex: [temp1/pose1, temp1/pose2, ..., tempN/pose1, ...] This groups the designs by temperature first\n        for data_type, data in sequences_and_scores.items():\n            if data_type == 'design_indices':\n                # These must vary by temperature\n                sequences_and_scores['design_indices'] = np.tile(data, (number_of_temps, 1))\n                # self.log.debug(f'Found design_indices with shape: {sequences_and_scores[\"design_indices\"].shape}')\n                continue\n            # print(f\"{data_type} has shape {data.shape}\")\n            sequences_and_scores[data_type] = np.concatenate(data, axis=1).reshape(-1, pose_length)\n\n        # self.log.debug(f'Found sequences with shape {sequences_and_scores[\"sequences\"].shape}')\n        # self.log.debug(f'Found proteinmpnn_loss_complex with shape {sequences_and_scores[\"proteinmpnn_loss_complex\"].shape}')\n        # self.log.debug(f'Found proteinmpnn_loss_unbound with shape {sequences_and_scores[\"proteinmpnn_loss_unbound\"].shape}')\n    else:\n        sequences_and_scores = {}\n        raise ValueError(\n            f\"The method '{method}' isn't a viable design protocol\")\n\n    return sequences_and_scores\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_termini_accessibility","title":"get_termini_accessibility","text":"<pre><code>get_termini_accessibility(entity: Entity = None, report_if_helix: bool = False) -&gt; dict[str, bool]\n</code></pre> <p>Returns a dictionary indicating which termini are not buried. Coarsely locates termini which face outward</p> <p>Parameters:</p> <ul> <li> <code>entity</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>The Structure to query which originates in the pose</p> </li> <li> <code>report_if_helix</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the query should additionally report on the helicity of the termini</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, bool]</code>         \u2013          <p>A dictionary with the mapping from termini to True if the termini is exposed Ex: {'n': True, 'c': False}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_termini_accessibility(self, entity: Entity = None, report_if_helix: bool = False) -&gt; dict[str, bool]:\n    \"\"\"Returns a dictionary indicating which termini are not buried. Coarsely locates termini which face outward\n\n    Args:\n        entity: The Structure to query which originates in the pose\n        report_if_helix: Whether the query should additionally report on the helicity of the termini\n\n    Returns:\n        A dictionary with the mapping from termini to True if the termini is exposed\n            Ex: {'n': True, 'c': False}\n    \"\"\"\n    assembly = self.assembly\n    if not assembly.sasa:\n        assembly.get_sasa()\n\n    # Find the chain that matches the Entity\n    for chain in assembly.chains:\n        if chain.entity_id == entity.name:\n            entity_chain = chain\n            break\n    else:\n        raise DesignError(\n            f\"Couldn't find a corresponding {repr(assembly)}.chain for the passed entity={repr(entity)}\")\n\n    n_term = c_term = False\n    entity_reference = None\n    if self.is_symmetric():\n        if self.dimension &gt; 0:\n            self.log.critical(\"Locating termini accessibility for lattice symmetries hasn't been tested\")\n            for _entity, entity_transformation in zip(self.entities, self.entity_transformations):\n                if entity == _entity:\n                    entity_reference = entity_transformation.get('translation2', None)\n                    break\n            else:\n                raise ValueError(\n                    f\"Can't measure point of reference for entity={repr(entity)} as a matching instance wasn't \"\n                    f\"found in the {repr(self)}\")\n\n    if entity.termini_proximity_from_reference(reference=entity_reference) == 1:  # if outward\n        if entity_chain.n_terminal_residue.relative_sasa &gt; metrics.default_sasa_burial_threshold:\n            n_term = True\n\n    if entity.termini_proximity_from_reference(termini='c', reference=entity_reference) == 1:  # if outward\n        if entity_chain.c_terminal_residue.relative_sasa &gt; metrics.default_sasa_burial_threshold:\n            c_term = True\n\n    if report_if_helix:\n        try:\n            retrieve_stride_info = resources.structure_db.structure_database_factory().stride.retrieve_data\n        except AttributeError:\n            pass\n        else:\n            parsed_secondary_structure = retrieve_stride_info(name=entity.name)\n            if parsed_secondary_structure:\n                entity.secondary_structure = parsed_secondary_structure\n\n        n_term = True if n_term and entity.is_termini_helical() else False\n        c_term = True if c_term and entity.is_termini_helical(termini='c') else False\n\n    return dict(n=n_term, c=c_term)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.per_residue_contact_order","title":"per_residue_contact_order","text":"<pre><code>per_residue_contact_order(oligomeric_interfaces: bool = False, **kwargs) -&gt; dict[str, ndarray]\n</code></pre> <p>Calculate the contact order separating calculation for chain breaks as would be expected for 3 state folding</p> <p>Parameters:</p> <ul> <li> <code>oligomeric_interfaces</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to query oligomeric interfaces</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>The dictionary of {'contact_order': array of shape (number_of_residues,)}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_contact_order(self, oligomeric_interfaces: bool = False, **kwargs) -&gt; dict[str, np.ndarray]:\n    \"\"\"Calculate the contact order separating calculation for chain breaks as would be expected for 3 state folding\n\n    Args:\n        oligomeric_interfaces: Whether to query oligomeric interfaces\n\n    Returns:\n        The dictionary of {'contact_order': array of shape (number_of_residues,)}\n    \"\"\"\n    if oligomeric_interfaces:\n        raise NotImplementedError(\n            \"Need to perform 'oligomeric_interfaces' calculation on the Entity.oligomer\")\n\n    contact_order = []\n    for idx, entity in enumerate(self.entities):\n        contact_order.append(entity.contact_order)\n\n    return {'contact_order': np.concatenate(contact_order)}\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_folding_metrics","title":"get_folding_metrics","text":"<pre><code>get_folding_metrics(profile_type: profile_types = 'evolutionary', **kwargs) -&gt; tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Calculate metrics relating to the Pose folding, separating calculation for chain breaks. These include contact_order, hydrophobic_collapse, and hydrophobic_collapse_profile (each Entity MUST have a .*_profile attribute to return the hydrophobic collapse profile!)</p> <p>Parameters:</p> <ul> <li> <code>profile_type</code>             (<code>profile_types</code>, default:                 <code>'evolutionary'</code> )         \u2013          <p>The type of profile to use to calculate the hydrophobic collapse profile</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>hydrophobicity</code>         \u2013          <p>str = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILYVW), or provide one with 'custom' keyword argument</p> </li> <li> <code>custom</code>         \u2013          <p>mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs</p> </li> <li> <code>alphabet_type</code>         \u2013          <p>alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters</p> </li> <li> <code>lower_window</code>         \u2013          <p>int = 3 \u2013 The smallest window used to measure</p> </li> <li> <code>upper_window</code>         \u2013          <p>int = 9 \u2013 The largest window used to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray]</code>         \u2013          <p>The per-residue contact_order_z_score (number_of_residues), a per-residue hydrophobic_collapse (number_of_residues), and the hydrophobic_collapse profile (number_of_residues) based on Entity.evolutionary_profile instances</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_folding_metrics(\n    self, profile_type: profile_types = 'evolutionary', **kwargs\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Calculate metrics relating to the Pose folding, separating calculation for chain breaks. These include\n    contact_order, hydrophobic_collapse, and hydrophobic_collapse_profile (each Entity MUST have a .*_profile\n    attribute to return the hydrophobic collapse profile!)\n\n    Args:\n        profile_type: The type of profile to use to calculate the hydrophobic collapse profile\n\n    Keyword Args:\n        hydrophobicity: str = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV),\n            'expanded' (FMILYVW), or provide one with 'custom' keyword argument\n        custom: mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value\n            pairs\n        alphabet_type: alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer\n            characters\n        lower_window: int = 3 \u2013 The smallest window used to measure\n        upper_window: int = 9 \u2013 The largest window used to measure\n\n    Returns:\n        The per-residue contact_order_z_score (number_of_residues),\n            a per-residue hydrophobic_collapse (number_of_residues),\n            and the hydrophobic_collapse profile (number_of_residues) based on Entity.evolutionary_profile instances\n    \"\"\"\n    #       and the hydrophobic_collapse profile (msa.length, msa.number_of_residues) based on Entity.msa instances\n    # Measure the wild type (reference) entity versus modified entity(ies) to find the hci delta\n    # Calculate Reference sequence statistics\n    contact_order_z, hydrophobic_collapse, hydrophobic_collapse_profile = [], [], []\n    missing = []\n    msa_metrics = True\n    for idx, entity in enumerate(self.entities):\n        contact_order = entity.contact_order\n        # This calculation shouldn't depend on oligomers... Only assumes unfolded -&gt; folded\n        # contact_order = entity_oligomer.contact_order[:entity.number_of_residues]\n        entity_residue_contact_order_z = metrics.z_score(contact_order, contact_order.mean(), contact_order.std())\n        contact_order_z.append(entity_residue_contact_order_z)\n        # inverse_residue_contact_order_z.append(entity_residue_contact_order_z * -1)\n        hydrophobic_collapse.append(entity.hydrophobic_collapse(**kwargs))\n\n        # Set the entity.msa which makes a copy and adjusts for any disordered residues\n        # This method is more accurate as it uses full sequences from MSA. However,\n        # more time-consuming and may not matter much\n        # if chain.msa and msa_metrics:\n        #     hydrophobic_collapse_profile.append(chain.collapse_profile(**kwargs))\n        #     collapse = chain.collapse_profile()\n        #     entity_collapse_mean.append(collapse.mean(axis=-2))\n        #     entity_collapse_std.append(collapse.std(axis=-2))\n        #     reference_collapse_z_score.append(utils.z_score(reference_collapse, entity_collapse_mean[idx],\n        #                                                     entity_collapse_std[idx]))\n        if msa_metrics and entity.evolutionary_profile:\n            try:\n                profile = getattr(entity, f'{profile_type}_profile')\n                if profile_type == 'fragment':\n                    profile_array = profile.as_array()\n                else:\n                    profile_array = pssm_as_array(profile)\n            except AttributeError:  # No profile from getattr()\n                raise ValueError(\n                    f\"The profile_type '{profile_type}' isn't available on {type(entity).__name__} {entity.name}\")\n\n            hydrophobic_collapse_profile.append(\n                metrics.hydrophobic_collapse_index(profile_array, alphabet_type='protein_letters_alph1', **kwargs))\n        else:\n            missing.append(1)\n            msa_metrics = False\n\n    contact_order_z = np.concatenate(contact_order_z)\n    hydrophobic_collapse = np.concatenate(hydrophobic_collapse)\n    if sum(missing):  # Need to sum as it could be empty from no .entities and then wouldn't collect either\n        hydrophobic_collapse_profile = np.empty(0)\n        self.log.warning(f'There were missing .evolutionary_profile attributes for {sum(missing)} Entity instances.'\n                         f' The collapse_profile will not be captured for the entire {self.__class__.__name__}')\n    #     self.log.warning(f'There were missing MultipleSequenceAlignment objects on {sum(missing)} Entity '\n    #                      'instances. The collapse_profile will not be captured for the entire '\n    #                      f'{self.__class__.__name__}.')\n    # else:\n    #     # We have to concatenate where the values will be different\n    #     # axis=1 is along the residues, so the result should be the length of the pose\n    #     # axis=0 will be different for each individual entity, so we pad to the maximum for lesser ones\n    #     array_sizes = [array.shape[0] for array in hydrophobic_collapse_profile]\n    #     axis0_max_length = max(array_sizes)\n    #     # full_hydrophobic_collapse_profile = \\\n    #     #     np.full((axis0_max_length, self.number_of_residues), np.nan)  # , dtype=np.float32)\n    #     for idx, array in enumerate(hydrophobic_collapse_profile):\n    #         hydrophobic_collapse_profile[idx] = \\\n    #             np.pad(array, ((0, axis0_max_length - array_sizes[idx]), (0, 0)), constant_values=np.nan)\n    #\n    #     hydrophobic_collapse_profile = np.concatenate(hydrophobic_collapse_profile, axis=1)\n    else:\n        hydrophobic_collapse_profile = np.concatenate(hydrophobic_collapse_profile)\n\n    return contact_order_z, hydrophobic_collapse, hydrophobic_collapse_profile\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics","title":"interface_metrics","text":"<pre><code>interface_metrics() -&gt; dict[str, Any]\n</code></pre> <p>Gather all metrics relating to the Pose and the interfaces within the Pose</p> <p>Calls self.get_fragment_metrics(), self.calculate_secondary_structure()</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>Metrics measured as: { 'entity_max_radius_average_deviation', 'entity_min_radius_average_deviation', 'entity_radius_average_deviation', 'interface_b_factor', 'interface1_secondary_structure_fragment_topology', 'interface1_secondary_structure_fragment_count', 'interface1_secondary_structure_topology', 'interface1_secondary_structure_count', 'interface2_secondary_structure_fragment_topology', 'interface2_secondary_structure_fragment_count', 'interface2_secondary_structure_topology', 'interface2_secondary_structure_count', 'maximum_radius', 'minimum_radius', 'multiple_fragment_ratio', 'nanohedra_score_normalized', 'nanohedra_score_center_normalized', 'nanohedra_score', 'nanohedra_score_center', 'number_residues_interface_fragment_total', 'number_residues_interface_fragment_center', 'number_fragments_interface', 'number_residues_interface', 'number_residues_interface_non_fragment', 'percent_fragment_helix', 'percent_fragment_strand', 'percent_fragment_coil', 'percent_residues_fragment_interface_total', 'percent_residues_fragment_interface_center', 'percent_residues_non_fragment_interface', 'pose_length', 'symmetric_interface'</p> </li> <li> <code>dict[str, Any]</code>         \u2013          <p>}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def interface_metrics(self) -&gt; dict[str, Any]:\n    \"\"\"Gather all metrics relating to the Pose and the interfaces within the Pose\n\n    Calls self.get_fragment_metrics(), self.calculate_secondary_structure()\n\n    Returns:\n        Metrics measured as: {\n            'entity_max_radius_average_deviation',\n            'entity_min_radius_average_deviation',\n            'entity_radius_average_deviation',\n            'interface_b_factor',\n            'interface1_secondary_structure_fragment_topology',\n            'interface1_secondary_structure_fragment_count',\n            'interface1_secondary_structure_topology',\n            'interface1_secondary_structure_count',\n            'interface2_secondary_structure_fragment_topology',\n            'interface2_secondary_structure_fragment_count',\n            'interface2_secondary_structure_topology',\n            'interface2_secondary_structure_count',\n            'maximum_radius',\n            'minimum_radius',\n            'multiple_fragment_ratio',\n            'nanohedra_score_normalized',\n            'nanohedra_score_center_normalized',\n            'nanohedra_score',\n            'nanohedra_score_center',\n            'number_residues_interface_fragment_total',\n            'number_residues_interface_fragment_center',\n            'number_fragments_interface',\n            'number_residues_interface',\n            'number_residues_interface_non_fragment',\n            'percent_fragment_helix',\n            'percent_fragment_strand',\n            'percent_fragment_coil',\n            'percent_residues_fragment_interface_total',\n            'percent_residues_fragment_interface_center',\n            'percent_residues_non_fragment_interface',\n            'pose_length',\n            'symmetric_interface'\n        }\n            # 'entity_radius_ratio_#v#',\n            # 'entity_min_radius_ratio_#v#',\n            # 'entity_max_radius_ratio_#v#',\n            # 'entity_number_of_residues_ratio_#v#',\n            # 'entity_number_of_residues_average_deviation,\n    \"\"\"\n    pose_metrics = self.get_fragment_metrics(total_interface=True)\n    # Remove *_indices from further analysis\n    interface_fragment_residue_indices = self.interface_fragment_residue_indices = (\n        pose_metrics.pop('center_indices', []))\n    pose_metrics.pop('total_indices')\n    number_residues_fragment_center = pose_metrics.pop('number_residues_fragment_center')\n    number_residues_fragment_total = pose_metrics.pop('number_residues_fragment_total')\n\n    interface_residues = self.interface_residues\n    number_residues_interface = len(interface_residues)\n    number_residues_interface_non_fragment = number_residues_interface - number_residues_fragment_total\n    # if number_residues_interface_non_fragment &lt; 0:\n    #     raise ValueError(f'Fragment metrics are broken due to \"number_residues_interface_non_fragment\" &gt; 1')\n    # Interface B Factor\n    int_b_factor = sum(residue.b_factor for residue in interface_residues)\n    try:  # If interface_distance is different from interface query and fragment generation these can be &lt; 0 or &gt; 1\n        percent_residues_fragment_interface_center = number_residues_fragment_center / number_residues_interface\n        # This value can be more than 1 so not a percent...\n        # percent_residues_fragment_interface_total = number_residues_fragment_total / number_residues_interface\n        percent_residues_fragment_interface_total = \\\n            min(number_residues_fragment_total / number_residues_interface, 1)\n        percent_interface_residues_non_fragment = number_residues_interface_non_fragment / number_residues_interface\n        # if percent_residues_fragment_interface_center &gt; 1:\n        #     raise ValueError(f'Fragment metrics are broken due to \"percent_residues_fragment_interface_center\"&gt;1')\n        # if percent_interface_residues_non_fragment &gt; 1:\n        #     raise ValueError(f'Fragment metrics are broken due to \"percent_interface_residues_non_fragment\" &gt; 1')\n        ave_b_factor = int_b_factor / number_residues_interface\n    except ZeroDivisionError:\n        self.log.warning(f'{self.name}: No interface residues were found. Is there an interface in your design?')\n        ave_b_factor = percent_residues_fragment_interface_center = percent_residues_fragment_interface_total = \\\n            percent_interface_residues_non_fragment = 0.\n\n    pose_metrics.update({\n        'interface_b_factor': ave_b_factor,\n        'number_residues_interface': number_residues_interface,\n        'number_residues_interface_fragment_center': number_residues_fragment_center,\n        'number_residues_interface_fragment_total': number_residues_fragment_total,\n        'number_residues_interface_non_fragment': number_residues_interface_non_fragment,\n        'percent_residues_non_fragment_interface': percent_interface_residues_non_fragment,\n        'percent_residues_fragment_interface_total': percent_residues_fragment_interface_total,\n        'percent_residues_fragment_interface_center': percent_residues_fragment_interface_center})\n\n    interface_residues_by_interface_unique = self.interface_residues_by_interface_unique\n    if not self.ss_sequence_indices or not self.ss_type_sequence:\n        self.calculate_secondary_structure()\n\n    # interface_ss_topology = {}  # {1: 'HHLH', 2: 'HSH'}\n    # interface_ss_fragment_topology = {}  # {1: 'HHH', 2: 'HH'}\n    ss_type_array = self.ss_type_sequence\n    total_interface_ss_topology = total_interface_ss_fragment_topology = ''\n    for number, elements in self.split_interface_ss_elements.items():\n        # Use unique as 2-fold interfaces 'interface_residues_by_interface_unique' duplicate fragment_elements\n        fragment_elements = {\n            element for residue, element in zip(interface_residues_by_interface_unique[number], elements)\n            if residue.index in interface_fragment_residue_indices}\n        # Take the set of elements as there are element repeats if SS is continuous over residues\n        interface_ss_topology = ''.join(ss_type_array[element] for element in set(elements))\n        interface_ss_fragment_topology = ''.join(ss_type_array[element] for element in fragment_elements)\n\n        pose_metrics[f'interface{number}_secondary_structure_topology'] = interface_ss_topology\n        pose_metrics[f'interface{number}_secondary_structure_count'] = len(interface_ss_topology)\n        total_interface_ss_topology += interface_ss_topology\n        pose_metrics[f'interface{number}_secondary_structure_fragment_topology'] = interface_ss_fragment_topology\n        pose_metrics[f'interface{number}_secondary_structure_fragment_count'] = len(interface_ss_fragment_topology)\n        total_interface_ss_fragment_topology += interface_ss_fragment_topology\n\n    pose_metrics['interface_secondary_structure_fragment_topology'] = total_interface_ss_fragment_topology\n    pose_metrics['interface_secondary_structure_fragment_count'] = len(total_interface_ss_fragment_topology)\n    pose_metrics['interface_secondary_structure_topology'] = total_interface_ss_topology\n    pose_metrics['interface_secondary_structure_count'] = len(total_interface_ss_topology)\n\n    # Calculate secondary structure percent for the entire interface\n    helical_designations = ['H', 'G', 'I']\n    strand_designations = ['E', 'B']\n    coil_designations = ['C', 'T']\n    number_helical_residues = number_strand_residues = number_loop_residues = 0\n    for residue in interface_residues:\n        if residue.secondary_structure in helical_designations:\n            number_helical_residues += 1\n        elif residue.secondary_structure in strand_designations:\n            number_strand_residues += 1\n        elif residue.secondary_structure in coil_designations:\n            number_loop_residues += 1\n\n    pose_metrics['percent_interface_helix'] = number_helical_residues / number_residues_interface\n    pose_metrics['percent_interface_strand'] = number_strand_residues / number_residues_interface\n    pose_metrics['percent_interface_coil'] = number_loop_residues / number_residues_interface\n    if self.interface_residues_by_interface == interface_residues_by_interface_unique:\n        pose_metrics['symmetric_interface'] = False\n    else:\n        pose_metrics['symmetric_interface'] = True\n\n    return pose_metrics\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics--entity_radius_ratio_v","title":"'entity_radius_ratio_#v#',","text":""},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics--entity_min_radius_ratio_v","title":"'entity_min_radius_ratio_#v#',","text":""},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics--entity_max_radius_ratio_v","title":"'entity_max_radius_ratio_#v#',","text":""},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics--entity_number_of_residues_ratio_v","title":"'entity_number_of_residues_ratio_#v#',","text":""},{"location":"reference/structure/model/#structure.model.Pose.interface_metrics--entity_number_of_residues_average_deviation","title":"'entity_number_of_residues_average_deviation,","text":""},{"location":"reference/structure/model/#structure.model.Pose.per_residue_errat","title":"per_residue_errat","text":"<pre><code>per_residue_errat() -&gt; dict[str, list[float]]\n</code></pre> <p>Return per-residue metrics for the interface surface area</p> <p>Returns:</p> <ul> <li> <code>dict[str, list[float]]</code>         \u2013          <p>The dictionary of errat metrics {errat_deviation, } mapped to arrays of shape (number_of_residues,)</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_errat(self) -&gt; dict[str, list[float]]:\n    \"\"\"Return per-residue metrics for the interface surface area\n\n    Returns:\n        The dictionary of errat metrics {errat_deviation, } mapped to arrays of shape (number_of_residues,)\n    \"\"\"\n    per_residue_data = {}\n    # pose_length = self.number_of_residues\n    assembly_minimally_contacting = self.assembly_minimally_contacting\n    self.log.debug(f'Starting {repr(self)} Errat')\n    errat_start = time.time()\n    _, per_residue_errat = assembly_minimally_contacting.errat(out_path=os.devnull)\n    self.log.debug(f'Finished Errat, took {time.time() - errat_start:6f} s')\n    per_residue_data['errat_deviation'] = per_residue_errat[:self.number_of_residues].tolist()\n\n    return per_residue_data\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.per_residue_interface_surface_area","title":"per_residue_interface_surface_area","text":"<pre><code>per_residue_interface_surface_area() -&gt; dict[str, list[float]]\n</code></pre> <p>Return per-residue metrics for the interface surface area</p> <p>Returns:</p> <ul> <li> <code>dict[str, list[float]]</code>         \u2013          <p>The dictionary of metrics mapped to arrays of values with shape (number_of_residues,) Metrics include sasa_hydrophobic_complex, sasa_polar_complex, sasa_relative_complex, sasa_hydrophobic_bound, sasa_polar_bound, sasa_relative_bound</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_interface_surface_area(self) -&gt; dict[str, list[float]]:\n    \"\"\"Return per-residue metrics for the interface surface area\n\n    Returns:\n        The dictionary of metrics mapped to arrays of values with shape (number_of_residues,)\n            Metrics include sasa_hydrophobic_complex, sasa_polar_complex, sasa_relative_complex,\n            sasa_hydrophobic_bound, sasa_polar_bound, sasa_relative_bound\n    \"\"\"\n    per_residue_data = {}\n    pose_length = self.number_of_residues\n    assembly_minimally_contacting = self.assembly_minimally_contacting\n\n    # Perform SASA measurements\n    if not assembly_minimally_contacting.sasa:\n        assembly_minimally_contacting.get_sasa()\n    assembly_asu_residues = assembly_minimally_contacting.residues[:pose_length]\n    per_residue_data['sasa_hydrophobic_complex'] = [residue.sasa_apolar for residue in assembly_asu_residues]\n    per_residue_data['sasa_polar_complex'] = [residue.sasa_polar for residue in assembly_asu_residues]\n    per_residue_data['sasa_relative_complex'] = [residue.relative_sasa for residue in assembly_asu_residues]\n    per_residue_sasa_unbound_apolar, per_residue_sasa_unbound_polar, per_residue_sasa_unbound_relative = [], [], []\n    for entity in self.entities:\n        if not entity.assembly.sasa:\n            entity.assembly.get_sasa()\n        oligomer_asu_residues = entity.assembly.residues[:entity.number_of_residues]\n        per_residue_sasa_unbound_apolar.extend([residue.sasa_apolar for residue in oligomer_asu_residues])\n        per_residue_sasa_unbound_polar.extend([residue.sasa_polar for residue in oligomer_asu_residues])\n        per_residue_sasa_unbound_relative.extend([residue.relative_sasa for residue in oligomer_asu_residues])\n\n    per_residue_data['sasa_hydrophobic_bound'] = per_residue_sasa_unbound_apolar\n    per_residue_data['sasa_polar_bound'] = per_residue_sasa_unbound_polar\n    per_residue_data['sasa_relative_bound'] = per_residue_sasa_unbound_relative\n\n    return per_residue_data\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.per_residue_relative_surface_area","title":"per_residue_relative_surface_area","text":"<pre><code>per_residue_relative_surface_area() -&gt; dict[str, ndarray]\n</code></pre> <p>Returns the relative solvent accessible surface area (SASA), per residue type compared to ideal three residue peptides, in both the bound (but not repacked) and complex states of the constituent Entity instances</p> <p>Returns:</p> <ul> <li> <code>dict[str, ndarray]</code>         \u2013          <p>Mapping of 'sasa_relative_complex' and 'sasa_relative_bound' to array with the per-residue relative SASA</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_relative_surface_area(self) -&gt; dict[str, np.ndarray]:\n    \"\"\"Returns the relative solvent accessible surface area (SASA), per residue type compared to ideal three residue\n    peptides, in both the bound (but not repacked) and complex states of the constituent Entity instances\n\n    Returns:\n        Mapping of 'sasa_relative_complex' and 'sasa_relative_bound' to array with the per-residue relative SASA\n    \"\"\"\n    pose_length = self.number_of_residues\n    assembly_minimally_contacting = self.assembly_minimally_contacting\n\n    # Perform SASA measurements\n    if not assembly_minimally_contacting.sasa:\n        assembly_minimally_contacting.get_sasa()\n    assembly_asu_residues = assembly_minimally_contacting.residues[:pose_length]\n    per_residue_relative_sasa_complex = np.array([residue.relative_sasa for residue in assembly_asu_residues])\n    per_residue_relative_sasa_unbound = []\n    for entity in self.entities:\n        if not entity.assembly.sasa:\n            entity.assembly.get_sasa()\n        oligomer_asu_residues = entity.assembly.residues[:entity.number_of_residues]\n        per_residue_relative_sasa_unbound.extend([residue.relative_sasa for residue in oligomer_asu_residues])\n\n    per_residue_relative_sasa_unbound = np.array(per_residue_relative_sasa_unbound)\n\n    return {'sasa_relative_complex': per_residue_relative_sasa_complex,\n            'sasa_relative_bound': per_residue_relative_sasa_unbound}\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.per_residue_buried_surface_area","title":"per_residue_buried_surface_area","text":"<pre><code>per_residue_buried_surface_area() -&gt; ndarray\n</code></pre> <p>Returns the buried surface area (BSA) as a result of all interfaces between Entity instances</p> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>An array with the per-residue unbound solvent accessible surface area (SASA) minus the SASA of the complex</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_buried_surface_area(self) -&gt; np.ndarray:\n    \"\"\"Returns the buried surface area (BSA) as a result of all interfaces between Entity instances\n\n    Returns:\n        An array with the per-residue unbound solvent accessible surface area (SASA) minus the SASA of the complex\n    \"\"\"\n    pose_length = self.number_of_residues\n    assembly_minimally_contacting = self.assembly_minimally_contacting\n\n    # Perform SASA measurements\n    if not assembly_minimally_contacting.sasa:\n        assembly_minimally_contacting.get_sasa()\n    assembly_asu_residues = assembly_minimally_contacting.residues[:pose_length]\n    per_residue_sasa_complex = np.array([residue.sasa for residue in assembly_asu_residues])\n    per_residue_sasa_unbound = []\n    for entity in self.entities:\n        if not entity.assembly.sasa:\n            entity.assembly.get_sasa()\n        oligomer_asu_residues = entity.assembly.residues[:entity.number_of_residues]\n        per_residue_sasa_unbound.extend([residue.sasa for residue in oligomer_asu_residues])\n\n    per_residue_sasa_unbound = np.array(per_residue_sasa_unbound)\n\n    return per_residue_sasa_unbound - per_residue_sasa_complex\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.per_residue_spatial_aggregation_propensity","title":"per_residue_spatial_aggregation_propensity","text":"<pre><code>per_residue_spatial_aggregation_propensity(distance: float = 5.0) -&gt; dict[str, list[float]]\n</code></pre> <p>Return per-residue spatial_aggregation for the complexed and unbound states. Positive values are more aggregation prone, while negative values are less prone</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>5.0</code> )         \u2013          <p>The distance in angstroms to measure Atom instances in contact</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, list[float]]</code>         \u2013          <p>The dictionary of metrics mapped to arrays of values with shape (number_of_residues,) Metrics include 'spatial_aggregation_propensity' and 'spatial_aggregation_propensity_unbound'</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def per_residue_spatial_aggregation_propensity(self, distance: float = 5.0) -&gt; dict[str, list[float]]:\n    \"\"\"Return per-residue spatial_aggregation for the complexed and unbound states. Positive values are more\n    aggregation prone, while negative values are less prone\n\n    Args:\n        distance: The distance in angstroms to measure Atom instances in contact\n\n    Returns:\n        The dictionary of metrics mapped to arrays of values with shape (number_of_residues,)\n            Metrics include 'spatial_aggregation_propensity' and 'spatial_aggregation_propensity_unbound'\n    \"\"\"\n    per_residue_sap = {}\n    pose_length = self.number_of_residues\n    assembly_minimally_contacting = self.assembly_minimally_contacting\n    assembly_minimally_contacting.spatial_aggregation_propensity_per_residue(distance=distance)\n    assembly_asu_residues = assembly_minimally_contacting.residues[:pose_length]\n    per_residue_sap['spatial_aggregation_propensity'] = \\\n        [residue.spatial_aggregation_propensity for residue in assembly_asu_residues]\n    # Calculate sap for each Entity\n    per_residue_sap_unbound = []\n    for entity in self.entities:\n        entity.assembly.spatial_aggregation_propensity_per_residue(distance=distance)\n        oligomer_asu_residues = entity.assembly.residues[:entity.number_of_residues]\n        # per_residue_sap_unbound.extend(entity.assembly.spatial_aggregation_propensity[:entity.number_of_residues])\n        per_residue_sap_unbound.extend(\n            [residue.spatial_aggregation_propensity for residue in oligomer_asu_residues])\n\n    per_residue_sap['spatial_aggregation_propensity_unbound'] = per_residue_sap_unbound\n\n    return per_residue_sap\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_interface","title":"get_interface","text":"<pre><code>get_interface(distance: float = 8.0) -&gt; Model\n</code></pre> <p>Provide a view of the Pose interface by generating a Model containing only interface Residues</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>8.0</code> )         \u2013          <p>The distance across the interface to query for Residue contacts</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Model</code>         \u2013          <p>The Structure containing only the Residues in the interface</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_interface(self, distance: float = 8.) -&gt; Model:\n    \"\"\"Provide a view of the Pose interface by generating a Model containing only interface Residues\n\n    Args:\n        distance: The distance across the interface to query for Residue contacts\n\n    Returns:\n        The Structure containing only the Residues in the interface\n    \"\"\"\n    if not self.is_symmetric():\n        raise NotImplementedError('This function has not been properly converted to deal with non-symmetric poses')\n\n    # interface_residues = []\n    # interface_core_coords = []\n    # for residues1, residues2 in self.interface_residues_by_entity_pair.values():\n    #     if not residues1 and not residues2:  # no interface\n    #         continue\n    #     elif residues1 and not residues2:  # symmetric case\n    #         interface_residues.extend(residues1)\n    #         # This was useful when not doing the symmetrization below...\n    #         # symmetric_residues = []\n    #         # for _ in range(number_of_models):\n    #         #     symmetric_residues.extend(residues1)\n    #         # residues1_coords = np.concatenate([residue.coords for residue in residues1])\n    #         # # Add the number of symmetric observed structures to a single new Structure\n    #         # symmetric_residue_structure = Chain.from_residues(residues=symmetric_residues)\n    #         # # symmetric_residues2_coords = self.return_symmetric_coords(residues1_coords)\n    #         # symmetric_residue_structure.replace_coords(self.return_symmetric_coords(residues1_coords))\n    #         # # use a single instance of the residue coords to perform a distance query against symmetric coords\n    #         # residues_tree = BallTree(residues1_coords)\n    #         # symmetric_query = residues_tree.query_radius(symmetric_residue_structure.coords, distance)\n    #         # # symmetric_indices = [symmetry_idx for symmetry_idx, asu_contacts in enumerate(symmetric_query)\n    #         # #                      if asu_contacts.any()]\n    #         # # finally, add all correctly located, asu interface indexed symmetrical residues to the interface\n    #         # coords_indexed_residues = symmetric_residue_structure.coords_indexed_residues\n    #         # interface_residues.extend(set(coords_indexed_residues[sym_idx]\n    #         #                               for sym_idx, asu_contacts in enumerate(symmetric_query)\n    #         #                               if asu_contacts.any()))\n    #     else:  # non-symmetric case\n    #         interface_core_coords.extend([residue.cb_coords for residue in residues1])\n    #         interface_core_coords.extend([residue.cb_coords for residue in residues2])\n    #         interface_residues.extend(residues1), interface_residues.extend(residues2)\n\n    # return Chain.from_residues(residues=sorted(interface_residues, key=lambda residue: residue.number))\n    # interface_symmetry_mates = self.return_symmetric_copies(interface_asu_structure)\n    # interface_coords = interface_asu_structure.coords\n    # interface_cb_indices = interface_asu_structure.cb_indices\n    # print('NUMBER of RESIDUES:', interface_asu_structure.number_of_residues,\n    #       '\\nNUMBER of CB INDICES', len(interface_cb_indices))\n    # residue_number = interface_asu_structure.number_of_residues\n    # [interface_asu_structure.cb_indices + (residue_number * model) for model in self.number_of_symmetry_mates]\n    # symmetric_cb_indices = np.array([idx + (coords_length * model_num) for model_num in range(number_of_models)\n    #                                  for idx in interface_asu_structure.cb_indices])\n    # print('Number sym CB INDICES:\\n', len(symmetric_cb_indices))\n    # From the interface core, find the mean position to seed clustering\n    entities_asu_com = self.center_of_mass\n    # initial_interface_coords = self.return_symmetric_coords(entities_asu_com)\n    initial_interface_coords = self.center_of_mass_symmetric_models\n    # initial_interface_coords = self.return_symmetric_coords(np.array(interface_core_coords).mean(axis=0))\n\n    # Get all interface residues and sort to ensure the asu is ordered and entity breaks can be found\n    sorted_residues = sorted(self.interface_residues, key=lambda residue: residue.index)\n    interface_asu_structure = Structure.from_residues(residues=sorted_residues)  # , chains=False, entities=False)\n    # symmetric_cb_indices = self.make_indices_symmetric(interface_asu_structure.cb_indices)\n    number_of_models = self.number_of_symmetry_mates\n    coords_length = interface_asu_structure.number_of_atoms\n    jump_sizes = [coords_length * model_num for model_num in range(number_of_models)]\n    symmetric_cb_indices = [idx + jump_size for jump_size in jump_sizes\n                            for idx in interface_asu_structure.cb_indices]\n    symmetric_interface_coords = self.return_symmetric_coords(interface_asu_structure.coords)\n    # index_cluster_labels = KMeans(n_clusters=self.number_of_symmetry_mates).fit_predict(symmetric_interface_coords)\n    # symmetric_interface_cb_coords = symmetric_interface_coords[symmetric_cb_indices]\n    # print('Number sym CB COORDS:\\n', len(symmetric_interface_cb_coords))\n    # initial_cluster_indices = [interface_cb_indices[0] + (coords_length * model_number)\n    #                            for model_number in range(self.number_of_symmetry_mates)]\n    # Fit a KMeans model to the symmetric interface cb coords\n    kmeans_cluster_model = \\\n        KMeans(n_clusters=number_of_models, init=initial_interface_coords, n_init=1) \\\n        .fit(symmetric_interface_coords[symmetric_cb_indices])\n    # kmeans_cluster_model = \\\n    #     KMeans(n_clusters=self.number_of_symmetry_mates, init=symmetric_interface_coords[initial_cluster_indices],\n    #            n_init=1).fit(symmetric_interface_cb_coords)\n    # Find the label where the asu is nearest too\n    asu_label = kmeans_cluster_model.predict(entities_asu_com[None, :])  # &lt;- add new first axis\n    # asu_interface_labels = kmeans_cluster_model.predict(interface_asu_structure.cb_coords)\n\n    # closest_interface_indices = np.where(index_cluster_labels == 0, True, False)\n    # [False, False, False, True, True, True, True, True, True, False, False, False, False, False, ...]\n    # symmetric_residues = interface_asu_structure.residues * self.number_of_symmetry_mates\n    # [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, ...]\n    # asu_index = np.median(asu_interface_labels)\n    # grab the symmetric indices for a single interface cluster, matching spatial proximity to the asu_index\n    # closest_asu_sym_cb_indices = symmetric_cb_indices[index_cluster_labels == asu_index]\n    # index_cluster_labels = kmeans_cluster_model.labels_\n    closest_asu_sym_cb_indices = \\\n        np.where(kmeans_cluster_model.labels_ == asu_label, symmetric_cb_indices, 0)\n    # # find the cb indices of the closest interface asu\n    # closest_asu_cb_indices = closest_asu_sym_cb_indices % coords_length\n    # interface_asu_structure.coords_indexed_residues\n    # find the model indices of the closest interface asu\n    # print('Normal sym CB INDICES\\n:', closest_asu_sym_cb_indices)\n\n    # Create an array where each model is along axis=0 and each cb_index is along axis=1\n    # The values in the array correspond to the symmetric cb_index if the cb_index is in the ASU\n    # or 0 if the index isn't in the ASU\n    # In this example the numbers aren't actually cb atom indices, they are cb residue indices...\n    # [[ 0,  0, 2, 3, 4,  0, ...],\n    #  [10,  0, 0, 0, 0,  0, ...],\n    #  [ 0, 21, 0, 0, 0, 25, ...]].sum(axis=0) -&gt;\n    # [ 10, 21, 2, 3, 4, 25, ...]\n    sym_cb_index_per_cb = closest_asu_sym_cb_indices.reshape((number_of_models, -1)).sum(axis=0)\n    # print('FLATTENED CB INDICES to get MODEL\\n:', sym_cb_index_per_cb)\n    # Now divide that array by the number of atoms to get the model index\n    sym_model_indices_per_cb = list(sym_cb_index_per_cb // coords_length)\n    # print('FLOORED sym CB INDICES to get MODEL\\n:', sym_model_indices_per_cb)\n    symmetry_mate_index_symmetric_coords = symmetric_interface_coords.reshape((number_of_models, -1, 3))\n    # print('RESHAPED SYMMETRIC COORDS SHAPE:', symmetry_mate_index_symmetric_coords.shape,\n    #       '\\nCOORDS length:', coords_length)\n    # Get the cb coords from the symmetric coords that correspond to the asu indices\n    closest_interface_coords = \\\n        np.concatenate([symmetry_mate_index_symmetric_coords[model_idx, residue.atom_indices]\n                        for model_idx, residue in zip(sym_model_indices_per_cb, interface_asu_structure.residues)])\n    # closest_symmetric_coords = \\\n    #     np.where(index_cluster_labels[:, None] == asu_index, symmetric_interface_coords, np.array([0.0, 0.0, 0.0]))\n    # closest_interface_coords = \\\n    #     closest_symmetric_coords.reshape((self.number_of_symmetry_mates, interface_coords.shape[0], -1)).sum(axis=0)\n\n    interface_asu_structure.coords = closest_interface_coords\n\n    # Correct structure attributes\n    # Get the new indices where there are different Entity instances\n    entity_breaks = iter(self.entity_breaks)\n    next_entity_break = next(entity_breaks)\n    interface_residue_entity_breaks = []  # 0]\n    for idx, residue in enumerate(sorted_residues):\n        if residue.index &gt; next_entity_break:\n            interface_residue_entity_breaks.append(idx)\n            try:\n                next_entity_break = next(entity_breaks)\n            except StopIteration:\n                raise StopIteration(f'Reached the end of self.entity_breaks before sorted_residues ran out')\n    else:  # Add the final idx\n        interface_residue_entity_breaks.append(idx + 1)\n\n    # For each residue, rename the chain_id according to the model it belongs to\n    number_entities = self.number_of_entities\n    model_chain_ids = list(chain_id_generator())[:number_of_models * number_entities]\n    entity_idx = 0\n    next_interface_asu_entity_break = interface_residue_entity_breaks[entity_idx]\n    for model_idx, residue in zip(sym_model_indices_per_cb, interface_asu_structure.residues):\n        if residue.index &gt;= next_interface_asu_entity_break:  # Increment the entity_idx\n            entity_idx += 1\n            next_interface_asu_entity_break = interface_residue_entity_breaks[entity_idx]\n        residue.chain_id = model_chain_ids[(model_idx * number_entities) + entity_idx]\n\n    return interface_asu_structure\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_interface_residues","title":"get_interface_residues","text":"<pre><code>get_interface_residues(entity1: Entity = None, entity2: Entity = None, **kwargs) -&gt; tuple[list[Residue] | list, list[Residue] | list]\n</code></pre> <p>Get unique Residues across an interface between two Entities</p> <p>If the interface occurs between the same Entity which is non-symmetrically defined, but happens to occur along a dimeric axis of symmetry (evaluates to True when the same Residue is found on each side of the interface), then the residues are returned belonging to only one side of the interface</p> <p>Parameters:</p> <ul> <li> <code>entity1</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>First Entity to measure interface between</p> </li> <li> <code>entity2</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>Second Entity to measure interface between</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>oligomeric_interfaces</code>         \u2013          <p>bool = False - Whether to query oligomeric interfaces</p> </li> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[Residue] | list, list[Residue] | list]</code>         \u2013          <p>The Entity1 and Entity2 interface Residue instances</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_interface_residues(\n        self, entity1: Entity = None, entity2: Entity = None, **kwargs\n) -&gt; tuple[list[Residue] | list, list[Residue] | list]:\n    \"\"\"Get unique Residues across an interface between two Entities\n\n    If the interface occurs between the same Entity which is non-symmetrically defined, but happens to occur along a\n    dimeric axis of symmetry (evaluates to True when the same Residue is found on each side of the interface), then\n    the residues are returned belonging to only one side of the interface\n\n    Args:\n        entity1: First Entity to measure interface between\n        entity2: Second Entity to measure interface between\n\n    Keyword Args:\n        oligomeric_interfaces: bool = False - Whether to query oligomeric interfaces\n        distance: float = 8. - The distance to measure Residues across an interface\n\n    Returns:\n        The Entity1 and Entity2 interface Residue instances\n    \"\"\"\n    entity1_residues, entity2_residues = \\\n        split_residue_pairs(self._find_interface_residue_pairs(entity1=entity1, entity2=entity2, **kwargs))\n\n    if not entity1_residues or not entity2_residues:\n        self.log.info(f'Interface search at {entity1.name} | {entity2.name} found no interface residues')\n        return [], []\n\n    if entity1 == entity2:  # If symmetric query\n        # Is the interface across a dimeric interface?\n        for residue in entity2_residues:  # entity2 usually has fewer residues, this might be quickest\n            if residue in entity1_residues:  # The interface is dimeric\n                # Include all residues found to only one side and move on\n                entity1_residues = sorted(set(entity1_residues).union(entity2_residues), key=lambda res: res.number)\n                entity2_residues = []\n                break\n    self.log.info(f'At Entity {entity1.name} | Entity {entity2.name} interface:'\n                  f'\\n\\t{entity1.name} found residue numbers: {\", \".join(str(r.number) for r in entity1_residues)}'\n                  f'\\n\\t{entity2.name} found residue numbers: {\", \".join(str(r.number) for r in entity2_residues)}')\n\n    return entity1_residues, entity2_residues\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.local_density_interface","title":"local_density_interface","text":"<pre><code>local_density_interface(distance: float = 12.0, atom_distance: float = None, **kwargs) -&gt; float\n</code></pre> <p>Returns the density of heavy Atoms neighbors within 'distance' Angstroms to Atoms in the Pose interface</p> <p>Parameters:</p> <ul> <li> <code>distance</code>             (<code>float</code>, default:                 <code>12.0</code> )         \u2013          <p>The cutoff distance with which Atoms should be included in local density</p> </li> <li> <code>atom_distance</code>             (<code>float</code>, default:                 <code>None</code> )         \u2013          <p>The distance to measure contacts between atoms. By default, uses default_atom_count_distance</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>residue_distance</code>         \u2013          <p>float = 8. - The distance to residue contacts in the interface. Uses the default if None</p> </li> <li> <code>oligomeric_interfaces</code>         \u2013          <p>bool = False - Whether to query oligomeric interfaces</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The local atom density around the interface</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def local_density_interface(self, distance: float = 12., atom_distance: float = None, **kwargs) -&gt; float:\n    \"\"\"Returns the density of heavy Atoms neighbors within 'distance' Angstroms to Atoms in the Pose interface\n\n    Args:\n        distance: The cutoff distance with which Atoms should be included in local density\n        atom_distance: The distance to measure contacts between atoms. By default, uses default_atom_count_distance\n\n    Keyword Args:\n        residue_distance: float = 8. - The distance to residue contacts in the interface. Uses the default if None\n        oligomeric_interfaces: bool = False - Whether to query oligomeric interfaces\n\n    Returns:\n        The local atom density around the interface\n    \"\"\"\n    if atom_distance:\n        kwargs['distance'] = atom_distance\n\n    interface_indices1, interface_indices2 = [], []\n    for entity1, entity2 in self.interface_residues_by_entity_pair:\n        atoms_indices1, atoms_indices2 = \\\n            split_number_pairs_and_sort(self._find_interface_atom_pairs(entity1=entity1, entity2=entity2, **kwargs))\n        interface_indices1.extend(atoms_indices1)\n        interface_indices2.extend(atoms_indices2)\n\n    if not interface_indices1 and not interface_indices2:\n        self.log.warning(f'No interface atoms located during {self.local_density_interface.__name__}')\n        return 0.\n\n    interface_indices = list(set(interface_indices1).union(interface_indices2))\n    if self.is_symmetric():\n        interface_coords = self.symmetric_coords[interface_indices]\n    else:\n        interface_coords = self.coords[interface_indices]\n\n    interface_tree = BallTree(interface_coords)\n    interface_counts = interface_tree.query_radius(interface_coords, distance, count_only=True)\n\n    return interface_counts.mean()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.query_entity_pair_for_fragments","title":"query_entity_pair_for_fragments","text":"<pre><code>query_entity_pair_for_fragments(entity1: Entity = None, entity2: Entity = None, oligomeric_interfaces: bool = False, **kwargs)\n</code></pre> <p>For all found interface residues in an Entity/Entity interface, search for corresponding fragment pairs</p> <p>Parameters:</p> <ul> <li> <code>entity1</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>The first Entity to measure for interface fragments</p> </li> <li> <code>entity2</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>The second Entity to measure for interface fragments</p> </li> <li> <code>oligomeric_interfaces</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to query oligomeric interfaces</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>by_distance</code>         \u2013          <p>bool = False - Whether interface Residue instances should be found by inter-residue Cb distance</p> </li> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> </ul> Sets <p>self._fragment_info_by_entity_pair (dict[tuple[str, str], list[FragmentInfo]])</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def query_entity_pair_for_fragments(self, entity1: Entity = None, entity2: Entity = None,\n                                    oligomeric_interfaces: bool = False, **kwargs):\n    \"\"\"For all found interface residues in an Entity/Entity interface, search for corresponding fragment pairs\n\n    Args:\n        entity1: The first Entity to measure for interface fragments\n        entity2: The second Entity to measure for interface fragments\n        oligomeric_interfaces: Whether to query oligomeric interfaces\n\n    Keyword Args:\n        by_distance: bool = False - Whether interface Residue instances should be found by inter-residue Cb distance\n        distance: float = 8. - The distance to measure Residues across an interface\n\n    Sets:\n        self._fragment_info_by_entity_pair (dict[tuple[str, str], list[FragmentInfo]])\n    \"\"\"\n    if (entity1.name, entity2.name) in self._fragment_info_by_entity_pair:\n        # Due to asymmetry in fragment generation, (2, 1) isn't checked\n        return\n\n    try:\n        residues1, residues2 = self.interface_residues_by_entity_pair[(entity1, entity2)]\n    except KeyError:  # When interface_residues haven't been set\n        self.find_and_split_interface(**kwargs)\n        try:\n            residues1, residues2 = self.interface_residues_by_entity_pair[(entity1, entity2)]\n        except KeyError:\n            raise DesignError(\n                f\"{self.query_entity_pair_for_fragments.__name__} can't access 'interface_residues' as the Entity \"\n                f\"pair {entity1.name}, {entity2.name} hasn't located any 'interface_residues'\")\n\n    # Because of the way self.interface_residues_by_entity_pair is set, when there isn't an interface, a check on\n    # residues1 is sufficient, however residues2 is empty with an interface present across a\n    # non-oligomeric dimeric 2-fold\n    if not residues1:\n        self.log.debug(f'No residues at the {entity1.name} | {entity2.name} interface. Fragments not available')\n        self._fragment_info_by_entity_pair[(entity1.name, entity2.name)] = []\n        return\n\n    frag_residues1 = entity1.get_fragment_residues(residues=residues1, fragment_db=self.fragment_db)\n    if not residues2:  # entity1 == entity2 and not residues2:\n        frag_residues2 = frag_residues1.copy()\n    else:\n        frag_residues2 = entity2.get_fragment_residues(residues=residues2, fragment_db=self.fragment_db)\n\n    if not frag_residues1 or not frag_residues2:\n        self.log.info(f'No fragments found at the {entity1.name} | {entity2.name} interface')\n        self._fragment_info_by_entity_pair[(entity1.name, entity2.name)] = []\n        return\n    else:\n        self.log.debug(f'At Entity {entity1.name} | Entity {entity2.name} interface:\\n\\t'\n                       f'{entity1.name} has {len(frag_residues1)} interface fragments at residues: '\n                       f'{\",\".join(map(str, [res.number for res in frag_residues1]))}\\n\\t'\n                       f'{entity2.name} has {len(frag_residues2)} interface fragments at residues: '\n                       f'{\",\".join(map(str, [res.number for res in frag_residues2]))}')\n\n    if self.is_symmetric():\n        # Even if entity1 == entity2, only need to expand the entity2 fragments due to surface/ghost frag mechanics\n        skip_models = []\n        if entity1 == entity2:\n            if oligomeric_interfaces:  # Intra-oligomeric contacts are desired\n                self.log.info(f'Including oligomeric models: '\n                              f'{\", \".join(map(str, self.oligomeric_model_indices.get(entity1)))}')\n            elif entity1.is_symmetric():\n                # Remove interactions with the intra-oligomeric contacts (contains asu)\n                skip_models = self.oligomeric_model_indices.get(entity1)\n                self.log.info(f'Skipping oligomeric models: {\", \".join(map(str, skip_models))}')\n            # else:  # Probably a C1\n\n        symmetric_frags2 = [self.return_symmetric_copies(residue) for residue in frag_residues2]\n        frag_residues2.clear()\n        for frag_mates in symmetric_frags2:\n            frag_residues2.extend([frag for sym_idx, frag in enumerate(frag_mates) if sym_idx not in skip_models])\n        self.log.debug(f'Entity {entity2.name} has {len(frag_residues2)} symmetric fragments')\n\n    # For clash check, only the backbone and Cb are desired\n    entity1_coords = entity1.backbone_and_cb_coords\n    fragment_time_start = time.time()\n    ghostfrag_surfacefrag_pairs = \\\n        find_fragment_overlap(frag_residues1, frag_residues2, clash_coords=entity1_coords)\n    self.log.info(f'Found {len(ghostfrag_surfacefrag_pairs)} overlapping fragment pairs at the {entity1.name} | '\n                  f'{entity2.name} interface')\n    self.log.debug(f'Took {time.time() - fragment_time_start:.8f}s')\n\n    self._fragment_info_by_entity_pair[(entity1.name, entity2.name)] = \\\n        create_fragment_info_from_pairs(ghostfrag_surfacefrag_pairs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.find_and_split_interface","title":"find_and_split_interface","text":"<pre><code>find_and_split_interface(by_distance: bool = False, **kwargs)\n</code></pre> <p>Locate interfaces regions for the designable entities and split into two contiguous interface residues sets</p> <p>Parameters:</p> <ul> <li> <code>by_distance</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether interface Residue instances should be found by inter-residue Cb distance</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> <li> <code>oligomeric_interfaces</code>         \u2013          <p>bool = False - Whether to query oligomeric interfaces</p> </li> </ul> Sets <p>self._interface_residues_by_entity_pair (dict[tuple[str, str], tuple[list[int], list[int]]]):     The Entity1/Entity2 interface mapped to the interface Residues self._interface_residues_by_interface (dict[int, list[int]]): Residue instances separated by     interface topology self._interface_residues_by_interface_unique (dict[int, list[int]]): Residue instances separated by     interface topology removing any dimeric duplicates</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def find_and_split_interface(self, by_distance: bool = False, **kwargs):\n    \"\"\"Locate interfaces regions for the designable entities and split into two contiguous interface residues sets\n\n    Args:\n        by_distance: Whether interface Residue instances should be found by inter-residue Cb distance\n\n    Keyword Args:\n        distance: float = 8. - The distance to measure Residues across an interface\n        oligomeric_interfaces: bool = False - Whether to query oligomeric interfaces\n\n    Sets:\n        self._interface_residues_by_entity_pair (dict[tuple[str, str], tuple[list[int], list[int]]]):\n            The Entity1/Entity2 interface mapped to the interface Residues\n        self._interface_residues_by_interface (dict[int, list[int]]): Residue instances separated by\n            interface topology\n        self._interface_residues_by_interface_unique (dict[int, list[int]]): Residue instances separated by\n            interface topology removing any dimeric duplicates\n    \"\"\"\n    if self._interface_residue_indices_by_interface:\n        self.log.debug(\"Interface residues weren't set as they're already set. If they've been changed, the \"\n                       \"attribute 'interface_residues_by_interface' should be reset\")\n        return\n\n    active_entities = self.active_entities\n    self.log.debug('Find and split interface using active_entities: '\n                   f'{\", \".join(entity.name for entity in active_entities)}')\n\n    if by_distance:\n        if self.is_symmetric():\n            entity_combinations = combinations_with_replacement(active_entities, 2)\n        else:\n            entity_combinations = combinations(active_entities, 2)\n\n        entity_pair: tuple[Entity, Entity]\n        for entity1, entity2 in entity_combinations:\n            residues1, residues2 = self.get_interface_residues(entity1, entity2, **kwargs)\n            self._interface_residue_indices_by_entity_name_pair[(entity1.name, entity2.name)] = (\n                [res.index for res in residues1],\n                [res.index for res in residues2]\n            )\n    else:\n        self._find_interface_residues_by_buried_surface_area()\n\n    self.check_interface_topology()\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.check_interface_topology","title":"check_interface_topology","text":"<pre><code>check_interface_topology()\n</code></pre> <p>From each pair of entities that share an interface, split the identified residues into two distinct groups. If an interface can't be composed into two distinct groups, raise DesignError</p> Sets <p>self._interface_residues_by_interface (dict[int, list[int]]): Residue instances separated by     interface topology self._interface_residues_by_interface_unique (dict[int, list[int]]): Residue instances separated by     interface topology removing any dimeric duplicates</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def check_interface_topology(self):\n    \"\"\"From each pair of entities that share an interface, split the identified residues into two distinct groups.\n    If an interface can't be composed into two distinct groups, raise DesignError\n\n    Sets:\n        self._interface_residues_by_interface (dict[int, list[int]]): Residue instances separated by\n            interface topology\n        self._interface_residues_by_interface_unique (dict[int, list[int]]): Residue instances separated by\n            interface topology removing any dimeric duplicates\n    \"\"\"\n    first_side, second_side = 0, 1\n    first_interface_side = defaultdict(list)  # {}\n    second_interface_side = defaultdict(list)  # {}\n    # interface = {first_side: {}, second_side: {}}\n    # Assume no symmetric contacts to start, i.e. [False, False]\n    self_indication = [False, False]\n    \"\"\"Set to True if the interface side (1 or 2) contains self-symmetric contacts\"\"\"\n    symmetric_dimer = {entity: False for entity in self.entities}\n    terminate = False\n    for (entity1, entity2), (residues1, residues2) in self.interface_residues_by_entity_pair.items():\n        # if not entity_residues:\n        if not residues1:  # No residues were found at this interface\n            continue\n        # Partition residues from each entity to the correct interface side\n        # Check for any existing symmetry\n        if entity1 == entity2:  # The query is with itself. Record as a self interaction\n            _self = True\n            if not residues2:  # The interface is a symmetric dimer and residues were removed from interface 2\n                symmetric_dimer[entity1] = True\n                # Set residues2 as residues1\n                residues2 = residues1  # .copy()  # Add residues1 to residues2\n        else:\n            _self = False\n\n        if not first_interface_side:  # This is first interface observation\n            # Add the pair to the dictionary in their indexed order\n            first_interface_side[entity1].extend(residues1)  # residues1.copy()\n            second_interface_side[entity2].extend(residues2)  # residues2.copy()\n            # Indicate whether the interface is a self symmetric interface by marking side 2 with _self\n            self_indication[second_side] = _self\n        else:  # Interface already assigned, so interface observation &gt;= 2\n            # Need to check if either Entity is in either side before adding correctly\n            if entity1 in first_interface_side:  # is Entity1 on the interface side 1?\n                # Is Entity1 in interface1 here as a result of self symmetric interaction?\n                if self_indication[first_side]:\n                    # Yes. Ex4 - self Entity was added to index 0 while ASU added to index 1\n                    # Add Entity1 to interface side 2\n                    second_interface_side[entity1].extend(residues1)\n                    # Add new Entity2 to interface side 1\n                    first_interface_side[entity2].extend(residues2)  # residues2.copy()\n                else:  # Entities are properly indexed\n                    # Add Entity1 to the first\n                    first_interface_side[entity1].extend(residues1)\n                    # Because of combinations with replacement Entity search, the second Entity isn't in\n                    # interface side 2, UNLESS the Entity self interaction is on interface 1 (above if check)\n                    # Therefore, add Entity2 to the second without checking for overwrite\n                    second_interface_side[entity2].extend(residues2)  # residues2.copy()\n                    # This can't happen, it would VIOLATE RULES\n                    # if _self:\n                    #     self_indication[second_side] = _self\n            # Entity1 isn't in the first index. It may be in the second, it may not\n            elif entity1 in second_interface_side:\n                # Yes. Ex5, add to interface side 2\n                second_interface_side[entity1].extend(residues1)\n                # Also, add Entity2 to the first side\n                # Entity 2 can't be in interface side 1 due to combinations with replacement check\n                first_interface_side[entity2].extend(residues2)  # residues2.copy()\n                if _self:  # Only modify if self is True, don't want to overwrite an existing True value\n                    self_indication[first_side] = _self\n            # If Entity1 is missing, check Entity2 to see if it has been identified yet\n            # This is more likely from combinations with replacement\n            elif entity2 in second_interface_side:\n                # Possible in an iteration Ex: (A:D) (C:D)\n                second_interface_side[entity2].extend(residues2)\n                # Entity 1 was not in first interface (from if #1), therefore we can set directly\n                first_interface_side[entity1].extend(residues1)  # residues1.copy()\n                # Ex3\n                if _self:  # Only modify if self is True, don't want to overwrite an existing True value\n                    self_indication[first_side] = _self\n            elif entity2 in first_interface_side:\n                # The first Entity wasn't found in either interface, but both interfaces are already set,\n                # therefore Entity pair isn't self, so the only way this works is if entity1 is further in the\n                # iterative process which is an impossible topology given combinations with replacement.\n                # Violates interface separation rules\n                second_interface_side[entity1] = False\n                terminate = True\n                break\n            # Neither of our Entities were found, thus we would add 2 entities to each interface side, violation\n            else:\n                first_interface_side[entity1] = second_interface_side[entity2] = False\n                terminate = True\n                break\n\n        if len(first_interface_side) == 2 and len(second_interface_side) == 2 and all(self_indication):\n            pass\n        elif len(first_interface_side) == 1 or len(second_interface_side) == 1:\n            pass\n        else:\n            terminate = True\n            break\n\n    if terminate:\n        self.log.critical('The set of interfaces found during interface search generated a topologically '\n                          'disallowed combination.\\n\\t %s\\n This cannot be modeled by a simple split for residues '\n                          'on either side while respecting the requirements of polymeric Entities. '\n                          '%sPlease correct your design_selectors to reduce the number of Entities you are '\n                          'attempting to design'\n                          % (' | '.join(':'.join(entity.name for entity in interface_entities)\n                                        for interface_entities in (first_interface_side, second_interface_side)),\n                             'Symmetry was set which may have influenced this unfeasible topology, you can try to '\n                             'set it False. ' if self.is_symmetric() else ''))\n        raise DesignError('The specified interfaces generated a topologically disallowed combination')\n\n    if not first_interface_side:\n        # raise utils.DesignError('Interface was unable to be split because no residues were found on one side of '\n        self.log.warning(\"The interface wasn't able to be split because no residues were found on one side. \"\n                         \"Check that your input has an interface or your flags aren't too stringent\")\n        return\n\n    for interface_number, entity_residues in enumerate((first_interface_side, second_interface_side), 1):\n        _residue_indices = [residue.index for _, residues in entity_residues.items() for residue in residues]\n        self._interface_residue_indices_by_interface[interface_number] = sorted(set(_residue_indices))\n\n    if any(symmetric_dimer.values()):\n        # For each entity, find the maximum residue observations on each interface side\n        entity_observations = {entity: [0, 0] for entity, dimer in symmetric_dimer.items() if dimer}\n        for interface_index, entity_residues in enumerate((first_interface_side, second_interface_side)):\n            for entity, observations in entity_observations.items():\n                observations[interface_index] += len(entity_residues[entity])\n\n        # Remove non-unique occurrences by entity if there are fewer observations\n        for entity, observations in entity_observations.items():\n            interface_obs1, interface_obs2 = entity_observations[entity]\n            if interface_obs1 &gt; interface_obs2:\n                second_interface_side.pop(entity)\n            elif interface_obs1 &lt; interface_obs2:\n                first_interface_side.pop(entity)\n            elif len(entity_observations) == 1:\n                # This is a homo-dimer, by convention, get rid of the second side\n                second_interface_side.pop(entity)\n            else:\n                raise SymmetryError(\n                    f\"Couldn't separate {entity.name} dimeric interface into unique residues. The number of \"\n                    f\"residues in each interface is equal: {interface_obs1} == {interface_obs2}\")\n\n        # Perform the sort as without dimeric constraints\n        for interface_number, entity_residues in enumerate((first_interface_side, second_interface_side), 1):\n            _residue_indices = [residue.index for _, residues in entity_residues.items() for residue in residues]\n            self._interface_residue_indices_by_interface_unique[interface_number] = sorted(set(_residue_indices))\n\n    else:  # Just make a copy of the internal list... Avoids unforeseen issues if these are ever modified\n        self._interface_residue_indices_by_interface_unique = \\\n            {number: residue_indices.copy()\n             for number, residue_indices in self._interface_residue_indices_by_interface.items()}\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.calculate_secondary_structure","title":"calculate_secondary_structure","text":"<pre><code>calculate_secondary_structure()\n</code></pre> <p>Curate the secondary structure topology for each Entity</p> Sets <p>self.ss_sequence_indices (list[int]):     Index which indicates the Residue membership to the secondary structure type element sequence self.ss_type_sequence (list[str]):     The ordered secondary structure type sequence which contains one character/secondary structure element self.split_interface_ss_elements (dict[int, list[int]]):     Maps the interface number to a list of indices corresponding to the secondary structure type     Ex: {1: [0, 0, 1, 2, ...] , 2: [9, 9, 9, 13, ...]]}</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_secondary_structure(self):\n    \"\"\"Curate the secondary structure topology for each Entity\n\n    Sets:\n        self.ss_sequence_indices (list[int]):\n            Index which indicates the Residue membership to the secondary structure type element sequence\n        self.ss_type_sequence (list[str]):\n            The ordered secondary structure type sequence which contains one character/secondary structure element\n        self.split_interface_ss_elements (dict[int, list[int]]):\n            Maps the interface number to a list of indices corresponding to the secondary structure type\n            Ex: {1: [0, 0, 1, 2, ...] , 2: [9, 9, 9, 13, ...]]}\n    \"\"\"\n    pose_secondary_structure = ''\n    for entity in self.entities:  # self.active_entities:\n        pose_secondary_structure += entity.secondary_structure\n\n    # Increment a secondary structure index which changes with every secondary structure transition\n    # Simultaneously, map the secondary structure type to an array of pose length\n    ss_increment_index = 0\n    ss_sequence_indices = [ss_increment_index]\n    ss_type_sequence = [pose_secondary_structure[0]]\n    for prior_ss_type, ss_type in zip(pose_secondary_structure[:-1], pose_secondary_structure[1:]):\n        if prior_ss_type != ss_type:\n            ss_increment_index += 1\n            ss_type_sequence.append(ss_type)\n        ss_sequence_indices.append(ss_increment_index)\n\n    # Clear any information if it exists\n    self.ss_sequence_indices.clear(), self.ss_type_sequence.clear()\n    self.ss_sequence_indices.extend(ss_sequence_indices)\n    self.ss_type_sequence.extend(ss_type_sequence)\n\n    for number, residue_indices in self._interface_residue_indices_by_interface_unique.items():\n        self.split_interface_ss_elements[number] = [ss_sequence_indices[idx] for idx in residue_indices]\n    self.log.debug(f'Found interface secondary structure: {self.split_interface_ss_elements}')\n    self.secondary_structure = pose_secondary_structure\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.calculate_fragment_profile","title":"calculate_fragment_profile","text":"<pre><code>calculate_fragment_profile(**kwargs)\n</code></pre> <p>Take the fragment_profile from each member Entity and combine</p> <p>Other Parameters:</p> <ul> <li> <code>evo_fill</code>         \u2013          <p>bool = False - Whether to fill missing positions with evolutionary profile values</p> </li> <li> <code>alpha</code>         \u2013          <p>float = 0.5 - The maximum contribution of the fragment profile to use, bounded between (0, 1]. 0 means no use of fragments in the .profile, while 1 means only use fragments</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def calculate_fragment_profile(self, **kwargs):\n    \"\"\"Take the fragment_profile from each member Entity and combine\n\n    Keyword Args:\n        evo_fill: bool = False - Whether to fill missing positions with evolutionary profile values\n        alpha: float = 0.5 - The maximum contribution of the fragment profile to use, bounded between (0, 1].\n            0 means no use of fragments in the .profile, while 1 means only use fragments\n    \"\"\"\n    #   keep_extras: bool = True - Whether to keep values for all that are missing data\n    for (entity1, entity2), fragment_info in self.fragment_info_by_entity_pair.items():\n        if fragment_info:\n            self.log.debug(f'Query Pair: {entity1.name}, {entity2.name}'\n                           f'\\n\\tFragment Info:{fragment_info}')\n            entity1.add_fragments_to_profile(fragments=fragment_info, alignment_type='mapped')\n            entity2.add_fragments_to_profile(fragments=fragment_info, alignment_type='paired')\n\n    # The order of this and below could be switched by combining self.fragment_map too\n    # Also, need to extract the entity.fragment_map to process_fragment_profile()\n    fragments_available = False\n    entities = self.entities\n    for entity in entities:\n        if entity.fragment_map:\n            entity.simplify_fragment_profile(**kwargs)\n            fragments_available = True\n        else:\n            entity.fragment_profile = Profile(\n                list(entity.create_null_profile(nan=True, zero_index=True).values()), dtype='fragment')\n\n    if fragments_available:\n        # # This assumes all values are present. What if they are not?\n        # self.fragment_profile = concatenate_profile([entity.fragment_profile for entity in self.entities],\n        #                                             start_at=0)\n        # self.alpha = concatenate_profile([entity.alpha for entity in self.entities])\n        fragment_profile = []\n        alpha = []\n        for entity in entities:\n            fragment_profile.extend(entity.fragment_profile)\n            alpha.extend(entity.alpha)\n        fragment_profile = Profile(fragment_profile, dtype='fragment')\n        self._alpha = entity._alpha  # Logic enforces entity is always referenced here\n    else:\n        alpha = [0 for _ in range(self.number_of_residues)]  # Reset the data\n        fragment_profile = Profile(\n            list(self.create_null_profile(nan=True, zero_index=True).values()), dtype='fragment')\n    self.alpha = alpha\n    self.fragment_profile = fragment_profile\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_fragment_observations","title":"get_fragment_observations","text":"<pre><code>get_fragment_observations(interface: bool = True) -&gt; list[FragmentInfo] | list\n</code></pre> <p>Return the fragment observations identified on the Pose for various types of tertiary structure interactions</p> <p>Parameters:</p> <ul> <li> <code>interface</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to return fragment observations from only the Pose interface</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FragmentInfo] | list</code>         \u2013          <p>The fragment observations</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_fragment_observations(self, interface: bool = True) -&gt; list[FragmentInfo] | list:\n    \"\"\"Return the fragment observations identified on the Pose for various types of tertiary structure interactions\n\n    Args:\n        interface: Whether to return fragment observations from only the Pose interface\n\n    Returns:\n        The fragment observations\n    \"\"\"\n    # Ensure fragments are generated if they aren't already\n    if interface:\n        self.generate_interface_fragments()\n    else:\n        self.generate_fragments()\n\n    interface_residues_by_entity_pair = self.interface_residues_by_entity_pair\n    observations = []\n    # {(ent1, ent2): [{mapped: res_num1, paired: res_num2, cluster: (int, int, int), match: score}, ...], ...}\n    for entity_pair, fragment_info in self.fragment_info_by_entity_pair.items():\n        if interface:\n            if entity_pair not in interface_residues_by_entity_pair:\n                continue\n\n        observations.extend(fragment_info)\n\n    return observations\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.get_fragment_metrics","title":"get_fragment_metrics","text":"<pre><code>get_fragment_metrics(fragments: list[FragmentInfo] = None, total_interface: bool = True, by_interface: bool = False, by_entity: bool = False, entity1: Entity = None, entity2: Entity = None, **kwargs) -&gt; dict[str, Any]\n</code></pre> <p>Return fragment metrics from the Pose. Returns the entire Pose unless by_interface or by_entity is True</p> <p>Uses data from self.fragment_queries unless fragments are passed</p> <p>Parameters:</p> <ul> <li> <code>fragments</code>             (<code>list[FragmentInfo]</code>, default:                 <code>None</code> )         \u2013          <p>A list of fragment observations</p> </li> <li> <code>total_interface</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Return all fragment metrics for every interface found in the Pose</p> </li> <li> <code>by_interface</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Return fragment metrics for each particular interface between Chain instances in the Pose</p> </li> <li> <code>by_entity</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Return fragment metrics for each Entity found in the Pose</p> </li> <li> <code>entity1</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>The first Entity object to identify the interface if per_interface=True</p> </li> <li> <code>entity2</code>             (<code>Entity</code>, default:                 <code>None</code> )         \u2013          <p>The second Entity object to identify the interface if per_interface=True</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> <li> <code>oligomeric_interfaces</code>         \u2013          <p>bool = False - Whether to query oligomeric interfaces</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>A mapping of the following metrics for the requested structural region - {'center_indices',  'total_indices',  'nanohedra_score',  'nanohedra_score_center',  'nanohedra_score_normalized',  'nanohedra_score_center_normalized',  'number_residues_fragment_total',  'number_residues_fragment_center',  'multiple_fragment_ratio',  'number_fragments_interface'  'percent_fragment_helix'  'percent_fragment_strand'  'percent_fragment_coil'  }</p> </li> <li> <code>dict[str, Any]</code>         \u2013          <p>Will include a single mapping if total_interface, a mapping for each interface if by_interface, and a</p> </li> <li> <code>dict[str, Any]</code>         \u2013          <p>mapping for each Entity if by_entity</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def get_fragment_metrics(self, fragments: list[FragmentInfo] = None, total_interface: bool = True,\n                         by_interface: bool = False, by_entity: bool = False,\n                         entity1: Entity = None, entity2: Entity = None, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Return fragment metrics from the Pose. Returns the entire Pose unless by_interface or by_entity is True\n\n    Uses data from self.fragment_queries unless fragments are passed\n\n    Args:\n        fragments: A list of fragment observations\n        total_interface: Return all fragment metrics for every interface found in the Pose\n        by_interface: Return fragment metrics for each particular interface between Chain instances in the Pose\n        by_entity: Return fragment metrics for each Entity found in the Pose\n        entity1: The first Entity object to identify the interface if per_interface=True\n        entity2: The second Entity object to identify the interface if per_interface=True\n\n    Keyword Args:\n        distance: float = 8. - The distance to measure Residues across an interface\n        oligomeric_interfaces: bool = False - Whether to query oligomeric interfaces\n\n    Returns:\n        A mapping of the following metrics for the requested structural region -\n            {'center_indices',\n             'total_indices',\n             'nanohedra_score',\n             'nanohedra_score_center',\n             'nanohedra_score_normalized',\n             'nanohedra_score_center_normalized',\n             'number_residues_fragment_total',\n             'number_residues_fragment_center',\n             'multiple_fragment_ratio',\n             'number_fragments_interface'\n             'percent_fragment_helix'\n             'percent_fragment_strand'\n             'percent_fragment_coil'\n             }\n        Will include a single mapping if total_interface, a mapping for each interface if by_interface, and a\n        mapping for each Entity if by_entity\n    \"\"\"\n\n    if fragments is not None:\n        fragment_db = self.fragment_db\n        return fragment_db.format_fragment_metrics(fragment_db.calculate_match_metrics(fragments))\n\n    fragment_metrics = self.fragment_metrics_by_entity_pair\n\n    if by_interface:\n        fragment_info = self.fragment_info_by_entity_pair\n        # Check for either orientation as the final interface score will be the same\n        if (entity1, entity2) not in fragment_info or (entity2, entity1) not in fragment_info:\n            self.query_entity_pair_for_fragments(entity1=entity1, entity2=entity2, **kwargs)\n\n        metric_d = deepcopy(fragment_metric_template)\n        for query_pair, _metrics in fragment_metrics.items():\n            # Check either orientation as the function query could vary from self.fragment_metrics\n            if (entity1, entity2) in query_pair or (entity2, entity1) in query_pair:\n                if _metrics:\n                    metric_d = self.fragment_db.format_fragment_metrics(_metrics)\n                    break\n        else:\n            self.log.warning(f\"Couldn't locate query metrics for Entity pair {entity1.name}, {entity2.name}\")\n    elif by_entity:\n        metric_d = {}\n        for query_pair, _metrics in fragment_metrics.items():\n            if not _metrics:\n                continue\n            for align_type, entity in zip(alignment_types, query_pair):\n                if entity not in metric_d:\n                    metric_d[entity] = deepcopy(fragment_metric_template)\n\n                metric_d[entity]['center_indices'].update(_metrics[align_type]['center']['indices'])\n                metric_d[entity]['total_indices'].update(_metrics[align_type]['total']['indices'])\n                metric_d[entity]['nanohedra_score'] += _metrics[align_type]['total']['score']\n                metric_d[entity]['nanohedra_score_center'] += _metrics[align_type]['center']['score']\n                metric_d[entity]['multiple_fragment_ratio'] += _metrics[align_type]['multiple_ratio']\n                metric_d[entity]['number_fragments_interface'] += _metrics['total']['observations']\n                metric_d[entity]['percent_fragment_helix'] += _metrics[align_type]['index_count'][1]\n                metric_d[entity]['percent_fragment_strand'] += _metrics[align_type]['index_count'][2]\n                metric_d[entity]['percent_fragment_coil'] += _metrics[align_type]['index_count'][3] \\\n                    + _metrics[align_type]['index_count'][4] + _metrics[align_type]['index_count'][5]\n\n        # Finally, tabulate based on the total for each Entity\n        for entity, _metrics in metric_d.items():\n            _metrics['number_residues_fragment_total'] = len(_metrics['total_indices'])\n            _metrics['number_residues_fragment_center'] = len(_metrics['center_indices'])\n            number_fragments_interface = _metrics['number_fragments_interface']\n            _metrics['percent_fragment_helix'] /= number_fragments_interface\n            _metrics['percent_fragment_strand'] /= number_fragments_interface\n            _metrics['percent_fragment_coil'] /= number_fragments_interface\n            try:\n                _metrics['nanohedra_score_normalized'] = \\\n                    _metrics['nanohedra_score'] / _metrics['number_residues_fragment_total']\n                _metrics['nanohedra_score_center_normalized'] = \\\n                    _metrics['nanohedra_score_center'] / _metrics['number_residues_fragment_center']\n            except ZeroDivisionError:\n                self.log.warning(f'{self.name}: No interface residues were found. Is there an interface in your '\n                                 f'design?')\n                _metrics['nanohedra_score_normalized'] = _metrics['nanohedra_score_center_normalized'] = 0.\n\n    elif total_interface:  # For the entire interface\n        metric_d = deepcopy(fragment_metric_template)\n        for query_pair, _metrics in fragment_metrics.items():\n            if not _metrics:\n                continue\n            metric_d['center_indices'].update(\n                _metrics['mapped']['center']['indices'].union(_metrics['paired']['center']['indices']))\n            metric_d['total_indices'].update(\n                _metrics['mapped']['total']['indices'].union(_metrics['paired']['total']['indices']))\n            metric_d['nanohedra_score'] += _metrics['total']['total']['score']\n            metric_d['nanohedra_score_center'] += _metrics['total']['center']['score']\n            metric_d['multiple_fragment_ratio'] += _metrics['total']['multiple_ratio']\n            metric_d['number_fragments_interface'] += _metrics['total']['observations']\n            metric_d['percent_fragment_helix'] += _metrics['total']['index_count'][1]\n            metric_d['percent_fragment_strand'] += _metrics['total']['index_count'][2]\n            metric_d['percent_fragment_coil'] += _metrics['total']['index_count'][3] \\\n                + _metrics['total']['index_count'][4] + _metrics['total']['index_count'][5]\n\n        # Finally, tabulate based on the total\n        metric_d['number_residues_fragment_total'] = len(metric_d['total_indices'])\n        metric_d['number_residues_fragment_center'] = len(metric_d['center_indices'])\n        total_observations = metric_d['number_fragments_interface'] * 2  # 2x observations in ['total']['index_count']\n        try:\n            metric_d['percent_fragment_helix'] /= total_observations\n            metric_d['percent_fragment_strand'] /= total_observations\n            metric_d['percent_fragment_coil'] /= total_observations\n        except ZeroDivisionError:\n            metric_d['percent_fragment_helix'] = metric_d['percent_fragment_strand'] = \\\n                metric_d['percent_fragment_coil'] = 0.\n        try:\n            metric_d['nanohedra_score_normalized'] = \\\n                metric_d['nanohedra_score'] / metric_d['number_residues_fragment_total']\n            metric_d['nanohedra_score_center_normalized'] = \\\n                metric_d['nanohedra_score_center']/metric_d['number_residues_fragment_center']\n        except ZeroDivisionError:\n            self.log.debug(f'{self.name}: No fragment residues were found')\n            metric_d['nanohedra_score_normalized'] = metric_d['nanohedra_score_center_normalized'] = 0.\n\n    else:  # For the entire Pose?\n        raise NotImplementedError(\"There isn't a mechanism to return fragments for the mode specified\")\n\n    return metric_d\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.residue_processing","title":"residue_processing","text":"<pre><code>residue_processing(design_scores: dict[str, dict[str, float | str]], columns: list[str]) -&gt; dict[str, dict[int, dict[str, float | list]]]\n</code></pre> <p>Process Residue Metrics from Rosetta score dictionary (One-indexed residues)</p> <p>Parameters:</p> <ul> <li> <code>design_scores</code>             (<code>dict[str, dict[str, float | str]]</code>)         \u2013          <p>{'001': {'buns': 2.0, 'res_energy_complex_15A': -2.71, ...,             'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...'}, ...}</p> </li> <li> <code>columns</code>             (<code>list[str]</code>)         \u2013          <p>['per_res_energy_complex_5', 'per_res_energy_1_unbound_5', ...]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict[int, dict[str, float | list]]]</code>         \u2013          <p>{'001': {15: {'type': 'T', 'energy': {'complex': -2.71, 'unbound': [-1.9, 0]}, 'fsp': 0., 'cst': 0.}, ...}, ...}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def residue_processing(\n    self, design_scores: dict[str, dict[str, float | str]], columns: list[str]\n) -&gt; dict[str, dict[int, dict[str, float | list]]]:\n    \"\"\"Process Residue Metrics from Rosetta score dictionary (One-indexed residues)\n\n    Args:\n        design_scores: {'001': {'buns': 2.0, 'res_energy_complex_15A': -2.71, ...,\n                        'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...'}, ...}\n        columns: ['per_res_energy_complex_5', 'per_res_energy_1_unbound_5', ...]\n\n    Returns:\n        {'001': {15: {'type': 'T', 'energy': {'complex': -2.71, 'unbound': [-1.9, 0]}, 'fsp': 0., 'cst': 0.}, ...},\n         ...}\n    \"\"\"\n    # energy_template = {'complex': 0., 'unbound': 0., 'fsp': 0., 'cst': 0.}\n    residue_template = {'energy': {'complex': 0., 'unbound': [0. for ent in self.entities], 'fsp': 0., 'cst': 0.}}\n    pose_length = self.number_of_residues\n    # adjust the energy based on pose specifics\n    pose_energy_multiplier = self.number_of_symmetry_mates  # Will be 1 if not self.is_symmetric()\n    entity_energy_multiplier = [entity.number_of_symmetry_mates for entity in self.entities]\n\n    warn = False\n    parsed_design_residues = {}\n    for design, scores in design_scores.items():\n        residue_data = {}\n        for column in columns:\n            if column not in scores:\n                continue\n            metadata = column.strip('_').split('_')\n            # remove chain_id in rosetta_numbering=\"False\"\n            # if we have enough chains, weird chain characters appear \"per_res_energy_complex_19_\" which mess up\n            # split. Also numbers appear, \"per_res_energy_complex_1161\" which may indicate chain \"1\" or residue 1161\n            residue_number = int(metadata[-1].translate(utils.keep_digit_table))\n            if residue_number &gt; pose_length:\n                if not warn:\n                    warn = True\n                    logger.warning(\n                        'Encountered %s which has residue number &gt; the pose length (%d). If this system is '\n                        'NOT a large symmetric system and output_as_pdb_nums=\"true\" was used in Rosetta '\n                        'PerResidue SimpleMetrics, there is an error in processing that requires your '\n                        'debugging. Otherwise, this is likely a numerical chain and will be treated under '\n                        'that assumption. Always ensure that output_as_pdb_nums=\"true\" is set'\n                        % (column, pose_length))\n                residue_number = residue_number[:-1]\n            if residue_number not in residue_data:\n                residue_data[residue_number] = deepcopy(residue_template)  # deepcopy(energy_template)\n\n            metric = metadata[2]  # energy [or sasa]\n            if metric != 'energy':\n                continue\n            pose_state = metadata[-2]  # unbound or complex [or fsp (favor_sequence_profile) or cst (constraint)]\n            entity_or_complex = metadata[3]  # 1,2,3,... or complex\n\n            # use += because instances of symmetric residues from symmetry related chains are summed\n            try:  # to convert to int. Will succeed if we have an entity value, ex: 1,2,3,...\n                entity = int(entity_or_complex) - ZERO_OFFSET\n                residue_data[residue_number][metric][pose_state][entity] += \\\n                    (scores.get(column, 0) / entity_energy_multiplier[entity])\n            except ValueError:  # complex is the value, use the pose state\n                residue_data[residue_number][metric][pose_state] += (scores.get(column, 0) / pose_energy_multiplier)\n        parsed_design_residues[design] = residue_data\n\n    return parsed_design_residues\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.process_rosetta_residue_scores","title":"process_rosetta_residue_scores","text":"<pre><code>process_rosetta_residue_scores(design_scores: dict[str, dict[str, float | str]]) -&gt; dict[str, dict[int, dict[str, float | list]]]\n</code></pre> <p>Process Residue Metrics from Rosetta score dictionary (One-indexed residues) accounting for symmetric energy</p> <p>Parameters:</p> <ul> <li> <code>design_scores</code>             (<code>dict[str, dict[str, float | str]]</code>)         \u2013          <p>{'001': {'buns': 2.0, 'res_energy_complex_15A': -2.71, ...,             'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...'}, ...}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, dict[int, dict[str, float | list]]]</code>         \u2013          <p>The parsed design information where the outer key is the design alias, and the next key is the Residue.index</p> </li> <li> <code>dict[str, dict[int, dict[str, float | list]]]</code>         \u2013          <p>where the corresponding information belongs. Only returns Residue metrics for those positions where metrics</p> </li> <li> <code>dict[str, dict[int, dict[str, float | list]]]</code>         \u2013          <p>were taken. Example: {'001':     {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],           'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],           'fsp': 0., 'cst': 0.},      ...},  ...}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def process_rosetta_residue_scores(self, design_scores: dict[str, dict[str, float | str]]) -&gt; \\\n        dict[str, dict[int, dict[str, float | list]]]:\n    \"\"\"Process Residue Metrics from Rosetta score dictionary (One-indexed residues) accounting for symmetric energy\n\n    Args:\n        design_scores: {'001': {'buns': 2.0, 'res_energy_complex_15A': -2.71, ...,\n                        'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...'}, ...}\n\n    Returns:\n        The parsed design information where the outer key is the design alias, and the next key is the Residue.index\n        where the corresponding information belongs. Only returns Residue metrics for those positions where metrics\n        were taken. Example:\n            {'001':\n                {15: {'complex': -2.71, 'bound': [-1.9, 0], 'unbound': [-1.9, 0],\n                      'solv_complex': -2.71, 'solv_bound': [-1.9, 0], 'solv_unbound': [-1.9, 0],\n                      'fsp': 0., 'cst': 0.},\n                 ...},\n             ...}\n    \"\"\"\n    res_slice = slice(0, 4)\n    pose_length = self.number_of_residues\n    # Adjust the energy based on pose specifics\n    pose_energy_multiplier = self.number_of_symmetry_mates  # Will be 1 if not self.is_symmetric()\n    entity_energy_multiplier = [entity.number_of_symmetry_mates for entity in self.entities]\n\n    def get_template(): return deepcopy({\n        'complex': 0., 'bound': [0. for _ in self.entities], 'unbound': [0. for _ in self.entities],\n        'solv_complex': 0., 'solv_bound': [0. for _ in self.entities],\n        'solv_unbound': [0. for _ in self.entities], 'fsp': 0., 'cst': 0.})\n\n    warn_additional = True\n    parsed_design_residues = {}\n    for design, scores in design_scores.items():\n        residue_data = defaultdict(get_template)\n        for key, value in scores.items():\n            if key[res_slice] != 'res_':\n                continue\n            # key contains: 'per_res_energysolv_complex_15W' or 'per_res_energysolv_2_bound_415B'\n            res, metric, entity_or_complex, *_ = metadata = key.strip('_').split('_')\n            # metadata[1] is energy [or sasa]\n            # metadata[2] is entity designation or complex such as '1','2','3',... or 'complex'\n\n            # Take the \"possibly\" symmetric Rosetta residue index (one-indexed) and convert to python,\n            # then take the modulus for the pose numbering\n            try:\n                residue_index = (int(metadata[-1])-1) % pose_length\n            except ValueError:\n                continue  # This is a residual metric\n            # remove chain_id in rosetta_numbering=\"False\"\n            if metric == 'energysolv':\n                metric_str = f'solv_{metadata[-2]}'  # pose_state - unbound, bound, complex\n            elif metric == 'energy':\n                metric_str = metadata[-2]  # pose_state - unbound, bound, complex\n            else:  # Other residual such as sasa or something else old/new\n                if warn_additional:\n                    warn_additional = False\n                    logger.warning(f\"Found additional metrics that aren't being processed. Ex {key}={value}\")\n                continue\n\n            # Use += because instances of symmetric residues from symmetry related chains are summed\n            try:  # To convert to int. Will succeed if we have an entity as a string integer, ex: 1,2,3,...\n                entity = int(entity_or_complex) - ZERO_OFFSET\n            except ValueError:  # 'complex' is the value, use the pose state\n                residue_data[residue_index][metric_str] += (value / pose_energy_multiplier)\n            else:\n                residue_data[residue_index][metric_str][entity] += (value / entity_energy_multiplier[entity])\n\n        parsed_design_residues[design] = residue_data\n\n    return parsed_design_residues\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.rosetta_hbond_processing","title":"rosetta_hbond_processing","text":"<pre><code>rosetta_hbond_processing(design_scores: dict[str, dict]) -&gt; dict[str, set[int]]\n</code></pre> <p>Process Hydrogen bond Metrics from Rosetta score dictionary</p> <p>if rosetta_numbering=\"true\" in .xml then use offset, otherwise, hbonds are PDB numbering</p> <p>Parameters:</p> <ul> <li> <code>design_scores</code>             (<code>dict[str, dict]</code>)         \u2013          <p>{'001': {'buns': 2.0, 'per_res_energy_complex_15A': -2.71, ...,                     'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...',                     'hbonds_res_selection_1_bound': '26A'}, ...}</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, set[int]]</code>         \u2013          <p>{'001': {34, 54, 67, 68, 106, 178}, ...}</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def rosetta_hbond_processing(self, design_scores: dict[str, dict]) -&gt; dict[str, set[int]]:\n    \"\"\"Process Hydrogen bond Metrics from Rosetta score dictionary\n\n    if rosetta_numbering=\"true\" in .xml then use offset, otherwise, hbonds are PDB numbering\n\n    Args:\n        design_scores: {'001': {'buns': 2.0, 'per_res_energy_complex_15A': -2.71, ...,\n                                'yhh_planarity':0.885, 'hbonds_res_selection_complex': '15A,21A,26A,35A,...',\n                                'hbonds_res_selection_1_bound': '26A'}, ...}\n\n    Returns:\n        {'001': {34, 54, 67, 68, 106, 178}, ...}\n    \"\"\"\n    pose_length = self.number_of_residues\n    hbonds = {}\n    for design, scores in design_scores.items():\n        unbound_bonds, complex_bonds = set(), set()\n        for column, value in scores.items():\n            if 'hbonds_res_' not in column:  # if not column.startswith('hbonds_res_selection'):\n                continue\n            meta_data = column.split('_')  # ['hbonds', 'res', 'selection', 'complex/interface_number', '[unbound]']\n            # Offset rosetta numbering to python index and make asu index using the modulus\n            parsed_hbond_indices = set((int(hbond)-1) % pose_length\n                                       for hbond in value.split(',') if hbond != '')  # '' in case no hbonds\n            # if meta_data[-1] == 'bound' and offset:  # find offset according to chain\n            #     res_offset = offset[meta_data[-2]]\n            #     parsed_hbonds = set(residue + res_offset for residue in parsed_hbonds)\n            if meta_data[3] == 'complex':\n                complex_bonds = parsed_hbond_indices\n            else:  # From another state\n                unbound_bonds = unbound_bonds.union(parsed_hbond_indices)\n        if complex_bonds:  # 'complex', '1', '2'\n            hbonds[design] = complex_bonds.difference(unbound_bonds)\n        else:  # No hbonds were found in the complex\n            hbonds[design] = complex_bonds\n\n    return hbonds\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.generate_interface_fragments","title":"generate_interface_fragments","text":"<pre><code>generate_interface_fragments(oligomeric_interfaces: bool = False, **kwargs)\n</code></pre> <p>Generate fragments between the Pose interface(s). Finds interface(s) if not already available</p> <p>Parameters:</p> <ul> <li> <code>oligomeric_interfaces</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to query oligomeric interfaces</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>by_distance</code>         \u2013          <p>bool = False - Whether interface Residue instances should be found by inter-residue Cb distance</p> </li> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to measure Residues across an interface</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def generate_interface_fragments(self, oligomeric_interfaces: bool = False, **kwargs):\n    \"\"\"Generate fragments between the Pose interface(s). Finds interface(s) if not already available\n\n    Args:\n        oligomeric_interfaces: Whether to query oligomeric interfaces\n\n    Keyword Args:\n        by_distance: bool = False - Whether interface Residue instances should be found by inter-residue Cb distance\n        distance: float = 8. - The distance to measure Residues across an interface\n    \"\"\"\n    if not self._interface_residue_indices_by_interface:\n        self.find_and_split_interface(oligomeric_interfaces=oligomeric_interfaces, **kwargs)\n\n    if self.is_symmetric():\n        entity_combinations = combinations_with_replacement(self.active_entities, 2)\n    else:\n        entity_combinations = combinations(self.active_entities, 2)\n\n    entity_pair: Iterable[Entity]\n    for entity_pair in entity_combinations:\n        self.log.debug(f'Querying Entity pair: {\", \".join(entity.name for entity in entity_pair)} '\n                       f'for interface fragments')\n        self.query_entity_pair_for_fragments(*entity_pair, oligomeric_interfaces=oligomeric_interfaces, **kwargs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.generate_fragments","title":"generate_fragments","text":"<pre><code>generate_fragments(**kwargs)\n</code></pre> <p>Generate fragments pairs between every possible Residue instance in the Pose</p> <p>Other Parameters:</p> <ul> <li> <code>distance</code>         \u2013          <p>float = 8. - The distance to query for neighboring fragments</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def generate_fragments(self, **kwargs):\n    \"\"\"Generate fragments pairs between every possible Residue instance in the Pose\n\n    Keyword Args:\n        distance: float = 8. - The distance to query for neighboring fragments\n    \"\"\"\n    for entity in self.active_entities:\n        self.log.info(f'Querying Entity: {entity} for internal fragments')\n        search_start_time = time.time()\n        ghostfrag_surfacefrag_pairs = entity.find_fragments(**kwargs)\n        self.log.info(f'Internal fragment search took {time.time() - search_start_time:8f}s')\n        self._fragment_info_by_entity_pair[(entity.name, entity.name)] = \\\n            create_fragment_info_from_pairs(ghostfrag_surfacefrag_pairs)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.write_fragment_pairs","title":"write_fragment_pairs","text":"<pre><code>write_fragment_pairs(out_path: AnyStr = os.getcwd(), multimodel: bool = False) -&gt; AnyStr | None\n</code></pre> <p>Write the fragments associated with the pose to disk</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The path to the directory to output files to</p> </li> <li> <code>multimodel</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to write all fragments as a multimodel file. File written to \"'out_path'/all_frags.pdb\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The path to the written file if one was written</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def write_fragment_pairs(self, out_path: AnyStr = os.getcwd(), multimodel: bool = False) -&gt; AnyStr | None:\n    \"\"\"Write the fragments associated with the pose to disk\n\n    Args:\n        out_path: The path to the directory to output files to\n        multimodel: Whether to write all fragments as a multimodel file. File written to \"'out_path'/all_frags.pdb\"\n\n    Returns:\n        The path to the written file if one was written\n    \"\"\"\n    residues = self.residues\n    ghost_frags = []\n    clusters = []\n    for entity_pair, fragment_info in self.fragment_info_by_entity_pair.items():\n        for info in fragment_info:\n            ijk = info.cluster\n            clusters.append(ijk)\n            # match_score = info.match\n            aligned_residue = residues[info.mapped]\n            ghost_frag = aligned_residue.ghost_fragments[ijk]\n            ghost_frags.append(ghost_frag)\n\n    putils.make_path(out_path)\n    file_path = None\n    if multimodel:\n        file_path = os.path.join(out_path, 'all_frags.pdb')\n        write_fragments_as_multimodel(ghost_frags, file_path)\n    else:\n        match_count = count(1)\n        for ghost_frag, ijk in zip(ghost_frags, clusters):\n            file_path = os.path.join(\n                out_path, '{}_{}_{}_fragment_match_{}.pdb'.format(*ijk, next(match_count)))\n            ghost_frag.representative.write(out_path=file_path)\n\n    # frag_file = Path(out_path, putils.frag_text_file)\n    # frag_file.unlink(missing_ok=True)  # Ensure old file is removed before new write\n    # for match_count, (ghost_frag, surface_frag, match_score) in enumerate(ghost_mono_frag_pairs, 1):\n    #     ijk = ghost_frag.ijk\n    #     fragment_pdb, _ = ghost_frag.fragment_db.paired_frags[ijk]\n    #     trnsfmd_fragment = fragment_pdb.get_transformed_copy(*ghost_frag.transformation)\n    # write_frag_match_info_file(ghost_frag=ghost_frag, matched_frag=surface_frag,\n    #                                     overlap_error=z_value_from_match_score(match_score),\n    #                                     match_number=match_count, out_path=out_path)\n    return file_path\n</code></pre>"},{"location":"reference/structure/model/#structure.model.Pose.debug_pose","title":"debug_pose","text":"<pre><code>debug_pose(out_dir: AnyStr = os.getcwd(), tag: str = None)\n</code></pre> <p>Write out all Structure objects for the Pose PDB</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def debug_pose(self, out_dir: AnyStr = os.getcwd(), tag: str = None):\n    \"\"\"Write out all Structure objects for the Pose PDB\"\"\"\n    entity_debug_path = os.path.join(out_dir, f'{f\"{tag}_\" if tag else \"\"}POSE_DEBUG_Entities_{self.name}.pdb')\n    with open(entity_debug_path, 'w') as f:\n        available_chain_ids = chain_id_generator()\n        for entity_idx, entity in enumerate(self.entities, 1):\n            # f.write(f'REMARK 999   Entity {entity_idx} - ID {entity.name}\\n')\n            # entity.write(file_handle=f, chain_id=next(available_chain_ids))\n            for chain_idx, chain in enumerate(entity.chains, 1):\n                f.write(f'REMARK 999   Entity {entity_idx} - ID {entity.name}   '\n                        f'Chain {chain_idx} - ID {chain.chain_id}\\n')\n                chain.write(file_handle=f, chain_id=next(available_chain_ids))\n\n    debug_path = os.path.join(out_dir, f'{f\"{tag}_\" if tag else \"\"}POSE_DEBUG_{self.name}.pdb')\n    assembly_debug_path = os.path.join(out_dir, f'{f\"{tag}_\" if tag else \"\"}POSE_DEBUG_Assembly_{self.name}.pdb')\n\n    self.log.critical(f'Wrote debugging Pose Entities to: {entity_debug_path}')\n    self.write(out_path=debug_path)\n    self.log.critical(f'Wrote debugging Pose to: {debug_path}')\n    self.write(assembly=True, out_path=assembly_debug_path)\n    self.log.critical(f'Wrote debugging Pose assembly to: {assembly_debug_path}')\n</code></pre>"},{"location":"reference/structure/model/#structure.model.softmax","title":"softmax","text":"<pre><code>softmax(x: ndarray) -&gt; ndarray\n</code></pre> <p>Take the softmax operation from an input array</p> <p>Parameters:</p> <ul> <li> <code>x</code>             (<code>ndarray</code>)         \u2013          <p>The array to calculate softmax on. Uses the axis=-1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The array with a softmax performed</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def softmax(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Take the softmax operation from an input array\n\n    Args:\n        x: The array to calculate softmax on. Uses the axis=-1\n\n    Returns:\n        The array with a softmax performed\n    \"\"\"\n    input_exp = np.exp(x)\n    return input_exp / input_exp.sum(axis=-1, keepdims=True)\n</code></pre>"},{"location":"reference/structure/model/#structure.model.split_residue_pairs","title":"split_residue_pairs","text":"<pre><code>split_residue_pairs(interface_pairs: list[tuple[Residue, Residue]]) -&gt; tuple[list[Residue], list[Residue]]\n</code></pre> <p>Used to split Residue pairs, take the set, then sort by Residue.number, and return pairs separated by index</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def split_residue_pairs(interface_pairs: list[tuple[Residue, Residue]]) -&gt; tuple[list[Residue], list[Residue]]:\n    \"\"\"Used to split Residue pairs, take the set, then sort by Residue.number, and return pairs separated by index\"\"\"\n    if interface_pairs:\n        residues1, residues2 = zip(*interface_pairs)\n        return sorted(set(residues1), key=lambda residue: residue.number), \\\n            sorted(set(residues2), key=lambda residue: residue.number)\n    else:\n        return [], []\n</code></pre>"},{"location":"reference/structure/model/#structure.model.split_number_pairs_and_sort","title":"split_number_pairs_and_sort","text":"<pre><code>split_number_pairs_and_sort(pairs: list[tuple[int, int]]) -&gt; tuple[list, list]\n</code></pre> <p>Used to split integer pairs and sort, and return pairs separated by index</p> Source code in <code>symdesign/structure/model.py</code> <pre><code>def split_number_pairs_and_sort(pairs: list[tuple[int, int]]) -&gt; tuple[list, list]:\n    \"\"\"Used to split integer pairs and sort, and return pairs separated by index\"\"\"\n    if pairs:\n        numbers1, numbers2 = zip(*pairs)\n        return sorted(set(numbers1), key=int), sorted(set(numbers2), key=int)\n    else:\n        return [], []\n</code></pre>"},{"location":"reference/structure/model/#structure.model.parse_cryst_record","title":"parse_cryst_record","text":"<pre><code>parse_cryst_record(cryst_record: str) -&gt; tuple[list[float], str]\n</code></pre> <p>Get the unit cell length, height, width, and angles alpha, beta, gamma and the space group</p> <p>Parameters:</p> <ul> <li> <code>cryst_record</code>             (<code>str</code>)         \u2013          <p>The CRYST1 record as found in .pdb file format</p> </li> </ul> Source code in <code>symdesign/structure/model.py</code> <pre><code>def parse_cryst_record(cryst_record: str) -&gt; tuple[list[float], str]:\n    \"\"\"Get the unit cell length, height, width, and angles alpha, beta, gamma and the space group\n\n    Args:\n        cryst_record: The CRYST1 record as found in .pdb file format\n    \"\"\"\n    try:\n        cryst, a, b, c, ang_a, ang_b, ang_c, *space_group = cryst_record.split()\n        # a = [6:15], b = [15:24], c = [24:33], ang_a = [33:40], ang_b = [40:47], ang_c = [47:54]\n    except ValueError:  # split() or unpacking went wrong\n        a = b = c = ang_a = ang_b = ang_c = 0\n\n    return list(map(float, [a, b, c, ang_a, ang_b, ang_c])), cryst_record[55:66].strip()\n</code></pre>"},{"location":"reference/structure/sequence/","title":"sequence","text":""},{"location":"reference/structure/sequence/#structure.sequence.blank_profile_entry","title":"blank_profile_entry  <code>module-attribute</code>","text":"<pre><code>blank_profile_entry = copy()\n</code></pre> <p>{utils.profile_keys, repeat(0))}</p>"},{"location":"reference/structure/sequence/#structure.sequence.aa_nan_counts_alph3","title":"aa_nan_counts_alph3  <code>module-attribute</code>","text":"<pre><code>aa_nan_counts_alph3 = dict(zip(protein_letters_alph3, repeat(nan)))\n</code></pre> <p>{protein_letters_alph3, repeat(numpy.nan))}</p>"},{"location":"reference/structure/sequence/#structure.sequence.nan_profile_entry","title":"nan_profile_entry  <code>module-attribute</code>","text":"<pre><code>nan_profile_entry = copy()\n</code></pre> <p>{utils.profile_keys, repeat(numpy.nan))}</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile","title":"Profile","text":"<pre><code>Profile(entries: Iterable[ProfileEntry], dtype: str = 'profile', **kwargs)\n</code></pre> <p>             Bases: <code>UserList[ProfileEntry]</code></p> <p>Parameters:</p> <ul> <li> <code>entries</code>             (<code>Iterable[ProfileEntry]</code>)         \u2013          <p>The per-residue entries to create the instance</p> </li> <li> <code>dtype</code>             (<code>str</code>, default:                 <code>'profile'</code> )         \u2013          <p>The datatype of the profile.</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def __init__(self, entries: Iterable[ProfileEntry], dtype: str = 'profile', **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        entries: The per-residue entries to create the instance\n        dtype: The datatype of the profile.\n        **kwargs:\n    \"\"\"\n    super().__init__(initlist=entries, **kwargs)  # initlist sets UserList.data\n    self.dtype = dtype\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: list[ProfileEntry]\n</code></pre> <p>[{'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...},  'type': 'W', 'info': 3.20, 'weight': 0.73}, {}, ...]</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.available_keys","title":"available_keys  <code>property</code>","text":"<pre><code>available_keys: tuple[Any, ...]\n</code></pre> <p>Returns the available ProfileEntry keys that are present in the Profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.lods","title":"lods  <code>property</code>","text":"<pre><code>lods: list[AminoAcidDistribution]\n</code></pre> <p>The log of odds values, given for each amino acid type, for each entry in the Profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.types","title":"types  <code>property</code>","text":"<pre><code>types: list[protein_letters_literal]\n</code></pre> <p>The amino acid type, for each entry in the Profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.weights","title":"weights  <code>property</code>","text":"<pre><code>weights: list[float]\n</code></pre> <p>The weight assigned to each entry in the Profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.info","title":"info  <code>property</code>","text":"<pre><code>info: list[float]\n</code></pre> <p>The information present for each entry in the Profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.as_array","title":"as_array","text":"<pre><code>as_array(alphabet: str = protein_letters_alph1, lod: bool = False) -&gt; ndarray\n</code></pre> <p>Convert the Profile into a numeric array</p> <p>Parameters:</p> <ul> <li> <code>alphabet</code>             (<code>str</code>, default:                 <code>protein_letters_alph1</code> )         \u2013          <p>The amino acid alphabet to use. Array values will be returned in this order</p> </li> <li> <code>lod</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return the array for the log of odds values</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The numerically encoded pssm where each entry along axis 0 is the position, and the entries on axis 1 are the frequency data at every indexed amino acid. Indices are according to the specified amino acid alphabet, i.e. array([[0.1, 0.01, 0.12, ...], ...])</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def as_array(self, alphabet: str = protein_letters_alph1, lod: bool = False) -&gt; np.ndarray:\n    \"\"\"Convert the Profile into a numeric array\n\n    Args:\n        alphabet: The amino acid alphabet to use. Array values will be returned in this order\n        lod: Whether to return the array for the log of odds values\n\n    Returns:\n        The numerically encoded pssm where each entry along axis 0 is the position, and the entries on axis 1 are\n            the frequency data at every indexed amino acid. Indices are according to the specified amino acid\n            alphabet, i.e. array([[0.1, 0.01, 0.12, ...], ...])\n    \"\"\"\n    if lod:\n        if self.lods:\n            data_source = self.lods\n        else:\n            raise ValueError(\n                f\"There aren't any values available for {self.__class__.__name__}.lods\")\n    else:\n        data_source = self\n\n    return np.array([[position_info[aa] for aa in alphabet]\n                     for position_info in data_source], dtype=np.float32)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.Profile.write","title":"write","text":"<pre><code>write(file_name: AnyStr = None, name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Create a PSI-BLAST format PSSM file from a PSSM dictionary. Assumes residue numbering is 1 to last entry</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The explicit name of the file</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the file. Will be used as the default file_name base name if file_name not provided</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output the file. Only used if file_name not explicitly provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>Disk location of newly created .pssm file</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def write(self, file_name: AnyStr = None, name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Create a PSI-BLAST format PSSM file from a PSSM dictionary. Assumes residue numbering is 1 to last entry\n\n    Args:\n        file_name: The explicit name of the file\n        name: The name of the file. Will be used as the default file_name base name if file_name not provided\n        out_dir: The location on disk to output the file. Only used if file_name not explicitly provided\n\n    Returns:\n        Disk location of newly created .pssm file\n    \"\"\"\n    # Format the Profile according to the write_pssm_file mechanism\n    # This takes the shape of 1 to the last entry\n    data: list[list[float]] = [[position_info[aa] for aa in protein_letters_alph3] for position_info in self]\n    if self.dtype == 'fragment':\n        # Need to convert np.nan to zeros\n        data = [[0 if np.nan else num for num in entry] for entry in data]\n        logger.warning(f'Converting {self.dtype} type Profile np.nan values to 0.0')\n\n    # Find out if the pssm has values expressed as frequencies (percentages) or as counts and modify accordingly\n    lods = self.lods\n    if isinstance(lods[0]['A'], float):\n        separation1 = \" \" * 4\n    else:\n        separation1 = \" \" * 3\n        # lod_freq = True\n    # if type(pssm[first_key]['A']) == float:\n    #     counts_freq = True\n\n    if file_name is None:\n        if name is None:\n            raise ValueError(\n                f\"Must provide argument 'file_name' or 'name' as a str to {self.write.__name__}\")\n        else:\n            file_name = os.path.join(out_dir, name)\n\n    if os.path.splitext(file_name)[-1] == '':  # No extension\n        file_name = f'{file_name}.pssm'\n\n    with open(file_name, 'w') as f:\n        f.write(f'\\n\\n{\" \" * 12}{separation1.join(protein_letters_alph3)}'\n                f'{separation1}{(\" \" * 3).join(protein_letters_alph3)}\\n')\n        for residue_number, (entry, lod, _type, info, weight) in enumerate(\n                zip(data, lods, self.types, self.info, self.weights), 1):\n            if isinstance(lod['A'], float):  # relevant for favor_fragment\n                lod_string = \\\n                    ' '.join(f'{lod[aa]:&gt;4.2f}' for aa in protein_letters_alph3) + ' '\n            else:\n                lod_string = \\\n                    ' '.join(f'{lod[aa]:&gt;3d}' for aa in protein_letters_alph3) + ' '\n\n            if isinstance(entry[0], float):  # relevant for freq calculations\n                counts_string = ' '.join(f'{floor(value * 100):&gt;3.0f}' for value in entry) + ' '\n            else:\n                counts_string = ' '.join(f'{value:&gt;3d}' for value in entry) + ' '\n\n            f.write(f'{residue_number:&gt;5d} {_type:1s}   {lod_string:80s} {counts_string:80s} '\n                    f'{round(info, 4):4.2f} {round(weight, 4):4.2f}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity","title":"GeneEntity","text":"<pre><code>GeneEntity(**kwargs)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Contains the sequence information for a ContainsResidues.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # GeneEntity\n    self._evolutionary_profile = {}  # position specific scoring matrix\n    self.h_fields = None\n    self.j_couplings = None\n    self.profile = {}  # design/structure specific scoring matrix\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.log","title":"log  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>log: Logger\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.sequence","title":"sequence  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>sequence: str\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.reference_sequence","title":"reference_sequence  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>reference_sequence: str\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.number_of_residues","title":"number_of_residues  <code>property</code>","text":"<pre><code>number_of_residues: int\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.evolutionary_profile","title":"evolutionary_profile  <code>property</code> <code>writable</code>","text":"<pre><code>evolutionary_profile: dict\n</code></pre> <p>Access the evolutionary_profile</p>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.msa","title":"msa  <code>property</code> <code>writable</code>","text":"<pre><code>msa: MultipleSequenceAlignment | None\n</code></pre> <p>The MultipleSequenceAlignment object for the instance</p>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.sequence_numeric","title":"sequence_numeric  <code>property</code>","text":"<pre><code>sequence_numeric: ndarray\n</code></pre> <p>Return the sequence as an integer array (number_of_residuces, alphabet_length) of the amino acid characters</p> <p>Maps \"ACDEFGHIKLMNPQRSTVWY-\" to the resulting index</p>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.hydrophobic_collapse","title":"hydrophobic_collapse","text":"<pre><code>hydrophobic_collapse(**kwargs) -&gt; array\n</code></pre> <p>Return the hydrophobic collapse for the Sequence</p> <p>Other Parameters:</p> <ul> <li> <code>hydrophobicity</code>         \u2013          <p>int = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILYVW), or provide one with 'custom' keyword argument</p> </li> <li> <code>custom</code>         \u2013          <p>mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs</p> </li> <li> <code>alphabet_type</code>         \u2013          <p>alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters</p> </li> <li> <code>lower_window</code>         \u2013          <p>int = 3 \u2013 The smallest window used to measure</p> </li> <li> <code>upper_window</code>         \u2013          <p>int = 9 \u2013 The largest window used to measure</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def hydrophobic_collapse(self, **kwargs) -&gt; np.array:\n    \"\"\"Return the hydrophobic collapse for the Sequence\n\n    Keyword Args:\n        hydrophobicity: int = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV),\n            'expanded' (FMILYVW), or provide one with 'custom' keyword argument\n        custom: mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs\n        alphabet_type: alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters\n        lower_window: int = 3 \u2013 The smallest window used to measure\n        upper_window: int = 9 \u2013 The largest window used to measure\n    \"\"\"\n    try:\n        return self._hydrophobic_collapse\n    except AttributeError:\n        self._hydrophobic_collapse = metrics.hydrophobic_collapse_index(self.sequence, **kwargs)\n        return self._hydrophobic_collapse\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.add_evolutionary_profile","title":"add_evolutionary_profile","text":"<pre><code>add_evolutionary_profile(file: AnyStr = None, out_dir: AnyStr = os.getcwd(), profile_source: alignment_programs_literal = putils.hhblits, force: bool = False, **kwargs)\n</code></pre> <p>Add the evolutionary profile to the GeneEntity. If the profile isn't provided, it is generated through search of homologous protein sequences using the profile_source argument</p> <p>Parameters:</p> <ul> <li> <code>file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>Location where profile file should be loaded from</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Location where sequence files should be written</p> </li> <li> <code>profile_source</code>             (<code>alignment_programs_literal</code>, default:                 <code>hhblits</code> )         \u2013          <p>One of 'hhblits' or 'psiblast'</p> </li> <li> <code>force</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to force generation of a new profile</p> </li> </ul> Sets <p>self.evolutionary_profile (ProfileDict)</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def add_evolutionary_profile(self, file: AnyStr = None, out_dir: AnyStr = os.getcwd(),\n                             profile_source: alignment_programs_literal = putils.hhblits, force: bool = False,\n                             **kwargs):\n    \"\"\"Add the evolutionary profile to the GeneEntity. If the profile isn't provided, it is generated through search\n    of homologous protein sequences using the profile_source argument\n\n    Args:\n        file: Location where profile file should be loaded from\n        out_dir: Location where sequence files should be written\n        profile_source: One of 'hhblits' or 'psiblast'\n        force: Whether to force generation of a new profile\n\n    Sets:\n        self.evolutionary_profile (ProfileDict)\n    \"\"\"\n    if profile_source not in alignment_programs:  # [putils.hhblits, 'psiblast']:\n        raise ValueError(\n            f'{self.add_evolutionary_profile.__name__}: Profile generation only possible from '\n            f'{\", \".join(alignment_programs)}, not {profile_source}')\n\n    if file is not None:\n        pssm_file = file\n    else:  # Check to see if the files of interest already exist\n        # Extract/Format Sequence Information. SEQRES is prioritized if available\n        name = self.name\n        sequence_file = self.write_sequence_to_fasta(out_dir=out_dir)\n        temp_file = Path(out_dir, f'{name}.hold')\n        pssm_file = os.path.join(out_dir, f'{name}.hmm')\n        if not os.path.exists(pssm_file) or force:\n            if not os.path.exists(temp_file):  # No work on this pssm file has been initiated\n                # Create blocking file to prevent excess work\n                with open(temp_file, 'w') as f:\n                    self.log.info(f\"Fetching '{name}' sequence data\")\n\n                self.log.info(f'Generating Evolutionary Profile for {name}')\n                getattr(self, profile_source)(sequence_file=sequence_file, out_dir=out_dir)\n                temp_file.unlink(missing_ok=True)\n                # if os.path.exists(temp_file):\n                #     os.remove(temp_file)\n            else:  # Block is in place, another process is working\n                self.log.info(f\"Waiting for '{name}' profile generation...\")\n                while not os.path.exists(pssm_file):\n                    if int(time.time()) - int(os.path.getmtime(temp_file)) &gt; 5400:  # &gt; 1 hr 30 minutes have passed\n                        # os.remove(temp_file)\n                        temp_file.unlink(missing_ok=True)\n                        raise StructureException(\n                            f'{self.add_evolutionary_profile.__name__}: Generation of the profile for {name} '\n                            'took longer than the time limit\\nKilled')\n                    time.sleep(20)\n\n    # Set self.evolutionary_profile\n    self.evolutionary_profile = parse_hhblits_pssm(pssm_file)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.create_null_profile","title":"create_null_profile","text":"<pre><code>create_null_profile(nan: bool = False, zero_index: bool = False, **kwargs) -&gt; ProfileDict\n</code></pre> <p>Make a blank profile</p> <p>Parameters:</p> <ul> <li> <code>nan</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to fill the null profile with np.nan</p> </li> <li> <code>zero_index</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>bool = False - If True, return the dictionary with zero indexing</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ProfileDict</code>         \u2013          <p>Dictionary containing profile information with keys as the index (zero or one-indexed), values as PSSM</p> </li> <li> <code>Ex</code> (            <code>ProfileDict</code> )        \u2013          <p>{1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},  2: {}, ...}</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def create_null_profile(self, nan: bool = False, zero_index: bool = False, **kwargs) -&gt; ProfileDict:\n    \"\"\"Make a blank profile\n\n    Args:\n        nan: Whether to fill the null profile with np.nan\n        zero_index: bool = False - If True, return the dictionary with zero indexing\n\n    Returns:\n        Dictionary containing profile information with keys as the index (zero or one-indexed), values as PSSM\n        Ex: {1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},\n             2: {}, ...}\n    \"\"\"\n    offset = 0 if zero_index else ZERO_OFFSET\n\n    if nan:\n        _profile_entry = nan_profile_entry\n    else:\n        _profile_entry = blank_profile_entry\n\n    profile = {residue: _profile_entry.copy()\n               for residue in range(offset, offset + self.number_of_residues)}\n\n    for residue_data, residue_type in zip(profile.values(), self.sequence):\n        residue_data['type'] = residue_type\n\n    return profile\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.create_null_entries","title":"create_null_entries  <code>staticmethod</code>","text":"<pre><code>create_null_entries(entry_numbers: Iterable[int], nan: bool = False, **kwargs) -&gt; ProfileDict\n</code></pre> <p>Make a blank profile</p> <p>Parameters:</p> <ul> <li> <code>entry_numbers</code>             (<code>Iterable[int]</code>)         \u2013          <p>The numbers to generate null entries for</p> </li> <li> <code>nan</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to fill the null profile with np.nan</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ProfileDict</code>         \u2013          <p>Dictionary containing profile information with the specified entries as the index, values as PSSM</p> </li> <li> <code>Ex</code> (            <code>ProfileDict</code> )        \u2013          <p>{1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'info': 3.20, 'weight': 0.73},  2: {}, ...}</p> </li> <li> <code>ProfileDict</code>         \u2013          <p>Importantly, there is no 'type' key. This must be added</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>@staticmethod\ndef create_null_entries(entry_numbers: Iterable[int], nan: bool = False, **kwargs) -&gt; ProfileDict:\n    \"\"\"Make a blank profile\n\n    Args:\n        entry_numbers: The numbers to generate null entries for\n        nan: Whether to fill the null profile with np.nan\n\n    Returns:\n        Dictionary containing profile information with the specified entries as the index, values as PSSM\n        Ex: {1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'info': 3.20, 'weight': 0.73},\n             2: {}, ...}\n        Importantly, there is no 'type' key. This must be added\n    \"\"\"\n    # offset = 0 if zero_index else ZERO_OFFSET\n\n    if nan:\n        _profile_entry = nan_profile_entry\n    else:\n        _profile_entry = blank_profile_entry\n\n    return {entry: _profile_entry.copy() for entry in entry_numbers}\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.hhblits","title":"hhblits","text":"<pre><code>hhblits(out_dir: AnyStr = os.getcwd(), **kwargs) -&gt; list[str] | None\n</code></pre> <p>Generate a position specific scoring matrix from hhblits using Hidden Markov Models</p> <p>Parameters:</p> <ul> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Disk location where generated file should be written</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>sequence_file</code>         \u2013          <p>AnyStr = None - The file containing the sequence to use</p> </li> <li> <code>threads</code>         \u2013          <p>Number of cpu's to use for the process</p> </li> <li> <code>return_command</code>         \u2013          <p>Whether to simply return the hhblits command</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str] | None</code>         \u2013          <p>The command if return_command is True, otherwise None</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def hhblits(self, out_dir: AnyStr = os.getcwd(), **kwargs) -&gt; list[str] | None:\n    \"\"\"Generate a position specific scoring matrix from hhblits using Hidden Markov Models\n\n    Args:\n        out_dir: Disk location where generated file should be written\n\n    Keyword Args:\n        sequence_file: AnyStr = None - The file containing the sequence to use\n        threads: Number of cpu's to use for the process\n        return_command: Whether to simply return the hhblits command\n\n    Returns:\n        The command if return_command is True, otherwise None\n    \"\"\"\n    result = hhblits(self.name, out_dir=out_dir, **kwargs)\n\n    if result:  # return_command is True\n        return result\n    # Otherwise, make alignment file(s)\n    name = self.name\n    # Set file attributes according to logic of hhblits()\n    a3m_file = os.path.join(out_dir, f'{name}.a3m')\n    msa_file = os.path.join(out_dir, f'{name}.sto')\n    fasta_msa = os.path.join(out_dir, f'{name}.fasta')\n    # Preferred alignment type\n    p = subprocess.Popen([putils.reformat_msa_exe_path, a3m_file, msa_file, '-num', '-uc'])\n    p.communicate()\n    p = subprocess.Popen([putils.reformat_msa_exe_path, a3m_file, fasta_msa, '-M', 'first', '-r'])\n    p.communicate()\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.add_msa_from_file","title":"add_msa_from_file","text":"<pre><code>add_msa_from_file(msa_file: AnyStr, file_format: msa_supported_types_literal = 'stockholm')\n</code></pre> <p>Add a multiple sequence alignment to the profile. Handles correct sizing of the MSA</p> <p>Parameters:</p> <ul> <li> <code>msa_file</code>             (<code>AnyStr</code>)         \u2013          <p>The multiple sequence alignment file to add to the Entity</p> </li> <li> <code>file_format</code>             (<code>msa_supported_types_literal</code>, default:                 <code>'stockholm'</code> )         \u2013          <p>The file type to read the multiple sequence alignment</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def add_msa_from_file(self, msa_file: AnyStr, file_format: msa_supported_types_literal = 'stockholm'):\n    \"\"\"Add a multiple sequence alignment to the profile. Handles correct sizing of the MSA\n\n    Args:\n        msa_file: The multiple sequence alignment file to add to the Entity\n        file_format: The file type to read the multiple sequence alignment\n    \"\"\"\n    if file_format == 'stockholm':\n        constructor = MultipleSequenceAlignment.from_stockholm\n    elif file_format == 'fasta':\n        constructor = MultipleSequenceAlignment.from_fasta\n    else:\n        raise ValueError(\n            f\"The file format '{file_format}' isn't an available format. Available formats include \"\n            f\"{msa_supported_types}\")\n\n    self.msa = constructor(msa_file)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.collapse_profile","title":"collapse_profile","text":"<pre><code>collapse_profile(msa_file: AnyStr = None, **kwargs) -&gt; ndarray\n</code></pre> <p>Make a profile out of the hydrophobic collapse index (HCI) for each sequence in a multiple sequence alignment</p> <p>Takes ~5-10 seconds depending on the size of the msa</p> <p>Calculate HCI for each sequence in the MSA (which are different lengths). This is the Hydro Collapse array. For each sequence, make a Gap mask, with full shape (length, number_of_residues) to account for gaps in each sequence. Apply the mask using a map between the Gap mask and the Hydro Collapse array. Finally, drop the columns from the array that are gaps in the reference sequence.</p> <p>iter array   -   Gap mask      -       Hydro Collapse array     -     Aligned HCI     - -     Final HCI</p> <p>iter - - - - - - 0 is gap    - -     compute for each     -     account for gaps   -  (drop idx 2)</p> <p>it 1 2 3 4  - - 0 | 1 | 2 - - - - - - - 0 | 1 | 2 - - - - - - 0 | 1 | 2 - - - - - - - 0 | 1 | 3 | ... N</p> <p>0 0 1 2 2  - - 1 | 1 | 0 - - -   - - - 0.5 0.2 0.5 - -  = - - 0.5 0.2 0.0 -  -&gt;   - - 0.5 0.2 0.4 ... 0.3</p> <p>1 0 0 1 2  - - 0 | 1 | 1 - - -   - - - 0.4 0.7 0.4 - -  = - - 0.0 0.4 0.7 -  -&gt;   - - 0.0 0.4 0.4 ... 0.1</p> <p>2 0 0 1 2  - - 0 | 1 | 1 - - -   - - - 0.3 0.6 0.3 - -  = - - 0.0 0.3 0.6 -  -&gt;   - - 0.0 0.3 0.4 ... 0.0</p> <p>Where index 0 is the MSA query sequence</p> <p>After iteration cumulative summation, the iterator is multiplied by the gap mask. Next the Hydro Collapse array value is accessed by the gaped iterator. This places the Hydro Collapse array or np.nan (if there is a 0 index, i.e. a gap). After calculation, the element at index 2 is dropped from the array when the aligned sequence gaps are removed. Finally, only the indices of the query sequence are left in the profile, essentially giving the HCI for each sequence in the native context, adjusted to the specific context of the protein sequence at hand</p> <p>Parameters:</p> <ul> <li> <code>msa_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The multiple sequence alignment file to use for collapse. Will use .msa attribute if not provided</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>file_format</code>         \u2013          <p>msa_supported_types_literal = 'stockholm' - The file type to read the multiple sequence alignment</p> </li> <li> <code>hydrophobicity</code>         \u2013          <p>int = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV), 'expanded' (FMILYVW), or provide one with 'custom' keyword argument</p> </li> <li> <code>custom</code>         \u2013          <p>mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value pairs</p> </li> <li> <code>alphabet_type</code>         \u2013          <p>alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer characters</p> </li> <li> <code>lower_window</code>         \u2013          <p>int = 3 \u2013 The smallest window used to measure</p> </li> <li> <code>upper_window</code>         \u2013          <p>int = 9 \u2013 The largest window used to measure</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Array with shape (length, number_of_residues) containing the hydrophobic collapse values for per-residue, per-sequence in the profile. The \"query\" sequence from the MultipleSequenceAlignment.query is located at index 0 on axis=0</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def collapse_profile(self, msa_file: AnyStr = None, **kwargs) -&gt; np.ndarray:\n    \"\"\"Make a profile out of the hydrophobic collapse index (HCI) for each sequence in a multiple sequence alignment\n\n    Takes ~5-10 seconds depending on the size of the msa\n\n    Calculate HCI for each sequence in the MSA (which are different lengths). This is the Hydro Collapse array. For\n    each sequence, make a Gap mask, with full shape (length, number_of_residues) to account for gaps in\n    each sequence. Apply the mask using a map between the Gap mask and the Hydro Collapse array. Finally, drop the\n    columns from the array that are gaps in the reference sequence.\n\n    iter array   -   Gap mask      -       Hydro Collapse array     -     Aligned HCI     - -     Final HCI\n\n    ------------\n\n    iter - - - - - - 0 is gap    - -     compute for each     -     account for gaps   -  (drop idx 2)\n\n    it 1 2 3 4  - - 0 | 1 | 2 - - - - - - - 0 | 1 | 2 - - - - - - 0 | 1 | 2 - - - - - - - 0 | 1 | 3 | ... N\n\n    0 0 1 2 2  - - 1 | 1 | 0 - - -   - - - 0.5 0.2 0.5 - -  = - - 0.5 0.2 0.0 -  -&gt;   - - 0.5 0.2 0.4 ... 0.3\n\n    1 0 0 1 2  - - 0 | 1 | 1 - - -   - - - 0.4 0.7 0.4 - -  = - - 0.0 0.4 0.7 -  -&gt;   - - 0.0 0.4 0.4 ... 0.1\n\n    2 0 0 1 2  - - 0 | 1 | 1 - - -   - - - 0.3 0.6 0.3 - -  = - - 0.0 0.3 0.6 -  -&gt;   - - 0.0 0.3 0.4 ... 0.0\n\n    Where index 0 is the MSA query sequence\n\n    After iteration cumulative summation, the iterator is multiplied by the gap mask. Next the Hydro Collapse array\n    value is accessed by the gaped iterator. This places the Hydro Collapse array or np.nan (if there is a 0 index,\n    i.e. a gap). After calculation, the element at index 2 is dropped from the array when the aligned sequence gaps\n    are removed. Finally, only the indices of the query sequence are left in the profile, essentially giving the HCI\n    for each sequence in the native context, adjusted to the specific context of the protein sequence at hand\n\n    Args:\n        msa_file: The multiple sequence alignment file to use for collapse. Will use .msa attribute if not provided\n\n    Keyword Args:\n        file_format: msa_supported_types_literal = 'stockholm' - The file type to read the multiple sequence\n            alignment\n        hydrophobicity: int = 'standard' \u2013 The hydrophobicity scale to consider. Either 'standard' (FILV),\n            'expanded' (FMILYVW), or provide one with 'custom' keyword argument\n        custom: mapping[str, float | int] = None \u2013 A user defined mapping of amino acid type, hydrophobicity value\n            pairs\n        alphabet_type: alphabet_types = None \u2013 The amino acid alphabet if the sequence consists of integer\n            characters\n        lower_window: int = 3 \u2013 The smallest window used to measure\n        upper_window: int = 9 \u2013 The largest window used to measure\n\n    Returns:\n        Array with shape (length, number_of_residues) containing the hydrophobic collapse values for\n            per-residue, per-sequence in the profile. The \"query\" sequence from the MultipleSequenceAlignment.query\n            is located at index 0 on axis=0\n    \"\"\"\n    try:\n        return self._collapse_profile\n    except AttributeError:\n        msa = self.msa\n        if not msa:\n            self.add_msa_from_file(msa_file)\n            msa = self.msa\n\n        # Make the output array. Use one additional length to add np.nan value at the 0 index for gaps\n        evolutionary_collapse_np = np.zeros((msa.length, msa.number_of_positions + 1))\n        evolutionary_collapse_np[:, 0] = np.nan  # np.nan for all missing indices\n        for idx, sequence in enumerate(msa.sequences):\n            non_gaped_sequence = str(sequence).replace('-', '')\n            evolutionary_collapse_np[idx, 1:len(non_gaped_sequence) + 1] = \\\n                metrics.hydrophobic_collapse_index(non_gaped_sequence, **kwargs)\n        # Todo this should be possible now metrics.hydrophobic_collapse_index(self.msa.array)\n\n        msa_sequence_indices = msa.sequence_indices\n        iterator_np = np.cumsum(msa_sequence_indices, axis=1) * msa_sequence_indices\n        aligned_hci_np = np.take_along_axis(evolutionary_collapse_np, iterator_np, axis=1)\n        # Select only the query sequence indices\n        # sequence_hci_np = aligned_hci_np[:, self.msa.query_indices]\n        # print('aligned_hci_np', aligned_hci_np.shape, aligned_hci_np)\n        # print('self.msa.query_indices', self.msa.query_indices.shape, self.msa.query_indices)\n        self._collapse_profile = aligned_hci_np[:, msa.query_indices]\n        # self._collapse_profile = pd.DataFrame(aligned_hci_np[:, self.msa.query_indices],\n        #                                       columns=list(range(1, self.msa.query_length + 1)))  # One-indexed\n        # summary = pd.concat([sequence_hci_df, pd.concat([sequence_hci_df.mean(), sequence_hci_df.std()], axis=1,\n        #                                                 keys=['mean', 'std']).T])\n\n        return self._collapse_profile\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.direct_coupling_analysis","title":"direct_coupling_analysis","text":"<pre><code>direct_coupling_analysis(msa_file: AnyStr = None, **kwargs) -&gt; ndarray\n</code></pre> <p>Using boltzmann machine direct coupling analysis (bmDCA), score each sequence in an alignment based on the  statistical energy compared to the learned DCA model</p> <p>Parameters:</p> <ul> <li> <code>msa_file</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The multiple sequence alignment file to use for collapse. Will use .msa attribute if not provided</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>file_format</code>         \u2013          <p>msa_supported_types_literal = 'stockholm' - The file type to read the multiple sequence alignment</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Array with shape (length, number_of_residues) where the values are the energy for each residue/sequence based on direct coupling analysis parameters</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def direct_coupling_analysis(self, msa_file: AnyStr = None, **kwargs) -&gt; np.ndarray:\n    \"\"\"Using boltzmann machine direct coupling analysis (bmDCA), score each sequence in an alignment based on the\n     statistical energy compared to the learned DCA model\n\n    Args:\n        msa_file: The multiple sequence alignment file to use for collapse. Will use .msa attribute if not provided\n\n    Keyword Args:\n        file_format: msa_supported_types_literal = 'stockholm' - The file type to read the multiple sequence\n            alignment\n\n    Returns:\n        Array with shape (length, number_of_residues) where the values are the energy for each residue/sequence\n            based on direct coupling analysis parameters\n    \"\"\"\n    # Check if required attributes are present\n    _raise = False\n    missing_attrs = []\n    msa = self.msa\n    if not msa:\n        if msa_file:\n            self.add_msa_from_file(msa_file)\n            msa = self.msa\n        else:\n            missing_attrs.append('.msa')\n    h_fields = self.h_fields\n    if not h_fields:\n        missing_attrs.append('.h_fields')\n    j_couplings = self.j_couplings\n    if not j_couplings:\n        missing_attrs.append('.j_couplings')\n\n    if missing_attrs:\n        raise AttributeError(\n            f\"The required attribute(s) {', '.join(missing_attrs)} aren't available. Add to the \"\n            f'Entity before {self.direct_coupling_analysis.__name__}')\n\n    analysis_length = msa.query_length\n    idx_range = np.arange(analysis_length)\n    # h_fields = bmdca.load_fields(os.path.join(data_dir, '%s_bmDCA' % self.name, 'parameters_h_final.bin'))\n    # h_fields = h_fields.T  # this isn't required when coming in Fortran order, i.e. (21, analysis_length)\n    # sum the h_fields values for each sequence position in every sequence\n    h_values = h_fields[msa.numerical_alignment, idx_range[None, :]].sum(axis=1)\n    h_sum = h_values.sum(axis=1)\n\n    # coming in as a 4 dimension (analysis_length, analysis_length, alphabet_number, alphabet_number) ndarray\n    # j_couplings = bmdca.load_couplings(os.path.join(data_dir, '%s_bmDCA' % self.name, 'parameters_J_final.bin'))\n    i_idx = np.repeat(idx_range, analysis_length)\n    j_idx = np.tile(idx_range, analysis_length)\n    i_aa = np.repeat(msa.numerical_alignment, analysis_length)\n    j_aa = np.tile(msa.numerical_alignment, msa.query_length)\n    j_values = np.zeros((msa.length, len(i_idx)))\n    for idx in range(msa.length):\n        j_values[idx] = j_couplings[i_idx, j_idx, i_aa, j_aa]\n    # this mask is not necessary when the array comes in as a non-symmetry matrix. All i &gt; j result in 0 values...\n    # mask = np.triu(np.ones((analysis_length, analysis_length)), k=1).flatten()\n    # j_sum = j_values[:, mask].sum(axis=1)\n    # sum the j_values for every design (axis 0) at every residue position (axis 1)\n    j_values = np.array(np.split(j_values, 3, axis=1)).sum(axis=2).T\n    j_sum = j_values.sum(axis=1)\n    # couplings_idx = np.stack((i_idx, j_idx, i_aa, j_aa), axis=1)\n    # this stacks all arrays like so\n    #  [[[ i_idx1, i_idx2, ..., i_idxN],\n    #    [ j_idx1, j_idx2, ..., j_idxN],  &lt;- this is for one sequence\n    #    [ i_aa 1, i_aa 2, ..., i_aa N],\n    #    [ j_aa 1, j_aa 2, ..., j_aa N]],\n    #   [[NEXT SEQUENCE],\n    #    [\n    # this stacks all arrays the transpose, which would match the indexing style on j_couplings much better...\n    # couplings_idx = np.stack((i_idx, j_idx, i_aa, j_aa), axis=2)\n    # j_sum = np.zeros((self.msa.length, len(couplings_idx)))\n    # for idx in range(self.msa.length):\n    #     j_sum[idx] = j_couplings[couplings_idx[idx]]\n    # return -h_sum - j_sum\n    return -h_values - j_values\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.GeneEntity.write_sequence_to_fasta","title":"write_sequence_to_fasta","text":"<pre><code>write_sequence_to_fasta(dtype: sequence_type_literal = 'reference_sequence', file_name: AnyStr = None, name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write a sequence to a .fasta file with fasta format and return the file location. '.fasta' is appended if not specified in the name argument</p> <p>Parameters:</p> <ul> <li> <code>dtype</code>             (<code>sequence_type_literal</code>, default:                 <code>'reference_sequence'</code> )         \u2013          <p>The type of sequence to write. Can be the the keywords 'reference_sequence' or 'sequence'</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The explicit name of the file</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the sequence record. If not provided, the instance name will be used. Will be used as the default file_name base name if file_name not provided</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output the file. Only used if file_name not explicitly provided</p> </li> </ul> <p>Returns:     The path to the output file</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def write_sequence_to_fasta(self, dtype: sequence_type_literal = 'reference_sequence', file_name: AnyStr = None,\n                            name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write a sequence to a .fasta file with fasta format and return the file location.\n    '.fasta' is appended if not specified in the name argument\n\n    Args:\n        dtype: The type of sequence to write. Can be the the keywords 'reference_sequence' or 'sequence'\n        file_name: The explicit name of the file\n        name: The name of the sequence record. If not provided, the instance name will be used.\n            Will be used as the default file_name base name if file_name not provided\n        out_dir: The location on disk to output the file. Only used if file_name not explicitly provided\n    Returns:\n        The path to the output file\n    \"\"\"\n    if dtype in sequence_types:\n        # Get the attribute from the instance\n        sequence = getattr(self, dtype)\n    else:\n        raise ValueError(\n            f\"Couldn't find a sequence matching the {dtype=}\"\n        )\n\n    if name is None:\n        name = self.name\n    if file_name is None:\n        file_name = os.path.join(out_dir, name)\n        if not file_name.endswith('.fasta'):\n            file_name = f'{file_name}.fasta'\n\n    return write_sequence_to_fasta(sequence=sequence, name=name, file_name=file_name)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.sequence_to_one_hot","title":"sequence_to_one_hot","text":"<pre><code>sequence_to_one_hot(sequence: Sequence[str], translation_table: dict[str, int] = None, alphabet_order: int = 1) -&gt; ndarray\n</code></pre> <p>Convert a sequence into a numeric array</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str]</code>)         \u2013          <p>The sequence to encode</p> </li> <li> <code>translation_table</code>             (<code>dict[str, int]</code>, default:                 <code>None</code> )         \u2013          <p>If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order</p> </li> <li> <code>alphabet_order</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The alphabetical order of the amino acid alphabet. Can be either 1 or 3</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The one-hot encoded sequence with shape (sequence length, translation_table length)</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def sequence_to_one_hot(sequence: Sequence[str], translation_table: dict[str, int] = None,\n                        alphabet_order: int = 1) -&gt; np.ndarray:\n    \"\"\"Convert a sequence into a numeric array\n\n    Args:\n        sequence: The sequence to encode\n        translation_table: If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order\n        alphabet_order: The alphabetical order of the amino acid alphabet. Can be either 1 or 3\n\n    Returns:\n        The one-hot encoded sequence with shape (sequence length, translation_table length)\n    \"\"\"\n    numeric_sequence = sequence_to_numeric(sequence, translation_table, alphabet_order=alphabet_order)\n    if translation_table is None:\n        # Assumes that alphabet_order is used and there aren't missing letters...\n        num_entries = 20\n    else:\n        num_entries = len(translation_table)\n    one_hot = np.zeros((len(sequence), num_entries), dtype=np.int32)\n    try:\n        one_hot[:, numeric_sequence] = 1\n    except IndexError:  # Our assumption above was wrong\n        from symdesign import sequence as _seq\n        embedding = getattr(_seq, f'numerical_translation_alph{alphabet_order}_bytes') \\\n            if translation_table is None else translation_table\n        raise ValueError(\n            f\"Couldn't produce a proper one-hot encoding for the provided sequence embedding: {embedding}\")\n    return one_hot\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.sequence_to_numeric","title":"sequence_to_numeric","text":"<pre><code>sequence_to_numeric(sequence: Sequence[str], translation_table: dict[str, int] = None, alphabet_order: int = 1) -&gt; ndarray\n</code></pre> <p>Convert a sequence into a numeric array</p> <p>Parameters:</p> <ul> <li> <code>sequence</code>             (<code>Sequence[str]</code>)         \u2013          <p>The sequence to encode</p> </li> <li> <code>translation_table</code>             (<code>dict[str, int]</code>, default:                 <code>None</code> )         \u2013          <p>If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order</p> </li> <li> <code>alphabet_order</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The alphabetical order of the amino acid alphabet. Can be either 1 or 3</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The numerically encoded sequence where each entry along axis=0 is the indexed amino acid. Indices are according to the 1 letter alphabetical amino acid</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def sequence_to_numeric(sequence: Sequence[str], translation_table: dict[str, int] = None,\n                        alphabet_order: int = 1) -&gt; np.ndarray:\n    \"\"\"Convert a sequence into a numeric array\n\n    Args:\n        sequence: The sequence to encode\n        translation_table: If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order\n        alphabet_order: The alphabetical order of the amino acid alphabet. Can be either 1 or 3\n\n    Returns:\n        The numerically encoded sequence where each entry along axis=0 is the indexed amino acid. Indices are according\n            to the 1 letter alphabetical amino acid\n    \"\"\"\n    _array = np.array(list(sequence), np.string_)\n    if translation_table is not None:\n        return np.vectorize(translation_table.__getitem__)(_array)\n    else:\n        if alphabet_order == 1:\n            return np.vectorize(numerical_translation_alph1_bytes.__getitem__)(_array)\n        elif alphabet_order == 3:\n            raise NotImplementedError('Need to make the \"numerical_translation_alph3_bytes\" table')\n            return np.vectorize(numerical_translation_alph3_bytes.__getitem__)(_array)\n        else:\n            raise ValueError(\n                f\"The 'alphabet_order' {alphabet_order} isn't valid. Choose from either 1 or 3\")\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.sequences_to_numeric","title":"sequences_to_numeric","text":"<pre><code>sequences_to_numeric(sequences: Iterable[Sequence[str]], translation_table: dict[str, int] = None, alphabet_order: int = 1) -&gt; ndarray\n</code></pre> <p>Convert sequences into a numeric array</p> <p>Parameters:</p> <ul> <li> <code>sequences</code>             (<code>Iterable[Sequence[str]]</code>)         \u2013          <p>The sequences to encode</p> </li> <li> <code>translation_table</code>             (<code>dict[str, int]</code>, default:                 <code>None</code> )         \u2013          <p>If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order</p> </li> <li> <code>alphabet_order</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The alphabetical order of the amino acid alphabet. Can be either 1 or 3</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The numerically encoded sequence where each entry along axis=0 is the indexed amino acid. Indices are according to the 1 letter alphabetical amino acid</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def sequences_to_numeric(sequences: Iterable[Sequence[str]], translation_table: dict[str, int] = None,\n                         alphabet_order: int = 1) -&gt; np.ndarray:\n    \"\"\"Convert sequences into a numeric array\n\n    Args:\n        sequences: The sequences to encode\n        translation_table: If a translation table (in bytes) is provided, it will be used. If not, use alphabet_order\n        alphabet_order: The alphabetical order of the amino acid alphabet. Can be either 1 or 3\n\n    Returns:\n        The numerically encoded sequence where each entry along axis=0 is the indexed amino acid. Indices are according\n            to the 1 letter alphabetical amino acid\n    \"\"\"\n    _array = np.array([list(sequence) for sequence in sequences], np.string_)\n    if translation_table is not None:\n        return np.vectorize(translation_table.__getitem__)(_array)\n    else:\n        if alphabet_order == 1:\n            return np.vectorize(numerical_translation_alph1_bytes.__getitem__)(_array)\n        elif alphabet_order == 3:\n            raise NotImplementedError('Need to make the \"numerical_translation_alph3_bytes\" table')\n            return np.vectorize(numerical_translation_alph3_bytes.__getitem__)(_array)\n        else:\n            raise ValueError(\n                f\"The 'alphabet_order' {alphabet_order} isn't valid. Choose from either 1 or 3\")\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.pssm_as_array","title":"pssm_as_array","text":"<pre><code>pssm_as_array(pssm: ProfileDict, alphabet: str = protein_letters_alph1, lod: bool = False) -&gt; ndarray\n</code></pre> <p>Convert a position specific profile matrix into a numeric array</p> <p>Parameters:</p> <ul> <li> <code>pssm</code>             (<code>ProfileDict</code>)         \u2013          <p>{1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},       2: {}, ...}</p> </li> <li> <code>alphabet</code>             (<code>str</code>, default:                 <code>protein_letters_alph1</code> )         \u2013          <p>The amino acid alphabet to use. Array values will be returned in this order</p> </li> <li> <code>lod</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return the array for the log of odds values</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>The numerically encoded pssm where each entry along axis 0 is the position, and the entries on axis 1 are the frequency data at every indexed amino acid. Indices are according to the specified amino acid alphabet, i.e array([[0.1, 0.01, 0.12, ...], ...])</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def pssm_as_array(pssm: ProfileDict, alphabet: str = protein_letters_alph1, lod: bool = False) \\\n        -&gt; np.ndarray:\n    \"\"\"Convert a position specific profile matrix into a numeric array\n\n    Args:\n        pssm: {1: {'A': 0, 'R': 0, ..., 'lod': {'A': -5, 'R': -5, ...}, 'type': 'W', 'info': 3.20, 'weight': 0.73},\n                  2: {}, ...}\n        alphabet: The amino acid alphabet to use. Array values will be returned in this order\n        lod: Whether to return the array for the log of odds values\n\n    Returns:\n        The numerically encoded pssm where each entry along axis 0 is the position, and the entries on axis 1 are the\n            frequency data at every indexed amino acid. Indices are according to the specified amino acid alphabet,\n            i.e array([[0.1, 0.01, 0.12, ...], ...])\n    \"\"\"\n    if lod:\n        return np.array([[position_info['lod'][aa] for aa in alphabet]\n                         for position_info in pssm.values()], dtype=np.float32)\n    else:\n        return np.array([[position_info[aa] for aa in alphabet]\n                         for position_info in pssm.values()], dtype=np.float32)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.concatenate_profile","title":"concatenate_profile","text":"<pre><code>concatenate_profile(profiles: Iterable[Any], start_at: int = 1) -&gt; dict[int, Any]\n</code></pre> <p>Combine a list of profiles (parsed PSSMs) by incrementing the entry index for each additional profile</p> <p>Parameters:</p> <ul> <li> <code>profiles</code>             (<code>Iterable[Any]</code>)         \u2013          <p>The profiles to concatenate</p> </li> <li> <code>start_at</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>The integer to start the resulting dictionary at</p> </li> </ul> <p>Returns     The concatenated input profiles, make a concatenated PSSM         {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...}, 'type': 'W', 'info': 0.00,              'weight': 0.00}, ...}}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def concatenate_profile(profiles: Iterable[Any], start_at: int = 1) -&gt; dict[int, Any]:\n    \"\"\"Combine a list of profiles (parsed PSSMs) by incrementing the entry index for each additional profile\n\n    Args:\n        profiles: The profiles to concatenate\n        start_at: The integer to start the resulting dictionary at\n\n    Returns\n        The concatenated input profiles, make a concatenated PSSM\n            {1: {'A': 0.04, 'C': 0.12, ..., 'lod': {'A': -5, 'C': -9, ...}, 'type': 'W', 'info': 0.00,\n                 'weight': 0.00}, ...}}\n    \"\"\"\n    _count = count(start_at)\n    return {next(_count): position_profile for profile in profiles for position_profile in profile.values()}\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.write_pssm_file","title":"write_pssm_file","text":"<pre><code>write_pssm_file(pssm: ProfileDict, file_name: AnyStr = None, name: str = None, out_dir: AnyStr = os.getcwd()) -&gt; AnyStr | None\n</code></pre> <p>Create a PSI-BLAST format PSSM file from a PSSM dictionary. Assumes residue numbering is correct!</p> <p>Parameters:</p> <ul> <li> <code>pssm</code>             (<code>ProfileDict</code>)         \u2013          <p>A dictionary which has the keys: 'A', 'C', ... (all aa's), 'lod', 'type', 'info', 'weight'</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The explicit name of the file</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the file. Will be used as the default file_name base name if file_name not provided</p> </li> <li> <code>out_dir</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location on disk to output the file. Only used if file_name not explicitly provided</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>Disk location of newly created .pssm file</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def write_pssm_file(pssm: ProfileDict, file_name: AnyStr = None, name: str = None,\n                    out_dir: AnyStr = os.getcwd()) -&gt; AnyStr | None:\n    \"\"\"Create a PSI-BLAST format PSSM file from a PSSM dictionary. Assumes residue numbering is correct!\n\n    Args:\n        pssm: A dictionary which has the keys: 'A', 'C', ... (all aa's), 'lod', 'type', 'info', 'weight'\n        file_name: The explicit name of the file\n        name: The name of the file. Will be used as the default file_name base name if file_name not provided\n        out_dir: The location on disk to output the file. Only used if file_name not explicitly provided\n\n    Returns:\n        Disk location of newly created .pssm file\n    \"\"\"\n    if not pssm:\n        return None\n\n    # Find out if the pssm has values expressed as frequencies (percentages) or as counts and modify accordingly\n    if isinstance(list(pssm.values())[0]['lod']['A'], float):\n        separation1 = \" \" * 4\n    else:\n        separation1 = \" \" * 3\n\n    if file_name is None:\n        if name is None:\n            raise ValueError(f'Must provide argument \"file_name\" or \"name\" as a str to {write_sequences.__name__}')\n        else:\n            file_name = os.path.join(out_dir, name)\n\n    if os.path.splitext(file_name)[-1] == '':  # No extension\n        file_name = f'{file_name}.pssm'\n\n    with open(file_name, 'w') as f:\n        f.write(f'\\n\\n{\" \" * 12}{separation1.join(protein_letters_alph3)}'\n                f'{separation1}{(\" \" * 3).join(protein_letters_alph3)}\\n')\n        for residue_number, profile in pssm.items():\n            if isinstance(profile['lod']['A'], float):  # lod_freq:  # relevant for favor_fragment\n                lod_string = ' '.join(f'{profile[\"lod\"][aa]:&gt;4.2f}' for aa in protein_letters_alph3) \\\n                    + ' '\n            else:\n                lod_string = ' '.join(f'{profile[\"lod\"][aa]:&gt;3d}' for aa in protein_letters_alph3) \\\n                    + ' '\n\n            if isinstance(profile['A'], float):  # counts_freq: # relevant for freq calculations\n                counts_string = ' '.join(f'{floor(profile[aa] * 100):&gt;3.0f}' for aa in\n                                         protein_letters_alph3) \\\n                    + ' '\n            else:\n                counts_string = ' '.join(f'{profile[aa]:&gt;3d}' for aa in protein_letters_alph3) \\\n                    + ' '\n            f.write(f'{residue_number:&gt;5d} {profile[\"type\"]:1s}   {lod_string:80s} {counts_string:80s} '\n                    f'{round(profile.get(\"info\", 0.), 4):4.2f} {round(profile.get(\"weight\", 0.), 4):4.2f}\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.format_frequencies","title":"format_frequencies","text":"<pre><code>format_frequencies(frequency_list: list, flip: bool = False) -&gt; dict[str, dict[str, float]]\n</code></pre> <p>Format list of paired frequency data into parsable paired format</p> <p>Parameters:</p> <ul> <li> <code>frequency_list</code>             (<code>list</code>)         \u2013          <p>[(('D', 'A'), 0.0822), (('D', 'V'), 0.0685), ...]</p> </li> <li> <code>flip</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to invert the mapping of internal tuple</p> </li> </ul> <p>Returns:     {'A': {'S': 0.02, 'T': 0.12}, ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def format_frequencies(frequency_list: list, flip: bool = False) -&gt; dict[str, dict[str, float]]:\n    \"\"\"Format list of paired frequency data into parsable paired format\n\n    Args:\n        frequency_list: [(('D', 'A'), 0.0822), (('D', 'V'), 0.0685), ...]\n        flip: Whether to invert the mapping of internal tuple\n    Returns:\n        {'A': {'S': 0.02, 'T': 0.12}, ...}\n    \"\"\"\n    if flip:\n        i, j = 1, 0\n    else:\n        i, j = 0, 1\n    freq_d = {}\n    for tup in frequency_list:\n        aa_mapped = tup[0][i]  # 0\n        aa_paired = tup[0][j]  # 1\n        freq = tup[1]\n        if aa_mapped in freq_d:\n            freq_d[aa_mapped][aa_paired] = freq\n        else:\n            freq_d[aa_mapped] = {aa_paired: freq}\n\n    return freq_d\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.overlap_consensus","title":"overlap_consensus","text":"<pre><code>overlap_consensus(issm, aa_set)\n</code></pre> <p>Find the overlap constrained consensus sequence</p> <p>Parameters:</p> <ul> <li> <code>issm</code>             (<code>dict</code>)         \u2013          <p>{1: {'A': 0.1, 'C': 0.0, ...}, 14: {...}, ...}</p> </li> <li> <code>aa_set</code>             (<code>dict</code>)         \u2013          <p>{residue: {'A', 'I', 'M', 'V'}, ...}</p> </li> </ul> <p>Returns:     (dict): {23: 'T', 29: 'A', ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def overlap_consensus(issm, aa_set):  # UNUSED\n    \"\"\"Find the overlap constrained consensus sequence\n\n    Args:\n        issm (dict): {1: {'A': 0.1, 'C': 0.0, ...}, 14: {...}, ...}\n        aa_set (dict): {residue: {'A', 'I', 'M', 'V'}, ...}\n    Returns:\n        (dict): {23: 'T', 29: 'A', ...}\n    \"\"\"\n    consensus = {}\n    for res in aa_set:\n        max_freq = 0.0\n        for aa in aa_set[res]:\n            # if max_freq &lt; issm[(res, partner)][]:\n            if issm[res][aa] &gt; max_freq:\n                max_freq = issm[res][aa]\n                consensus[res] = aa\n\n    return consensus\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.get_cluster_dicts","title":"get_cluster_dicts","text":"<pre><code>get_cluster_dicts(db: str = putils.biological_interfaces, id_list: list[str] = None) -&gt; dict[str, dict]\n</code></pre> <p>Generate an interface specific scoring matrix from the fragment library</p> <p>Parameters:</p> <ul> <li> <code>db</code>             (<code>str</code>, default:                 <code>biological_interfaces</code> )         \u2013          <p>The source of the fragment information</p> </li> <li> <code>id_list</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>[1_2_24, ...]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cluster_dict</code> (            <code>dict[str, dict]</code> )        \u2013          <p>{'1_2_45': {'size': ..., 'rmsd': ..., 'rep': ..., 'mapped': ..., 'paired': ...}, ...}</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def get_cluster_dicts(db: str = putils.biological_interfaces, id_list: list[str] = None) -&gt; dict[str, dict]:\n    \"\"\"Generate an interface specific scoring matrix from the fragment library\n\n    Args:\n        db: The source of the fragment information\n        id_list: [1_2_24, ...]\n\n    Returns:\n         cluster_dict: {'1_2_45': {'size': ..., 'rmsd': ..., 'rep': ..., 'mapped': ..., 'paired': ...}, ...}\n    \"\"\"\n    info_db = putils.frag_directory[db]\n    if id_list is None:\n        directory_list = utils.get_base_root_paths_recursively(info_db)\n    else:\n        directory_list = []\n        for _id in id_list:\n            c_id = _id.split('_')\n            _dir = os.path.join(info_db, c_id[0], c_id[0] + '_' + c_id[1], c_id[0] + '_' + c_id[1] + '_' + c_id[2])\n            directory_list.append(_dir)\n\n    cluster_dict = {}\n    for cluster in directory_list:\n        filename = os.path.join(cluster, os.path.basename(cluster) + '.pkl')\n        cluster_dict[os.path.basename(cluster)] = utils.unpickle(filename)\n\n    return cluster_dict\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.fragment_overlap","title":"fragment_overlap","text":"<pre><code>fragment_overlap(residues, interaction_graph, freq_map)\n</code></pre> <p>Take fragment contact list to find the possible AA types allowed in fragment pairs from the contact list</p> <p>Parameters:</p> <ul> <li> <code>residues</code>             (<code>iter</code>)         \u2013          <p>Iterable of residue numbers</p> </li> <li> <code>interaction_graph</code>             (<code>dict</code>)         \u2013          <p>{52: [54, 56, 72, 206], ...}</p> </li> <li> <code>freq_map</code>             (<code>dict</code>)         \u2013          <p>{(78, 87, ...): {'A': {'S': 0.02, 'T': 0.12}, ...}, ...}</p> </li> </ul> <p>Returns:     overlap (dict): {residue: {'A', 'I', 'M', 'V'}, ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def fragment_overlap(residues, interaction_graph, freq_map):\n    \"\"\"Take fragment contact list to find the possible AA types allowed in fragment pairs from the contact list\n\n    Args:\n        residues (iter): Iterable of residue numbers\n        interaction_graph (dict): {52: [54, 56, 72, 206], ...}\n        freq_map (dict): {(78, 87, ...): {'A': {'S': 0.02, 'T': 0.12}, ...}, ...}\n    Returns:\n        overlap (dict): {residue: {'A', 'I', 'M', 'V'}, ...}\n    \"\"\"\n    overlap = {}\n    for res in residues:\n        overlap[res] = set()\n        if res in interaction_graph:  # check for existence as some fragment info is not in the interface set\n            # overlap[res] = set()\n            for partner in interaction_graph[res]:\n                if (res, partner) in freq_map:\n                    overlap[res] |= set(freq_map[(res, partner)].keys())\n\n    for res in residues:\n        if res in interaction_graph:  # check for existence as some fragment info is not in the interface set\n            for partner in interaction_graph[res]:\n                if (res, partner) in freq_map:\n                    overlap[res] &amp;= set(freq_map[(res, partner)].keys())\n\n    return overlap\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.offset_index","title":"offset_index","text":"<pre><code>offset_index(dictionary: dict[int, Any], to_zero: bool = False) -&gt; dict[int, dict]\n</code></pre> <p>Modify the index of a sequence dictionary. Default is to one-indexed. to_zero=True gives zero-indexed</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def offset_index(dictionary: dict[int, Any], to_zero: bool = False) -&gt; dict[int, dict]:\n    \"\"\"Modify the index of a sequence dictionary. Default is to one-indexed. to_zero=True gives zero-indexed\"\"\"\n    if to_zero:\n        return {residue - ZERO_OFFSET: dictionary[residue] for residue in dictionary}\n    else:\n        return {residue + ZERO_OFFSET: dictionary[residue] for residue in dictionary}\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.residue_object_to_number","title":"residue_object_to_number","text":"<pre><code>residue_object_to_number(residue_dict: dict[str, Iterable['structure.base.Residue']]) -&gt; dict[str, list[tuple[int, ...]]]\n</code></pre> <p>Convert sets of PDB.Residue objects to residue numbers</p> <p>Parameters:</p> <ul> <li> <code>residue_dict</code>             (<code>dict</code>)         \u2013          <p>{'key1': [(residue1_ca_atom, residue2_ca_atom, ...), ...] ...}</p> </li> </ul> <p>Returns:     residue_dict (dict): {'key1': [(78, 87, ...),], ...} - Entry mapped to residue sets</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def residue_object_to_number(\n    residue_dict: dict[str, Iterable['structure.base.Residue']]\n) -&gt; dict[str, list[tuple[int, ...]]]:\n    \"\"\"Convert sets of PDB.Residue objects to residue numbers\n\n    Args:\n        residue_dict (dict): {'key1': [(residue1_ca_atom, residue2_ca_atom, ...), ...] ...}\n    Returns:\n        residue_dict (dict): {'key1': [(78, 87, ...),], ...} - Entry mapped to residue sets\n    \"\"\"\n    for entry in residue_dict:\n        pairs = []\n        # for _set in range(len(residue_dict[entry])):\n        for j, _set in enumerate(residue_dict[entry]):\n            residue_num_set = []\n            # for i, residue in enumerate(residue_dict[entry][_set]):\n            for residue in _set:\n                resi_number = residue.residue_number\n                residue_num_set.append(resi_number)\n            pairs.append(tuple(residue_num_set))\n        residue_dict[entry] = pairs\n\n    return residue_dict\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.convert_to_residue_cluster_map","title":"convert_to_residue_cluster_map","text":"<pre><code>convert_to_residue_cluster_map(residue_cluster_dict, frag_range)\n</code></pre> <p>Make a residue and cluster/fragment index map</p> <p>Parameters:</p> <ul> <li> <code>residue_cluster_dict</code>             (<code>dict</code>)         \u2013          <p>{'1_2_45': [(residue1_ca_atom, residue2_ca_atom), ...] ...}</p> </li> <li> <code>frag_range</code>             (<code>dict</code>)         \u2013          <p>A range of the fragment size to search over. Ex: (-2, 3) for fragments of length 5</p> </li> </ul> <p>Returns:     cluster_map (dict): {48: {'source': 'mapped', 'cluster': [(-2, 1_1_54), ...]}, ...}         Where the key is the 0 indexed residue id</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def convert_to_residue_cluster_map(residue_cluster_dict, frag_range):\n    \"\"\"Make a residue and cluster/fragment index map\n\n    Args:\n        residue_cluster_dict (dict): {'1_2_45': [(residue1_ca_atom, residue2_ca_atom), ...] ...}\n        frag_range (dict): A range of the fragment size to search over. Ex: (-2, 3) for fragments of length 5\n    Returns:\n        cluster_map (dict): {48: {'source': 'mapped', 'cluster': [(-2, 1_1_54), ...]}, ...}\n            Where the key is the 0 indexed residue id\n    \"\"\"\n    cluster_map = {}\n    for cluster in residue_cluster_dict:\n        for pair in range(len(residue_cluster_dict[cluster])):\n            for i, residue_atom in enumerate(residue_cluster_dict[cluster][pair]):\n                # for each residue in map add the same cluster to the range of fragment residue numbers\n                residue_num = residue_atom.residue_number - ZERO_OFFSET  # zero index\n                for j in range(*frag_range):\n                    if residue_num + j not in cluster_map:\n                        if i == 0:\n                            cluster_map[residue_num + j] = {'source': 'mapped', 'cluster': []}\n                        else:\n                            cluster_map[residue_num + j] = {'source': 'paired', 'cluster': []}\n                    cluster_map[residue_num + j]['cluster'].append((j, cluster))\n\n    return cluster_map\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.consensus_sequence","title":"consensus_sequence","text":"<pre><code>consensus_sequence(pssm)\n</code></pre> <p>Return the consensus sequence from a PSSM</p> <p>Parameters:</p> <ul> <li> <code>pssm</code>             (<code>dict</code>)         \u2013          <p>pssm dictionary</p> </li> </ul> <p>Return:     consensus_identities (dict): {1: 'M', 2: 'H', ...} One-indexed</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def consensus_sequence(pssm):\n    \"\"\"Return the consensus sequence from a PSSM\n\n    Args:\n        pssm (dict): pssm dictionary\n    Return:\n        consensus_identities (dict): {1: 'M', 2: 'H', ...} One-indexed\n    \"\"\"\n    consensus_identities = {}\n    for residue in pssm:\n        max_lod = 0\n        max_res = pssm[residue]['type']\n        for aa in protein_letters_alph3:\n            if pssm[residue]['lod'][aa] &gt; max_lod:\n                max_lod = pssm[residue]['lod'][aa]\n                max_res = aa\n        consensus_identities[residue + ZERO_OFFSET] = max_res\n\n    return consensus_identities\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.sequence_difference","title":"sequence_difference","text":"<pre><code>sequence_difference(seq1: Sequence, seq2: Sequence, d: dict = None, matrix: str = 'BLOSUM62') -&gt; float\n</code></pre> <p>Returns the sequence difference between two sequence iterators</p> <p>Parameters:</p> <ul> <li> <code>seq1</code>             (<code>Sequence</code>)         \u2013          <p>Either an iterable with residue type as array, or key, with residue type as dseq1['type']</p> </li> <li> <code>seq2</code>             (<code>Sequence</code>)         \u2013          <p>Either an iterable with residue type as array, or key, with residue type as dseq2['type']</p> </li> <li> <code>d</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>The dictionary to look up seq1 and seq2 if they are keys in d</p> </li> <li> <code>matrix</code>             (<code>str</code>, default:                 <code>'BLOSUM62'</code> )         \u2013          <p>The type of matrix to score the sequence differences on</p> </li> </ul> <p>Returns:     The computed sequence difference between seq1 and seq2</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def sequence_difference(seq1: Sequence, seq2: Sequence, d: dict = None, matrix: str = 'BLOSUM62') -&gt; float:  # TODO AMS\n    \"\"\"Returns the sequence difference between two sequence iterators\n\n    Args:\n        seq1: Either an iterable with residue type as array, or key, with residue type as d[seq1][residue]['type']\n        seq2: Either an iterable with residue type as array, or key, with residue type as d[seq2][residue]['type']\n        d: The dictionary to look up seq1 and seq2 if they are keys in d\n        matrix: The type of matrix to score the sequence differences on\n    Returns:\n        The computed sequence difference between seq1 and seq2\n    \"\"\"\n    if d is not None:\n        # seq1 = d[seq1]\n        # seq2 = d[seq2]\n        # for residue in d[seq1]:\n        #     s.append((d[seq1][residue]['type'], d[seq2][residue]['type']))\n        pairs = [(d[seq1][residue]['type'], d[seq2][residue]['type']) for residue in d[seq1]]\n    else:\n        pairs = zip(seq1, seq2)\n\n    matrix_ = substitution_matrices.load(matrix)\n    scores = [matrix_.get((letter1, letter2), (letter2, letter1)) for letter1, letter2 in pairs]\n\n    return sum(scores)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.msa_from_dictionary","title":"msa_from_dictionary","text":"<pre><code>msa_from_dictionary(named_sequences: dict[str, str]) -&gt; MultipleSeqAlignment\n</code></pre> <p>Create a MultipleSequenceAlignment from a dictionary of named sequences</p> <p>Parameters:</p> <ul> <li> <code>named_sequences</code>             (<code>dict[str, str]</code>)         \u2013          <p>{name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}</p> </li> </ul> <p>Returns:     The MultipleSequenceAlignment object for the provided sequences</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def msa_from_dictionary(named_sequences: dict[str, str]) -&gt; MultipleSeqAlignment:\n    \"\"\"Create a MultipleSequenceAlignment from a dictionary of named sequences\n\n    Args:\n        named_sequences: {name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}\n    Returns:\n        The MultipleSequenceAlignment object for the provided sequences\n    \"\"\"\n    return MultipleSeqAlignment([SeqRecord(Seq(sequence), annotations={'molecule_type': 'Protein'}, id=name)\n                                 for name, sequence in named_sequences.items()])\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.msa_from_seq_records","title":"msa_from_seq_records","text":"<pre><code>msa_from_seq_records(seq_records: Iterable[SeqRecord]) -&gt; MultipleSeqAlignment\n</code></pre> <p>Create a BioPython Multiple Sequence Alignment from a SeqRecord Iterable</p> <p>Parameters:</p> <ul> <li> <code>seq_records</code>             (<code>Iterable[SeqRecord]</code>)         \u2013          <p>{name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}</p> </li> </ul> <p>Returns:     [SeqRecord(Seq('MNTEELQVAAFEI...', ...), id=\"Alpha\"),      SeqRecord(Seq('MNTEEL-VAAFEI...', ...), id=\"Beta\"), ...]</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def msa_from_seq_records(seq_records: Iterable[SeqRecord]) -&gt; MultipleSeqAlignment:\n    \"\"\"Create a BioPython Multiple Sequence Alignment from a SeqRecord Iterable\n\n    Args:\n        seq_records: {name: sequence, ...} ex: {'clean_asu': 'MNTEELQVAAFEI...', ...}\n    Returns:\n        [SeqRecord(Seq('MNTEELQVAAFEI...', ...), id=\"Alpha\"),\n         SeqRecord(Seq('MNTEEL-VAAFEI...', ...), id=\"Beta\"), ...]\n    \"\"\"\n    return MultipleSeqAlignment(seq_records)\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.make_mutations_chain_agnostic","title":"make_mutations_chain_agnostic","text":"<pre><code>make_mutations_chain_agnostic(mutations)\n</code></pre> <p>Remove chain identifier from mutation dictionary</p> <p>Parameters:</p> <ul> <li> <code>mutations</code>             (<code>dict</code>)         \u2013          <p>{design: {chain_id: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}</p> </li> </ul> <p>Returns:     (dict): {design: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def make_mutations_chain_agnostic(mutations):\n    \"\"\"Remove chain identifier from mutation dictionary\n\n    Args:\n        mutations (dict): {design: {chain_id: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}\n    Returns:\n        (dict): {design: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}\n    \"\"\"\n    flattened_mutations = {}\n    for design, chain_mutations in mutations.items():\n        flattened_mutations[design] = {}\n        for chain, mutations in chain_mutations.items():\n            flattened_mutations[design].update(mutations)\n\n    return flattened_mutations\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.simplify_mutation_dict","title":"simplify_mutation_dict","text":"<pre><code>simplify_mutation_dict(mutations: dict[str, mutation_dictionary], to: bool = True) -&gt; dict[str, mutation_dictionary]\n</code></pre> <p>Simplify mutation dictionary to 'to'/'from' AA key</p> <p>Parameters:</p> <ul> <li> <code>mutations</code>             (<code>dict[str, mutation_dictionary]</code>)         \u2013          <p>Ex: {alias: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}</p> </li> <li> <code>to</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to simplify with the 'to' AA key (True) or the 'from' AA key (False)</p> </li> </ul> <p>Returns:     The simplified mutation dictionary. Ex: {alias: {mutation_index: 'K', ...}, ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def simplify_mutation_dict(mutations: dict[str, mutation_dictionary], to: bool = True) \\\n        -&gt; dict[str, mutation_dictionary]:\n    \"\"\"Simplify mutation dictionary to 'to'/'from' AA key\n\n    Args:\n        mutations: Ex: {alias: {mutation_index: {'from': 'A', 'to': 'K'}, ...}, ...}, ...}\n        to: Whether to simplify with the 'to' AA key (True) or the 'from' AA key (False)\n    Returns:\n        The simplified mutation dictionary. Ex: {alias: {mutation_index: 'K', ...}, ...}\n    \"\"\"\n    simplification = 'to' if to else 'from'\n\n    for alias, indexed_mutations in mutations.items():\n        for index, mutation in indexed_mutations.items():\n            mutations[alias][index] = mutation[simplification]\n\n    return mutations\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.weave_mutation_dict","title":"weave_mutation_dict","text":"<pre><code>weave_mutation_dict(sorted_freq, mut_prob, resi_divergence, int_divergence, des_divergence)\n</code></pre> <p>Make final dictionary, index to sequence</p> <p>Parameters:</p> <ul> <li> <code>sorted_freq</code>             (<code>dict</code>)         \u2013          <p>{15: ['S', 'A', 'T'], ... }</p> </li> <li> <code>mut_prob</code>             (<code>dict</code>)         \u2013          <p>{15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}</p> </li> <li> <code>resi_divergence</code>             (<code>dict</code>)         \u2013          <p>{15: 0.732, 16: 0.552, ...}</p> </li> <li> <code>int_divergence</code>             (<code>dict</code>)         \u2013          <p>{15: 0.732, 16: 0.552, ...}</p> </li> <li> <code>des_divergence</code>             (<code>dict</code>)         \u2013          <p>{15: 0.732, 16: 0.552, ...}</p> </li> </ul> <p>Returns:     weaved_dict (dict): {16: {'S': 0.134, 'A': 0.050, ..., 'jsd': 0.732, 'int_jsd': 0.412}, ...}</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def weave_mutation_dict(sorted_freq, mut_prob, resi_divergence, int_divergence, des_divergence):\n    \"\"\"Make final dictionary, index to sequence\n\n    Args:\n        sorted_freq (dict): {15: ['S', 'A', 'T'], ... }\n        mut_prob (dict): {15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}\n        resi_divergence (dict): {15: 0.732, 16: 0.552, ...}\n        int_divergence (dict): {15: 0.732, 16: 0.552, ...}\n        des_divergence (dict): {15: 0.732, 16: 0.552, ...}\n    Returns:\n        weaved_dict (dict): {16: {'S': 0.134, 'A': 0.050, ..., 'jsd': 0.732, 'int_jsd': 0.412}, ...}\n    \"\"\"\n    weaved_dict = {}\n    for residue in sorted_freq:\n        final_resi = residue + ZERO_OFFSET\n        weaved_dict[final_resi] = {}\n        for aa in sorted_freq[residue]:\n            weaved_dict[final_resi][aa] = round(mut_prob[residue][aa], 3)\n        weaved_dict[final_resi]['jsd'] = resi_divergence[residue]\n        weaved_dict[final_resi]['int_jsd'] = int_divergence[residue]\n        weaved_dict[final_resi]['des_jsd'] = des_divergence[residue]\n\n    return weaved_dict\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.clean_gaped_columns","title":"clean_gaped_columns","text":"<pre><code>clean_gaped_columns(alignment_dict, correct_index)\n</code></pre> <p>Cleans an alignment dictionary by revising key list with correctly indexed positions. 0 indexed</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def clean_gaped_columns(alignment_dict, correct_index):  # UNUSED\n    \"\"\"Cleans an alignment dictionary by revising key list with correctly indexed positions. 0 indexed\"\"\"\n    return {i: alignment_dict[index] for i, index in enumerate(correct_index)}\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.msa_to_prob_distribution","title":"msa_to_prob_distribution","text":"<pre><code>msa_to_prob_distribution(alignment)\n</code></pre> <p>Turn Alignment dictionary into a probability distribution</p> <p>Parameters:</p> <ul> <li> <code>alignment</code>             (<code>dict</code>)         \u2013          <p>{'meta': {'num_sequences': 214, 'query': 'MGSTHLVLK...', 'query_with_gaps': 'MGS--THLVLK...'},                'msa': (Bio.Align.MultipleSeqAlignment)                'counts': {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...},                'frequencies': {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...},                'rep': {1: 210, 2:211, ...}} The msa formatted with counts and indexed by residue</p> </li> </ul> <p>Returns:     (dict): {'meta': {'num_sequences': 214, 'query': 'MGSTHLVLK...', 'query_with_gaps': 'MGS--THLVLK...'},              'msa': (Bio.Align.MultipleSeqAlignment)              'counts': {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...},              'frequencies': {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...},              'rep': {1: 210, 2:211, ...}}         The msa formatted with counts and indexed by residue</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def msa_to_prob_distribution(alignment):\n    \"\"\"Turn Alignment dictionary into a probability distribution\n\n    Args:\n        alignment (dict): {'meta': {'num_sequences': 214, 'query': 'MGSTHLVLK...', 'query_with_gaps': 'MGS--THLVLK...'},\n                           'msa': (Bio.Align.MultipleSeqAlignment)\n                           'counts': {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...},\n                           'frequencies': {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...},\n                           'rep': {1: 210, 2:211, ...}}\n            The msa formatted with counts and indexed by residue\n    Returns:\n        (dict): {'meta': {'num_sequences': 214, 'query': 'MGSTHLVLK...', 'query_with_gaps': 'MGS--THLVLK...'},\n                 'msa': (Bio.Align.MultipleSeqAlignment)\n                 'counts': {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...},\n                 'frequencies': {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...},\n                 'rep': {1: 210, 2:211, ...}}\n            The msa formatted with counts and indexed by residue\n    \"\"\"\n    alignment['frequencies'] = {}\n    for residue, amino_acid_counts in alignment['counts'].items():\n        total_column_weight = alignment['rep'][residue]\n        assert total_column_weight != 0, '%s: Processing error... Downstream cannot divide by 0. Position = %s' \\\n                                         % (msa_to_prob_distribution.__name__, residue)\n        alignment['frequencies'][residue] = {aa: count / total_column_weight for aa, count in amino_acid_counts.items()}\n\n    return alignment\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.window_score","title":"window_score","text":"<pre><code>window_score(scores: dict[int, float] | Sequence[float], window_len: int, lambda_: float = 0.5) -&gt; dict\n</code></pre> <p>Takes an MSA score dict and transforms so that each position is a weighted average of the surrounding positions. Positions with scores less than zero are not changed and are ignored calculation</p> <p>Modified from Capra and Singh 2007 code</p> Notes <p>The input should be a one-indexed dictionary (if a dictionary)</p> <p>Parameters:</p> <ul> <li> <code>scores</code>             (<code>dict[int, float] | Sequence[float]</code>)         \u2013          <p>A dictionary with scores.</p> </li> <li> <code>window_len</code>             (<code>int</code>)         \u2013          <p>Number of residues on either side of the current residue</p> </li> <li> <code>lambda_</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>Float between 0 and 1 which parameterizes the amount of the score to pull from</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code>         \u2013          <p>A dictionary with the modified scores for the specified window, one-indexed</p> </li> </ul> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def window_score(scores: dict[int, float] | Sequence[float], window_len: int, lambda_: float = 0.5) -&gt; dict:  # UNUSED  incorporate into MultipleSequenceAlignment\n    \"\"\"Takes an MSA score dict and transforms so that each position is a weighted average of the surrounding positions.\n    Positions with scores less than zero are not changed and are ignored calculation\n\n    Modified from Capra and Singh 2007 code\n\n    Notes:\n        The input should be a one-indexed dictionary (if a dictionary)\n\n    Args:\n        scores: A dictionary with scores.\n        window_len: Number of residues on either side of the current residue\n        lambda_: Float between 0 and 1 which parameterizes the amount of the score to pull from\n\n    Returns:\n        A dictionary with the modified scores for the specified window, one-indexed\n    \"\"\"\n    if window_len == 0:\n        try:\n            return dict(enumerate(scores.values(), ZERO_OFFSET))\n        except AttributeError:  # Not a dict\n            return dict(enumerate(scores, ZERO_OFFSET))\n    else:\n        window_scores = {}\n        number_scores = len(scores)\n        for i in range(number_scores + ZERO_OFFSET):\n            s = number_terms = 0\n            if i &lt;= window_len:\n                for j in range(ZERO_OFFSET, i + window_len + ZERO_OFFSET):\n                    if i != j:\n                        number_terms += 1\n                        s += scores[j]\n            elif i + window_len &gt; number_scores:\n                for j in range(i - window_len, number_scores + ZERO_OFFSET):\n                    if i != j:\n                        number_terms += 1\n                        s += scores[j]\n            else:\n                for j in range(i - window_len, i + window_len + ZERO_OFFSET):\n                    if i != j:\n                        number_terms += 1\n                        s += scores[j]\n            window_scores[i] = (1 - lambda_) * (s / number_terms) + lambda_ * scores[i]\n\n        return window_scores\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.rank_possibilities","title":"rank_possibilities","text":"<pre><code>rank_possibilities(probability_dict)\n</code></pre> <p>Gather alternative residues and sort them by probability.</p> <p>Parameters:</p> <ul> <li> <code>probability_dict</code>             (<code>dict</code>)         \u2013          <p>{15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}</p> </li> </ul> <p>Returns:      sorted_alternates_dict (dict): {15: ['S', 'A', 'T'], ... }</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def rank_possibilities(probability_dict):  # UNUSED  incorporate into MultipleSequenceAlignment\n    \"\"\"Gather alternative residues and sort them by probability.\n\n    Args:\n        probability_dict (dict): {15: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 16: {}, ...}\n    Returns:\n         sorted_alternates_dict (dict): {15: ['S', 'A', 'T'], ... }\n    \"\"\"\n    sorted_alternates_dict = {}\n    for residue in probability_dict:\n        residue_probability_list = []\n        for aa in probability_dict[residue]:\n            if probability_dict[residue][aa] &gt; 0:\n                residue_probability_list.append((aa, round(probability_dict[residue][aa], 5)))  # tuple instead of list\n        residue_probability_list.sort(key=lambda tup: tup[1], reverse=True)\n        # [('S', 0.13190), ('A', 0.0500), ...]\n        sorted_alternates_dict[residue] = [aa[0] for aa in residue_probability_list]\n\n    return sorted_alternates_dict\n</code></pre>"},{"location":"reference/structure/sequence/#structure.sequence.multi_chain_alignment","title":"multi_chain_alignment","text":"<pre><code>multi_chain_alignment(mutated_sequences, **kwargs)\n</code></pre> <p>Combines different chain's Multiple Sequence Alignments into a single MSA. One-indexed</p> <p>Parameters:</p> <ul> <li> <code>mutated_sequences</code>             (<code>dict</code>)         \u2013          <p>{chain: {name: sequence, ...}</p> </li> </ul> <p>Returns:     (MultipleSequenceAlignment): The MSA object with counts, frequencies, sequences, and indexed by residue</p> Source code in <code>symdesign/structure/sequence.py</code> <pre><code>def multi_chain_alignment(mutated_sequences, **kwargs):\n    \"\"\"Combines different chain's Multiple Sequence Alignments into a single MSA. One-indexed\n\n    Args:\n        mutated_sequences (dict): {chain: {name: sequence, ...}\n    Returns:\n        (MultipleSequenceAlignment): The MSA object with counts, frequencies, sequences, and indexed by residue\n    \"\"\"\n    #         (dict): {'meta': {'num_sequences': 214, 'query': 'MGSTHLVLK...', 'query_with_gaps': 'MGS--THLVLK...'},\n    #                  'msa': (Bio.Align.MultipleSeqAlignment)\n    #                  'counts': {1: {'A': 13, 'C': 1, 'D': 23, ...}, 2: {}, ...},\n    #                  'frequencies': {1: {'A': 0.05, 'C': 0.001, 'D': 0.1, ...}, 2: {}, ...},\n    #                  'rep': {1: 210, 2:211, ...}}\n    #             The msa formatted with counts and indexed by residue\n\n    # Combine alignments for all chains from design file Ex: A: 1-102, B: 1-130. Alignment: 1-232\n    total_alignment = None\n    for idx, named_sequences in enumerate(mutated_sequences.values()):\n        if idx == 0:\n            total_alignment = msa_from_dictionary(named_sequences)[:, :]\n        else:\n            total_alignment += msa_from_dictionary(named_sequences)[:, :]\n\n    if total_alignment:\n        return MultipleSequenceAlignment(alignment=total_alignment, **kwargs)\n    else:\n        raise RuntimeError(f'{multi_chain_alignment.__name__} - No sequences were found!')\n</code></pre>"},{"location":"reference/structure/utils/","title":"utils","text":""},{"location":"reference/structure/utils/#structure.utils.chain_id_generator","title":"chain_id_generator","text":"<pre><code>chain_id_generator() -&gt; Generator[str, None, None]\n</code></pre> <p>Provide a generator which produces all combinations of chain ID strings</p> <p>Returns     The generator producing a maximum 2 character string where single characters are exhausted,         first in uppercase, then in lowercase</p> Source code in <code>symdesign/structure/utils.py</code> <pre><code>def chain_id_generator() -&gt; Generator[str, None, None]:\n    \"\"\"Provide a generator which produces all combinations of chain ID strings\n\n    Returns\n        The generator producing a maximum 2 character string where single characters are exhausted,\n            first in uppercase, then in lowercase\n    \"\"\"\n    # Todo\n    #  Some PDB file use numeric chains... so chain 1 for instance couldn't be parsed correctly\n    return (first + second for modification in ['upper', 'lower']\n            for first in [''] + list(getattr(available_letters, modification)())\n            for second in list(getattr(available_letters, 'upper')()) +\n            list(getattr(available_letters, 'lower')()))\n</code></pre>"},{"location":"reference/structure/utils/#structure.utils.parse_stride","title":"parse_stride","text":"<pre><code>parse_stride(stride_file, **kwargs)\n</code></pre> <p>From a Stride file, parse information for residue level secondary structure assignment</p> Sets <p>self.secondary_structure</p> Source code in <code>symdesign/structure/utils.py</code> <pre><code>def parse_stride(stride_file, **kwargs):\n    \"\"\"From a Stride file, parse information for residue level secondary structure assignment\n\n    Sets:\n        self.secondary_structure\n    \"\"\"\n    with open(stride_file, 'r') as f:\n        stride_output = f.readlines()\n\n    return ''.join(line[24:25] for line in stride_output if line[0:3] == 'ASG')\n</code></pre>"},{"location":"reference/structure/fragment/","title":"fragment","text":""},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment","title":"GhostFragment","text":"<pre><code>GhostFragment(guide_coords: ndarray, i_type: int, j_type: int, k_type: int, ijk_rmsd: float, aligned_fragment: Fragment)\n</code></pre> <p>Stores the mapping from a Fragment to a \"paired\" Fragment. Handles spatial manipulation for the mapped pair</p> <p>Parameters:</p> <ul> <li> <code>guide_coords</code>             (<code>ndarray</code>)         \u2013          <p>The array of shape (3, 3) with the set of x, y, z coordinates to map this instance to the standard fragment reference frame at the origin</p> </li> <li> <code>i_type</code>             (<code>int</code>)         \u2013          <p>The particular index which identifies the aligned fragment type, i.e. the aligned_fragment</p> </li> <li> <code>j_type</code>             (<code>int</code>)         \u2013          <p>The particular index which identifies the paired fragment type, i.e. this instance</p> </li> <li> <code>k_type</code>             (<code>int</code>)         \u2013          <p>The particular index which identifies the spatial orientation of the i and j fragment types</p> </li> <li> <code>ijk_rmsd</code>             (<code>float</code>)         \u2013          <p>The root-mean-square deviation which applies to all members particular i,j,k index type</p> </li> <li> <code>aligned_fragment</code>             (<code>Fragment</code>)         \u2013          <p>The Fragment instance which this instance is paired with</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def __init__(self, guide_coords: np.ndarray, i_type: int, j_type: int, k_type: int, ijk_rmsd: float,\n             aligned_fragment: Fragment):\n    \"\"\"Construct the instance\n\n    Args:\n        guide_coords: The array of shape (3, 3) with the set of x, y, z coordinates to map this instance to the\n            standard fragment reference frame at the origin\n        i_type: The particular index which identifies the aligned fragment type, i.e. the aligned_fragment\n        j_type: The particular index which identifies the paired fragment type, i.e. this instance\n        k_type: The particular index which identifies the spatial orientation of the i and j fragment types\n        ijk_rmsd: The root-mean-square deviation which applies to all members particular i,j,k index type\n        aligned_fragment: The Fragment instance which this instance is paired with\n    \"\"\"\n    self._guide_coords = guide_coords\n    self.i_type = i_type\n    self.j_type = self.frag_type = j_type\n    self.k_type = k_type\n    self.rmsd = ijk_rmsd\n    self.aligned_fragment = aligned_fragment\n    # Assign both attributes for API compatibility with Fragment\n    self.fragment_db = self._fragment_db = aligned_fragment.fragment_db\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.rmsd","title":"rmsd  <code>instance-attribute</code>","text":"<pre><code>rmsd: float = ijk_rmsd\n</code></pre> <p>The deviation from the representative ghost fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.aligned_fragment","title":"aligned_fragment  <code>instance-attribute</code>","text":"<pre><code>aligned_fragment: Fragment = aligned_fragment\n</code></pre> <p>The Fragment instance that this GhostFragment is aligned too.  Must support .chain_id, .number, .index, .rotation, .translation, and .transformation attributes</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.type","title":"type  <code>property</code>","text":"<pre><code>type: int\n</code></pre> <p>The secondary structure of the Fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.ijk","title":"ijk  <code>property</code>","text":"<pre><code>ijk: tuple[int, int, int]\n</code></pre> <p>The Fragment cluster index information</p> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int]</code>         \u2013          <p>I cluster index, J cluster index, K cluster index</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.aligned_chain_and_residue","title":"aligned_chain_and_residue  <code>property</code>","text":"<pre><code>aligned_chain_and_residue: tuple[str, int]\n</code></pre> <p>Return the Fragment identifiers that the GhostFragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>tuple[str, int]</code>         \u2013          <p>aligned chain_id, aligned residue_number</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.number","title":"number  <code>property</code>","text":"<pre><code>number: int\n</code></pre> <p>The Residue number of the aligned Fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.index","title":"index  <code>property</code>","text":"<pre><code>index: int\n</code></pre> <p>The Residue index of the aligned Fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.guide_coords","title":"guide_coords  <code>property</code>","text":"<pre><code>guide_coords: ndarray\n</code></pre> <p>Return the guide coordinates of the GhostFragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.rotation","title":"rotation  <code>property</code>","text":"<pre><code>rotation: ndarray\n</code></pre> <p>The rotation of the aligned Fragment from the Fragment Database</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.translation","title":"translation  <code>property</code>","text":"<pre><code>translation: ndarray\n</code></pre> <p>The translation of the aligned Fragment from the Fragment Database</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.transformation","title":"transformation  <code>property</code>","text":"<pre><code>transformation: tuple[ndarray, ndarray]\n</code></pre> <p>The transformation of the aligned Fragment from the Fragment Database</p> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray]</code>         \u2013          <p>The rotation (3, 3) and the translation (3,)</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.representative","title":"representative  <code>property</code>","text":"<pre><code>representative: 'structure.model.Model'\n</code></pre> <p>Access the Representative GhostFragment StructureBase</p>"},{"location":"reference/structure/fragment/#structure.fragment.GhostFragment.write","title":"write","text":"<pre><code>write(out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, **kwargs) -&gt; AnyStr | None\n</code></pre> <p>Write the GhostFragment to a file specified by out_path or with a passed file_handle</p> <p>If a file_handle is passed, no header information will be written. Arguments are mutually exclusive</p> <p>Parameters:</p> <ul> <li> <code>out_path</code>             (<code>bytes | str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The location where the StructureBase object should be written to disk</p> </li> <li> <code>file_handle</code>             (<code>IO</code>, default:                 <code>None</code> )         \u2013          <p>Used to write to an open FileObject</p> </li> <li> <code>header</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A string that is desired at the top of the file</p> </li> </ul> <p>Other Parameters:</p> <ul> <li> <code>chain_id</code>         \u2013          <p>str = None - The chain ID to use</p> </li> <li> <code>atom_offset</code>         \u2013          <p>int = 0 - How much to offset the atom number by. Default returns one-indexed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>AnyStr | None</code>         \u2013          <p>The name of the written file if out_path is used</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def write(\n    self, out_path: bytes | str = os.getcwd(), file_handle: IO = None, header: str = None, **kwargs\n) -&gt; AnyStr | None:\n    \"\"\"Write the GhostFragment to a file specified by out_path or with a passed file_handle\n\n    If a file_handle is passed, no header information will be written. Arguments are mutually exclusive\n\n    Args:\n        out_path: The location where the StructureBase object should be written to disk\n        file_handle: Used to write to an open FileObject\n        header: A string that is desired at the top of the file\n\n    Keyword Args:\n        chain_id: str = None - The chain ID to use\n        atom_offset: int = 0 - How much to offset the atom number by. Default returns one-indexed\n\n    Returns:\n        The name of the written file if out_path is used\n    \"\"\"\n    representative = self.representative\n    if file_handle:\n        file_handle.write(f'{representative.get_atom_record(**kwargs)}\\n')\n        return None\n    else:  # out_path always has default argument current working directory\n        _header = representative.format_header(**kwargs)\n        if header is not None:\n            if not isinstance(header, str):\n                header = str(header)\n            _header += (header if header[-2:] == '\\n' else f'{header}\\n')\n\n        with open(out_path, 'w') as outfile:\n            outfile.write(_header)\n            outfile.write(f'{representative.get_atom_record(**kwargs)}\\n')\n        return out_path\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment","title":"Fragment","text":"<pre><code>Fragment(fragment_type: int = None, fragment_db: FragmentDatabase = None, **kwargs)\n</code></pre> <p>             Bases: <code>ABC</code></p> <p>Parameters:</p> <ul> <li> <code>fragment_type</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The particular index for the class of fragment this instance belongs to</p> </li> <li> <code>fragment_db</code>             (<code>FragmentDatabase</code>, default:                 <code>None</code> )         \u2013          <p>The FragmentDatabase from where this instance was derived</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def __init__(self, fragment_type: int = None, fragment_db: db.FragmentDatabase = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        fragment_type: The particular index for the class of fragment this instance belongs to\n        fragment_db: The FragmentDatabase from where this instance was derived\n        **kwargs:\n    \"\"\"\n    self._fragment_coords = None\n    self.ghost_fragments = None\n    self.i_type = fragment_type\n    self.rotation = utils.symmetry.identity_matrix\n    self.translation = utils.symmetry.origin\n    self._fragment_db = fragment_db\n\n    super().__init__(**kwargs)  # Fragment\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.fragment_db","title":"fragment_db  <code>property</code> <code>writable</code>","text":"<pre><code>fragment_db: FragmentDatabase\n</code></pre> <p>The FragmentDatabase that the Fragment was created from</p>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.frag_type","title":"frag_type  <code>property</code> <code>writable</code>","text":"<pre><code>frag_type: int | None\n</code></pre> <p>The secondary structure of the Fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.aligned_chain_and_residue","title":"aligned_chain_and_residue  <code>property</code>","text":"<pre><code>aligned_chain_and_residue: tuple[str, int]\n</code></pre> <p>Return the Fragment identifiers that the Fragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>tuple[str, int]</code>         \u2013          <p>aligned chain_id, aligned residue_number</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.chain_id","title":"chain_id  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>chain_id: str\n</code></pre> <p>Return the Fragment identifiers that the Fragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The aligned Residue.chain_id attribute</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.number","title":"number  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>number: int\n</code></pre> <p>Return the Fragment identifiers that the Fragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The aligned Residue.number attribute</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.index","title":"index  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>index: int\n</code></pre> <p>Return the Fragment identifiers that the Fragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The aligned Residue.index attribute</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.guide_coords","title":"guide_coords  <code>property</code>","text":"<pre><code>guide_coords: ndarray\n</code></pre> <p>Return the guide coordinates of the mapped Fragment</p>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.transformation","title":"transformation  <code>property</code>","text":"<pre><code>transformation: tuple[ndarray, ndarray]\n</code></pre> <p>The transformation of the Fragment from the FragmentDatabase to its current position</p>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.find_ghost_fragments","title":"find_ghost_fragments","text":"<pre><code>find_ghost_fragments(clash_tree: BinaryTreeType = None, clash_dist: float = 2.1) -&gt; list[GhostFragment]\n</code></pre> <p>Find all the GhostFragments associated with the Fragment</p> <p>Parameters:</p> <ul> <li> <code>clash_tree</code>             (<code>BinaryTreeType</code>, default:                 <code>None</code> )         \u2013          <p>Allows clash prevention during search. Typical use is the backbone and CB atoms of the ContainsAtomsMixin that the Fragment is assigned</p> </li> <li> <code>clash_dist</code>             (<code>float</code>, default:                 <code>2.1</code> )         \u2013          <p>The distance to check for backbone clashes</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def find_ghost_fragments(self, clash_tree: BinaryTreeType = None, clash_dist: float = 2.1) -&gt; list[GhostFragment]:\n    \"\"\"Find all the GhostFragments associated with the Fragment\n\n    Args:\n        clash_tree: Allows clash prevention during search. Typical use is the backbone and CB atoms of the\n            ContainsAtomsMixin that the Fragment is assigned\n        clash_dist: The distance to check for backbone clashes\n    \"\"\"\n    try:\n        ghost_i_type_arrays = self._fragment_db.indexed_ghosts.get(self.i_type, None)\n    except AttributeError:  # _fragment_db is None\n        raise NotImplementedError(\n            f\"Can't {self.find_ghost_fragments.__name__} without first setting '.fragment_db' to a \"\n            f\"{db.FragmentDatabase.__name__}\"\n        )\n    if ghost_i_type_arrays is None:\n        self.ghost_fragments = {}\n        return []\n\n    stacked_bb_coords, stacked_guide_coords, ijk_types, rmsd_array = ghost_i_type_arrays\n    # No need to transform stacked_guide_coords as these will be transformed upon .guide_coords access\n    if clash_tree is None:\n        # Ensure no slice occurs\n        viable_indices = Ellipsis\n    else:\n        # Ensure that the backbone coords are transformed to the Fragment reference frame\n        transformed_bb_coords = transform_coordinate_sets(stacked_bb_coords, *self.transformation)\n        # with .reshape(), we query on a np.view saving memory\n        neighbors = clash_tree.query_radius(transformed_bb_coords.reshape(-1, 3), clash_dist)\n        neighbor_counts = np.array([neighbor.size for neighbor in neighbors.tolist()])\n        # reshape to original size then query for existence of any neighbors for each fragment individually\n        clashing_indices = neighbor_counts.reshape(len(transformed_bb_coords), -1).any(axis=1)\n        viable_indices = ~clashing_indices\n\n    # self.ghost_fragments = [GhostFragment(*info) for info in zip(list(transformed_guide_coords[viable_indices]),\n    selected_ijk_types = ijk_types[viable_indices].tolist()\n    ghost_fragments = [\n        GhostFragment(*info_)\n        for info_ in zip(\n            list(stacked_guide_coords[viable_indices]), *zip(*selected_ijk_types),\n            rmsd_array[viable_indices].tolist(), repeat(self))\n    ]\n    self.ghost_fragments = {ijk: frag for ijk, frag in zip(selected_ijk_types, ghost_fragments)}\n\n    return ghost_fragments\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.Fragment.get_ghost_fragments","title":"get_ghost_fragments","text":"<pre><code>get_ghost_fragments(**kwargs) -&gt; list | list[GhostFragment]\n</code></pre> <p>Retrieve the GhostFragments associated with the Fragment. Will generate if none are available, otherwise, will return the already generated instances.</p> <p>Optionally, check clashing with the original structure backbone by passing clash_tree</p> <p>Other Parameters:</p> <ul> <li> <code>clash_tree</code>         \u2013          <p>BinaryTreeType = None - Allows clash prevention during search. Typical use is the backbone and CB coordinates of the ContainsAtomsMixin that the Fragment is assigned</p> </li> <li> <code>clash_dist</code>         \u2013          <p>float = 2.1 - The distance to check for backbone clashes</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list | list[GhostFragment]</code>         \u2013          <p>The ghost fragments associated with the fragment</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def get_ghost_fragments(self, **kwargs) -&gt; list | list[GhostFragment]:\n    \"\"\"Retrieve the GhostFragments associated with the Fragment. Will generate if none are available, otherwise,\n    will return the already generated instances.\n\n    Optionally, check clashing with the original structure backbone by passing clash_tree\n\n    Keyword Args:\n        clash_tree: BinaryTreeType = None - Allows clash prevention during search.\n            Typical use is the backbone and CB coordinates of the ContainsAtomsMixin that the Fragment is assigned\n        clash_dist: float = 2.1 - The distance to check for backbone clashes\n\n    Returns:\n        The ghost fragments associated with the fragment\n    \"\"\"\n    #         Args:\n    #             indexed_ghost_fragments: The paired fragment database to match to the Fragment instance\n    # self.find_ghost_fragments(**kwargs)\n    if self.ghost_fragments is None:\n        ghost_fragments = self.find_ghost_fragments(**kwargs)\n    else:\n        # This routine is necessary when the ghost_fragments are already generated on a residue,\n        # but that residue is copied.\n        logger.debug('Using previously generated ghost fragments. Updating their .aligned_fragment attribute')\n        ghost_fragments = list(self.ghost_fragments.values())\n        for ghost in ghost_fragments:\n            ghost.aligned_fragment = self\n\n    return ghost_fragments\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.MonoFragment","title":"MonoFragment","text":"<pre><code>MonoFragment(residues: Sequence['structure.base.Residue'], **kwargs)\n</code></pre> <p>             Bases: <code>Fragment</code></p> <p>Used to represent Fragment information when treated as a continuous Fragment of length fragment_length</p> <pre><code>**kwargs:\n</code></pre> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def __init__(self, residues: Sequence['structure.base.Residue'], **kwargs):\n    \"\"\"\n\n    Args:\n        residues: The Residue instances which comprise the MonoFragment\n        **kwargs:\n    \"\"\"\n    super().__init__(**kwargs)  # MonoFragment\n\n    fragment_db = self.fragment_db\n    try:\n        fragment_length = fragment_db.fragment_length\n    except AttributeError:  # fragment_db is None\n        raise ValueError(\n            f\"Can't construct {self.__class__.__name__} without passing 'fragment_db'\")\n\n    if not residues:\n        raise ValueError(\n            f\"Can't find {self.__class__.__name__} without passing {fragment_length} Residue instances\")\n    self.central_residue = residues[int(fragment_length / 2)]\n\n    try:\n        fragment_representatives = fragment_db.representatives\n    except AttributeError:\n        raise TypeError(\n            f\"The 'fragment_db' is not of the required type '{db.FragmentDatabase.__name__}'\")\n\n    fragment_ca_coords = np.array([residue.ca_coords for residue in residues])\n    min_rmsd = float('inf')\n\n    for fragment_type, representative in fragment_representatives.items():\n        rmsd, rot, tx = superposition3d(fragment_ca_coords, representative.ca_coords)\n        if rmsd &lt;= self.rmsd_thresh and rmsd &lt;= min_rmsd:\n            self.i_type = fragment_type\n            min_rmsd, self.rotation, self.translation = rmsd, rot, tx\n\n    if self.i_type:\n        # self.guide_coords = \\\n        #     np.matmul(self.template_coords, np.transpose(self.rotation)) + self.translation\n        self._fragment_coords = fragment_representatives[self.i_type].backbone_coords\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.MonoFragment.chain_id","title":"chain_id  <code>property</code>","text":"<pre><code>chain_id: str\n</code></pre> <p>The Residue chainID</p>"},{"location":"reference/structure/fragment/#structure.fragment.MonoFragment.number","title":"number  <code>property</code>","text":"<pre><code>number: int\n</code></pre> <p>The Residue number</p>"},{"location":"reference/structure/fragment/#structure.fragment.MonoFragment.index","title":"index  <code>property</code>","text":"<pre><code>index: int\n</code></pre> <p>Return the Fragment identifiers that the Fragment was mapped to</p> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>The aligned Residue.index attribute</p> </li> </ul>"},{"location":"reference/structure/fragment/#structure.fragment.ResidueFragment","title":"ResidueFragment","text":"<pre><code>ResidueFragment(**kwargs)\n</code></pre> <p>             Bases: <code>Fragment</code>, <code>ABC</code></p> <p>Represent Fragment information for a single Residue</p> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def __init__(self, **kwargs):\n    super().__init__(**kwargs)  # ResidueFragment\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.ResidueFragment.backbone_coords","title":"backbone_coords  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>backbone_coords: ndarray\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.ResidueFragment.transformation","title":"transformation  <code>property</code>","text":"<pre><code>transformation: tuple[ndarray, ndarray]\n</code></pre> <p>The transformation of the ResidueFragment from the FragmentDatabase to its current position</p>"},{"location":"reference/structure/fragment/#structure.fragment.find_fragment_overlap","title":"find_fragment_overlap","text":"<pre><code>find_fragment_overlap(fragments1: Iterable[Fragment], fragments2: Sequence[Fragment], clash_coords: ndarray = None, min_match_value: float = 2.0, **kwargs) -&gt; list[tuple[GhostFragment, Fragment, float]]\n</code></pre> <p>From two sets of Residues, score the fragment overlap according to Nanohedra's fragment matching</p> <p>Parameters:</p> <ul> <li> <code>fragments1</code>             (<code>Iterable[Fragment]</code>)         \u2013          <p>The Fragment instances that will be used to search for GhostFragment instances</p> </li> <li> <code>fragments2</code>             (<code>Sequence[Fragment]</code>)         \u2013          <p>The Fragment instances to pair against fragments1 GhostFragment instances</p> </li> <li> <code>clash_coords</code>             (<code>ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The coordinates to use for checking for GhostFragment clashes</p> </li> <li> <code>min_match_value</code>             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>The minimum value which constitutes an acceptable fragment z_score</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[tuple[GhostFragment, Fragment, float]]</code>         \u2013          <p>The GhostFragment, Fragment pairs, along with their match score</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def find_fragment_overlap(fragments1: Iterable[Fragment], fragments2: Sequence[Fragment],\n                          clash_coords: np.ndarray = None, min_match_value: float = 2.,  # .2,\n                          **kwargs) -&gt; list[tuple[GhostFragment, Fragment, float]]:\n    #           entity1, entity2, entity1_interface_residue_numbers, entity2_interface_residue_numbers, max_z_value=2):\n    \"\"\"From two sets of Residues, score the fragment overlap according to Nanohedra's fragment matching\n\n    Args:\n        fragments1: The Fragment instances that will be used to search for GhostFragment instances\n        fragments2: The Fragment instances to pair against fragments1 GhostFragment instances\n        clash_coords: The coordinates to use for checking for GhostFragment clashes\n        min_match_value: The minimum value which constitutes an acceptable fragment z_score\n\n    Returns:\n        The GhostFragment, Fragment pairs, along with their match score\n    \"\"\"\n    # min_match_value: The minimum value which constitutes an acceptable fragment match score\n    # 0.2 with match score, 2 with z_score\n    # Todo memoize this variable into a function default... The load time is kinda significant and shouldn't be used\n    #  until needed. Getting the factory everytime is a small overhead that is really unnecessary. Perhaps this function\n    #  should be refactored to structure.fragment.db or something and imported upon usage...\n\n    # logger.debug('Starting Ghost Frag Lookup')\n    if clash_coords is not None:\n        clash_tree = BallTree(clash_coords)\n    else:\n        clash_tree = None\n\n    ghost_frags1: list[GhostFragment] = []\n    for fragment in fragments1:\n        ghost_frags1.extend(fragment.get_ghost_fragments(clash_tree=clash_tree))\n\n    logger.debug(f'Residues 1 has {len(ghost_frags1)} ghost fragments')\n\n    # Get fragment guide coordinates\n    residue1_ghost_guide_coords = np.array([ghost_frag.guide_coords for ghost_frag in ghost_frags1])\n    residue2_guide_coords = np.array([fragment.guide_coords for fragment in fragments2])\n    # interface_surf_frag_guide_coords = np.array([residue.guide_coords for residue in interface_residues2])\n\n    # Check for matching Euler angles\n    # Todo create a stand alone function\n    # logger.debug('Starting Euler Lookup')\n    euler_lookup = fragments2[0].fragment_db.euler_lookup\n\n    overlapping_ghost_indices, overlapping_frag_indices = \\\n        euler_lookup.check_lookup_table(residue1_ghost_guide_coords, residue2_guide_coords)\n    # logger.debug('Finished Euler Lookup')\n    logger.debug(f'Found {len(overlapping_ghost_indices)} overlapping fragments in the same Euler rotational space')\n    # filter array by matching type for surface (i) and ghost (j) frags\n    ghost_type_array = np.array([ghost_frags1[idx].frag_type for idx in overlapping_ghost_indices.tolist()])\n    mono_type_array = np.array([fragments2[idx].frag_type for idx in overlapping_frag_indices.tolist()])\n    ij_type_match = mono_type_array == ghost_type_array\n\n    passing_ghost_indices = overlapping_ghost_indices[ij_type_match]\n    passing_frag_indices = overlapping_frag_indices[ij_type_match]\n    logger.debug(f'Found {len(passing_ghost_indices)} overlapping fragments in the same i/j type')\n\n    passing_ghost_coords = residue1_ghost_guide_coords[passing_ghost_indices]\n    passing_frag_coords = residue2_guide_coords[passing_frag_indices]\n    # # Todo keep without euler_lookup?\n    # ghost_type_array = np.array([ghost_frag.frag_type for ghost_frag in ghost_frags1])\n    # mono_type_array = np.array([residue.frag_type for residue in fragments2])\n    # # Using only ij_type_match, no euler_lookup\n    # int_ghost_shape = len(ghost_frags1)\n    # int_surf_shape = len(fragments2)\n    # # maximum_number_of_pairs = int_ghost_shape*int_surf_shape\n    # ghost_indices_repeated = np.repeat(ghost_type_array, int_surf_shape)\n    # surf_indices_tiled = np.tile(mono_type_array, int_ghost_shape)\n    # # ij_type_match = ij_type_match_lookup_table[ghost_indices_repeated, surf_indices_tiled]\n    # # ij_type_match = np.where(ghost_indices_repeated == surf_indices_tiled, True, False)\n    # # ij_type_match = ghost_indices_repeated == surf_indices_tiled\n    # ij_type_match_lookup_table = (ghost_indices_repeated == surf_indices_tiled).reshape(int_ghost_shape, -1)\n    # ij_type_match = ij_type_match_lookup_table[ghost_indices_repeated, surf_indices_tiled]\n    # # possible_fragments_pairs = ghost_indices_repeated.shape[0]\n    # passing_ghost_indices = ghost_indices_repeated[ij_type_match]\n    # passing_surf_indices = surf_indices_tiled[ij_type_match]\n    # # passing_ghost_coords = residue1_ghost_guide_coords[ij_type_match]\n    # # passing_frag_coords = residue2_guide_coords[ij_type_match]\n    # passing_ghost_coords = residue1_ghost_guide_coords[passing_ghost_indices]\n    # passing_frag_coords = residue2_guide_coords[passing_surf_indices]\n    # Precalculate the reference_rmsds for each ghost fragment\n    reference_rmsds = np.array([ghost_frags1[ghost_idx].rmsd for ghost_idx in passing_ghost_indices.tolist()])\n    # # Todo keep without euler_lookup?\n    # reference_rmsds = np.array([ghost_frag.rmsd for ghost_frag in ghost_frags1])[passing_ghost_indices]\n\n    # logger.debug('Calculating passing fragment overlaps by RMSD')\n    # all_fragment_match = metrics.calculate_match(passing_ghost_coords, passing_frag_coords, reference_rmsds)\n    # passing_overlaps_indices = np.flatnonzero(all_fragment_match &gt; min_match_value)\n    all_fragment_z_score = metrics.rmsd_z_score(passing_ghost_coords, passing_frag_coords, reference_rmsds)\n    passing_overlaps_indices = np.flatnonzero(all_fragment_z_score &lt; min_match_value)\n    # logger.debug('Finished calculating fragment overlaps')\n    # logger.debug(f'Found {len(passing_overlaps_indices)} overlapping fragments over the {min_match_value} threshold')\n    logger.debug(f'Found {len(passing_overlaps_indices)} overlapping fragments under the {min_match_value} threshold')\n\n    # interface_ghostfrags = [ghost_frags1[idx] for idx in passing_ghost_indices[passing_overlap_indices].tolist()]\n    # interface_monofrags2 = [fragments2[idx] for idx in passing_surf_indices[passing_overlap_indices].tolist()]\n    # passing_z_values = all_fragment_overlap[passing_overlap_indices]\n    # match_scores = utils.match_score_from_z_value(all_fragment_overlap[passing_overlap_indices])\n\n    return list(zip([ghost_frags1[idx] for idx in passing_ghost_indices[passing_overlaps_indices].tolist()],\n                    [fragments2[idx] for idx in passing_frag_indices[passing_overlaps_indices].tolist()],\n                    # all_fragment_match[passing_overlaps_indices].tolist()))\n                    metrics.match_score_from_z_value(all_fragment_z_score[passing_overlaps_indices]).tolist()))\n</code></pre>"},{"location":"reference/structure/fragment/#structure.fragment.create_fragment_info_from_pairs","title":"create_fragment_info_from_pairs","text":"<pre><code>create_fragment_info_from_pairs(ghostfrag_frag_pairs: list[tuple[GhostFragment, Fragment, float]]) -&gt; list[FragmentInfo]\n</code></pre> <p>From a ghost fragment/surface fragment pair and corresponding match score, return the pertinent interface information</p> <p>Parameters:</p> <ul> <li> <code>ghostfrag_frag_pairs</code>             (<code>list[tuple[GhostFragment, Fragment, float]]</code>)         \u2013          <p>Observed ghost and surface fragment overlaps and their match score</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[FragmentInfo]</code>         \u2013          <p>The formatted fragment information for each pair {'mapped': int, 'paired': int, 'match': float, 'cluster': tuple(int, int, int)}</p> </li> </ul> Source code in <code>symdesign/structure/fragment/__init__.py</code> <pre><code>def create_fragment_info_from_pairs(\n    ghostfrag_frag_pairs: list[tuple[GhostFragment, Fragment, float]]\n) -&gt; list[db.FragmentInfo]:\n    \"\"\"From a ghost fragment/surface fragment pair and corresponding match score, return the pertinent interface\n    information\n\n    Args:\n        ghostfrag_frag_pairs: Observed ghost and surface fragment overlaps and their match score\n\n    Returns:\n        The formatted fragment information for each pair\n            {'mapped': int, 'paired': int, 'match': float, 'cluster': tuple(int, int, int)}\n    \"\"\"\n    fragment_matches = [db.FragmentInfo(mapped=ghost_frag.index, paired=surf_frag.index,\n                                        match=match_score, cluster=ghost_frag.ijk)\n                        for ghost_frag, surf_frag, match_score in ghostfrag_frag_pairs]\n\n    logger.debug(f'Fragments for Entity1 found at indices: {[frag.mapped for frag in fragment_matches]}')\n    logger.debug(f'Fragments for Entity2 found at indices: {[frag.paired for frag in fragment_matches]}')\n\n    return fragment_matches\n</code></pre>"},{"location":"reference/structure/fragment/db/","title":"db","text":""},{"location":"reference/structure/fragment/db/#structure.fragment.db.fragment_factory","title":"fragment_factory  <code>module-attribute</code>","text":"<pre><code>fragment_factory: Annotated[FragmentDatabaseFactory, 'Calling this factory method returns the single instance of the FragmentDatabase class containing fragment information specified by the \"source\" keyword argument'] = FragmentDatabaseFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the FragmentDatabase class containing fragment information specified by the \"source\" keyword argument</p>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.euler_factory","title":"euler_factory  <code>module-attribute</code>","text":"<pre><code>euler_factory: Annotated[EulerLookupFactory, 'Calling this factory method returns the single instance of the EulerLookup class'] = EulerLookupFactory()\n</code></pre> <p>Calling this factory method returns the single instance of the EulerLookup class</p>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentObservation","title":"FragmentObservation","text":""},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentObservation.match","title":"match  <code>instance-attribute</code>","text":"<pre><code>match: float\n</code></pre> <p>The match of the entire fragment observation to the fragment cluster representative</p>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentObservation.weight","title":"weight  <code>instance-attribute</code>","text":"<pre><code>weight: float\n</code></pre> <p>The contribution of the FragObservation to atomic interactions from the entire fragment observation</p>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabase","title":"FragmentDatabase","text":"<pre><code>FragmentDatabase(**kwargs)\n</code></pre> <p>             Bases: <code>FragmentInfo</code></p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __init__(self, **kwargs):  # init_db: bool = True, fragment_length: int = 5\n    super().__init__(**kwargs)\n    if self.source == putils.biological_interfaces:  # Todo parameterize\n        self.cluster_representatives_path = putils.intfrag_cluster_rep_dirpath\n        self.monofrag_representatives_path = putils.monofrag_cluster_rep_dirpath\n\n    self.representatives = {}\n    self.paired_frags = {}\n    self.indexed_ghosts = {}\n    self.euler_lookup = None\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabase.calculate_match_metrics","title":"calculate_match_metrics","text":"<pre><code>calculate_match_metrics(fragment_matches: list[FragmentInfo]) -&gt; dict[str, Any]\n</code></pre> <p>Return the various metrics calculated by overlapping fragments at the interface of two proteins</p> <p>Parameters:</p> <ul> <li> <code>fragment_matches</code>             (<code>list[FragmentInfo]</code>)         \u2013          <p>[{'mapped': entity1_index, 'paired': entity2_index,                 'cluster': tuple(int, int, int), 'match': score_term}, ...]</p> </li> </ul> <p>Returns:     {'mapped': {'center': {'indices': (set[int]), 'score': (float),},                 'total': {'indices': (set[int]), 'score': (float),},                 'match_scores': {residue number(int): (list[score (float)]), ...},                 'index_count': {index (int): count (int), ...},                 'multiple_ratio': (float)}      'paired': {'center': , 'total': , 'match_scores': , 'index_count': , 'multiple_ratio': },      'total':  {'center': {'score': , 'number': },                 'total': {'score': , 'number': },                 'index_count': , 'multiple_ratio': , 'observations': (int)}      }</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def calculate_match_metrics(self, fragment_matches: list[FragmentInfo]) -&gt; dict[str, Any]:\n    \"\"\"Return the various metrics calculated by overlapping fragments at the interface of two proteins\n\n    Args:\n        fragment_matches: [{'mapped': entity1_index, 'paired': entity2_index,\n                            'cluster': tuple(int, int, int), 'match': score_term}, ...]\n    Returns:\n        {'mapped': {'center': {'indices': (set[int]), 'score': (float),},\n                    'total': {'indices': (set[int]), 'score': (float),},\n                    'match_scores': {residue number(int): (list[score (float)]), ...},\n                    'index_count': {index (int): count (int), ...},\n                    'multiple_ratio': (float)}\n         'paired': {'center': , 'total': , 'match_scores': , 'index_count': , 'multiple_ratio': },\n         'total':  {'center': {'score': , 'number': },\n                    'total': {'score': , 'number': },\n                    'index_count': , 'multiple_ratio': , 'observations': (int)}\n         }\n    \"\"\"\n    if not fragment_matches:\n        return {}\n\n    fragment_i_index_count_d = {frag_idx: 0 for frag_idx in range(1, self.fragment_length + 1)}\n    fragment_j_index_count_d = fragment_i_index_count_d.copy()\n    total_fragment_content = fragment_i_index_count_d.copy()\n\n    # entity1_center_match_scores, entity2_center_match_scores = {}, {}\n    # entity1_match_scores, entity2_match_scores = {}, {}\n    entity1_center_match_scores = defaultdict(list)\n    entity2_center_match_scores = defaultdict(list)\n    entity1_match_scores = defaultdict(list)\n    entity2_match_scores = defaultdict(list)\n    total_observations = len(fragment_matches)\n    separated_fragment_metrics = {}\n    # separated_fragment_metrics = {'mapped': {'center': {'indices': set()}, 'total': {'indices': set()}},\n    #                               'paired': {'center': {'indices': set()}, 'total': {'indices': set()}},\n    #                               'total': {'observations': len(fragment_matches), 'center': {}, 'total': {}}}\n    for fragment in fragment_matches:\n        center_residx1 = fragment.mapped  # fragment['mapped']\n        center_residx2 = fragment.paired  # fragment['paired']\n        match_score = fragment.match  # fragment['match']\n        i, j, k = fragment.cluster  # fragment['cluster']\n        fragment_i_index_count_d[i] += 1\n        fragment_j_index_count_d[j] += 1\n\n        entity1_center_match_scores[center_residx1].append(match_score)\n        entity2_center_match_scores[center_residx2].append(match_score)\n\n        for idx1, idx2 in [(center_residx1 + j, center_residx2 + j) for j in self.fragment_range]:\n            entity1_match_scores[idx1].append(match_score)\n            entity2_match_scores[idx2].append(match_score)\n\n    # -------------------------------------------\n    # Score the interface individually\n    mapped_center_score = nanohedra_fragment_match_score(entity1_center_match_scores.values())\n    paired_center_score = nanohedra_fragment_match_score(entity2_center_match_scores.values())\n    mapped_total_score = nanohedra_fragment_match_score(entity1_match_scores.values())\n    paired_total_score = nanohedra_fragment_match_score(entity2_match_scores.values())\n    # Combine\n    all_residue_score = mapped_total_score + paired_total_score\n    center_residue_score = mapped_center_score + paired_center_score\n    # -------------------------------------------\n    entity1_indices = set(entity1_center_match_scores.keys())\n    entity1_total_indices = set(entity1_match_scores.keys())\n    entity2_indices = set(entity2_center_match_scores.keys())\n    entity2_total_indices = set(entity2_match_scores.keys())\n    # Get individual number of CENTRAL residues with overlapping fragments given z_value criteria\n    mapped_central_residues_with_fragment_overlap = len(entity1_indices)\n    paired_central_residues_with_fragment_overlap = len(entity2_indices)\n    # Combine\n    central_residues_with_fragment_overlap = \\\n        mapped_central_residues_with_fragment_overlap + paired_central_residues_with_fragment_overlap\n    # -------------------------------------------\n    # Get the individual number of TOTAL residues with overlapping fragments given z_value criteria\n    mapped_total_indices_with_fragment_overlap = len(entity1_total_indices)\n    paired_total_indices_with_fragment_overlap = len(entity2_total_indices)\n    # Combine\n    total_indices_with_fragment_overlap = \\\n        mapped_total_indices_with_fragment_overlap + paired_total_indices_with_fragment_overlap\n    # -------------------------------------------\n    # Get the individual multiple fragment observation ratio observed for each side of the fragment query\n    mapped_multiple_fragment_ratio = total_observations / mapped_central_residues_with_fragment_overlap\n    paired_multiple_fragment_ratio = total_observations / paired_central_residues_with_fragment_overlap\n    # Combine\n    multiple_fragment_ratio = total_observations*2 / central_residues_with_fragment_overlap\n    # -------------------------------------------\n    # Turn individual index counts into paired counts\n    for index, count in fragment_i_index_count_d.items():\n        total_fragment_content[index] += count\n        # separated_fragment_metrics['mapped']['index'][index_count] = count / separated_fragment_metrics['number']\n    for index, count in fragment_j_index_count_d.items():\n        total_fragment_content[index] += count\n        # separated_fragment_metrics['paired']['index'][index_count] = count / separated_fragment_metrics['number']\n    # combined\n    # for index, count in total_fragment_content.items():\n    #     total_fragment_content[index] = count / (separated_fragment_metrics['total']['observations'] * 2)\n    # -------------------------------------------\n    separated_fragment_metrics['mapped'] = dict(\n        center=dict(score=mapped_center_score,\n                    # number=mapped_central_residues_with_fragment_overlap,\n                    indices=entity1_indices),\n        # center_match_scores=entity1_center_match_scores,\n        # match_scores=entity1_match_scores,\n        # index_count=fragment_i_index_count_d,\n        total=dict(score=mapped_total_score,\n                   # number=mapped_total_indices_with_fragment_overlap,\n                   indices=entity1_total_indices),\n        multiple_ratio=mapped_multiple_fragment_ratio\n    )\n    separated_fragment_metrics['paired'] = dict(\n        center=dict(score=paired_center_score,\n                    # number=paired_central_residues_with_fragment_overlap,\n                    indices=entity2_indices),\n        # center_match_scores=entity2_center_match_scores,\n        # match_scores=entity2_match_scores,\n        # index_count=fragment_j_index_count_d,\n        total=dict(score=paired_total_score,\n                   # number=paired_total_indices_with_fragment_overlap,\n                   indices=entity2_total_indices),\n        multiple_ratio=paired_multiple_fragment_ratio\n    )\n    # separated_fragment_metrics['mapped']['center']['score'] = mapped_center_score\n    # separated_fragment_metrics['mapped']['center']['number'] = mapped_central_residues_with_fragment_overlap\n    # separated_fragment_metrics['mapped']['total']['score'] = mapped_total_score\n    # separated_fragment_metrics['mapped']['total']['number'] = mapped_total_indices_with_fragment_overlap\n    # separated_fragment_metrics['mapped']['multiple_ratio'] = mapped_multiple_fragment_ratio\n    # separated_fragment_metrics['paired']['center']['score'] = paired_center_score\n    # separated_fragment_metrics['paired']['center']['number'] = paired_central_residues_with_fragment_overlap\n    # separated_fragment_metrics['paired']['total']['score'] = paired_total_score\n    # separated_fragment_metrics['paired']['total']['number'] = paired_total_indices_with_fragment_overlap\n    # separated_fragment_metrics['paired']['multiple_ratio'] = paired_multiple_fragment_ratio\n\n    separated_fragment_metrics['total'] = dict(\n        observations=total_observations,\n        multiple_ratio=multiple_fragment_ratio,\n        index_count=total_fragment_content,\n        center=dict(score=center_residue_score, number=central_residues_with_fragment_overlap),\n        total=dict(score=all_residue_score, number=total_indices_with_fragment_overlap)\n    )\n\n    return separated_fragment_metrics\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabase.format_fragment_metrics","title":"format_fragment_metrics  <code>staticmethod</code>","text":"<pre><code>format_fragment_metrics(metrics: dict) -&gt; dict\n</code></pre> <p>For a set of fragment metrics, return the formatted total fragment metrics</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>             (<code>dict</code>)         \u2013          </li> </ul> <p>Returns:     {center_indices, total_indices,      nanohedra_score, nanohedra_score_center, multiple_fragment_ratio, number_residues_fragment_total,      number_residues_fragment_center, number_fragments,      percent_fragment_helix, percent_fragment_strand, percent_fragment_coil}</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>@staticmethod\ndef format_fragment_metrics(metrics: dict) -&gt; dict:\n    \"\"\"For a set of fragment metrics, return the formatted total fragment metrics\n\n    Args:\n        metrics:\n    Returns:\n        {center_indices, total_indices,\n         nanohedra_score, nanohedra_score_center, multiple_fragment_ratio, number_residues_fragment_total,\n         number_residues_fragment_center, number_fragments,\n         percent_fragment_helix, percent_fragment_strand, percent_fragment_coil}\n    \"\"\"\n    return {\n        'center_indices': metrics['mapped']['center']['indices'].union(metrics['paired']['center']['indices']),\n        'total_indices': metrics['mapped']['total']['indices'].union(metrics['paired']['total']['indices']),\n        'nanohedra_score': metrics['total']['total']['score'],\n        'nanohedra_score_center': metrics['total']['center']['score'],\n        'multiple_fragment_ratio': metrics['total']['multiple_ratio'],\n        'number_residues_fragment_total': metrics['total']['total']['number'],\n        'number_residues_fragment_center': metrics['total']['center']['number'],\n        'number_fragments': metrics['total']['observations'],\n        # Todo ensure these metrics are accounted for if using a different cluster index\n        'percent_fragment_helix': (metrics['total']['index_count'][1] / (metrics['total']['observations'] * 2)),\n        'percent_fragment_strand': (metrics['total']['index_count'][2] / (metrics['total']['observations'] * 2)),\n        'percent_fragment_coil': (metrics['total']['index_count'][3] + metrics['total']['index_count'][4]\n                                  + metrics['total']['index_count'][5]) / (metrics['total']['observations'] * 2)}\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabaseFactory","title":"FragmentDatabaseFactory","text":"<pre><code>FragmentDatabaseFactory(**kwargs)\n</code></pre> <p>Return a FragmentDatabase instance by calling the Factory instance with the FragmentDatabase source name</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer to the named FragmentDatabase</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __init__(self, **kwargs):\n    self._databases = {}\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabaseFactory.__call__","title":"__call__","text":"<pre><code>__call__(source: str = putils.biological_interfaces, token: int = None, **kwargs) -&gt; FragmentDatabase | None\n</code></pre> <p>Return the specified FragmentDatabase object singleton</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>biological_interfaces</code> )         \u2013          <p>The FragmentDatabase source name</p> </li> <li> <code>token</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>Provide the initialization token to skip construction</p> </li> </ul> <p>Returns:     The instance of the specified FragmentDatabase</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __call__(\n    self, source: str = putils.biological_interfaces, token: int = None, **kwargs\n) -&gt; FragmentDatabase | None:\n    \"\"\"Return the specified FragmentDatabase object singleton\n\n    Args:\n        source: The FragmentDatabase source name\n        token: Provide the initialization token to skip construction\n    Returns:\n        The instance of the specified FragmentDatabase\n    \"\"\"\n    fragment_db = self._databases.get(source)\n    if fragment_db:\n        return fragment_db\n    elif source == putils.biological_interfaces:\n        if token == RELOAD_DB:\n            return None\n        try:\n            self._databases[source] = utils.unpickle(putils.biological_fragment_db_pickle)\n        except (ModuleNotFoundError, FileNotFoundError):\n            raise RuntimeError(\n                f\"Couldn't access the serialized {FragmentDatabase.__name__} which is required for operation. \"\n                f\"Please reload this by executing '{putils.pickle_program_requirements_cmd}'\")\n    else:\n        self._databases[source] = FragmentDatabase(source=source, **kwargs)\n\n    logger.info(f'Initializing {FragmentDatabase.__name__}({source})')\n    # Attach the EulerLookup singleton\n    self._databases[source].euler_lookup = euler_factory()\n\n    return self._databases[source]\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.FragmentDatabaseFactory.get","title":"get","text":"<pre><code>get(**kwargs) -&gt; FragmentDatabase\n</code></pre> <p>Return the specified FragmentDatabase object singleton</p> <p>Other Parameters:</p> <ul> <li> <code>source</code>         \u2013          <p>str = putils.biological_interfaces - The FragmentDatabase source name</p> </li> <li> <code>token</code>         \u2013          <p>int = None - Provide the initialization token to skip construction</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>FragmentDatabase</code>         \u2013          <p>The instance of the specified FragmentDatabase</p> </li> </ul> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def get(self, **kwargs) -&gt; FragmentDatabase:\n    \"\"\"Return the specified FragmentDatabase object singleton\n\n    Keyword Args:\n        source: str = putils.biological_interfaces - The FragmentDatabase source name\n        token: int = None - Provide the initialization token to skip construction\n\n    Returns:\n        The instance of the specified FragmentDatabase\n    \"\"\"\n    return self.__call__(**kwargs)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher","title":"TransformHasher","text":"<pre><code>TransformHasher(max_width: float, translation_bin_width: float = 0.2, rotation_bin_width: float = 0.5, dimensions: int = 6)\n</code></pre> <p>Constructs a 6D mapping of transformation bins</p> <p>Parameters:</p> <ul> <li> <code>max_width</code>             (<code>float</code>)         \u2013          <p>The radius of a box to define the boundaries of the transformation bin space</p> </li> <li> <code>translation_bin_width</code>             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>The size of the translation bins</p> </li> <li> <code>rotation_bin_width</code>             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>The size of the rotation bins</p> </li> <li> <code>dimensions</code>             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>The number of dimensions to construct</p> </li> </ul> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __init__(self, max_width: float, translation_bin_width: float = 0.2, rotation_bin_width: float = 0.5,\n             dimensions: int = 6):\n    \"\"\"Construct the instance\n\n    Args:\n        max_width: The radius of a box to define the boundaries of the transformation bin space\n        translation_bin_width: The size of the translation bins\n        rotation_bin_width:  The size of the rotation bins\n        dimensions: The number of dimensions to construct\n    \"\"\"\n    half_box_width = max_width\n    self.box_width = 2 * half_box_width\n    self.dimensions = dimensions\n    self.translation_bin_width = translation_bin_width  # Angstrom minimum amount of search difference\n    self.rotation_bin_width = rotation_bin_width  # Degree to bin euler angles\n    self.offset_scipy_euler_to_bin = [180, 90, 180]\n    logger.debug(f'translation_bin_width: {self.translation_bin_width}')\n    logger.debug(f'rotation_bin_width: {self.rotation_bin_width}')\n    # half_box_width = raidus1 - radius2\n    # box_width = half_box_width * 2\n    # half_box_width = max_width / 2\n    self.box_lower = np.array([-half_box_width, -half_box_width, -half_box_width])\n    logger.debug(f'self.box_lower: {self.box_lower}')\n    # box_upper = self.box_lower * -1\n\n    # Create dimsizes_\n    number_of_bins = math.ceil(self.box_width / self.translation_bin_width)\n    # number_tx_dimensions = np.full(3, number_of_bins, np.int)\n    number_tx_bins = [number_of_bins, number_of_bins, number_of_bins]\n    self.index_with_360 = [0, 2]\n    number_of_rot_bins_360, remainder360 = divmod(360, self.rotation_bin_width)\n    number_of_rot_bins_180, remainder180 = divmod(180, self.rotation_bin_width)\n    # Ensure the values are integers\n    number_of_rot_bins_360 = int(number_of_rot_bins_360)\n    number_of_rot_bins_180 = int(number_of_rot_bins_180)\n    # The number of bins always should be an integer. Ensure quotient is an integer for both 360/X and 180/X\n    # otherwise the bins wouldn't space correctly as they wrap around\n    if remainder180 != 0 or remainder360 != 0:\n        raise ValueError(\n            f\"The value of the rotation_bin_size {self.rotation_bin_width} must be an integer denominator of\"\n            f\" both 360 and 180. Got 360/rotation_bin_size={360 / self.rotation_bin_width},\"\n            f\" 180/rotation_bin_size={180 / self.rotation_bin_width}\")\n    # Keeping with convention, keep the smaller euler angle on the second axis...\n    # number_euler_dimensions = \\\n    #     np.array([number_of_rot_bins_360, number_of_rot_bins_180, number_of_rot_bins_360], np.int)\n    number_euler_bins = [number_of_rot_bins_360, number_of_rot_bins_180, number_of_rot_bins_360]\n\n    # Create dimprods_ . The form of this is different given the placement of the \"theta\" angle in the last position\n    # of the euler array in SixDHasher.cc and using the conventional 2nd position here\n    # dimension_products = np.ones(6, np.int)\n    # for i in range(2, 7):  # [-2,-3,-4,-5,-6]\n    #     dimension_products[-i] =\n    dimension_products = [1, 0, 0, 0, 0, 0]\n    try:\n        for idx, (product, dimension_size) in enumerate(zip(dimension_products,\n                                                            number_euler_bins\n                                                            + number_tx_bins), 1):\n            # input(f'product:{product}dimension_size:{dimension_size}')\n            dimension_products[idx] = product * dimension_size\n    except IndexError:  # Reached the end of dimension_products\n        # Reverse the order for future usage\n        self.dimension_products = [int(product) for product in dimension_products[::-1]]\n        logger.debug(f'Found dimension_products: {self.dimension_products} from {dimension_products}')\n\n    self.number_tx_bins = np.array(number_tx_bins)\n    self.number_euler_bins = np.array(number_euler_bins)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.transforms_to_bin_integers","title":"transforms_to_bin_integers","text":"<pre><code>transforms_to_bin_integers(rotations: ndarray, translations: ndarray) -&gt; ndarray\n</code></pre> <p>From transformations consisting of stacked rotations and translations, identify the bin that each transformation falls into along each transformation dimension</p> <p>Parameters:</p> <ul> <li> <code>rotations</code>             (<code>ndarray</code>)         \u2013          <p>Array with shape (N, 3, 3) where each N is the rotation paired with the translation N</p> </li> <li> <code>translations</code>             (<code>ndarray</code>)         \u2013          <p>Array with shape (N, 3) where each N is the translation paired with the rotation N</p> </li> </ul> <p>Returns:     Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,         that each transform occupies</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def transforms_to_bin_integers(self, rotations: np.ndarray, translations: np.ndarray) -&gt; np.ndarray:\n    \"\"\"From transformations consisting of stacked rotations and translations, identify the bin that each\n    transformation falls into along each transformation dimension\n\n\n    Args:\n        rotations: Array with shape (N, 3, 3) where each N is the rotation paired with the translation N\n        translations: Array with shape (N, 3) where each N is the translation paired with the rotation N\n    Returns:\n        Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,\n            that each transform occupies\n    \"\"\"\n    #     -&gt; tuple[int, int, int, int, int, int]:\n    # For each of the translations, set in reference to the lower bound and divide by the bin widths to find the bin\n    translation_bin_int = (translations - self.box_lower) // self.translation_bin_width\n    if (translation_bin_int &lt; 0).any():\n        raise ValueError(\n            'Found a value for the binned translation integers less than 0. The transform box for the'\n            f\" requested {self.__class__.__name__} can't properly handle the passed transformation\")\n    rotation = scipy.spatial.transform.Rotation.from_matrix(rotations)\n    # 'xyz' lowercase denotes the rotation is external\n    euler_angles = rotation.as_euler('xyz', degrees=True)\n    euler_angles += self.offset_scipy_euler_to_bin\n    rotation_bin_int = euler_angles // self.rotation_bin_width\n    # Using the modulo operator should enable the returned euler_angles, which are in the range:\n    # -180,180, -90,90, and -180,180\n    # To be in the correct bins between 0,360, 0,180, 0,360\n    # rotation_bin_int %= self.number_euler_bins\n    # rotation_bin_int[:, self.index_with_360] %= self.number_euler_bins\n    logger.debug(f'First 3 euler_angles:\\n{euler_angles[:3]}')\n    logger.debug(f'First 3 rotation_bin_int:\\n{rotation_bin_int[:3]}')\n    logger.debug(f'First 3 translation_bin_int:\\n{translation_bin_int[:3]}')\n\n    return np.concatenate((translation_bin_int.astype(int), rotation_bin_int.astype(int)), axis=1)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.hash_bin_integers","title":"hash_bin_integers","text":"<pre><code>hash_bin_integers(bin_integers: ndarray) -&gt; ndarray\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bin_integers</code>             (<code>ndarray</code>)         \u2013          <p>Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension, that each transform occupies</p> </li> </ul> <p>Returns:     An integer hashing the binned integers which describe a position in the described 3D space</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def hash_bin_integers(self, bin_integers: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n\n    Args:\n        bin_integers: Array with shape (number_of_transforms, dimensions) representing the unique bins along each\n            dimension, that each transform occupies\n    Returns:\n        An integer hashing the binned integers which describe a position in the described 3D space\n    \"\"\"\n    # # This is used for boost library c++ stuff\n    # self.max_int32_prime = 999879\n    # return (bin_integers * self.dimension_products).sum(axis=-1) % self.max_int32_prime\n    # logger.debug(f'bin_integers[:3]: {bin_integers[:3]}')\n    # result = (bin_integers * self.dimension_products).sum(axis=-1)\n    # input(result[:3])\n    # input(result.dtype)\n    return (bin_integers * self.dimension_products).sum(axis=-1).astype(int)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.transforms_to_hash","title":"transforms_to_hash","text":"<pre><code>transforms_to_hash(rotations: ndarray, translations: ndarray) -&gt; ndarray\n</code></pre> <p>From pairs of rotations and translations, create a hash to describe the complete transformation</p> <p>Parameters:</p> <ul> <li> <code>rotations</code>             (<code>ndarray</code>)         \u2013          <p>Array with shape (N, 3, 3) where each N is the rotation paired with the translation N</p> </li> <li> <code>translations</code>             (<code>ndarray</code>)         \u2013          <p>Array with shape (N, 3) where each N is the translation paired with the rotation N</p> </li> </ul> <p>Returns:     An integer hashing the transforms which relates to distinct orientational offset in the described space</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def transforms_to_hash(self, rotations: np.ndarray, translations: np.ndarray) -&gt; np.ndarray:\n    \"\"\"From pairs of rotations and translations, create a hash to describe the complete transformation\n\n    Args:\n        rotations: Array with shape (N, 3, 3) where each N is the rotation paired with the translation N\n        translations: Array with shape (N, 3) where each N is the translation paired with the rotation N\n    Returns:\n        An integer hashing the transforms which relates to distinct orientational offset in the described space\n    \"\"\"\n    return self.hash_bin_integers(self.transforms_to_bin_integers(rotations, translations))\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.hash_to_bins","title":"hash_to_bins","text":"<pre><code>hash_to_bins(index: int | ndarray) -&gt; ndarray\n</code></pre> <p>From a hash or multiple hashes describing a transformation, return the bins that that hash belongs too</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int | ndarray</code>)         \u2013          <p>The hash(es) to calculate bins for</p> </li> </ul> <p>Returns:     Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,         that each hash maps to the transformation space</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def hash_to_bins(self, index: int | np.ndarray) -&gt; np.ndarray:\n    \"\"\"From a hash or multiple hashes describing a transformation, return the bins that that hash belongs too\n\n    Args:\n        index: The hash(es) to calculate bins for\n    Returns:\n        Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,\n            that each hash maps to the transformation space\n    \"\"\"\n    try:\n        bins = np.zeros((len(index), self.dimensions))\n    except TypeError:  # int doesn't have len()\n        bins = [0 for _ in range(self.dimensions)]\n        for idx, product in enumerate(self.dimension_products):\n            bins[idx], index = divmod(index, product)\n        bins = np.array(bins)\n    else:\n        for idx, product in enumerate(self.dimension_products):\n            bins[:, idx], index = divmod(index, product)\n\n    return bins\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.bins_to_transform","title":"bins_to_transform","text":"<pre><code>bins_to_transform(bins: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Parameters:</p> <ul> <li> <code>bins</code>             (<code>ndarray</code>)         \u2013          <p>The integers which express particular bins in the transformation space</p> </li> </ul> <p>Returns:     The tuple of stacked rotations and stacked translations</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def bins_to_transform(self, bins: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n\n    Args:\n        bins: The integers which express particular bins in the transformation space\n    Returns:\n        The tuple of stacked rotations and stacked translations\n    \"\"\"\n    translation_bin_int: np.ndarray = bins[:, :3]\n    translations = translation_bin_int*self.translation_bin_width + self.box_lower\n    rotation_bin_int: np.ndarray = bins[:, 3:]\n    offset_euler_angles = rotation_bin_int * self.rotation_bin_width\n    euler_angles = offset_euler_angles - self.offset_scipy_euler_to_bin\n    # 'xyz' lowercase denotes the rotation is external\n    rotation = scipy.spatial.transform.Rotation.from_euler('xyz', euler_angles, degrees=True)\n    rotations = rotation.as_matrix()\n\n    return rotations, translations\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.TransformHasher.hash_to_transforms","title":"hash_to_transforms","text":"<pre><code>hash_to_transforms(index: int | ndarray) -&gt; ndarray\n</code></pre> <p>From a hash or multiple hashes describing a transformation, return the bins that that hash belongs too</p> <p>Parameters:</p> <ul> <li> <code>index</code>             (<code>int | ndarray</code>)         \u2013          <p>The hash(es) to calculate bins for</p> </li> </ul> <p>Returns:     Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,         that each hash maps to the transformation space</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def hash_to_transforms(self, index: int | np.ndarray) -&gt; np.ndarray:\n    \"\"\"From a hash or multiple hashes describing a transformation, return the bins that that hash belongs too\n\n    Args:\n        index: The hash(es) to calculate bins for\n    Returns:\n        Array with shape (number_of_transforms, dimensions) representing the unique bins along each dimension,\n            that each hash maps to the transformation space\n    \"\"\"\n    # bins = self.hash_to_bins(index)\n    # input(bins[:3])\n    return self.bins_to_transform(self.hash_to_bins(index))\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup","title":"EulerLookup","text":"<pre><code>EulerLookup(scale: float = 3.0)\n</code></pre> <p>Handle the lookup of Euler angles which are roughly segregated into similar rotational bins</p> <p>Parameters:</p> <ul> <li> <code>scale</code>             (<code>float</code>, default:                 <code>3.0</code> )         \u2013          <p>The size of the vectors used to describe the basis vectors</p> </li> </ul> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __init__(self, scale: float = 3.):\n    \"\"\"Construct the instance\n\n    Args:\n        scale: The size of the vectors used to describe the basis vectors\n    \"\"\"\n    # 6-d bool array [[[[[[True, False, ...], ...]]]]] with shape (37, 19, 37, 37, 19, 37)\n    self.eul_lookup_40 = np.load(putils.binary_lookup_table_path)['a']\n    \"\"\"Indicates whether two sets of triplet integer values for each Euler angle are within an acceptable angular \n    offset. Acceptable offset is approximately +/- 40 degrees, or +/- 3 integers in one of the rotation angles and\n    a couple of integers in another i.e.\n    eul_lookup_40[1,5,1,1,8,1] -&gt; True\n    eul_lookup_40[1,5,1,1,9,1] -&gt; False\n    eul_lookup_40[1,5,1,1,7,4] -&gt; True\n    KM doesn't completely know how Todd made these\n    \"\"\"\n    self.indices_lens = [0, 0]\n    self.normalization = 1. / scale\n    self.one_tolerance = 1. - 1.e-6\n    self.eulint_divisor = 180. / np.pi * 0.1 * self.one_tolerance\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.eul_lookup_40","title":"eul_lookup_40  <code>instance-attribute</code>","text":"<pre><code>eul_lookup_40 = load(binary_lookup_table_path)['a']\n</code></pre> <p>Indicates whether two sets of triplet integer values for each Euler angle are within an acceptable angular  offset. Acceptable offset is approximately +/- 40 degrees, or +/- 3 integers in one of the rotation angles and a couple of integers in another i.e. eul_lookup_40[1,5,1,1,8,1] -&gt; True eul_lookup_40[1,5,1,1,9,1] -&gt; False eul_lookup_40[1,5,1,1,7,4] -&gt; True KM doesn't completely know how Todd made these</p>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.get_eulint_from_guides_as_array","title":"get_eulint_from_guides_as_array","text":"<pre><code>get_eulint_from_guides_as_array(guide_coords: ndarray) -&gt; ndarray\n</code></pre> <p>Take a set of guide atoms (3 xyz positions) and return integer indices for the euler angles describing the orientations of the axes they form. Note that the positions are in a 3D array. Each guide_coords[i,:,:] is a 3x3 array with the vectors stored in columns, i.e. one vector is in [i,:,j]. Use known scale value to normalize, to save repeated sqrt calculations</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def get_eulint_from_guides_as_array(self, guide_coords: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Take a set of guide atoms (3 xyz positions) and return integer indices for the euler angles describing the\n    orientations of the axes they form. Note that the positions are in a 3D array. Each guide_coords[i,:,:] is a 3x3\n    array with the vectors stored *in columns*, i.e. one vector is in [i,:,j]. Use known scale value to normalize,\n    to save repeated sqrt calculations\n    \"\"\"\n    # Todo Alternative\n    e_array = np.zeros_like(guide_coords)\n    # Subtract the guide coord origin from the other two dimensions to get unit basis vectors\n    e_array[:, :2, :] = (guide_coords[:, 1:, :] - guide_coords[:, :1, :]) * self.normalization\n    # Cross the basis vectors to get the orthogonal vector\n    e_array[:, 2, :] = np.cross(e_array[:, 0], e_array[:, 1])\n    # Bound by a min of -1 and max of 1 as arccos is valid in the domain of [1 to -1]\n    e_array[:, 2, 2] = np.minimum(1, e_array[:, 2, 2])\n    e_array[:, 2, 2] = np.maximum(-1, e_array[:, 2, 2])\n    third_angle_degenerate = np.abs(e_array[:, 2, 2]) &gt; self.one_tolerance\n    # Put the results in the first position of every instances first vector\n    # Second and third position are disregarded\n    e_array[:, 0, 0] = np.where(third_angle_degenerate,\n                                np.arctan2(*e_array[:, :2, 0].T),\n                                np.arctan2(e_array[:, 0, 2], -e_array[:, 1, 2]))\n    # Put the results in the first position of every instances second vector\n    # arccos returns values of [0 to pi]\n    e_array[:, 1, 0] = np.arccos(e_array[:, 2, 2])\n    # Put the results in the first position of every instances third vector\n    e_array[:, 2, 0] = np.where(third_angle_degenerate, 0, np.arctan2(*e_array[:, 2, :2].T))\n    # Values in range (-18 to 18) (not inclusive) v. Then add 36 and divide by 36 to get the abs\n    np.rint(e_array * self.eulint_divisor, out=e_array)\n    e_array[:, [0, 2]] += 36\n    e_array[:, [0, 2]] %= 36\n    # # This way allows unpacking for check_lookup_table()\n    # return e_array[:, :, 0].T.astype(int)\n    # This returns in the intuitive column orientation with \"C\" array ordering\n    return e_array[:, :, 0].astype(int)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.get_eulint_from_guides","title":"get_eulint_from_guides","text":"<pre><code>get_eulint_from_guides(guide_coords: ndarray) -&gt; tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Take a set of guide atoms (3 xyz positions) and return integer indices for the euler angles describing the orientations of the axes they form. Note that the positions are in a 3D array. Each guide_ats[i,:,:] is a 3x3 array with the vectors stored in columns, i.e. one vector is in [i,:,j]. Use known scale value to normalize, to save repeated sqrt calculations</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def get_eulint_from_guides(self, guide_coords: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Take a set of guide atoms (3 xyz positions) and return integer indices for the euler angles describing the\n    orientations of the axes they form. Note that the positions are in a 3D array. Each guide_ats[i,:,:] is a 3x3\n    array with the vectors stored *in columns*, i.e. one vector is in [i,:,j]. Use known scale value to normalize,\n    to save repeated sqrt calculations\n    \"\"\"\n    \"\"\"\n    v1_a: An array of vectors containing the first vector which is orthogonal to v2_a (canonically on x)\n    v2_a: An array of vectors containing the second vector which is orthogonal to v1_a (canonically on y)\n    v3_a: An array of vectors containing the third vector which is the cross product of v1_a and v2_a\n    \"\"\"\n    v1_a = (guide_coords[:, 1, :] - guide_coords[:, 0, :]) * self.normalization\n    v2_a = (guide_coords[:, 2, :] - guide_coords[:, 0, :]) * self.normalization\n    v3_a = np.cross(v1_a, v2_a)\n\n    \"\"\"Convert rotation matrix to euler angles in the form of an integer triplet (integer values are degrees\n    divided by 10; these become indices for a lookup table)\n    \"\"\"\n    # Bound by a min of -1 and max of 1 as arccos is valid in the domain of [1 to -1]\n    e2_v = np.minimum(1, v3_a[:, 2])\n    np.maximum(-1, e2_v, out=e2_v)\n\n    # Check if the third angle is degenerate\n    third_angle_degenerate = np.abs(e2_v) &gt; self.one_tolerance\n    e1_v = np.where(third_angle_degenerate,\n                    np.arctan2(v2_a[:, 0], v1_a[:, 0]),\n                    np.arctan2(v1_a[:, 2], -v2_a[:, 2]))\n\n    # arccos returns values of [0 to pi]\n    np.arccos(e2_v, out=e2_v)\n    e3_v = np.where(third_angle_degenerate, 0, np.arctan2(*v3_a[:, :2].T))\n\n    # np.floor(e1_v*self.eulint_divisor + .5, out=e1_v)  # More accurate\n    np.rint(e1_v * self.eulint_divisor, out=e1_v)\n    e1_v += 36\n    e1_v %= 36\n    # np.floor(e3_v*self.eulint_divisor + .5, out=e3_v)  # More accurate\n    np.rint(e3_v * self.eulint_divisor, out=e3_v)\n    e3_v += 36\n    e3_v %= 36\n    # Values in range (0 to 18) (not inclusive)\n    # np.floor(e2_v*self.eulint_divisor + .5, out=e2_v)  # More accurate\n    np.rint(e2_v * self.eulint_divisor, out=e2_v)\n\n    return e1_v.astype(int), e2_v.astype(int), e3_v.astype(int)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.lookup_by_euler_integers","title":"lookup_by_euler_integers","text":"<pre><code>lookup_by_euler_integers(*args: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a tuple with the index of the first fragment and second fragment where they overlap</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def lookup_by_euler_integers(self, *args: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a tuple with the index of the first fragment and second fragment where they overlap\n    \"\"\"\n    # Unpack each of the integer arrays\n    eulintarray1_1, eulintarray1_2, eulintarray1_3, eulintarray2_1, eulintarray2_2, eulintarray2_3 = args\n\n    indices1_len, indices2_len = len(eulintarray1_1), len(eulintarray2_1)\n\n    index_array1 = np.repeat(np.arange(indices1_len), indices2_len)\n    index_array2 = np.tile(np.arange(indices2_len), indices1_len)\n\n    # Construct the correctly sized arrays to lookup euler space matching pairs from the all to all guide_coords\n    # there may be some solution where numpy.meshgrid is used to broadcast the euler ints\n    # check lookup table\n    # start = time.time()\n    overlap = self.eul_lookup_40[np.repeat(eulintarray1_1, indices2_len),\n                                 np.repeat(eulintarray1_2, indices2_len),\n                                 np.repeat(eulintarray1_3, indices2_len),\n                                 np.tile(eulintarray2_1, indices1_len),\n                                 np.tile(eulintarray2_2, indices1_len),\n                                 np.tile(eulintarray2_3, indices1_len)]\n    # logger.debug(f'took {time.time() - start:8f}s')\n    # overlap = self.eul_lookup_40[*stacked_eulintarray1.T,\n    #                              stacked_eulintarray2.T]\n    # there may be some solution where numpy.ix_ is used to broadcast the index operation\n\n    return index_array1[overlap], index_array2[overlap]  # these are the overlapping ij pairs\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.lookup_by_euler_integers_as_array","title":"lookup_by_euler_integers_as_array","text":"<pre><code>lookup_by_euler_integers_as_array(eulintarray1: ndarray, eulintarray2: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a tuple with the index of the first fragment and second fragment where they overlap</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def lookup_by_euler_integers_as_array(self, eulintarray1: np.ndarray, eulintarray2: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns a tuple with the index of the first fragment and second fragment where they overlap\n    \"\"\"\n    indices1_len, indices2_len = len(eulintarray1), len(eulintarray2)\n\n    index_array1 = np.repeat(np.arange(indices1_len), indices2_len)\n    index_array2 = np.tile(np.arange(indices2_len), indices1_len)\n\n    # Construct the correctly sized arrays to lookup euler space matching pairs from the all to all guide_coords\n    # there may be some solution where numpy.meshgrid is used to broadcast the euler ints\n    # stacked_eulintarray1 = np.tile(eulintarray1, (indices2_len, 1))\n    # stacked_eulintarray2 = np.tile(eulintarray2, (indices1_len, 1))\n    # # check lookup table\n    # overlap = self.eul_lookup_40[stacked_eulintarray1[:, 0],\n    #                              stacked_eulintarray1[:, 1],\n    #                              stacked_eulintarray1[:, 2],\n    #                              stacked_eulintarray2[:, 0],\n    #                              stacked_eulintarray2[:, 1],\n    #                              stacked_eulintarray2[:, 2]]\n    # Transpose the intarray to have each of the columns unpacked as individual arrays\n    eulintarray1_1, eulintarray1_2, eulintarray1_3 = eulintarray1.T\n    eulintarray2_1, eulintarray2_2, eulintarray2_3 = eulintarray2.T\n\n    # Construct the correctly sized arrays to lookup euler space matching pairs from the all to all guide_coords\n    # there may be some solution where numpy.meshgrid is used to broadcast the euler ints\n    # check lookup table\n    # start = time.time()\n    overlap = self.eul_lookup_40[np.repeat(eulintarray1_1, indices2_len),\n                                 np.repeat(eulintarray1_2, indices2_len),\n                                 np.repeat(eulintarray1_3, indices2_len),\n                                 np.tile(eulintarray2_1, indices1_len),\n                                 np.tile(eulintarray2_2, indices1_len),\n                                 np.tile(eulintarray2_3, indices1_len)]\n    # logger.debug(f'took {time.time() - start:8f}s')\n    # overlap = self.eul_lookup_40[*stacked_eulintarray1.T,\n    #                              stacked_eulintarray2.T]\n    # there may be some solution where numpy.ix_ is used to broadcast the index operation\n\n    return index_array1[overlap], index_array2[overlap]  # these are the overlapping ij pairs\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookup.check_lookup_table","title":"check_lookup_table","text":"<pre><code>check_lookup_table(guide_coords1: ndarray, guide_coords2: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Returns a tuple with the index of the first fragment and second fragment where they overlap</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def check_lookup_table(self, guide_coords1: np.ndarray, guide_coords2: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    #                    return_bool: bool = False\n    \"\"\"Returns a tuple with the index of the first fragment and second fragment where they overlap\n    \"\"\"\n    # Ensure the atoms are passed as an array of (n, 3x3) matrices\n    try:\n        for idx, guide_coords in enumerate([guide_coords1, guide_coords2]):\n            indices_len, *remainder = guide_coords.shape\n            if remainder != [3, 3]:\n                logger.error(f'ERROR: guide coordinate array with wrong dimensions. '\n                             f'{guide_coords.shape} != (n, 3, 3)')\n                return np.array([]), np.array([])\n            self.indices_lens[idx] = indices_len\n    except (AttributeError, ValueError):  # guide_coords are the wrong format or the shape couldn't be unpacked\n        logger.error(f'ERROR: guide coordinate array wrong type {type(guide_coords).__name__} != (n, 3, 3)')\n        return np.array([]), np.array([])\n\n    eulintarray1_1, eulintarray1_2, eulintarray1_3 = self.get_eulint_from_guides(guide_coords1)\n    eulintarray2_1, eulintarray2_2, eulintarray2_3 = self.get_eulint_from_guides(guide_coords2)\n\n    indices1_len, indices2_len = self.indices_lens\n    index_array1 = np.repeat(np.arange(indices1_len), indices2_len)\n    index_array2 = np.tile(np.arange(indices2_len), indices1_len)\n\n    # Construct the correctly sized arrays to lookup euler space matching pairs from the all to all guide_coords\n    # eulintarray1_1_r = np.repeat(eulintarray1_1, indices2_len)\n    # eulintarray1_2_r = np.repeat(eulintarray1_2, indices2_len)\n    # eulintarray1_3_r = np.repeat(eulintarray1_3, indices2_len)\n    # eulintarray2_1_r = np.tile(eulintarray2_1, indices1_len)\n    # eulintarray2_2_r = np.tile(eulintarray2_2, indices1_len)\n    # eulintarray2_3_r = np.tile(eulintarray2_3, indices1_len)\n    # check lookup table\n    overlap = self.eul_lookup_40[np.repeat(eulintarray1_1, indices2_len),\n                                 np.repeat(eulintarray1_2, indices2_len),\n                                 np.repeat(eulintarray1_3, indices2_len),\n                                 np.tile(eulintarray2_1, indices1_len),\n                                 np.tile(eulintarray2_2, indices1_len),\n                                 np.tile(eulintarray2_3, indices1_len)]\n    # if return_bool:\n    #     return overlap\n    # else:\n    return index_array1[overlap], index_array2[overlap]  # these are the overlapping ij pairs\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookupFactory","title":"EulerLookupFactory","text":"<pre><code>EulerLookupFactory(**kwargs)\n</code></pre> <p>Return an EulerLookup instance by calling the Factory instance</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __init__(self, **kwargs):\n    self._lookup_tables = {}\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookupFactory.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs) -&gt; EulerLookup\n</code></pre> <p>Return the specified EulerLookup object singleton</p> <p>Returns:</p> <ul> <li> <code>EulerLookup</code>         \u2013          <p>The instance of the specified EulerLookup</p> </li> </ul> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def __call__(self, **kwargs) -&gt; EulerLookup:\n    \"\"\"Return the specified EulerLookup object singleton\n\n    Returns:\n        The instance of the specified EulerLookup\n    \"\"\"\n    lookup = self._lookup_tables.get('euler')\n    if lookup:\n        return lookup\n    else:\n        logger.info(f'Initializing {EulerLookup.__name__}()')\n        self._lookup_tables['euler'] = EulerLookup(**kwargs)\n\n    return self._lookup_tables['euler']\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.EulerLookupFactory.get","title":"get","text":"<pre><code>get(**kwargs) -&gt; EulerLookup\n</code></pre> <p>Return the specified EulerLookup object singleton</p> <p>Returns:</p> <ul> <li> <code>EulerLookup</code>         \u2013          <p>The instance of the specified EulerLookup</p> </li> </ul> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def get(self, **kwargs) -&gt; EulerLookup:\n    \"\"\"Return the specified EulerLookup object singleton\n\n    Returns:\n        The instance of the specified EulerLookup\n    \"\"\"\n    return self.__call__(**kwargs)\n</code></pre>"},{"location":"reference/structure/fragment/db/#structure.fragment.db.nanohedra_fragment_match_score","title":"nanohedra_fragment_match_score","text":"<pre><code>nanohedra_fragment_match_score(per_residue_match_scores: Iterable[Iterable[float]]) -&gt; float\n</code></pre> <p>Calculate the Nanohedra score from a dictionary with the 'center' residues and 'match_scores'</p> <p>Parameters:</p> <ul> <li> <code>per_residue_match_scores</code>             (<code>Iterable[Iterable[float]]</code>)         \u2013          <p>The residue mapped to its match score measurements</p> </li> </ul> <p>Returns:     The Nanohedra score</p> Source code in <code>symdesign/structure/fragment/db.py</code> <pre><code>def nanohedra_fragment_match_score(per_residue_match_scores: Iterable[Iterable[float]]) -&gt; float:\n    \"\"\"Calculate the Nanohedra score from a dictionary with the 'center' residues and 'match_scores'\n\n    Args:\n        per_residue_match_scores: The residue mapped to its match score measurements\n    Returns:\n        The Nanohedra score\n    \"\"\"\n    score = 0\n    for scores in per_residue_match_scores:\n        n = 1\n        for observation in sorted(scores, reverse=True):\n            score += observation / n\n            n *= 2\n\n    return score\n</code></pre>"},{"location":"reference/structure/fragment/info/","title":"info","text":""},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo","title":"FragmentInfo","text":"<pre><code>FragmentInfo(source: str = utils.path.biological_interfaces, fragment_length: int = 5, sql: bool = False, **kwargs)\n</code></pre> <p>Stores all Fragment metadata for a particular FragmentDatabase</p> <p>Parameters:</p> <ul> <li> <code>source</code>             (<code>str</code>, default:                 <code>biological_interfaces</code> )         \u2013          <p>Which type of information to use</p> </li> <li> <code>fragment_length</code>             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>What is the length of the fragment database</p> </li> <li> <code>sql</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the database is stored in SQL table</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>def __init__(self, source: str = utils.path.biological_interfaces, fragment_length: int = 5, sql: bool = False,\n             **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        source: Which type of information to use\n        fragment_length: What is the length of the fragment database\n        sql: Whether the database is stored in SQL table\n        **kwargs:\n    \"\"\"\n    super().__init__()  # object\n    self.cluster_info_path = utils.path.intfrag_cluster_info_dirpath\n    self.fragment_length = fragment_length\n    self.fragment_range = parameterize_frag_length(fragment_length)\n    self.info = {}\n    self.source = source\n    self.statistics = {}\n    # {cluster_id: [[mapped, paired, {max_weight_counts}, ...], ..., frequencies: {'A': 0.11, ...}}\n    #  ex: {'1_0_0': [[0.540, 0.486, {-2: 67, -1: 326, ...}, {-2: 166, ...}], 2749]\n\n    if sql:\n        raise NotImplementedError(\"Can't connect to SQL database yet\")\n        self.db = True\n    else:  # self.source == 'directory':\n        # Todo initialize as local directory\n        self.db = False\n\n    self._load_db_statistics()\n</code></pre>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.location","title":"location  <code>property</code>","text":"<pre><code>location: AnyStr | None\n</code></pre> <p>Provide the location where fragments are stored</p>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.aa_frequencies","title":"aa_frequencies  <code>property</code>","text":"<pre><code>aa_frequencies: dict[protein_letters_alph1, float]\n</code></pre> <p>Retrieve database specific amino acid representation frequencies</p> <p>Returns:</p> <ul> <li> <code>dict[protein_letters_alph1, float]</code>         \u2013          <p>{'A': 0.11, 'C': 0.03, 'D': 0.53, ...}</p> </li> </ul>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.retrieve_cluster_info","title":"retrieve_cluster_info","text":"<pre><code>retrieve_cluster_info(cluster: str = None, source: source_literal = None, index: str = None) -&gt; dict[str, int | float | str | dict[int, dict[protein_letters_literal | str, float | tuple[int, float]]]]\n</code></pre> <p>Return information from the fragment information database by cluster_id, information source, and source index</p> <p>Parameters:</p> <ul> <li> <code>cluster</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A cluster_id to get information about</p> </li> <li> <code>source</code>             (<code>source_literal</code>, default:                 <code>None</code> )         \u2013          <p>The source of information to retrieve. Must be one of 'size', 'rmsd', 'rep', 'mapped', or 'paired'</p> </li> <li> <code>index</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The index to gather information from. Source must be one of 'mapped' or 'paired' to use</p> </li> </ul> <p>Returns:     {'size': ..., 'rmsd': ..., 'rep': ..., 'mapped': indexed_frequencies, 'paired': indexed_frequencies}     Where indexed_frequencies has format {-2: {'A': 0.1, 'C': 0., ..., 'info': (12, 0.41)}, -1: {}, ..., 2: {}}</p> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>def retrieve_cluster_info(self, cluster: str = None, source: source_literal = None, index: str = None) -&gt; \\\n        dict[str, int | float | str | dict[int, dict[sequence.protein_letters_literal | str, float | tuple[int, float]]]]:\n    \"\"\"Return information from the fragment information database by cluster_id, information source, and source index\n\n    Args:\n        cluster: A cluster_id to get information about\n        source: The source of information to retrieve. Must be one of 'size', 'rmsd', 'rep', 'mapped', or 'paired'\n        index: The index to gather information from. Source must be one of 'mapped' or 'paired' to use\n    Returns:\n        {'size': ..., 'rmsd': ..., 'rep': ..., 'mapped': indexed_frequencies, 'paired': indexed_frequencies}\n        Where indexed_frequencies has format {-2: {'A': 0.1, 'C': 0., ..., 'info': (12, 0.41)}, -1: {}, ..., 2: {}}\n    \"\"\"\n    try:\n        cluster_data = self.info[cluster]\n    except KeyError:\n        self.load_cluster_info(ids=[cluster])\n        cluster_data = self.info[cluster]\n\n    if source is None:\n        return cluster_data\n    else:\n        if index is None:  # Must check for None, index can be 0\n            return cluster_data[source]\n        else:  # source in ['mapped', 'paired']:\n            try:\n                return cluster_data[source][index]\n            except KeyError:\n                raise KeyError(f'The source {source} is not available. '\n                               f'Try one of {\", \".join(get_args(source_literal))}')\n            except IndexError:\n                raise IndexError(f'The index {index} is outside of the fragment range. '\n                                 f'Try one of {\", \".join(cluster_data[\"mapped\"].keys())}')\n            except TypeError:\n                raise TypeError(f'You must provide \"mapped\" or \"paired\" if you wish to use an index')\n</code></pre>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.load_cluster_info","title":"load_cluster_info","text":"<pre><code>load_cluster_info(ids: Sequence[str] = None)\n</code></pre> <p>Load cluster information from the fragment database source into attribute .info</p> <p>Parameters:</p> <ul> <li> <code>ids</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>['1_2_123', ...]</p> </li> </ul> Sets <p>self.info (dict[str, dict]): {'1_2_123': {'size': , 'rmsd': , 'rep': , 'mapped': , 'paired': }, ...}</p> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>def load_cluster_info(self, ids: Sequence[str] = None):\n    \"\"\"Load cluster information from the fragment database source into attribute .info\n\n    Args:\n        ids: ['1_2_123', ...]\n\n    Sets:\n        self.info (dict[str, dict]): {'1_2_123': {'size': , 'rmsd': , 'rep': , 'mapped': , 'paired': }, ...}\n    \"\"\"\n    if self.db:\n        raise NotImplementedError(\"Can't connect to MySQL database yet\")\n    else:\n        if ids is None:  # Load all data\n            identified_files = [(os.path.splitext(os.path.basename(cluster_file))[0], cluster_file)\n                                for cluster_file in utils.get_file_paths_recursively(self.location,\n                                                                                     extension='.pkl')]\n        else:\n            identified_files = \\\n                [(_id, os.path.join(self.location, c_id1, f'{c_id1}_{c_id2}', _id, f'{_id}.pkl'))\n                 for _id, (c_id1, c_id2, c_id3) in zip(ids, map(str.split, ids, repeat('_')))]\n\n        self.info.update({tuple(map(int, cluster_id.split('_'))):\n                          ClusterInfo(name=cluster_id, **utils.unpickle(cluster_file))\n                          for cluster_id, cluster_file in identified_files})\n</code></pre>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.load_cluster_info_from_text","title":"load_cluster_info_from_text","text":"<pre><code>load_cluster_info_from_text(ids: Sequence[str] = None)\n</code></pre> <p>Load cluster information from the fragment database source text files into attribute .info</p> <p>Parameters:</p> <ul> <li> <code>ids</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>['1_2_123', ...]</p> </li> </ul> Sets <p>self.info (dict[str, dict]): {'1_2_123': {'size': , 'rmsd': , 'rep': , 'mapped': , 'paired': }, ...}</p> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>def load_cluster_info_from_text(self, ids: Sequence[str] = None):\n    \"\"\"Load cluster information from the fragment database source text files into attribute .info\n\n    Args:\n        ids: ['1_2_123', ...]\n\n    Sets:\n        self.info (dict[str, dict]): {'1_2_123': {'size': , 'rmsd': , 'rep': , 'mapped': , 'paired': }, ...}\n    \"\"\"\n    if self.db:\n        raise NotImplementedError(\"Can't connect to MySQL database yet\")\n    else:\n        if ids is None:  # Load all data\n            identified_files = \\\n                [(os.path.splitext(os.path.basename(cluster_directory))[0], cluster_directory)\n                 for cluster_directory in utils.get_file_paths_recursively(self.cluster_info_path)]\n            # for root, dirs, files in os.walk(self.cluster_info_path):\n            #     if not dirs:\n            #         i_cluster_type, j_cluster_type, k_cluster_type = map(int, os.path.basename(root).split('_'))\n            #\n            #         # if i_cluster_type not in self.info:\n            #         #     self.info[i_cluster_type] = {}\n            #         # if j_cluster_type not in self.info[i_cluster_type]:\n            #         #     self.info[i_cluster_type][j_cluster_type] = {}\n            #\n            #         # for file in files:\n            #         # There is only one file\n            #         self.info[(i_cluster_type, j_cluster_type, k_cluster_type)] = \\\n            #             ClusterInfo.from_file(os.path.join(root, files[0]))\n        else:\n            identified_files = [(_id, os.path.join(self.cluster_info_path, c_id1,\n                                                   f'{c_id1}_{c_id2}', _id, f'{_id}.txt'))\n                                for _id, (c_id1, c_id2, c_id3) in zip(ids, map(str.split, ids, repeat('_')))]\n            # for _id, (c_id1, c_id2, c_id3) in zip(ids, map(str.split, ids, repeat('_'))):\n            #     identified_directories[_id] = os.path.join(self.cluster_info_path, c_id1,\n            #                                                f'{c_id1}_{c_id2}', _id, f'{_id}.txt')\n\n        self.info.update({tuple(map(int, cluster_id.split('_'))): ClusterInfo.from_file(cluster_file)\n                          for cluster_id, cluster_file in identified_files})\n</code></pre>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.FragmentInfo.get_cluster_id","title":"get_cluster_id  <code>staticmethod</code>","text":"<pre><code>get_cluster_id(cluster_id: str, index: int = 3) -&gt; str\n</code></pre> <p>Returns the cluster identification string according the specified index</p> <p>Parameters:</p> <ul> <li> <code>cluster_id</code>             (<code>str</code>)         \u2013          <p>The id of the fragment cluster. Ex: \"1_2_123\"</p> </li> <li> <code>index</code>             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>The index on which to return. Ex: index_number=2 gives 1_2</p> </li> </ul> <p>Returns:     The cluster_id modified by the requested index_number</p> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>@staticmethod\ndef get_cluster_id(cluster_id: str, index: int = 3) -&gt; str:  # Todo Unused, DEPRECIATE\n    \"\"\"Returns the cluster identification string according the specified index\n\n    Args:\n        cluster_id: The id of the fragment cluster. Ex: \"1_2_123\"\n        index: The index on which to return. Ex: index_number=2 gives 1_2\n    Returns:\n        The cluster_id modified by the requested index_number\n    \"\"\"\n    while len(cluster_id) &lt; 3:\n        cluster_id += '0'\n\n    cluster_id_split = cluster_id.split('_')\n    if len(cluster_id_split) == 1:  # in case of 12123? -&gt; ['12123', '']\n        id_l = [cluster_id[:1], cluster_id[1:2], cluster_id[2:]]\n    else:\n        id_l = cluster_id_split\n\n    info = id_l[:index]\n\n    while len(info) &lt; 3:  # Ensure the returned string has at least 3 indices\n        info.append('0')\n\n    return '_'.join(info)\n</code></pre>"},{"location":"reference/structure/fragment/info/#structure.fragment.info.parameterize_frag_length","title":"parameterize_frag_length","text":"<pre><code>parameterize_frag_length(length: int) -&gt; tuple[int, int]\n</code></pre> <p>Generate fragment length range parameters for use in fragment functions</p> <p>Parameters:</p> <ul> <li> <code>length</code>             (<code>int</code>)         \u2013          <p>The length of the fragment</p> </li> </ul> <p>Returns:     The tuple that provide the range for the specified length centered around 0         ex: length=5 -&gt; (-2, 3), length=6 -&gt; (-3, 3)</p> Source code in <code>symdesign/structure/fragment/info.py</code> <pre><code>def parameterize_frag_length(length: int) -&gt; tuple[int, int]:\n    \"\"\"Generate fragment length range parameters for use in fragment functions\n\n    Args:\n        length: The length of the fragment\n    Returns:\n        The tuple that provide the range for the specified length centered around 0\n            ex: length=5 -&gt; (-2, 3), length=6 -&gt; (-3, 3)\n    \"\"\"\n    if length % 2 == 1:  # fragment length is odd\n        index_offset = 1\n    else:  # length is even\n        logger.warning(f\"{parameterize_frag_length.__name__}: {length} is an even integer which isn't symmetric about \"\n                       'a single residue. Ensure this is what you want')\n        index_offset = 0\n\n    # Get the number of residues extending to each side\n    _range = math.floor(length / 2)\n\n    return 0 - _range, 0 + _range + index_offset\n</code></pre>"},{"location":"reference/structure/fragment/metrics/","title":"metrics","text":""},{"location":"reference/structure/fragment/metrics/#structure.fragment.metrics.calculate_match","title":"calculate_match","text":"<pre><code>calculate_match(coords1: float | ndarray = None, coords2: float | ndarray = None, coords_rmsd_reference: float | ndarray = None) -&gt; float | ndarray\n</code></pre> <p>Calculate the match score(s) between two sets of coordinates given a reference rmsd</p> <p>Parameters:</p> <ul> <li> <code>coords1</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first set of coordinates</p> </li> <li> <code>coords2</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second set of coordinates</p> </li> <li> <code>coords_rmsd_reference</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The reference RMSD to compare each pair of coordinates against</p> </li> </ul> <p>Returns:     The match score(s)</p> Source code in <code>symdesign/structure/fragment/metrics.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef calculate_match(coords1: float | np.ndarray = None, coords2: float | np.ndarray = None,\n                    coords_rmsd_reference: float | np.ndarray = None) -&gt; float | np.ndarray:\n    \"\"\"Calculate the match score(s) between two sets of coordinates given a reference rmsd\n\n    Args:\n        coords1: The first set of coordinates\n        coords2: The second set of coordinates\n        coords_rmsd_reference: The reference RMSD to compare each pair of coordinates against\n    Returns:\n        The match score(s)\n    \"\"\"\n    # rmsds = _rmsd(coords1, coords2)\n    # # Calculate Guide Atom Overlap Z-Value\n    # z_values = rmsds / coords_rmsd_reference\n    # # filter z_values by passing threshold\n    return match_score_from_z_value(_rmsd(coords1, coords2) / coords_rmsd_reference)\n</code></pre>"},{"location":"reference/structure/fragment/metrics/#structure.fragment.metrics.rmsd_z_score","title":"rmsd_z_score","text":"<pre><code>rmsd_z_score(coords1: float | ndarray = None, coords2: float | ndarray = None, coords_rmsd_reference: float | ndarray = None) -&gt; float | ndarray\n</code></pre> <p>Calculate the overlap between two sets of coordinates given a reference rmsd</p> <p>Parameters:</p> <ul> <li> <code>coords1</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The first set of coordinates</p> </li> <li> <code>coords2</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The second set of coordinates</p> </li> <li> <code>coords_rmsd_reference</code>             (<code>float | ndarray</code>, default:                 <code>None</code> )         \u2013          <p>The reference RMSD to compare each pair of coordinates against</p> </li> </ul> <p>Returns:     The overlap z-value</p> Source code in <code>symdesign/structure/fragment/metrics.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef rmsd_z_score(coords1: float | np.ndarray = None, coords2: float | np.ndarray = None,\n                 coords_rmsd_reference: float | np.ndarray = None) -&gt; float | np.ndarray:\n    \"\"\"Calculate the overlap between two sets of coordinates given a reference rmsd\n\n    Args:\n        coords1: The first set of coordinates\n        coords2: The second set of coordinates\n        coords_rmsd_reference: The reference RMSD to compare each pair of coordinates against\n    Returns:\n        The overlap z-value\n    \"\"\"\n    #         max_z_value: The z-score deviation threshold of the overlap to be considered a match\n    # Calculate Guide Atom Overlap Z-Value\n    return _rmsd(coords1, coords2) / coords_rmsd_reference\n</code></pre>"},{"location":"reference/structure/fragment/metrics/#structure.fragment.metrics.z_value_from_match_score","title":"z_value_from_match_score","text":"<pre><code>z_value_from_match_score(match_score: float | ndarray) -&gt; float | ndarray\n</code></pre> <p>Given a match score, convert to a z-value. sqrt(1/match_score - 1)</p> Source code in <code>symdesign/structure/fragment/metrics.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef z_value_from_match_score(match_score: float | np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Given a match score, convert to a z-value. sqrt(1/match_score - 1)\"\"\"\n    return np.sqrt(1/match_score - 1)\n</code></pre>"},{"location":"reference/structure/fragment/metrics/#structure.fragment.metrics.match_score_from_z_value","title":"match_score_from_z_value","text":"<pre><code>match_score_from_z_value(z_value: float | ndarray) -&gt; float | ndarray\n</code></pre> <p>Return the match score from a fragment z-value -&gt; 1 / (1 + z_value**2). Bounded between 0 and 1</p> Source code in <code>symdesign/structure/fragment/metrics.py</code> <pre><code>@jit(nopython=True)  # , cache=True)\ndef match_score_from_z_value(z_value: float | np.ndarray) -&gt; float | np.ndarray:\n    \"\"\"Return the match score from a fragment z-value -&gt; 1 / (1 + z_value**2). Bounded between 0 and 1\"\"\"\n    return 1 / (1 + z_value**2)\n</code></pre>"},{"location":"reference/structure/fragment/visuals/","title":"visuals","text":""},{"location":"reference/structure/fragment/visuals/#structure.fragment.visuals.write_fragment_pairs_as_accumulating_states","title":"write_fragment_pairs_as_accumulating_states","text":"<pre><code>write_fragment_pairs_as_accumulating_states(ghost_frags: list[GhostFragment], file_name: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write GhostFragments as an MultiModel trajectory where each successive Model has one more Fragment</p> <p>Parameters:</p> <ul> <li> <code>ghost_frags</code>             (<code>list[GhostFragment]</code>)         \u2013          <p>An iterable of GhostFragments</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The path that the file should be written to</p> </li> </ul> <p>Returns:     The file_name</p> Source code in <code>symdesign/structure/fragment/visuals.py</code> <pre><code>def write_fragment_pairs_as_accumulating_states(\n        ghost_frags: list[GhostFragment], file_name: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write GhostFragments as an MultiModel trajectory where each successive Model has one more Fragment\n\n    Args:\n        ghost_frags: An iterable of GhostFragments\n        file_name: The path that the file should be written to\n    Returns:\n        The file_name\n    \"\"\"\n    if '.pdb' not in file_name:\n        file_name += '.pdb'\n\n    with open(file_name, 'w') as f:\n        atom_iterator = 0\n        residue_iterator = 1\n        chain_generator = chain_id_generator()\n        mapped_chain_id = next(chain_generator)\n        model_number = 1\n        # Write the monofrag that ghost frags are paired against\n        f.write(f'MODEL    {model_number:&gt;4d}\\n')\n        ghost_frag_init = ghost_frags[0]\n        frag_model, frag_paired_chain = ghost_frag_init.fragment_db.paired_frags[ghost_frag_init.ijk]\n        trnsfmd_fragment = frag_model.get_transformed_copy(*ghost_frag_init.transformation)\n        # Set mapped_chain to A\n        mapped_chain = trnsfmd_fragment.chain(\n            tuple(set(frag_model.chain_ids).difference({frag_paired_chain, '9'}))[0])\n        mapped_chain.chain_id = mapped_chain_id\n        # Renumber residues\n        trnsfmd_fragment.renumber_residues(at=residue_iterator)\n        # Write\n        f.write('%s\\n' % mapped_chain.get_atom_record(atom_offset=atom_iterator))\n        # Iterate atom/residue numbers\n        atom_iterator += mapped_chain.number_of_atoms\n        residue_iterator += mapped_chain.number_of_residues\n        f.write('ENDMDL\\n')\n\n        # Write all subsequent models, stacking each subsequent model on the previous\n        fragment_lines = []\n        for model_number, ghost_frag in enumerate(ghost_frags, model_number + 1):\n            f.write(f'MODEL    {model_number:&gt;4d}\\n')\n            frag_model, frag_paired_chain = ghost_frag.fragment_db.paired_frags[ghost_frag.ijk]\n            trnsfmd_fragment = frag_model.get_transformed_copy(*ghost_frag.transformation)\n            # Iterate only the paired chain with new chainID\n            trnsfmd_fragment.chain(frag_paired_chain).chain_id = next(chain_generator)\n            # Set the mapped chain to the single mapped_chain_id\n            trnsfmd_fragment.chain(tuple(\n                set(frag_model.chain_ids).difference({frag_paired_chain, '9'})\n            )[0]).chain_id = mapped_chain_id\n            trnsfmd_fragment.renumber_residues(at=residue_iterator)\n            fragment_lines.append(trnsfmd_fragment.get_atom_record(atom_offset=atom_iterator))\n            # trnsfmd_fragment.write(file_handle=f)\n            f.write('%s\\n' % '\\n'.join(fragment_lines))\n            atom_iterator += frag_model.number_of_atoms\n            residue_iterator += frag_model.number_of_residues\n            f.write('ENDMDL\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/structure/fragment/visuals/#structure.fragment.visuals.write_fragments_as_multimodel","title":"write_fragments_as_multimodel","text":"<pre><code>write_fragments_as_multimodel(ghost_frags: list[GhostFragment], file_name: AnyStr = os.getcwd()) -&gt; AnyStr\n</code></pre> <p>Write GhostFragments as one MultiModel</p> <p>Parameters:</p> <ul> <li> <code>ghost_frags</code>             (<code>list[GhostFragment]</code>)         \u2013          <p>An iterable of GhostFragments</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The path that the file should be written to</p> </li> </ul> <p>Returns:     The file_name</p> Source code in <code>symdesign/structure/fragment/visuals.py</code> <pre><code>def write_fragments_as_multimodel(ghost_frags: list[GhostFragment], file_name: AnyStr = os.getcwd()) -&gt; AnyStr:\n    \"\"\"Write GhostFragments as one MultiModel\n\n    Args:\n        ghost_frags: An iterable of GhostFragments\n        file_name: The path that the file should be written to\n    Returns:\n        The file_name\n    \"\"\"\n    if '.pdb' not in file_name:\n        file_name += '.pdb'\n\n    with open(file_name, 'w') as f:\n        atom_iterator = 0\n        residue_iterator = 1\n        chain_generator = chain_id_generator()\n        mapped_chain_id = next(chain_generator)\n        paired_chain_id = next(chain_generator)\n        model_number = 1\n\n        # Write all models\n        for model_number, ghost_frag in enumerate(ghost_frags, model_number):\n            f.write(f'MODEL    {model_number:&gt;4d}\\n')\n            frag_model, frag_paired_chain = ghost_frag.fragment_db.paired_frags[ghost_frag.ijk]\n            trnsfmd_fragment = frag_model.get_transformed_copy(*ghost_frag.transformation)\n            # Set the paired chain to the single paired chainID\n            trnsfmd_fragment.chain(frag_paired_chain).chain_id = paired_chain_id\n            # Set the mapped chain to the single mapped chainID\n            trnsfmd_fragment.chain(tuple(\n                set(frag_model.chain_ids).difference({frag_paired_chain, '9'})\n            )[0]).chain_id = mapped_chain_id\n            trnsfmd_fragment.renumber_residues(at=residue_iterator)\n            trnsfmd_fragment.write(file_handle=f, atom_offset=atom_iterator)\n            atom_iterator += frag_model.number_of_atoms\n            residue_iterator += frag_model.number_of_residues\n            f.write('ENDMDL\\n')\n\n    return file_name\n</code></pre>"},{"location":"reference/tools/","title":"tools","text":""},{"location":"reference/tools/BenchmarkCollapseHydrophobicityScale/","title":"BenchmarkCollapseHydrophobicityScale","text":""},{"location":"reference/tools/BenchmarkCollapseHydrophobicityScale/#tools.BenchmarkCollapseHydrophobicityScale.human_tau40","title":"human_tau40  <code>module-attribute</code>","text":"<pre><code>human_tau40 = 'MAEPRQEFEVMEDHAGTYGLGDRKDQGGYTMHQDQEGDTDAGLKESPLQTPTEDGSEEPGSETSDAKSTPTAEDVTAPLVDEGAPGKQAAAQPHTEIPEGTTAEEAGIGDTPSLEDEAAGHVTQARMVSKSKDGTGSDDKKAKGADGKTKIATPRGAAPPGQKGQANATRIPAKTPPAPKTPPSSGEPPKSGDRSGYSSPGSPGTPGSRSRTPSLPTPPTREPKKVAVVRTPPKSPSSAKSRLQTAPVPMPDLKNVKSKIGSTENLKHQPGGGKVQIINKKLDLSNVQSKCGSKDNIKHVPGGGSVQIVYKPVDLSKVTSKCGSLGNIHHKPGGGQVEVKSEKLDFKDRVQSKIGSLDNITHVPGGGNKKIETHKLTFRENAKAKTDHGAEIVYKSPVVSGDTSPRHLSNVSSTGSIDMVDSPQLATLADEVSASLAKQGL'\n</code></pre> <p>human Tau-F 2N4R</p>"},{"location":"reference/tools/BenchmarkCollapseHydrophobicityScale/#tools.BenchmarkCollapseHydrophobicityScale.compare_hydrophobicity_scale_to_standard","title":"compare_hydrophobicity_scale_to_standard","text":"<pre><code>compare_hydrophobicity_scale_to_standard(hydrophobicity_scale: dict[str, float], sequences: Sequence[Sequence] = None) -&gt; float\n</code></pre> <p>Test a new hydrophobicity scale again the standard scale from https://doi.org/10.1073/pnas.1617873114</p> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The computed value that is analogous for the given hydrophobicity_scale</p> </li> </ul> Source code in <code>symdesign/tools/BenchmarkCollapseHydrophobicityScale.py</code> <pre><code>def compare_hydrophobicity_scale_to_standard(hydrophobicity_scale: dict[str, float],\n                                             sequences: Sequence[Sequence] = None) -&gt; float:\n    \"\"\"Test a new hydrophobicity scale again the standard scale from https://doi.org/10.1073/pnas.1617873114\n\n    Returns:\n        The computed value that is analogous for the given hydrophobicity_scale\n    \"\"\"\n    if sequences is None:\n        sequences = benchmark_sequences\n        sequence_source = 'standard'\n    else:\n        sequence_source = 'provided'\n\n    standard_collapse_indices = []\n    new_collapse_indices = []\n    for sequence in sequences:\n        standard_collapse_index = metrics.hydrophobic_collapse_index(sequence, hydrophobicity='standard')\n        standard_collapse_indices.append(standard_collapse_index)\n        new_collapse_index = metrics.hydrophobic_collapse_index(sequence, hydrophobicity='custom',\n                                                                custom=hydrophobicity_scale)\n        new_collapse_indices.append(new_collapse_index)\n\n    significance_threshold = metrics.collapse_thresholds['standard']\n    equivalent_values = []\n    for seq_idx, collapse in enumerate(standard_collapse_indices):\n        collapse_bool = collapse &gt; significance_threshold\n        # Check for the collapse \"transition\" positions by comparing neighboring residues\n        indices_around_transition_point = []\n        for prior_idx, idx in enumerate(range(1, len(collapse_bool))):\n            # Condition is only True when 0 -&gt; 1 transition occurs\n            if collapse_bool[prior_idx] &lt; collapse_bool[idx]:\n                indices_around_transition_point.extend([prior_idx, idx])\n\n        # Index the corresponding sequence from the new_collapse_indices\n        equivalent_collapse_under_new_scale = new_collapse_indices[seq_idx][indices_around_transition_point]\n        # equivalent_values.append(equivalent_collapse_under_new_scale.mean())\n        equivalent_values.extend(equivalent_collapse_under_new_scale.tolist())\n\n    print(f'For the {sequence_source} sequences, the equivalent hydrophobic collapse values are '\n          f'{equivalent_values}')\n\n    max_length = max([len(sequence) for sequence in sequences])\n    new_significance_threshold = sum(equivalent_values) / len(equivalent_values)\n    figure = False  # True  #\n    # standard_collapse_indices_np = np.array(standard_collapse_indices)\n    # new_collapse_indices_np = np.array(new_collapse_indices)\n    if figure:\n        # Set the base figure aspect ratio for all sequence designs\n        figure_aspect_ratio = (max_length / 25., 20)  # 20 is arbitrary size to fit all information in figure\n        fig = plt.figure(figsize=figure_aspect_ratio)\n        collapse_ax = fig.subplots(1)\n        for idx, indices in enumerate(standard_collapse_indices):\n            collapse_ax.plot(indices, label=f'standard{idx}')\n        for idx, indices in enumerate(new_collapse_indices):\n            collapse_ax.plot(indices, label=f'new{idx}')\n        # collapse_ax = collapse_graph_df.plot.line(ax=collapse_ax)\n        collapse_ax.xaxis.set_major_locator(MultipleLocator(20))\n        collapse_ax.xaxis.set_major_formatter('{x:.0f}')\n        # For the minor ticks, use no labels; default NullFormatter.\n        collapse_ax.xaxis.set_minor_locator(MultipleLocator(5))\n        collapse_ax.set_xlim(0, max_length)\n        collapse_ax.set_ylim(0, 1)\n        # # CAN'T SET FacetGrid object for most matplotlib elements...\n        # ax = graph_collapse.axes\n        # ax = plt.gca()  # gca &lt;- get current axis\n        # labels = [fill(index, legend_fill_value) for index in collapse_graph_df.index]\n        # collapse_ax.legend(labels, loc='lower left', bbox_to_anchor=(0., 1))\n        # collapse_ax.legend(loc='lower left', bbox_to_anchor=(0., 1))\n        # linestyles={'solid', 'dashed', 'dashdot', 'dotted'}\n        # Plot horizontal significance\n        collapse_ax.hlines([significance_threshold], 0, 1, transform=collapse_ax.get_yaxis_transform(),\n                           label='Collapse Threshold', colors='#fc554f', linestyle='dotted')  # tomato\n        collapse_ax.hlines([new_significance_threshold], 0, 1, transform=collapse_ax.get_yaxis_transform(),\n                           label='New Collapse Threshold', colors='#cccccc', linestyle='dotted')  # grey\n        collapse_ax.set_ylabel('Hydrophobic Collapse Index')\n        plt.legend()\n        plt.show()\n\n    return new_significance_threshold\n</code></pre>"},{"location":"reference/tools/ConcatenatePDBFiles/","title":"ConcatenatePDBFiles","text":""},{"location":"reference/tools/ProfileScripts/","title":"ProfileScripts","text":"<p>To profile CPU usage and execution times, run your script with the command format: python -m cProfile -o output_file.name [ARGS] Then run this with: python ProfileScripts.py -f output_file.name [-t]</p> <p>To profile memory (RAM) usage, run your script with the command format: python -m memory_profiler SCRIPT &gt; output_file.name</p>"},{"location":"reference/tools/SlurmControl/","title":"SlurmControl","text":""},{"location":"reference/tools/SlurmControl/#tools.SlurmControl.find_list_indices","title":"find_list_indices","text":"<pre><code>find_list_indices(reference_cmds, query_ids)\n</code></pre> <p>Search for ID's present in supplied list in a reference list and return the indices of the reference list where they are found</p> Source code in <code>symdesign/tools/SlurmControl.py</code> <pre><code>def find_list_indices(reference_cmds, query_ids):\n    \"\"\"Search for ID's present in supplied list in a reference list and return the indices of the reference list where\n    they are found\"\"\"\n    # full_lines_set = set(full_lines)\n    # cmd_lines_set = set(cmd_lines)\n    query_ids_sort = sorted(query_ids)\n    # cmd_lines_sort = sorted(cmd_lines)\n\n    idxs = []\n    for i, cmd_id in enumerate(reference_cmds):  # _sort\n        for id in query_ids_sort:\n            if id in cmd_id:\n                idxs.append(i)\n                break\n    idxs_sorted = sorted(idxs)\n    logger.info(','.join(str(i + 1) for i in idxs_sorted))\n\n    return idxs_sorted\n</code></pre>"},{"location":"reference/tools/SlurmControl/#tools.SlurmControl.filter_by_indices","title":"filter_by_indices","text":"<pre><code>filter_by_indices(index_array, _iterable, zero=True)\n</code></pre> <p>Return the indices from an iterable that match a specified input index array</p> Source code in <code>symdesign/tools/SlurmControl.py</code> <pre><code>def filter_by_indices(index_array, _iterable, zero=True):\n    \"\"\"Return the indices from an iterable that match a specified input index array\"\"\"\n    offset = 0 if zero else 1\n    return [_iterable[idx - offset] for idx in index_array]\n</code></pre>"},{"location":"reference/tools/SlurmControl/#tools.SlurmControl.link_pair","title":"link_pair","text":"<pre><code>link_pair(pair: tuple[str, str], force: bool = False) -&gt; None\n</code></pre> <p>Combine docking files of one docking combination with another</p> <p>Parameters:</p> <ul> <li> <code>pair</code>             (<code>tuple[str, str]</code>)         \u2013          <p>source file, destination file (link)</p> </li> <li> <code>force</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to remove links before creation</p> </li> </ul> Source code in <code>symdesign/tools/SlurmControl.py</code> <pre><code>def link_pair(pair: tuple[str, str], force: bool = False) -&gt; None:\n    \"\"\"Combine docking files of one docking combination with another\n\n    Args:\n        pair: source file, destination file (link)\n        force: Whether to remove links before creation\n    \"\"\"\n    if force:\n        os.remove(pair[1])\n    os.symlink(*pair)  # , target_is_directory=True)\n</code></pre>"},{"location":"reference/tools/SlurmControl/#tools.SlurmControl.investigate_job_array_failure","title":"investigate_job_array_failure","text":"<pre><code>investigate_job_array_failure(job_id, output_dir=os.path.join(os.getcwd(), 'output'))\n</code></pre> <p>Returns an array for each of the errors encountered. All=True returns the set</p> Source code in <code>symdesign/tools/SlurmControl.py</code> <pre><code>def investigate_job_array_failure(job_id, output_dir=os.path.join(os.getcwd(), 'output')):\n    \"\"\"Returns an array for each of the errors encountered. All=True returns the set\"\"\"\n    job_output_files = glob(os.path.join(output_dir, '*%s*' % job_id))\n    if not job_output_files:\n        raise RuntimeError('Found no files with %s glob. Did you provide the correct arguments? See --help'\n                           % os.path.join(output_dir, '*%s*' % job_id))\n    potential_errors = [job_file if os.path.getsize(job_file) &gt; 0 else None for job_file in job_output_files]\n    logger.info('Found array ids from job %s with SBATCH output:\\n\\t%s'\n                % (job_id, ','.join(str(i) for i, error in enumerate(potential_errors, 1) if error)))\n    parsed_errors = list(map(classify_slurm_error_type, potential_errors))\n    job_file_array_id_d = \\\n        {job_file: int(os.path.splitext(job_file.split('_')[-1])[0]) for job_file in job_output_files}\n    # generate a dictionary of the job_file to array_id\n    # for job in job_output_files:\n    #     array_id = os.path.splitext(job.split('_')[-1])[0]\n    #     job_file_array_id_d[array] = job\n    memory_array = \\\n        sorted(job_file_array_id_d[job_output_files[i]] for i, error in enumerate(parsed_errors) if error == 'memory')\n    failure_array = \\\n        sorted(job_file_array_id_d[job_output_files[i]] for i, error in enumerate(parsed_errors) if error == 'failure')\n    other_array = \\\n        sorted(job_file_array_id_d[job_output_files[i]] for i, error in enumerate(parsed_errors) if error == 'other')\n\n    return memory_array, failure_array, other_array\n</code></pre>"},{"location":"reference/tools/SlurmControl/#tools.SlurmControl.change_script_array","title":"change_script_array","text":"<pre><code>change_script_array(script_file, array)\n</code></pre> <p>Take a script file and replace the array line with a new array</p> Source code in <code>symdesign/tools/SlurmControl.py</code> <pre><code>def change_script_array(script_file, array):\n    \"\"\"Take a script file and replace the array line with a new array\"\"\"\n    with open(script_file, 'r') as f:\n        lines = f.readlines()\n        for i, line in enumerate(lines):\n            if '#SBATCH -a' in line or '#SBATCH --array' in line:\n                lines[i] = '#SBATCH --array=%s' % ','.join(str(a) for a in array)\n\n    new_script = '%s_%s' % (os.path.splitext(script_file)[0], 're-do_SLURM_failures.sh')\n    with open(new_script, 'w') as f:\n        f.write('\\n'.join(line for line in map(str.strip, lines)))\n\n    return new_script\n</code></pre>"},{"location":"reference/tools/distribute/","title":"distribute","text":""},{"location":"reference/tools/fetch_commands_from_slurm_output/","title":"fetch_commands_from_slurm_output","text":""},{"location":"reference/tools/format_commands/","title":"format_commands","text":""},{"location":"reference/tools/format_pose_ids/","title":"format_pose_ids","text":""},{"location":"reference/tools/generate_reference_docs/","title":"generate_reference_docs","text":"<p>Generate the code reference pages and navigation.</p>"},{"location":"reference/tools/get_array_numbers_from_identifiers/","title":"get_array_numbers_from_identifiers","text":""},{"location":"reference/tools/list_files_in_directory/","title":"list_files_in_directory","text":""},{"location":"reference/tools/list_overlap/","title":"list_overlap","text":""},{"location":"reference/tools/models_to_multimodel/","title":"models_to_multimodel","text":""},{"location":"reference/tools/profile_gpu_nodes/","title":"profile_gpu_nodes","text":""},{"location":"reference/tools/profile_gpu_nodes/#tools.profile_gpu_nodes.out","title":"out  <code>module-attribute</code>","text":"<pre><code>out = decode('UTF-8')\n</code></pre> <p>Searching for the line Partition=gpu AllocNode:Sid=cassini:21698 ReqNodeList=(null) ExcNodeList=compute-2-[3,5-7,10,22,27,31-48] NodeList=compute-2-28 BatchHost=compute-2-28 NumNodes=1 NumCPUs=1 CPUs/Task=1 ReqB:S:C:T=0:0::*</p>"},{"location":"reference/tools/retrieve_oligomers/","title":"retrieve_oligomers","text":""},{"location":"reference/tools/retrieve_pdbs_by_advanced_query/","title":"retrieve_pdbs_by_advanced_query","text":""},{"location":"reference/tools/write_indices_for_C1_docking/","title":"write_indices_for_C1_docking","text":""},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#utils.log_levels","title":"log_levels  <code>module-attribute</code>","text":"<pre><code>log_levels = dict(zip(log_level_keys, [DEBUG, INFO, WARNING, ERROR, CRITICAL, DEBUG, INFO, WARNING, ERROR, CRITICAL, DEBUG, INFO, WARNING, ERROR, CRITICAL, DEBUG, INFO, WARNING, ERROR, CRITICAL, DEBUG, INFO, WARNING, ERROR, CRITICAL, DEBUG, INFO, WARNING, ERROR, CRITICAL]))\n</code></pre> <p>log_level = { 'debug': DEBUG, 'info': INFO, 'warning': WARNING, 'error': ERROR, 'critical': CRITICAL, 'DEBUG': DEBUG, 'INFO': INFO, 'WARNING': WARNING, 'ERROR': ERROR, 'CRITICAL': CRITICAL, 1: DEBUG, 2: INFO, 3: WARNING, 4: ERROR, 5: CRITICAL, 10: DEBUG, 20: INFO, 30: WARNING, 40: ERROR, 50: CRITICAL}</p>"},{"location":"reference/utils/#utils.PoseSpecification","title":"PoseSpecification","text":"<pre><code>PoseSpecification(file: AnyStr)\n</code></pre> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def __init__(self, file: AnyStr):\n    self.directive_delimiter: str = ':'\n    self.file: AnyStr = file\n    self.directives: list[dict[int, str]] = []\n\n    all_poses, design_names, all_design_directives, = [], [], []\n    with open(self.file) as f:\n        # pose_identifiers, design_names, all_design_directives, *_ = zip(*reader(file, dialect=self))\n        all_info = list(zip(*csv.reader(f)))  # dialect=self)))\n\n    for idx in range(len(all_info)):\n        if idx == 0:\n            all_poses = all_info[idx]\n        elif idx == 1:\n            design_names = all_info[idx]\n        elif idx == 2:\n            all_design_directives = all_info[idx]\n\n    # logger.debug(f'Found poses {all_poses}')\n    # logger.debug(f'Found designs {design_names}')\n    # logger.debug(f'Found directives {all_design_directives}')\n    self.pose_identifiers: list[str] = list(map(str.strip, all_poses))\n    self.design_names: list[str] = list(map(str.strip, design_names))\n\n    # First, split directives by white space, then by directive_delimiter\n    # self.directives = \\\n    #     [dict((residue, directive) for residues_s, directive in [residue_directive.split(self.directive_delimiter)\n    #                                                              for residue_directive in design_directives.split()]\n    #           for residue in format_index_string(residues_s)) for design_directives in all_design_directives]\n    for design_directives in all_design_directives:\n        # print('Design Directives', design_directives)\n        parsed_directives = []\n        # for residues_s, directive in map(str.split, design_directives.split(), repeat(self.directive_delimiter)):\n        for design_directive in design_directives.split():\n            try:\n                design_specification, directive = design_directive.split(self.directive_delimiter)\n            except ValueError:  # Not enough values to unpack\n                break\n            else:\n                if design_specification.replace(',', '').replace('-', '') == design_specification:\n                    parsed_directives.append((design_specification, directive))\n                else:\n                    parsed_directives.extend([(spec, directive) for spec in format_index_string(design_specification)])\n        self.directives.append(dict(parsed_directives))\n</code></pre>"},{"location":"reference/utils/#utils.PoseSpecification.get_directives","title":"get_directives","text":"<pre><code>get_directives() -&gt; Generator[tuple[str, list[str] | None, list[dict[int, str]] | None], None, None]\n</code></pre> <p>Retrieve the parsed PoseID, Design Name, and Mutation Directive information from a Specification file</p> <p>Returns:</p> <ul> <li> <code>Generator[tuple[str, list[str] | None, list[dict[int, str]] | None], None, None]</code>         \u2013          <p>An generator of tuples where each tuple contains the PoseID, then if provided in the parsed file, the corresponding DesignID and then design directives. If they aren't provided then None will be returned for the DesignID and directives.</p> </li> </ul> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_directives(self) -&gt; Generator[tuple[str, list[str] | None, list[dict[int, str]] | None], None, None]:\n    \"\"\"Retrieve the parsed PoseID, Design Name, and Mutation Directive information from a Specification file\n\n    Returns:\n        An generator of tuples where each tuple contains the PoseID, then if provided in the parsed file, the\n            corresponding DesignID and then design directives. If they aren't provided then None will be returned\n            for the DesignID and directives.\n    \"\"\"\n    # Calculate whether there are multiple designs present per pose\n    found_poses = defaultdict(list)\n    for idx, pose in enumerate(self.pose_identifiers):\n        # if pose in found_poses:\n        found_poses[pose].append(idx)\n        # else:\n        #     found_poses[pose] = [idx]\n\n    # Ensure correctly sized inputs. Create blank data otherwise\n    number_pose_identifiers = len(self.pose_identifiers)\n    if self.design_names:  # design_file\n        if number_pose_identifiers != len(self.design_names):\n            raise ValueError(\n                f\"The 'design identifiers' provided to {self.__class__.__name__} are a different length \"\n                f\"({len(self.design_names)}) than the 'pose identifiers' ({number_pose_identifiers})\")\n        if self.directives:\n            if number_pose_identifiers != len(self.directives):\n                raise ValueError(\n                    f\"The 'directives' provided to {self.__class__.__name__} are a different length \"\n                    f\"({len(self.directives)}) than the 'pose identifiers' ({number_pose_identifiers})\")\n        else:\n            directives = list(repeat(None, number_pose_identifiers))\n    else:\n        design_names = list(repeat(None, number_pose_identifiers))\n        directives = design_names.copy()\n\n    # Group the pose_identifiers with the design_names and directives\n    if len(found_poses) == number_pose_identifiers:  # There is one design per pose\n        if self.design_names:\n            design_names = [[design_name] for design_name in self.design_names]\n            if self.directives:\n                directives = [[directive] for directive in self.directives]\n    else:  # More than one\n        if self.design_names:\n            design_names = [[self.design_names[index] for index in indices] for indices in found_poses.values()]\n            for pose_identifier, names in zip(found_poses, design_names):\n                if len(names) != len(set(names)):\n                    overlapping_designs = {design: names.count(design) for design in names}\n                    raise InputError(f\"Can't use a specification file with more than one entry for the same design\"\n                                     f\".\\nThe design{'' if len(overlapping_designs) == 1 else 's'} \"\n                                     f\"{', '.join(overlapping_designs)} for pose '{pose_identifier}'\")\n            if self.directives:\n                directives = [[self.directives[index] for index in indices] for indices in found_poses.values()]\n\n    # With above logic, it's impossible to have UnboundLocalError of design_names, directives\n    return zip(found_poses, design_names, directives)\n</code></pre>"},{"location":"reference/utils/#utils.dictionary_lookup","title":"dictionary_lookup","text":"<pre><code>dictionary_lookup(dictionary: dict, items: tuple[Any, ...]) -&gt; Any\n</code></pre> <p>Return the values of a dictionary for the item pairs nested within</p> <p>Parameters:</p> <ul> <li> <code>dictionary</code>             (<code>dict</code>)         \u2013          <p>The dictionary to search</p> </li> <li> <code>items</code>             (<code>tuple[Any, ...]</code>)         \u2013          <p>The tuple of keys to search for</p> </li> </ul> <p>Returns:     The value specified by dictionary keys</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def dictionary_lookup(dictionary: dict, items: tuple[Any, ...]) -&gt; Any:\n    \"\"\"Return the values of a dictionary for the item pairs nested within\n\n    Args:\n        dictionary: The dictionary to search\n        items: The tuple of keys to search for\n    Returns:\n        The value specified by dictionary keys\n    \"\"\"\n    return reduce(getitem, items, dictionary)\n</code></pre>"},{"location":"reference/utils/#utils.set_dictionary_by_path","title":"set_dictionary_by_path","text":"<pre><code>set_dictionary_by_path(root, items, value)\n</code></pre> <p>Set a value in a nested object in root by item sequence.</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def set_dictionary_by_path(root, items, value):\n    \"\"\"Set a value in a nested object in root by item sequence.\"\"\"\n    dictionary_lookup(root, items[:-1])[items[-1]] = value\n</code></pre>"},{"location":"reference/utils/#utils.handle_errors","title":"handle_errors","text":"<pre><code>handle_errors(errors: tuple[Type[Exception], ...] = (Exception)) -&gt; Any\n</code></pre> <p>Decorator to wrap a function with try: ... except errors:</p> <p>Parameters:</p> <ul> <li> <code>errors</code>             (<code>tuple[Type[Exception], ...]</code>, default:                 <code>(Exception)</code> )         \u2013          <p>A tuple of exceptions to monitor, even if single exception</p> </li> </ul> <p>Returns:     Function return upon proper execution, else the Exception if one was raised</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def handle_errors(errors: tuple[Type[Exception], ...] = (Exception,)) -&gt; Any:\n    \"\"\"Decorator to wrap a function with try: ... except errors:\n\n    Args:\n        errors: A tuple of exceptions to monitor, even if single exception\n    Returns:\n        Function return upon proper execution, else the Exception if one was raised\n    \"\"\"\n    def wrapper(func: Callable) -&gt; Any:\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except errors as error:\n                return error\n        return wrapped\n    return wrapper\n</code></pre>"},{"location":"reference/utils/#utils.timestamp","title":"timestamp","text":"<pre><code>timestamp() -&gt; str\n</code></pre> <p>Return the date/time formatted as YR-MO-DA-HRMNSC. Ex: 2022-Jan-01-245959</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def timestamp() -&gt; str:\n    \"\"\"Return the date/time formatted as YR-MO-DA-HRMNSC. Ex: 2022-Jan-01-245959\"\"\"\n    return time.strftime('%y-%m-%d-%H%M%S')\n</code></pre>"},{"location":"reference/utils/#utils.datestamp","title":"datestamp","text":"<pre><code>datestamp(short: bool = False) -&gt; str\n</code></pre> <p>Return the date/time formatted as Year-Mon-DA.</p> <p>Parameters:</p> <ul> <li> <code>short</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to return the short date</p> </li> </ul> <p>Returns:     Ex: 2022-Jan-01 or 01-Jan-22 if short</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def datestamp(short: bool = False) -&gt; str:\n    \"\"\"Return the date/time formatted as Year-Mon-DA.\n\n    Args:\n        short: Whether to return the short date\n    Returns:\n        Ex: 2022-Jan-01 or 01-Jan-22 if short\n    \"\"\"\n    if short:\n        return time.strftime('%d-%b-%y')  # Desired PDB format\n    else:\n        return time.strftime('%Y-%b-%d')  # Preferred format\n</code></pre>"},{"location":"reference/utils/#utils.start_log","title":"start_log","text":"<pre><code>start_log(name: str = '', handler: int = 1, level: logging_level_literal = 2, location: AnyStr = os.getcwd(), propagate: bool = False, format_log: bool = True, no_log_name: bool = False, handler_level: logging_level_literal = None) -&gt; Logger\n</code></pre> <p>Create a logger to handle program messages</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>The name of the logger. By default, the root logger is returned</p> </li> <li> <code>handler</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Whether to handle to stream (1), a file (2), or a NullHandler (3+)</p> </li> <li> <code>level</code>             (<code>logging_level_literal</code>, default:                 <code>2</code> )         \u2013          <p>What level of messages to emit (1-debug, 2-info, 3-warning, 4-error, 5-critical)</p> </li> <li> <code>location</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>If a FileHandler is used (handler=2) where should file be written? .log is appended to the filename</p> </li> <li> <code>propagate</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to propagate messages to parent loggers (such as root or parent.current_logger)</p> </li> <li> <code>format_log</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to format the log with logger specific formatting otherwise use message format</p> </li> <li> <code>no_log_name</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to omit the logger name from the output</p> </li> <li> <code>handler_level</code>             (<code>logging_level_literal</code>, default:                 <code>None</code> )         \u2013          <p>Whether to set the level for the logger handler on top of the overall level</p> </li> </ul> <p>Returns:     Logger object to handle messages</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def start_log(name: str = '', handler: int = 1, level: logging_level_literal = 2, location: AnyStr = os.getcwd(),\n              propagate: bool = False, format_log: bool = True, no_log_name: bool = False,\n              handler_level: logging_level_literal = None) -&gt; Logger:\n    \"\"\"Create a logger to handle program messages\n\n    Args:\n        name: The name of the logger. By default, the root logger is returned\n        handler: Whether to handle to stream (1), a file (2), or a NullHandler (3+)\n        level: What level of messages to emit (1-debug, 2-info, 3-warning, 4-error, 5-critical)\n        location: If a FileHandler is used (handler=2) where should file be written? .log is appended to the filename\n        propagate: Whether to propagate messages to parent loggers (such as root or parent.current_logger)\n        format_log: Whether to format the log with logger specific formatting otherwise use message format\n        no_log_name: Whether to omit the logger name from the output\n        handler_level: Whether to set the level for the logger handler on top of the overall level\n    Returns:\n        Logger object to handle messages\n    \"\"\"\n    _logger = getLogger(name)\n    _logger.setLevel(log_levels[level])\n    # Todo make a mechanism to only emit warning or higher if propagate=True\n    #  See below this function for adding handler[0].addFilter()\n    _logger.propagate = propagate\n    if format_log:\n        if no_log_name:\n            message_fmt = '\\033[38;5;208m{levelname}\\033[0;0m: {message}'\n        else:\n            message_fmt = '\\033[38;5;93m{name}\\033[0;0m-\\033[38;5;208m{levelname}\\033[0;0m: {message}'\n    else:\n        message_fmt = '{message}'\n\n    _handler = log_handler[handler]\n    if handler == 2:\n        # Check for extension. If one doesn't exist, add \".log\"\n        lh = _handler(f'{location}.log' if os.path.splitext(location)[1] == '' else location,\n                      delay=True)\n        # Set delay=True to prevent the log from opening until the first emit() is called\n        # Remove any coloring from the log\n        message_fmt = (message_fmt.replace('\\033[38;5;208m', '')\n                       .replace('\\033[38;5;93m', '')\n                       .replace('\\033[0;0m', ''))\n    else:\n        # Check if a StreamHandler already exists\n        remove_streams = []\n        for idx, handler in enumerate(_logger.handlers):\n            if getattr(handler, 'stream', None):\n                remove_streams.append(idx)\n        for stream_idx in reversed(remove_streams):\n            _logger.handlers.pop(stream_idx)\n\n        lh = _handler()\n\n    if handler_level is not None:\n        lh.setLevel(log_levels[handler_level])\n\n    log_format = Formatter(fmt=message_fmt, style='{')\n    lh.setFormatter(log_format)\n    _logger.addHandler(lh)\n\n    return _logger\n</code></pre>"},{"location":"reference/utils/#utils.set_logging_to_level","title":"set_logging_to_level","text":"<pre><code>set_logging_to_level(level: logging_level_literal = None, handler_level: logging_level_literal = None)\n</code></pre> <p>For each Logger in current run time, set the Logger or the Logger.handlers level to level</p> <p>level is debug by default if no arguments are specified</p> <p>Parameters:</p> <ul> <li> <code>level</code>             (<code>logging_level_literal</code>, default:                 <code>None</code> )         \u2013          <p>The level to set all loggers to</p> </li> <li> <code>handler_level</code>             (<code>logging_level_literal</code>, default:                 <code>None</code> )         \u2013          <p>The level to set all logger handlers to</p> </li> </ul> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def set_logging_to_level(level: logging_level_literal = None, handler_level: logging_level_literal = None):\n    \"\"\"For each Logger in current run time, set the Logger or the Logger.handlers level to level\n\n    level is debug by default if no arguments are specified\n\n    Args:\n        level: The level to set all loggers to\n        handler_level: The level to set all logger handlers to\n    \"\"\"\n    if level is not None:\n        _level = log_levels[level]\n        set_level_func = Logger.setLevel\n    elif handler_level is not None:  # Todo possibly rework this to accept both arguments\n        _level = log_levels[handler_level]\n\n        def set_level_func(logger_: Logger, level_: int):\n            for handler in logger_.handlers:\n                handler.setLevel(level_)\n    else:  # if level is None and handler_level is None:\n        _level = log_levels[1]\n        set_level_func = Logger.setLevel\n\n    # print(root_logger.manager.loggerDict)\n    for logger_name in root_logger.manager.loggerDict:\n        _logger = getLogger(logger_name)\n        set_level_func(_logger, _level)\n</code></pre>"},{"location":"reference/utils/#utils.set_loggers_to_propagate","title":"set_loggers_to_propagate","text":"<pre><code>set_loggers_to_propagate()\n</code></pre> <p>For each Logger in current run time, set the Logger to propagate</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def set_loggers_to_propagate():\n    \"\"\"For each Logger in current run time, set the Logger to propagate\"\"\"\n    for logger_name in root_logger.manager.loggerDict:\n        _logger = getLogger(logger_name)\n        _logger.propagate = True\n</code></pre>"},{"location":"reference/utils/#utils.pretty_format_table","title":"pretty_format_table","text":"<pre><code>pretty_format_table(data: Iterable[tuple | dict], justification: Sequence[str] = None, header: Sequence[str] = None, header_justification: Sequence[str] = None) -&gt; list[str]\n</code></pre> <p>Present a table in readable format by sizing and justifying columns in a nested data structure i.e. [row1[column1, column2, ...], row2[], ...]</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Iterable[tuple | dict]</code>)         \u2013          <p>Where each successive element is a row and each row's sub-elements are unique columns. The typical data structure would be [[i, j, k], [yes, 4, 0.1], [no, 5, 0.3]]</p> </li> <li> <code>justification</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>Iterable with elements 'l'/'left', 'r'/'right', or 'c'/'center' as justification values</p> </li> <li> <code>header</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>The names of values to place in the table header</p> </li> <li> <code>header_justification</code>             (<code>Sequence[str]</code>, default:                 <code>None</code> )         \u2013          <p>Iterable with elements 'l'/'left', 'r'/'right', or 'c'/'center' as justification values</p> </li> </ul> <p>Returns:     The formatted data with each input row justified as an individual element in the list</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def pretty_format_table(data: Iterable[tuple | dict], justification: Sequence[str] = None, header: Sequence[str] = None,\n                        header_justification: Sequence[str] = None) -&gt; list[str]:\n    \"\"\"Present a table in readable format by sizing and justifying columns in a nested data structure\n    i.e. [row1[column1, column2, ...], row2[], ...]\n\n    Args:\n        data: Where each successive element is a row and each row's sub-elements are unique columns.\n            The typical data structure would be [[i, j, k], [yes, 4, 0.1], [no, 5, 0.3]]\n        justification: Iterable with elements 'l'/'left', 'r'/'right', or 'c'/'center' as justification values\n        header: The names of values to place in the table header\n        header_justification: Iterable with elements 'l'/'left', 'r'/'right', or 'c'/'center' as justification values\n    Returns:\n        The formatted data with each input row justified as an individual element in the list\n    \"\"\"\n    justification_d = {'l': str.ljust, 'r': str.rjust, 'c': str.center,\n                       'left': str.ljust, 'right': str.rjust, 'center': str.center}\n    # Incase data is passed as a dictionary, we should turn into an iterator of key, value\n    if isinstance(data, dict):\n        data = data.items()\n\n    # Format data as list so we can insert header\n    # data = [[column for column in row] for row in data]\n    data = list(data)\n    number_columns = len(data[0])\n    if header is not None:\n        if len(header) == number_columns:\n            data.insert(0, header)  # list(header))\n            if header_justification is None:\n                header_justification = list(str.ljust for _ in range(number_columns))\n            elif len(header_justification) == number_columns:\n                header_justification = [justification_d.get(key.lower(), str.ljust) for key in header_justification]\n            else:\n                raise RuntimeError(\n                    f\"The header_justification length ({len(header_justification)}) doesn't match the number of columns\"\n                    f\" ({number_columns})\")\n        else:\n            raise RuntimeError(\n                f\"The header length ({len(header)}) doesn't match the number of columns ({number_columns})\")\n\n    column_widths = get_table_column_widths(data)\n    # number_columns = len(column_widths)\n    if not justification:\n        justifications = list(str.ljust for _ in range(number_columns))\n    elif len(justification) == number_columns:\n        justifications = [justification_d.get(key.lower(), str.ljust) for key in justification]\n    else:\n        raise RuntimeError(\n            f\"The justification length ({len(justification)}) doesn't match the number of columns ({number_columns})\")\n\n    return [' '.join(header_justification[idx](column, column_widths[idx]) if row_idx == 0 and header is not None\n                     else justifications[idx](column, column_widths[idx])\n                     for idx, column in enumerate(map(str, row_entry)))\n            for row_idx, row_entry in enumerate(data)]\n</code></pre>"},{"location":"reference/utils/#utils.get_table_column_widths","title":"get_table_column_widths","text":"<pre><code>get_table_column_widths(data: Iterable) -&gt; tuple[int]\n</code></pre> <p>Find the widths of each column in a nested data structure</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Iterable</code>)         \u2013          <p>Where each successive element is a row and each row's sub-elements are unique columns</p> </li> </ul> <p>Returns:     A tuple containing the width of each column from the input data</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_table_column_widths(data: Iterable) -&gt; tuple[int]:\n    \"\"\"Find the widths of each column in a nested data structure\n\n    Args:\n        data: Where each successive element is a row and each row's sub-elements are unique columns\n    Returns:\n        A tuple containing the width of each column from the input data\n    \"\"\"\n    return tuple(max(map(len, map(str, column))) for column in zip(*data))\n</code></pre>"},{"location":"reference/utils/#utils.read_json","title":"read_json","text":"<pre><code>read_json(file_name, **kwargs) -&gt; dict | None\n</code></pre> <p>Use json.load to read an object from a file</p> <p>Parameters:</p> <ul> <li> <code>file_name</code>         \u2013          <p>The location of the file to write</p> </li> </ul> <p>Returns:     The json data in the file</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def read_json(file_name, **kwargs) -&gt; dict | None:\n    \"\"\"Use json.load to read an object from a file\n\n    Args:\n        file_name: The location of the file to write\n    Returns:\n        The json data in the file\n    \"\"\"\n    with open(file_name, 'r') as f_save:\n        data = json.load(f_save)\n\n    return data\n</code></pre>"},{"location":"reference/utils/#utils.write_json","title":"write_json","text":"<pre><code>write_json(data: Any, file_name: AnyStr, **kwargs) -&gt; AnyStr\n</code></pre> <p>Use json.dump to write an object to a file</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Any</code>)         \u2013          <p>The object to write</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>)         \u2013          <p>The location of the file to write</p> </li> </ul> <p>Returns:     The name of the written file</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def write_json(data: Any, file_name: AnyStr, **kwargs) -&gt; AnyStr:\n    \"\"\"Use json.dump to write an object to a file\n\n    Args:\n        data: The object to write\n        file_name: The location of the file to write\n    Returns:\n        The name of the written file\n    \"\"\"\n    with open(file_name, 'w') as f_save:\n        json.dump(data, f_save, **kwargs)\n\n    return file_name\n</code></pre>"},{"location":"reference/utils/#utils.unpickle","title":"unpickle","text":"<pre><code>unpickle(file_name: AnyStr) -&gt; Any\n</code></pre> <p>Unpickle (deserialize) and return a python object located at filename</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def unpickle(file_name: AnyStr) -&gt; Any:  # , protocol=pickle.HIGHEST_PROTOCOL):\n    \"\"\"Unpickle (deserialize) and return a python object located at filename\"\"\"\n    if '.pkl' not in file_name and '.pickle' not in file_name:\n        file_name = '%s.pkl' % file_name\n    try:\n        with open(file_name, 'rb') as serial_f:\n            new_object = pickle.load(serial_f)\n    except EOFError as ex:\n        raise InputError(\n            f\"The serialized file '{file_name}' contains no data.\")\n\n    return new_object\n</code></pre>"},{"location":"reference/utils/#utils.pickle_object","title":"pickle_object","text":"<pre><code>pickle_object(target_object: Any, name: str = None, out_path: AnyStr = os.getcwd(), protocol: int = pickle.HIGHEST_PROTOCOL) -&gt; AnyStr\n</code></pre> <p>Pickle (serialize) an object into a file named \"out_path/name.pkl\". Automatically adds extension</p> <p>Parameters:</p> <ul> <li> <code>target_object</code>             (<code>Any</code>)         \u2013          <p>Any python object</p> </li> <li> <code>name</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The name of the pickled file</p> </li> <li> <code>out_path</code>             (<code>AnyStr</code>, default:                 <code>getcwd()</code> )         \u2013          <p>Where the file should be written</p> </li> <li> <code>protocol</code>             (<code>int</code>, default:                 <code>HIGHEST_PROTOCOL</code> )         \u2013          <p>The pickling protocol to use</p> </li> </ul> <p>Returns:     The pickled filename</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def pickle_object(target_object: Any, name: str = None, out_path: AnyStr = os.getcwd(),\n                  protocol: int = pickle.HIGHEST_PROTOCOL) -&gt; AnyStr:\n    \"\"\"Pickle (serialize) an object into a file named \"out_path/name.pkl\". Automatically adds extension\n\n    Args:\n        target_object: Any python object\n        name: The name of the pickled file\n        out_path: Where the file should be written\n        protocol: The pickling protocol to use\n    Returns:\n        The pickled filename\n    \"\"\"\n    if name is None:\n        file_name = out_path\n    else:\n        file_name = os.path.join(out_path, name)\n\n    if not file_name.endswith('.pkl'):\n        file_name = f'{file_name}.pkl'\n\n    with open(file_name, 'wb') as f:\n        pickle.dump(target_object, f, protocol)\n\n    return file_name\n</code></pre>"},{"location":"reference/utils/#utils.remove_interior_keys","title":"remove_interior_keys","text":"<pre><code>remove_interior_keys(dictionary: dict, keys: Iterable, keep: bool = False) -&gt; dict[Any, dict[Any, Any]]\n</code></pre> <p>Clean specified keys from a dictionaries internal dictionary. Default removes the specified keys</p> <p>Parameters:</p> <ul> <li> <code>dictionary</code>             (<code>dict</code>)         \u2013          <p>{outer_dictionary: {key: value, key2: value2, ...}, ...}</p> </li> <li> <code>keys</code>             (<code>Iterable</code>)         \u2013          <p>Keys to be removed from dictionary, such as [key2, key10]</p> </li> <li> <code>keep</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to keep (True) or remove (False) specified keys</p> </li> </ul> <p>Returns:     {outer_dictionary: {key: value, ...}, ...} - Cleaned dictionary</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def remove_interior_keys(dictionary: dict, keys: Iterable, keep: bool = False) -&gt; dict[Any, dict[Any, Any]]:\n    \"\"\"Clean specified keys from a dictionaries internal dictionary. Default removes the specified keys\n\n    Args:\n        dictionary: {outer_dictionary: {key: value, key2: value2, ...}, ...}\n        keys: Keys to be removed from dictionary, such as [key2, key10]\n        keep: Whether to keep (True) or remove (False) specified keys\n    Returns:\n        {outer_dictionary: {key: value, ...}, ...} - Cleaned dictionary\n    \"\"\"\n    if keep:\n        return {entry: {key: dictionary[entry][key] for key in dictionary[entry] if key in keys}\n                for entry in dictionary}\n    else:\n        for entry in dictionary:\n            for key in keys:\n                dictionary[entry].pop(key, None)\n\n        return dictionary\n</code></pre>"},{"location":"reference/utils/#utils.clean_comma_separated_string","title":"clean_comma_separated_string","text":"<pre><code>clean_comma_separated_string(s: str) -&gt; list[str]\n</code></pre> <p>Return a list from a comma separated string</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def clean_comma_separated_string(s: str) -&gt; list[str]:\n    \"\"\"Return a list from a comma separated string\"\"\"\n    return list(map(str.strip, s.strip().split(',')))\n</code></pre>"},{"location":"reference/utils/#utils.format_index_string","title":"format_index_string","text":"<pre><code>format_index_string(index_string: str) -&gt; list[int]\n</code></pre> <p>From a string with indices of interest, comma separated or in a range, format into individual, integer indices</p> <p>Parameters:</p> <ul> <li> <code>index_string</code>             (<code>str</code>)         \u2013          <p>23, 34,35,56-89, 290</p> </li> </ul> <p>Returns:     Indices in Pose formatting</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def format_index_string(index_string: str) -&gt; list[int]:\n    \"\"\"From a string with indices of interest, comma separated or in a range, format into individual, integer indices\n\n    Args:\n        index_string: 23, 34,35,56-89, 290\n    Returns:\n        Indices in Pose formatting\n    \"\"\"\n    final_index = []\n    for index in clean_comma_separated_string(index_string):\n        if '-' in index:  # This is a range, extract ranges\n            try:\n                low, high = index.split('-')\n            except ValueError:  # Too many values to unpack\n                raise InputError(\n                    f\"Couldn't coerce the range '{index}' to a compatible range. Use the format 1-4 to specify the \"\n                    f\"index consisting of 1,2,3,4\")\n            try:\n                final_index.extend([idx for idx in range(int(low), int(high) + 1)])  # Include the last integer in range\n            except ValueError:\n                raise InputError(f\"Couldn't coerce the input '{index}' to a compatible range({low}, {high})\")\n        else:  # Single integer\n            final_index.append(int(index))\n\n    return final_index\n</code></pre>"},{"location":"reference/utils/#utils.write_file","title":"write_file","text":"<pre><code>write_file(data: Iterable, file_name: AnyStr = None) -&gt; AnyStr\n</code></pre> <p>Take an iterable and either output to user, write to a file, or both. User defined choice</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Iterable</code>)         \u2013          <p>The data to write to file</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The name of the file to write to</p> </li> </ul> <p>Returns:     The name of the output file</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def write_file(data: Iterable, file_name: AnyStr = None) -&gt; AnyStr:\n    \"\"\"Take an iterable and either output to user, write to a file, or both. User defined choice\n\n    Args:\n        data: The data to write to file\n        file_name: The name of the file to write to\n    Returns:\n        The name of the output file\n    \"\"\"\n    if not file_name:\n        file_name = os.path.join(os.getcwd(), input('What is your desired filename? (appended to current working '\n                                                    f'directory){query.input_string}'))\n    with open(file_name, 'w') as f:\n        f.write('%s\\n' % '\\n'.join(map(str, data)))\n\n    return file_name\n</code></pre>"},{"location":"reference/utils/#utils.io_save","title":"io_save","text":"<pre><code>io_save(data: Iterable, file_name: AnyStr = None) -&gt; AnyStr\n</code></pre> <p>Take an iterable and either output to user, write to a file, or both. User defined choice</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>Iterable</code>)         \u2013          <p>The data to write to file</p> </li> <li> <code>file_name</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>The name of the file to write to</p> </li> </ul> <p>Returns:     The name of the output file</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def io_save(data: Iterable, file_name: AnyStr = None) -&gt; AnyStr:\n    \"\"\"Take an iterable and either output to user, write to a file, or both. User defined choice\n\n    Args:\n        data: The data to write to file\n        file_name: The name of the file to write to\n    Returns:\n        The name of the output file\n    \"\"\"\n    io_prompt = f\"Enter 'P' to print Data, 'W' to write Data to file, or 'B' for both{query.input_string}\"\n    response = ['W', 'P', 'B', 'w', 'p', 'b']\n    _input = query.validate_input(io_prompt, response=response).lower()\n\n    if _input in 'bp':\n        logger.info('%s\\n' % '\\n'.join(map(str, data)))\n\n    if _input in 'wb':\n        write_file(data, file_name)\n\n    return file_name\n</code></pre>"},{"location":"reference/utils/#utils.to_iterable","title":"to_iterable","text":"<pre><code>to_iterable(obj: AnyStr | list, ensure_file: bool = False, skip_comma: bool = False) -&gt; list[str]\n</code></pre> <p>Take an object and return a list of individual objects splitting on newline or comma</p> <p>Parameters:</p> <ul> <li> <code>obj</code>             (<code>AnyStr | list</code>)         \u2013          <p>The object to convert to an Iterable</p> </li> <li> <code>ensure_file</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to ensure the passed obj is a file</p> </li> <li> <code>skip_comma</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to skip commas when converting the records to an iterable</p> </li> </ul> <p>Returns:     The Iterable formed from the input obj</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def to_iterable(obj: AnyStr | list, ensure_file: bool = False, skip_comma: bool = False) -&gt; list[str]:\n    \"\"\"Take an object and return a list of individual objects splitting on newline or comma\n\n    Args:\n        obj: The object to convert to an Iterable\n        ensure_file: Whether to ensure the passed obj is a file\n        skip_comma: Whether to skip commas when converting the records to an iterable\n    Returns:\n        The Iterable formed from the input obj\n    \"\"\"\n    try:\n        with open(obj, 'r') as f:\n            iterable = f.readlines()\n    except (FileNotFoundError, TypeError) as error:\n        if isinstance(error, FileNotFoundError) and ensure_file:\n            raise error\n        if isinstance(obj, list):\n            iterable = obj\n        else:  # Assume that obj is a string\n            iterable = [obj]\n\n    clean_list = []\n    for item in iterable:\n        if skip_comma:\n            it_list = [item]\n        else:\n            it_list = item.split(',')\n        clean_list.extend(map(str.strip, it_list))\n\n    # # Remove duplicates but keep the order\n    # clean_list = remove_duplicates(clean_list)\n    try:\n        clean_list.pop(clean_list.index(''))  # Remove any missing values\n    except ValueError:\n        pass\n    return clean_list\n</code></pre>"},{"location":"reference/utils/#utils.remove_duplicates","title":"remove_duplicates","text":"<pre><code>remove_duplicates(iter_: Iterable[Any]) -&gt; list[Any]\n</code></pre> <p>An efficient, order maintaining, set function to remove duplicates</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def remove_duplicates(iter_: Iterable[Any]) -&gt; list[Any]:\n    \"\"\"An efficient, order maintaining, set function to remove duplicates\"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in iter_ if not (x in seen or seen_add(x))]\n</code></pre>"},{"location":"reference/utils/#utils.calculate_mp_cores","title":"calculate_mp_cores","text":"<pre><code>calculate_mp_cores(cores: int = None, mpi: bool = False, jobs: int = None) -&gt; int\n</code></pre> <p>Calculate the number of multiprocessing cores to use for a specific application, taking the minimum</p> <p>Default options specify to leave at least one CPU available for the machine. If a SLURM environment is used, the number of cores will reflect the environmental variable SLURM_CPUS_PER_TASK Args:     cores: How many cpu's to use     mpi: If commands use MPI     jobs: How many jobs to use Returns:     The number of cores to use taking the minimum of cores, jobs, and max cpus available</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def calculate_mp_cores(cores: int = None, mpi: bool = False, jobs: int = None) -&gt; int:\n    \"\"\"Calculate the number of multiprocessing cores to use for a specific application, taking the minimum\n\n    Default options specify to leave at least one CPU available for the machine. If a SLURM environment is used,\n    the number of cores will reflect the environmental variable SLURM_CPUS_PER_TASK\n    Args:\n        cores: How many cpu's to use\n        mpi: If commands use MPI\n        jobs: How many jobs to use\n    Returns:\n        The number of cores to use taking the minimum of cores, jobs, and max cpus available\n    \"\"\"\n    allocated_cpus = os.environ.get('SLURM_CPUS_PER_TASK')\n    if allocated_cpus:  # Should follow allocation from SLURM environment\n        max_cpus_to_use = int(allocated_cpus)\n    else:  # logical=False only uses physical cpus, not logical threads\n        max_cpus_to_use = psutil.cpu_count(logical=False) - 1  # Leave CPU available for computer\n\n    if cores or jobs:\n        # Take the minimum\n        infinity = float('inf')\n        return min((cores or infinity), (jobs or infinity), max_cpus_to_use)\n\n    if mpi:  # Todo grab an environmental variable for mpi cores?\n        return int(max_cpus_to_use / 6)  # distribute.mpi)\n    else:\n        return max_cpus_to_use\n</code></pre>"},{"location":"reference/utils/#utils.set_worker_affinity","title":"set_worker_affinity","text":"<pre><code>set_worker_affinity()\n</code></pre> <p>When a new worker process is created, use this initialization function to set the affinity for all CPUs. Especially important for multiprocessing in the context of numpy, scipy, pandas FROM Stack Overflow: https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy</p> http://manpages.ubuntu.com/manpages/precise/en/man1/taskset.1.html <p>-p is a mask for the logical cpu processors to use, the pid allows the affinity for an existing process to be specified instead of a new process being spawned</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def set_worker_affinity():\n    \"\"\"When a new worker process is created, use this initialization function to set the affinity for all CPUs.\n    Especially important for multiprocessing in the context of numpy, scipy, pandas\n    FROM Stack Overflow:\n    https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy\n\n    See: http://manpages.ubuntu.com/manpages/precise/en/man1/taskset.1.html\n        -p is a mask for the logical cpu processors to use, the pid allows the affinity for an existing process to be\n        specified instead of a new process being spawned\n    \"\"\"\n    _cmd = ['taskset', '-p', f'0x{\"f\" * int((psutil.cpu_count() / 4))}', str(os.getpid())]\n    logger.debug(subprocess.list2cmdline(_cmd))\n    p = subprocess.Popen(_cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n    p.communicate()\n</code></pre>"},{"location":"reference/utils/#utils.mp_map","title":"mp_map","text":"<pre><code>mp_map(function: Callable, arg: Iterable, processes: int = 1, context: str = 'spawn') -&gt; list[Any]\n</code></pre> <p>Maps an interable input with a single argument to a function using multiprocessing Pool</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable</code>)         \u2013          <p>Which function should be executed</p> </li> <li> <code>arg</code>             (<code>Iterable</code>)         \u2013          <p>Arguments to be unpacked in the defined function, order specific</p> </li> <li> <code>processes</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>How many workers/cores should be spawned to handle function(arguments)?</p> </li> <li> <code>context</code>             (<code>str</code>, default:                 <code>'spawn'</code> )         \u2013          <p>How to start new processes? One of 'spawn', 'fork', or 'forkserver'.</p> </li> </ul> <p>Returns:     The results produced from the function and arg</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def mp_map(function: Callable, arg: Iterable, processes: int = 1, context: str = 'spawn') -&gt; list[Any]:\n    \"\"\"Maps an interable input with a single argument to a function using multiprocessing Pool\n\n    Args:\n        function: Which function should be executed\n        arg: Arguments to be unpacked in the defined function, order specific\n        processes: How many workers/cores should be spawned to handle function(arguments)?\n        context: How to start new processes? One of 'spawn', 'fork', or 'forkserver'.\n    Returns:\n        The results produced from the function and arg\n    \"\"\"\n    # with mp.get_context(context).Pool(processes=processes) as p:  # , maxtasksperchild=100\n    with mp.get_context(context).Pool(processes=processes, initializer=set_worker_affinity) as p:\n        results = p.map(function, arg)\n\n    return results\n</code></pre>"},{"location":"reference/utils/#utils.mp_starmap","title":"mp_starmap","text":"<pre><code>mp_starmap(function: Callable, star_args: Iterable[tuple], processes: int = 1, context: str = 'spawn') -&gt; list[Any]\n</code></pre> <p>Maps an iterable input with multiple arguments to a function using multiprocessing Pool</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable</code>)         \u2013          <p>Which function should be executed</p> </li> <li> <code>star_args</code>             (<code>Iterable[tuple]</code>)         \u2013          <p>Arguments to be unpacked in the defined function, order specific</p> </li> <li> <code>processes</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>How many workers/cores should be spawned to handle function(arguments)?</p> </li> <li> <code>context</code>             (<code>str</code>, default:                 <code>'spawn'</code> )         \u2013          <p>How to start new processes? One of 'spawn', 'fork', or 'forkserver'.</p> </li> </ul> <p>Returns:     The results produced from the function and star_args</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def mp_starmap(function: Callable, star_args: Iterable[tuple], processes: int = 1, context: str = 'spawn') -&gt; list[Any]:\n    \"\"\"Maps an iterable input with multiple arguments to a function using multiprocessing Pool\n\n    Args:\n        function: Which function should be executed\n        star_args: Arguments to be unpacked in the defined function, order specific\n        processes: How many workers/cores should be spawned to handle function(arguments)?\n        context: How to start new processes? One of 'spawn', 'fork', or 'forkserver'.\n    Returns:\n        The results produced from the function and star_args\n    \"\"\"\n    # with mp.get_context(context).Pool(processes=processes) as p:  # , maxtasksperchild=100\n    with mp.get_context(context).Pool(processes=processes, initializer=set_worker_affinity) as p:\n        results = p.starmap(function, star_args)\n\n    return results\n</code></pre>"},{"location":"reference/utils/#utils.bytes2human","title":"bytes2human","text":"<pre><code>bytes2human(number: int, return_format: str = '{:.1f} {}') -&gt; str\n</code></pre> <p>Convert bytes to a human-readable format</p> <p>See: http://goo.gl/zeJZl</p> <p>bytes2human(10000) '9.8 K' bytes2human(100001221) '95.4 M'</p> <p>Parameters:</p> <ul> <li> <code>number</code>             (<code>int</code>)         \u2013          <p>The number of bytes</p> </li> <li> <code>return_format</code>             (<code>str</code>, default:                 <code>'{:.1f} {}'</code> )         \u2013          <p>The desired return format with '{}'.format() compatibility</p> </li> </ul> <p>Returns:     The human-readable expression of bytes from a number of bytes</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def bytes2human(number: int, return_format: str = \"{:.1f} {}\") -&gt; str:\n    \"\"\"Convert bytes to a human-readable format\n\n    See: http://goo.gl/zeJZl\n    &gt;&gt;&gt; bytes2human(10000)\n    '9.8 K'\n    &gt;&gt;&gt; bytes2human(100001221)\n    '95.4 M'\n\n    Args:\n        number: The number of bytes\n        return_format: The desired return format with '{}'.format() compatibility\n    Returns:\n        The human-readable expression of bytes from a number of bytes\n    \"\"\"\n    symbols = ('B', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')\n    prefix = {symbol: 1 &lt;&lt; idx * 10 for idx, symbol in enumerate(symbols)}\n\n    for symbol, symbol_number in reversed(prefix.items()):\n        if number &gt;= symbol_number:\n            value = number / symbol_number\n            break\n    else:  # Smaller than the smallest\n        symbol = symbols[0]\n        value = number\n    return return_format.format(value, symbol)\n</code></pre>"},{"location":"reference/utils/#utils.human2bytes","title":"human2bytes","text":"<pre><code>human2bytes(human_byte_str: AnyStr) -&gt; int\n</code></pre> <p>Convert human-readable bytes to a numeric format</p> <p>See: http://goo.gl/zeJZl</p> <p>human2bytes('0 B') 0 human2bytes('1 K') 1024 human2bytes('1 M') 1048576 human2bytes('1 Gi') 1073741824 human2bytes('1 tera') 1099511627776 human2bytes('0.5kilo') 512 human2bytes('0.1  byte') 0 human2bytes('1 k')  # k is an alias for K 1024 human2bytes('12 foo')</p> <p>Returns:     The number of bytes from a human-readable expression of bytes</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def human2bytes(human_byte_str: AnyStr) -&gt; int:\n    \"\"\"Convert human-readable bytes to a numeric format\n\n    See: http://goo.gl/zeJZl\n    &gt;&gt;&gt; human2bytes('0 B')\n    0\n    &gt;&gt;&gt; human2bytes('1 K')\n    1024\n    &gt;&gt;&gt; human2bytes('1 M')\n    1048576\n    &gt;&gt;&gt; human2bytes('1 Gi')\n    1073741824\n    &gt;&gt;&gt; human2bytes('1 tera')\n    1099511627776\n    &gt;&gt;&gt; human2bytes('0.5kilo')\n    512\n    &gt;&gt;&gt; human2bytes('0.1  byte')\n    0\n    &gt;&gt;&gt; human2bytes('1 k')  # k is an alias for K\n    1024\n    &gt;&gt;&gt; human2bytes('12 foo')\n\n    Raises:\n        ValueError if input can't be parsed\n    Returns:\n        The number of bytes from a human-readable expression of bytes\n    \"\"\"\n    # Find the scale prefix/abbreviation\n    letter = human_byte_str.translate(remove_digit_table).replace('.', '').replace(' ', '')\n    for name, symbol_set in SYMBOLS.items():\n        if letter in symbol_set:\n            break\n    else:\n        # if letter == 'k':\n        #     # treat 'k' as an alias for 'K' as per: http://goo.gl/kTQMs\n        #     sset = SYMBOLS['customary']\n        #     letter = letter.upper()\n        # else:\n        raise ValueError(f\"{human2bytes.__name__}: Can't interpret {human_byte_str}\")\n\n    # Find the size value\n    number = human_byte_str.strip(letter).strip()\n    try:\n        number = float(number)\n    except ValueError:\n        raise ValueError(f\"{human2bytes.__name__}: Can't interpret {human_byte_str}\")\n    else:\n        # Convert to numeric bytes\n        letter_index = symbol_set.index(letter)\n        return int(number * (1 &lt;&lt; letter_index * 10))\n</code></pre>"},{"location":"reference/utils/#utils.get_available_memory","title":"get_available_memory","text":"<pre><code>get_available_memory(human_readable: bool = False, gpu: bool = False) -&gt; int\n</code></pre> <p>Parameters:</p> <ul> <li> <code>human_readable</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the return value should be human-readable</p> </li> <li> <code>gpu</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether a GPU should be used</p> </li> </ul> <p>Returns:     The available memory (in bytes) depending on the compute environment</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_available_memory(human_readable: bool = False, gpu: bool = False) -&gt; int:\n    \"\"\"\n\n    Args:\n        human_readable: Whether the return value should be human-readable\n        gpu: Whether a GPU should be used\n    Returns:\n        The available memory (in bytes) depending on the compute environment\n    \"\"\"\n    # Check if job is allocated by SLURM\n    if 'SLURM_JOB_ID' in os.environ:\n        jobid = os.environ['SLURM_JOB_ID']  # SLURM_JOB_ID\n        # array_jobid = os.environ.get('SLURM_ARRAY_TASK_ID')\n        # if array_jobid:\n        #     jobid = f'{jobid}_{array_jobid}'  # SLURM_ARRAY_TASK_ID\n        if 'SLURM_ARRAY_TASK_ID' in os.environ:\n            jobid = f'{jobid}_{os.environ[\"SLURM_ARRAY_TASK_ID\"]}'  # SLURM_ARRAY_TASK_ID\n            logger.debug(f'The job is managed by SLURM with SLURM_ARRAY_TASK_ID={jobid}')\n        else:\n            logger.debug(f'The job is managed by SLURM with SLURM_JOB_ID={jobid}')\n\n        # Run the command 'scontrol show job {jobid}'\n        p = subprocess.Popen(['scontrol', 'show', 'job', jobid], stdout=subprocess.PIPE)\n        out, err = p.communicate()\n        out = out.decode('UTF-8')\n        \"\"\" When --mem-per-cpu=20G, searching for the line\n        MinCPUsNode=1 MinMemoryCPU=210000M MinTmpDiskNode=0\n        Features=(null) DelayBoot=00:00:00\n        \"\"\"\n        \"\"\" OR when --mem=20G, searching for the line\n        MinMemoryNode = 20G\n        \"\"\"\n        \"\"\" Additionally, the line with \n        TRES=cpu=1,mem=20G,node=1,billing=1\n        Is the same with either submission\n        \"\"\"\n        start_index = out.find('MinMemoryCPU=') + 13  # &lt;- 13 is length of search string\n        \"\"\"\n        Since default value is in M (MB), memory shouldn't be more than ~1000000 (1000 GB RAM?!)\n        Use plus 10 characters to parse. Value could be 50 I suppose and the split will get this variable only...\n        \"\"\"\n        # try:\n        memory_allocated = out[start_index:start_index + 10].split()[0]\n        # except IndexError:\n        #     print(out)\n        #     print(f\"start_index where 'MinMemoryCPU=' '=' was found: {start_index}\")\n        logger.debug(f'Found memory allocated: {memory_allocated}')\n        # memory_available = psutil.virtual_memory().available\n        # logger.debug(f'Found memory available: {bytes2human(memory_available)}')\n        process = psutil.Process()\n        memory_used = process.memory_info().rss\n        logger.debug(f'Found memory used: {bytes2human(memory_used)}')\n        try:\n            memory_constraint = human2bytes(memory_allocated) - memory_used\n        except ValueError:\n            logger.critical(f\"Found the scontrol out: {out}\")\n            raise\n    else:\n        memory_constraint = psutil.virtual_memory().available\n\n    if human_readable:\n        memory_constraint = bytes2human(memory_constraint)\n\n    return memory_constraint\n</code></pre>"},{"location":"reference/utils/#utils.get_base_root_paths_recursively","title":"get_base_root_paths_recursively","text":"<pre><code>get_base_root_paths_recursively(directory: AnyStr, sort: bool = True) -&gt; list[AnyStr]\n</code></pre> <p>Retrieve the bottom most directories recursively from a root directory</p> <p>Parameters:</p> <ul> <li> <code>directory</code>             (<code>AnyStr</code>)         \u2013          <p>The root directory of interest</p> </li> <li> <code>sort</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the files should be filtered by name before returning</p> </li> </ul> <p>Returns:     The list of directories matching the search</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_base_root_paths_recursively(directory: AnyStr, sort: bool = True) -&gt; list[AnyStr]:\n    \"\"\"Retrieve the bottom most directories recursively from a root directory\n\n    Args:\n        directory: The root directory of interest\n        sort: Whether the files should be filtered by name before returning\n    Returns:\n        The list of directories matching the search\n    \"\"\"\n    file_generator = (os.path.abspath(root) for root, dirs, files in os.walk(directory) if not dirs)\n    return sorted(file_generator) if sort else list(file_generator)\n</code></pre>"},{"location":"reference/utils/#utils.get_file_paths_recursively","title":"get_file_paths_recursively","text":"<pre><code>get_file_paths_recursively(directory: AnyStr, extension: str = None, sort: bool = True) -&gt; list[AnyStr]\n</code></pre> <p>Retrieve files recursively from a directory</p> <p>Parameters:</p> <ul> <li> <code>directory</code>             (<code>AnyStr</code>)         \u2013          <p>The directory of interest</p> </li> <li> <code>extension</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>A extension to filter by</p> </li> <li> <code>sort</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the files should be filtered by name before returning</p> </li> </ul> <p>Returns:     The list of files matching the search</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_file_paths_recursively(directory: AnyStr, extension: str = None, sort: bool = True) -&gt; list[AnyStr]:\n    \"\"\"Retrieve files recursively from a directory\n\n    Args:\n        directory: The directory of interest\n        extension: A extension to filter by\n        sort: Whether the files should be filtered by name before returning\n    Returns:\n        The list of files matching the search\n    \"\"\"\n    if extension is not None:\n        file_generator = (os.path.join(os.path.abspath(root), file)\n                          for root, dirs, files in os.walk(directory, followlinks=True) for file in files\n                          if extension in file)\n    else:\n        file_generator = (os.path.join(os.path.abspath(root), file)\n                          for root, dirs, files in os.walk(directory, followlinks=True) for file in files)\n\n    return sorted(file_generator) if sort else list(file_generator)\n</code></pre>"},{"location":"reference/utils/#utils.get_directory_file_paths","title":"get_directory_file_paths","text":"<pre><code>get_directory_file_paths(directory: AnyStr, suffix: str = '', extension: str = '', sort: bool = True) -&gt; list[AnyStr]\n</code></pre> <p>Return all files in a directory with specified extensions and suffixes</p> <p>Parameters:</p> <ul> <li> <code>directory</code>             (<code>AnyStr</code>)         \u2013          <p>The directory of interest</p> </li> <li> <code>suffix</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>A string to match before the extension. A glob pattern is built as follows \"suffixextension\" ex: suffix=\"model\" matches \"design_model.pdb\" and \"model1.pdb\"</p> </li> <li> <code>extension</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>A extension to filter by. Include the \".\" if there is one</p> </li> <li> <code>sort</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether the files should be filtered by name before returning</p> </li> </ul> <p>Returns:     The list of files matching the search</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_directory_file_paths(directory: AnyStr, suffix: str = '', extension: str = '', sort: bool = True) -&gt; \\\n        list[AnyStr]:\n    \"\"\"Return all files in a directory with specified extensions and suffixes\n\n    Args:\n        directory: The directory of interest\n        suffix: A string to match before the extension. A glob pattern is built as follows \"*suffix*extension\"\n            ex: suffix=\"model\" matches \"design_model.pdb\" and \"model1.pdb\"\n        extension: A extension to filter by. Include the \".\" if there is one\n        sort: Whether the files should be filtered by name before returning\n    Returns:\n        The list of files matching the search\n    \"\"\"\n    directory = os.path.abspath(directory)\n    file_generator = (file for file in glob(os.path.join(directory, f'*{suffix}*{extension}')))\n\n    return sorted(file_generator) if sort else list(file_generator)\n</code></pre>"},{"location":"reference/utils/#utils.collect_nanohedra_designs","title":"collect_nanohedra_designs","text":"<pre><code>collect_nanohedra_designs(files: Sequence = None, directory: str = None, dock: bool = False) -&gt; tuple[list[AnyStr], str]\n</code></pre> <p>Grab all poses from a Nanohedra directory via a file or a directory</p> <p>Parameters:</p> <ul> <li> <code>files</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>Iterable with disk location of files containing design directories</p> </li> <li> <code>directory</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Disk location of the program directory</p> </li> <li> <code>dock</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether the designs are in current docking run</p> </li> </ul> <p>Returns:     The absolute paths to Nanohedra output directories for all pose directories found</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def collect_nanohedra_designs(files: Sequence = None, directory: str = None, dock: bool = False) -&gt; \\\n        tuple[list[AnyStr], str]:\n    \"\"\"Grab all poses from a Nanohedra directory via a file or a directory\n\n    Args:\n        files: Iterable with disk location of files containing design directories\n        directory: Disk location of the program directory\n        dock: Whether the designs are in current docking run\n    Returns:\n        The absolute paths to Nanohedra output directories for all pose directories found\n    \"\"\"\n    if files:\n        all_paths = []\n        for file in files:\n            if not os.path.exists(file):\n                logger.critical(f'No \"{file}\" file found! Please ensure correct location/name!')\n                sys.exit(1)\n            if '.pdb' in file:  # single .pdb files were passed as input and should be loaded as such\n                all_paths.append(file)\n            else:  # assume a file that specifies individual designs was passed and load all design names in that file\n                try:\n                    with open(file, 'r') as f:\n                        # only strip the trailing 'os.sep' in case file names are passed\n                        paths = map(str.rstrip, [location.strip() for location in f.readlines()\n                                                 if location.strip() != ''], repeat(os.sep))\n                except IsADirectoryError:\n                    raise InputError(f'{file} is a directory not a file. Did you mean to run with --directory?')\n                all_paths.extend(paths)\n    elif directory:\n        if dock:\n            all_paths = get_docked_directories(directory)\n        else:\n            base_directories = get_base_nanohedra_dirs(directory)\n            all_paths = []\n            for base in base_directories:  # Todo we shouldn't allow multiple, it complicates SymEntry matching\n                all_paths.extend(get_docked_dirs_from_base(base))\n    else:  # this shouldn't happen\n        all_paths = []\n    location = (files or directory)\n\n    return sorted(set(all_paths)), location if isinstance(location, str) else location[0]\n</code></pre>"},{"location":"reference/utils/#utils.get_base_nanohedra_dirs","title":"get_base_nanohedra_dirs","text":"<pre><code>get_base_nanohedra_dirs(base_dir)\n</code></pre> <p>Find all master directories corresponding to the highest output level of Nanohedra.py outputs. This corresponds to the PoseJob symmetry attribute</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_base_nanohedra_dirs(base_dir):\n    \"\"\"Find all master directories corresponding to the highest output level of Nanohedra.py outputs. This corresponds\n    to the PoseJob symmetry attribute\n    \"\"\"\n    nanohedra_dirs = []\n    for root, dirs, files in os.walk(base_dir, followlinks=True):\n        if putils.master_log in files:\n            nanohedra_dirs.append(root)\n            del dirs[:]\n\n    return nanohedra_dirs\n</code></pre>"},{"location":"reference/utils/#utils.get_docked_directories","title":"get_docked_directories","text":"<pre><code>get_docked_directories(base_directory, directory_type='NanohedraEntry')\n</code></pre> <p>Useful for when your docked directory is basically known but the</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_docked_directories(base_directory, directory_type='NanohedraEntry'):  # '*DockedPoses'\n    \"\"\"Useful for when your docked directory is basically known but the \"\"\"\n    return [os.path.join(root, _dir) for root, dirs, files in os.walk(base_directory) for _dir in dirs\n            if directory_type in _dir]\n</code></pre>"},{"location":"reference/utils/#utils.get_docked_dirs_from_base","title":"get_docked_dirs_from_base","text":"<pre><code>get_docked_dirs_from_base(base: str) -&gt; list[AnyStr]\n</code></pre> <p>Find every Nanohedra output base directory where each of the poses and files is contained</p> <p>Parameters:</p> <ul> <li> <code>base</code>             (<code>str</code>)         \u2013          <p>The base of the filepath corresponding to the Nanohedra master output directory</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[AnyStr]</code>         \u2013          <p>The absolute path to every directory containing Nanohedra output</p> </li> </ul> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_docked_dirs_from_base(base: str) -&gt; list[AnyStr]:\n    \"\"\"Find every Nanohedra output base directory where each of the poses and files is contained\n\n    Args:\n        base: The base of the filepath corresponding to the Nanohedra master output directory\n\n    Returns:\n        The absolute path to every directory containing Nanohedra output\n    \"\"\"\n    # base/building_blocks/degen/rot/tx/'\n    # abspath removes trailing separator as well\n    return sorted(set(map(os.path.abspath, glob(f'{base}{f\"{os.sep}*\" * 4}{os.sep}'))))\n</code></pre>"},{"location":"reference/utils/#utils.collect_designs","title":"collect_designs","text":"<pre><code>collect_designs(files: Sequence = None, directory: AnyStr = None, projects: Sequence = None, singles: Sequence = None) -&gt; tuple[list, str]\n</code></pre> <p>Grab all poses from an input source</p> <p>Parameters:</p> <ul> <li> <code>files</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>Iterable with disk location of files containing design directories</p> </li> <li> <code>directory</code>             (<code>AnyStr</code>, default:                 <code>None</code> )         \u2013          <p>Disk location of the program directory</p> </li> <li> <code>projects</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>Disk location of a project directory</p> </li> <li> <code>singles</code>             (<code>Sequence</code>, default:                 <code>None</code> )         \u2013          <p>Disk location of a single design directory</p> </li> </ul> <p>Returns:     All pose directories found, the location where they are located</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def collect_designs(files: Sequence = None, directory: AnyStr = None, projects: Sequence = None,\n                    singles: Sequence = None) -&gt; tuple[list, str]:\n    \"\"\"Grab all poses from an input source\n\n    Args:\n        files: Iterable with disk location of files containing design directories\n        directory: Disk location of the program directory\n        projects: Disk location of a project directory\n        singles: Disk location of a single design directory\n    Returns:\n        All pose directories found, the location where they are located\n    \"\"\"\n    if files:\n        all_paths = []\n        for file in files:\n            if not os.path.exists(file):\n                logger.critical(f\"No '{file}' file found. Please ensure correct location/name\")\n                sys.exit(1)\n            if '.pdb' in file:  # Single .pdb file passed as input\n                all_paths.append(file)\n            elif '.cif' in file:  # Single .cif file passed as input\n                all_paths.append(file)\n            else:  # Assume a file that specifies individual designs was passed and load all design names in that file\n                try:\n                    with open(file, 'r') as f:\n                        # only strip the trailing 'os.sep' in case file names are passed\n                        paths = map(str.rstrip, [location.strip() for location in f.readlines()\n                                                 if location.strip() != ''], repeat(os.sep))\n                except IsADirectoryError:\n                    raise IsADirectoryError(\n                        f\"'{file}' is a directory not a file. Did you mean to run with --file?'\")\n                all_paths.extend(paths)\n    else:\n        base_directory = get_program_root_directory(directory)\n        # return all design directories within:\n        #  base directory -&gt; /base/Projects/project1, ... /base/Projects/projectN\n        #  specified projects -&gt; /base/Projects/project1, /base/Projects/project2, ...\n        #  specified singles -&gt; /base/Projects/project/design1, /base/Projects/project/design2, ...\n        if base_directory or projects or singles:\n            all_paths = get_program_directories(base=base_directory, projects=projects, singles=singles)\n        elif directory:  # This is probably an uninitialized project. Grab all .pdb files\n            all_paths = get_directory_file_paths(directory, extension='.pdb')\n            directory = os.path.basename(directory)  # This is for the location variable return\n        else:  # Function was called with all set to None. This shouldn't happen\n            raise ValueError(\n                f\"Can't {collect_designs.__name__}() with no arguments passed\")\n\n    location = (files or directory or projects or singles)\n\n    return sorted(set(all_paths)), location  # if isinstance(location, str) else location[0]  # Grab first index\n</code></pre>"},{"location":"reference/utils/#utils.get_program_root_directory","title":"get_program_root_directory","text":"<pre><code>get_program_root_directory(search_path: str = None) -&gt; AnyStr | None\n</code></pre> <p>Find the program_output variable in the specified path and return the path to it</p> <p>Parameters:</p> <ul> <li> <code>search_path</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The path to search</p> </li> </ul> <p>Returns:     The absolute path of the identified program root</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_program_root_directory(search_path: str = None) -&gt; AnyStr | None:\n    \"\"\"Find the program_output variable in the specified path and return the path to it\n\n    Args:\n        search_path: The path to search\n    Returns:\n        The absolute path of the identified program root\n    \"\"\"\n    root_directory = None\n    if search_path is not None:\n        # Search for the program_output name in the provided path\n        search_path = os.path.abspath(search_path)\n        if putils.program_output in search_path:   # directory1/program_output/directory2/directory3\n            # Return the path to that directory\n            for idx, dirname in enumerate(search_path.split(os.sep), 1):\n                if dirname == putils.program_output:\n                    root_directory = f'{os.sep}{os.path.join(*search_path.split(os.sep)[:idx])}'\n                    break\n            else:\n                raise InputError(\n                    f'{putils.program_output} is missing in search_path. This should never happen')\n        else:  # See if program_output is a child of the provided search_path\n            try:\n                all_files = os.listdir(search_path)\n            except (FileNotFoundError, NotADirectoryError):\n                all_files = []\n            if putils.program_output in all_files:  # directory_provided/program_output\n                for sub_directory in all_files:\n                    if sub_directory == putils.program_output:\n                        root_directory = os.path.join(search_path, sub_directory)\n                        break\n                else:\n                    raise InputError(\n                        f'{putils.program_output} is missing in all_files. This should never happen')\n\n    return root_directory\n</code></pre>"},{"location":"reference/utils/#utils.get_program_directories","title":"get_program_directories","text":"<pre><code>get_program_directories(base: str = None, projects: Iterable = None, singles: Iterable = None) -&gt; Generator[AnyStr, None, None]\n</code></pre> <p>Return the specific design directories from the specified hierarchy with the format /base(program_output)/Projects/project/design</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def get_program_directories(base: str = None, projects: Iterable = None, singles: Iterable = None) \\\n        -&gt; Generator[AnyStr, None, None]:\n    \"\"\"Return the specific design directories from the specified hierarchy with the format\n    /base(program_output)/Projects/project/design\n    \"\"\"\n    paths = []\n    if base:\n        paths.extend(glob(f'{base}{os.sep}{putils.projects}{os.sep}*{os.sep}*{os.sep}'))  # base/Projects/*/*/\n    if projects:\n        for project in projects:\n            paths.extend(glob(f'{project}{os.sep}*{os.sep}'))  # base/Projects/project/*/\n    if singles:\n        for single, extension in map(os.path.splitext, singles):  # Remove extensions\n            paths.extend(glob(f'{single}{os.sep}'))  # base/Projects/project/single/\n    return map(os.path.abspath, paths)\n</code></pre>"},{"location":"reference/utils/#utils.all_vs_all","title":"all_vs_all","text":"<pre><code>all_vs_all(iterable: Iterable, func: Callable, symmetrize: bool = True) -&gt; ndarray\n</code></pre> <p>Calculate an all versus all comparison using a defined function. Matrix is symmetrized by default</p> <p>Parameters:</p> <ul> <li> <code>iterable</code>             (<code>Iterable</code>)         \u2013          <p>Dictionary or array like object</p> </li> <li> <code>func</code>             (<code>Callable</code>)         \u2013          <p>Function to calculate different iterations of the iterable</p> </li> <li> <code>symmetrize</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to make the resulting matrix symmetric</p> </li> </ul> <p>Returns:     Matrix with resulting calculations</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def all_vs_all(iterable: Iterable, func: Callable, symmetrize: bool = True) -&gt; np.ndarray:\n    \"\"\"Calculate an all versus all comparison using a defined function. Matrix is symmetrized by default\n\n    Args:\n        iterable: Dictionary or array like object\n        func: Function to calculate different iterations of the iterable\n        symmetrize: Whether to make the resulting matrix symmetric\n    Returns:\n        Matrix with resulting calculations\n    \"\"\"\n    if isinstance(iterable, dict):\n        # func(iterable[obj1], iterable[obj2])\n        _dict = iterable\n    else:\n        _dict = None\n\n    pairwise = np.zeros((len(iterable), (len(iterable))))\n    for i, obj1 in enumerate(iterable[:-1]):\n        j = i+1\n        for j, obj2 in enumerate(iterable[j:], j):\n            # if type(iterable) == dict:  # _dict\n            pairwise[i][j] = func(obj1, obj2, d=_dict)\n            # pairwise[i][j] = func(obj1, obj2, iterable, d=_dict)\n            # else:\n            #     pairwise[i][j] = func(obj1, obj2, iterable, d=_dict)\n\n    if symmetrize:\n        return sym(pairwise)\n    else:\n        return pairwise\n</code></pre>"},{"location":"reference/utils/#utils.sym","title":"sym","text":"<pre><code>sym(a: ndarray) -&gt; ndarray\n</code></pre> <p>Symmetrize a numpy array. i.e. if a_ij = 0, then the returned array is such that a_ji = a_ij</p> <p>Parameters:</p> <ul> <li> <code>a</code>             (<code>ndarray</code>)         \u2013          <p>A 2D square array</p> </li> </ul> <p>Returns:     Symmetrized array</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def sym(a: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Symmetrize a numpy array. i.e. if a_ij = 0, then the returned array is such that a_ji = a_ij\n\n    Args:\n        a: A 2D square array\n    Returns:\n        Symmetrized array\n    \"\"\"\n    return a + a.T - np.diag(a.diagonal())\n</code></pre>"},{"location":"reference/utils/#utils.condensed_to_square","title":"condensed_to_square","text":"<pre><code>condensed_to_square(k, n)\n</code></pre> <p>Return the i, j indices of a scipy condensed matrix from element k and matrix dimension n</p> Source code in <code>symdesign/utils/__init__.py</code> <pre><code>def condensed_to_square(k, n):\n    \"\"\"Return the i, j indices of a scipy condensed matrix from element k and matrix dimension n\"\"\"\n    def calc_row_idx(_k, _n):\n        return int(math.ceil((1 / 2.) * (- (-8 * _k + 4 * _n ** 2 - 4 * _n - 7) ** 0.5 + 2 * _n - 1) - 1))\n\n    def elem_in_i_rows(_i, _n):\n        return _i * (_n - 1 - _i) + (_i * (_i + 1)) // 2\n\n    def calc_col_idx(_k, _i, _n):\n        return int(_n - elem_in_i_rows(_i + 1, _n) + _k)\n    i = calc_row_idx(k, n)\n    j = calc_col_idx(k, i, n)\n\n    return i, j\n</code></pre>"},{"location":"reference/utils/SymEntry/","title":"SymEntry","text":""},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry","title":"SymEntry","text":"<pre><code>SymEntry(entry: int, sym_map: list[str] = None, **kwargs)\n</code></pre> <p>Stores information for a particular symmetry definition which derives from a standard definition of a global symmetry type and provides access to operators and attributes which allow symmetric manipulation.</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>int</code>)         \u2013          <p>The entry integer which uniquely identifies this instance information</p> </li> <li> <code>sym_map</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>The mapping of individual groups to construct this instance</p> </li> <li> <code>**kwargs</code>         \u2013          </li> </ul> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def __init__(self, entry: int, sym_map: list[str] = None, **kwargs):\n    \"\"\"Construct the instance\n\n    Args:\n        entry: The entry integer which uniquely identifies this instance information\n        sym_map: The mapping of individual groups to construct this instance\n        **kwargs:\n    \"\"\"\n    try:\n        self.group_info, result_info = parsed_symmetry_combinations[entry]\n        # group_info, result_info = parsed_symmetry_combinations[entry]\n        # returns\n        #  {'group1': [self.int_dof_group1, self.rot_set_group1, self.ref_frame_tx_dof1],\n        #   'group2': [self.int_dof_group2, self.rot_set_group2, self.ref_frame_tx_dof2],\n        #   ...},\n        #  [point_group_symmetry, resulting_symmetry, dimension, unit_cell, tot_dof, cycle_size]\n    except KeyError:\n        raise ValueError(\n            f\"Invalid symmetry entry '{entry}'. Supported values are Nanohedra entries: \"\n            f'{1}-{len(nanohedra_symmetry_combinations)} and custom entries: '\n            f'{\", \".join(map(str, custom_entries))}')\n\n    try:  # To unpack the result_info. This will fail if a CRYST1 record placeholder\n        self.point_group_symmetry, self.resulting_symmetry, self.dimension, self.cell_lengths, self.cell_angles, \\\n            self.total_dof, self.cycle_size = result_info\n    except ValueError:  # Not enough values to unpack, probably a CRYST token\n        # Todo - Crystallographic symmetry could coincide with group symmetry...\n        self.group_info = [('C1', [['r:&lt;1,1,1,h,i,a&gt;', 't:&lt;j,k,b&gt;'], 1, None])]  # Assume for now that the groups are C1\n        # group_info = [('C1', [['r:&lt;1,1,1,h,i,a&gt;', 't:&lt;j,k,b&gt;'], 1, None])]  # Assume for now that the groups are C1\n        self.point_group_symmetry = None\n        # self.resulting_symmetry = kwargs.get('resulting_symmetry', None)\n        if 'space_group' in kwargs:\n            self.resulting_symmetry = kwargs['space_group']\n        elif sym_map is None:\n            # self.resulting_symmetry = None\n            raise utils.SymmetryInputError(\n                f\"Can't create a {self.__class__.__name__} without passing 'space_group' or 'sym_map'\")\n        else:\n            self.resulting_symmetry, *_ = sym_map\n        self.dimension = 2 if self.resulting_symmetry in utils.symmetry.layer_group_cryst1_fmt_dict else 3\n        self.cell_lengths = self.cell_angles = None\n        self.total_dof = self.cycle_size = 0\n\n    self._int_dof_groups, self._setting_matrices, self._setting_matrices_numbers, self._ref_frame_tx_dof, \\\n        self.__external_dof = [], [], [], [], []\n    self.number = entry\n    self.entry_groups = [group_name for group_name, group_params in self.group_info if group_name is not None]\n    # Check if this entry ever has the same symmetry\n    self._same_symmetry = len(self.entry_groups) != len(set(self.entry_groups))\n    # group1, group2, *extra = entry_groups\n    if sym_map is None:  # Assume standard SymEntry\n        # Assumes 2 component symmetry. index with only 2 options\n        self.sym_map = [self.resulting_symmetry] + self.entry_groups\n        groups = self.entry_groups\n    else:  # Requires full specification of all symmetry groups\n        # Clean any missing groups then remove the result, pass the groups\n        result, *groups = self.sym_map = [sym for sym in sym_map if sym is not None]\n\n    # Solve the group information for each passed symmetry\n    self.groups = []\n    for idx, group in enumerate(groups, 1):\n        self.append_group(group)\n        # self.groups = []\n        # for idx, group in enumerate(groups, 1):\n        #     self.append_group(group)\n        #     # if group not in valid_symmetries:\n        #     #     if group is None:\n        #     #         # Todo\n        #     #         #  Need to refactor symmetry_combinations for any number of elements\n        #     #         continue\n        #     #     else:  # Recurse to see if it is yet another symmetry specification\n        #     #         # raise ValueError(\n        #     #         logger.warning(\n        #     #             f\"The symmetry group '{group}' specified at index '{idx}' isn't a valid sub-symmetry. \"\n        #     #             f\"Trying to correct by applying another {self.__class__.__name__}()\")\n        #     #         raise NotImplementedError()\n        #     #\n        #     # if group not in entry_groups:\n        #     #     # This is probably a sub-symmetry of one of the groups. Is it allowed?\n        #     #     if not symmetry_groups_are_allowed_in_entry(groups, *entry_groups, result=self.resulting_symmetry):\n        #     #                                                 # group1=group1, group2=group2):\n        #     #         viable_groups = [group for group in entry_groups if group is not None]\n        #     #         raise utils.SymmetryInputError(\n        #     #             f\"The symmetry group '{group}' isn't an allowed sub-symmetry of the result \"\n        #     #             f'{self.resulting_symmetry}, or the group(s) {\", \".join(viable_groups)}')\n        #     # self.groups.append(group)\n\n    # def add_group():\n    #     # Todo\n    #     #  Can the accuracy of this creation method be guaranteed with the usage of the same symmetry\n    #     #  operator and different orientations? Think T33\n    #     self._int_dof_groups.append(int_dof)\n    #     self._setting_matrices.append(setting_matrices[set_mat_number])\n    #     self._setting_matrices_numbers.append(set_mat_number)\n    #     if ext_dof is None:\n    #         self._ref_frame_tx_dof.append(ext_dof)\n    #         self.__external_dof.append(construct_uc_matrix(('0', '0', '0')))\n    #     else:\n    #         ref_frame_tx_dof = ext_dof.split(',')\n    #         self._ref_frame_tx_dof.append(ref_frame_tx_dof)\n    #         if group_idx &lt;= 2:\n    #             # This isn't possible with more than 2 groups unless the groups is tethered to existing\n    #             self.__external_dof.append(construct_uc_matrix(ref_frame_tx_dof))\n    #         else:\n    #             if ref_frame_tx_dof:\n    #                 raise utils.SymmetryInputError(\n    #                     f\"Can't create {self.__class__.__name__} with external degrees of freedom and &gt; 2 groups\")\n    #\n    # for group_idx, group_symmetry in enumerate(self.groups, 1):\n    #     if isinstance(group_symmetry, SymEntry):\n    #         group_symmetry = group_symmetry.resulting_symmetry\n    #     # for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in self.group_info:\n    #     for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in group_info:\n    #         if group_symmetry == entry_group_symmetry:\n    #             add_group()\n    #             break\n    #     else:  # None was found for this group_symmetry\n    #         # raise utils.SymmetryInputError(\n    #         logger.critical(\n    #             f\"Trying to assign the group '{group_symmetry}' at index {group_idx} to \"\n    #             f\"{self.__class__.__name__}.number={self.number}\")\n    #         # See if the group is a sub-symmetry of a known group\n    #         # for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in self.group_info:\n    #         for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in group_info:\n    #             entry_sub_groups = sub_symmetries.get(entry_group_symmetry, [None])\n    #             if group_symmetry in entry_sub_groups:\n    #                 add_group()\n    #                 break\n    #         else:\n    #             raise utils.SymmetryInputError(\n    #                 f\"Assignment of the group '{group_symmetry}' failed\")\n\n    # Check construction is valid\n    if self.point_group_symmetry not in valid_symmetries:\n        if not self.number == 0:  # Anything besides CRYST entry\n            raise utils.SymmetryInputError(\n                f'Invalid point group symmetry {self.point_group_symmetry}')\n    try:\n        if self.dimension == 0:\n            self._expand_matrices = point_group_symmetry_operators[self.resulting_symmetry]\n        elif self.dimension in [2, 3]:\n            self._expand_matrices, expand_translations = space_group_symmetry_operators[self.resulting_symmetry]\n        else:\n            raise utils.SymmetryInputError(\n                'Invalid symmetry entry. Supported dimensions are 0, 2, and 3')\n    except KeyError:\n        raise utils.SymmetryInputError(\n            f\"The symmetry result '{self.resulting_symmetry}' isn't allowed as there aren't group operators \"\n            \"available for it\")\n\n    if self.cell_lengths:\n        self.unit_cell = (self.cell_lengths, self.cell_angles)\n    else:\n        self.unit_cell = None\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.number_of_operations","title":"number_of_operations  <code>property</code>","text":"<pre><code>number_of_operations: int\n</code></pre> <p>The number of symmetric copies in the full symmetric system</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.group_subunit_numbers","title":"group_subunit_numbers  <code>property</code>","text":"<pre><code>group_subunit_numbers: list[int]\n</code></pre> <p>Returns the number of subunits for each symmetry group</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.specification","title":"specification  <code>property</code>","text":"<pre><code>specification: str\n</code></pre> <p>Return the specification for the instance. Ex: RESULT:{SUBSYMMETRY1}{SUBSYMMETRY2}... -&gt; (T:{C3}{C3})</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.simple_specification","title":"simple_specification  <code>property</code>","text":"<pre><code>simple_specification: str\n</code></pre> <p>Return the simple specification for the instance. Ex: 'RESULTSUBSYMMETRY1SUBSYMMETRY2... -&gt; (T33)</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.uc_specification","title":"uc_specification  <code>property</code>","text":"<pre><code>uc_specification: tuple[tuple[str] | None, tuple[int] | None]\n</code></pre> <p>The external dof and angle parameters which constitute a viable lattice</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.uc_dimensions","title":"uc_dimensions  <code>property</code> <code>writable</code>","text":"<pre><code>uc_dimensions: tuple[float, float, float, float, float, float] | None\n</code></pre> <p>The unit cell dimensions for the lattice specified by lengths a, b, c and angles alpha, beta, gamma</p> <p>Returns:</p> <ul> <li> <code>tuple[float, float, float, float, float, float] | None</code>         \u2013          <p>length a, length b, length c, angle alpha, angle beta, angle gamma</p> </li> </ul>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.rotation_range1","title":"rotation_range1  <code>property</code>","text":"<pre><code>rotation_range1: float\n</code></pre> <p>Return the rotation range according the first symmetry group operator</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.rotation_range2","title":"rotation_range2  <code>property</code>","text":"<pre><code>rotation_range2: float\n</code></pre> <p>Return the rotation range according the second symmetry group operator</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.rotation_range3","title":"rotation_range3  <code>property</code>","text":"<pre><code>rotation_range3: float\n</code></pre> <p>Return the rotation range according the third symmetry group operator</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.number_dof_rotation","title":"number_dof_rotation  <code>property</code>","text":"<pre><code>number_dof_rotation: int\n</code></pre> <p>Return the number of internal rotational degrees of freedom</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.is_internal_rot1","title":"is_internal_rot1  <code>property</code>","text":"<pre><code>is_internal_rot1: bool\n</code></pre> <p>Whether there are rotational degrees of freedom for group 1</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.is_internal_rot2","title":"is_internal_rot2  <code>property</code>","text":"<pre><code>is_internal_rot2: bool\n</code></pre> <p>Whether there are rotational degrees of freedom for group 2</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.number_dof_translation","title":"number_dof_translation  <code>property</code>","text":"<pre><code>number_dof_translation: int\n</code></pre> <p>Return the number of internal translational degrees of freedom</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.is_internal_tx1","title":"is_internal_tx1  <code>property</code>","text":"<pre><code>is_internal_tx1: bool\n</code></pre> <p>Whether there are internal translational degrees of freedom for group 1</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.is_internal_tx2","title":"is_internal_tx2  <code>property</code>","text":"<pre><code>is_internal_tx2: bool\n</code></pre> <p>Whether there are internal translational degrees of freedom for group 2</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.number_dof_external","title":"number_dof_external  <code>property</code>","text":"<pre><code>number_dof_external: int\n</code></pre> <p>Return the number of external degrees of freedom</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.external_dof","title":"external_dof  <code>property</code>","text":"<pre><code>external_dof: ndarray\n</code></pre> <p>Return the total external degrees of freedom as a number DOF externalx3 array</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.external_dofs","title":"external_dofs  <code>property</code>","text":"<pre><code>external_dofs: list[ndarray]\n</code></pre> <p>Return the 3x3 external degrees of freedom for component1</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.external_dof1","title":"external_dof1  <code>property</code>","text":"<pre><code>external_dof1: ndarray\n</code></pre> <p>Return the 3x3 external degrees of freedom for component1</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.external_dof2","title":"external_dof2  <code>property</code>","text":"<pre><code>external_dof2: ndarray\n</code></pre> <p>Return the 3x3 external degrees of freedom for component2</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.degeneracy_matrices1","title":"degeneracy_matrices1  <code>property</code>","text":"<pre><code>degeneracy_matrices1: ndarray\n</code></pre> <p>Returns the (number of degeneracies, 3, 3) degeneracy matrices for component1</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.degeneracy_matrices2","title":"degeneracy_matrices2  <code>property</code>","text":"<pre><code>degeneracy_matrices2: ndarray\n</code></pre> <p>Returns the (number of degeneracies, 3, 3) degeneracy matrices for component2</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.cryst_record","title":"cryst_record  <code>property</code> <code>writable</code>","text":"<pre><code>cryst_record: str | None\n</code></pre> <p>Get the CRYST1 record associated with this SymEntry</p>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.from_cryst","title":"from_cryst  <code>classmethod</code>","text":"<pre><code>from_cryst(space_group: str, **kwargs)\n</code></pre> <p>Create a SymEntry from a specified symmetry in Hermann-Mauguin notation and the unit-cell dimensions</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>@classmethod\ndef from_cryst(cls, space_group: str, **kwargs):  # uc_dimensions: Iterable[float],\n    \"\"\"Create a SymEntry from a specified symmetry in Hermann-Mauguin notation and the unit-cell dimensions\"\"\"\n    return cls(0, space_group=space_group, **kwargs)\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.append_group","title":"append_group","text":"<pre><code>append_group(group: str)\n</code></pre> <p>Add an additional symmetry group to the SymEntry</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def append_group(self, group: str):\n    \"\"\"Add an additional symmetry group to the SymEntry\"\"\"\n    if group not in valid_symmetries:\n        if group is None:\n            # Todo\n            #  Need to refactor symmetry_combinations for any number of elements\n            return\n        else:  # Recurse to see if it is yet another symmetry specification\n            # raise ValueError(\n            logger.warning(\n                f\"The symmetry group '{group}' at index {len(self.groups)} isn't a valid sub-symmetry. \"\n                f\"Trying to correct by applying another {self.__class__.__name__}()\")\n            raise NotImplementedError()\n\n    if group not in self.entry_groups:\n        # This is probably a sub-symmetry of one of the groups. Is it allowed?\n        if not symmetry_groups_are_allowed_in_entry([group], *self.entry_groups, result=self.resulting_symmetry):\n            # group1=group1, group2=group2):\n            viable_groups = [group for group in self.entry_groups if group is not None]\n            raise utils.SymmetryInputError(\n                f\"The symmetry group '{group}' isn't an allowed sub-symmetry of the result \"\n                f'{self.resulting_symmetry}, or the group(s) {\", \".join(viable_groups)}')\n    self.groups.append(group)\n\n    def add_group() -&gt; bool:\n        if self._same_symmetry:\n            if int_dof in self._int_dof_groups:\n                return False\n        self._int_dof_groups.append(int_dof)\n        self._setting_matrices.append(setting_matrices[set_mat_number])\n        self._setting_matrices_numbers.append(set_mat_number)\n        if ext_dof is None:\n            self._ref_frame_tx_dof.append(ext_dof)\n            self.__external_dof.append(construct_uc_matrix(('0', '0', '0')))\n        else:\n            ref_frame_tx_dof = ext_dof.split(',')\n            self._ref_frame_tx_dof.append(ref_frame_tx_dof)\n            if len(self.groups) &lt;= 2:\n                # This isn't possible with more than 2 groups unless the groups is tethered to existing\n                self.__external_dof.append(construct_uc_matrix(ref_frame_tx_dof))\n            else:\n                if ref_frame_tx_dof:\n                    raise utils.SymmetryInputError(\n                        f\"Can't create {self.__class__.__name__} with external degrees of freedom and &gt; 2 groups\")\n\n        return True\n\n    if isinstance(group, SymEntry):\n        group = group.resulting_symmetry\n    for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in self.group_info:\n        if group == entry_group_symmetry:\n            # Try to add the group. If it doesn't work, then proceed\n            if add_group():\n                break\n    else:  # None was found for this group\n        # raise utils.SymmetryInputError(\n        logger.critical(\n            f\"Trying to assign the group '{group}' at index {len(self.groups)} to \"\n            f\"{self.__class__.__name__}.number={self.number}\")\n        # See if the group is a sub-symmetry of a known group\n        for entry_group_symmetry, (int_dof, set_mat_number, ext_dof) in self.group_info:\n            entry_sub_groups = sub_symmetries.get(entry_group_symmetry, [None])\n            if group in entry_sub_groups:\n                add_group()\n                break\n        else:\n            raise utils.SymmetryInputError(\n                f\"Assignment of the group '{group}' failed\")\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.is_token","title":"is_token","text":"<pre><code>is_token() -&gt; bool\n</code></pre> <p>Is the SymEntry utilizing a provided CRYST1 record</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def is_token(self) -&gt; bool:\n    \"\"\"Is the SymEntry utilizing a provided CRYST1 record\"\"\"\n    return self.number == 0\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.needs_cryst_record","title":"needs_cryst_record","text":"<pre><code>needs_cryst_record() -&gt; bool\n</code></pre> <p>Is the SymEntry utilizing a provided CRYST1 record</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def needs_cryst_record(self) -&gt; bool:\n    \"\"\"Is the SymEntry utilizing a provided CRYST1 record\"\"\"\n    # If .number is 0, then definitely yes. Otherwise, need to check if one is already set\n    return self.dimension &gt; 0 and self.uc_dimensions is None\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.get_uc_dimensions","title":"get_uc_dimensions","text":"<pre><code>get_uc_dimensions(optimal_shift_vec: ndarray) -&gt; ndarray | None\n</code></pre> <p>Return an array with the three unit cell lengths and three angles [20, 20, 20, 90, 90, 90] by combining UC basis vectors with component translation degrees of freedom</p> <p>Parameters:</p> <ul> <li> <code>optimal_shift_vec</code>             (<code>ndarray</code>)         \u2013          <p>An Nx3 array where N is the number of shift instances and 3 is number of possible external degrees of freedom (even if they are not utilized)</p> </li> </ul> <p>Returns:     The unit cell dimensions for each optimal shift vector passed</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def get_uc_dimensions(self, optimal_shift_vec: np.ndarray) -&gt; np.ndarray | None:\n    \"\"\"Return an array with the three unit cell lengths and three angles [20, 20, 20, 90, 90, 90] by combining UC\n    basis vectors with component translation degrees of freedom\n\n    Args:\n        optimal_shift_vec: An Nx3 array where N is the number of shift instances\n            and 3 is number of possible external degrees of freedom (even if they are not utilized)\n    Returns:\n        The unit cell dimensions for each optimal shift vector passed\n    \"\"\"\n    if self.unit_cell is None:\n        return None\n    # for entry 6 - self.cell_lengths is ('4*e', '4*e', '4*e')\n    # construct_uc_matrix() = [[4, 4, 4], [0, 0, 0], [0, 0, 0]]\n    uc_mat = construct_uc_matrix(self.cell_lengths) * optimal_shift_vec[:, :, None]\n    # [:, :, None] &lt;- expands axis so multiplication is accurate. eg. [[[1.], [0.], [0.]], [[0.], [0.], [0.]]]\n    lengths = np.abs(uc_mat.sum(axis=-2))\n    #               (^).sum(axis=-2) = [4, 4, 4]\n    if len(self.cell_lengths) == 2:\n        lengths[:, 2] = 1.\n\n    if len(self.cell_angles) == 1:\n        angles = [90., 90., float(self.cell_angles[0])]\n    else:\n        angles = [0., 0., 0.]  # Initialize incase there are &lt; 1 self.cell_angles\n        for idx, string_angle in enumerate(self.cell_angles):\n            angles[idx] = float(string_angle)\n\n    # return np.concatenate(lengths, np.tile(angles, len(lengths)))\n    return np.hstack((lengths, np.tile(angles, len(lengths)).reshape(-1, 3)))\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.get_optimal_shift_from_uc_dimensions","title":"get_optimal_shift_from_uc_dimensions","text":"<pre><code>get_optimal_shift_from_uc_dimensions(a: float, b: float, c: float, *angles: list) -&gt; ndarray | None\n</code></pre> <p>Return the optimal shifts provided unit cell dimensions and the external translation degrees of freedom</p> <p>Parameters:</p> <ul> <li> <code>a</code>             (<code>float</code>)         \u2013          <p>The unit cell parameter for the lattice dimension 'a'</p> </li> <li> <code>b</code>             (<code>float</code>)         \u2013          <p>The unit cell parameter for the lattice dimension 'b'</p> </li> <li> <code>c</code>             (<code>float</code>)         \u2013          <p>The unit cell parameter for the lattice dimension 'c'</p> </li> <li> <code>angles</code>             (<code>list</code>, default:                 <code>()</code> )         \u2013          <p>The unit cell parameters for the lattice angles alpha, beta, gamma. Not utilized!</p> </li> </ul> <p>Returns:     The optimal shifts in each direction a, b, and c if they are allowed</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def get_optimal_shift_from_uc_dimensions(self, a: float, b: float, c: float, *angles: list) -&gt; np.ndarray | None:\n    \"\"\"Return the optimal shifts provided unit cell dimensions and the external translation degrees of freedom\n\n    Args:\n        a: The unit cell parameter for the lattice dimension 'a'\n        b: The unit cell parameter for the lattice dimension 'b'\n        c: The unit cell parameter for the lattice dimension 'c'\n        angles: The unit cell parameters for the lattice angles alpha, beta, gamma. Not utilized!\n    Returns:\n        The optimal shifts in each direction a, b, and c if they are allowed\n    \"\"\"\n    if self.unit_cell is None:\n        return None\n    # uc_mat = construct_uc_matrix(string_lengths) * optimal_shift_vec[:, :, None]  # &lt;- expands axis so mult accurate\n    uc_mat = construct_uc_matrix(self.cell_lengths)\n    # To reverse the values from the incoming a, b, and c, divide by the uc_matrix_constraints\n    # given the matrix should only ever have one value in each column (max) a sum over the column should produce the\n    # desired vector to calculate the optimal shift.\n    # There is a possibility of returning inf when we divide 0 by a value so ignore this warning\n    with warnings.catch_warnings():\n        # Cause all warnings to always be ignored\n        warnings.simplefilter('ignore')\n        external_translation_shifts = [a, b, c] / np.abs(uc_mat.sum(axis=-2))\n        # Replace any inf with zero\n        external_translation_shifts = np.nan_to_num(external_translation_shifts, copy=False, posinf=0., neginf=0.)\n\n    if len(self.cell_lengths) == 2:\n        external_translation_shifts[2] = 1.\n\n    return external_translation_shifts\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.sdf_lookup","title":"sdf_lookup","text":"<pre><code>sdf_lookup() -&gt; AnyStr\n</code></pre> <p>Locate the proper symmetry definition file depending on the specified symmetry</p> <p>Returns:</p> <ul> <li> <code>AnyStr</code>         \u2013          <p>The location of the symmetry definition file on disk</p> </li> </ul> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def sdf_lookup(self) -&gt; AnyStr:\n    \"\"\"Locate the proper symmetry definition file depending on the specified symmetry\n\n    Returns:\n        The location of the symmetry definition file on disk\n    \"\"\"\n    if self.dimension &gt; 0:\n        return os.path.join(putils.symmetry_def_files, 'C1.sym')\n\n    symmetry = self.simple_specification\n    for file, ext in map(os.path.splitext, os.listdir(putils.symmetry_def_files)):\n        if symmetry == file:\n            return os.path.join(putils.symmetry_def_files, file + ext)\n\n    symmetry = self.resulting_symmetry\n    for file, ext in map(os.path.splitext, os.listdir(putils.symmetry_def_files)):\n        if symmetry == file:\n            return os.path.join(putils.symmetry_def_files, file + ext)\n\n    raise FileNotFoundError(\n        f\"Couldn't locate symmetry definition file at '{putils.symmetry_def_files}' for {self.__class__.__name__} \"\n        f\"{self.number}\")\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntry.log_parameters","title":"log_parameters","text":"<pre><code>log_parameters()\n</code></pre> <p>Log the SymEntry Parameters</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def log_parameters(self):\n    \"\"\"Log the SymEntry Parameters\"\"\"\n    #                pdb1_path, pdb2_path, master_outdir\n    # log.info('NANOHEDRA PROJECT INFORMATION')\n    # log.info(f'Oligomer 1 Input: {pdb1_path}')\n    # log.info(f'Oligomer 2 Input: {pdb2_path}')\n    # log.info(f'Master Output Directory: {master_outdir}\\n')\n    logger.info('SYMMETRY COMBINATION MATERIAL INFORMATION')\n    logger.info(f'Nanohedra Entry Number: {self.number}')\n    logger.info(f'Oligomer 1 Point Group Symmetry: {self.group1}')\n    logger.info(f'Oligomer 2 Point Group Symmetry: {self.group2}')\n    logger.info(f'SCM Point Group Symmetry: {self.point_group_symmetry}')\n    # logger.debug(f'Oligomer 1 Internal ROT DOF: {self.is_internal_rot1}')\n    # logger.debug(f'Oligomer 2 Internal ROT DOF: {self.is_internal_rot2}')\n    # logger.debug(f'Oligomer 1 Internal Tx DOF: {self.is_internal_tx1}')\n    # logger.debug(f'Oligomer 2 Internal Tx DOF: {self.is_internal_tx2}')\n    # Todo textwrap.textwrapper() prettify these matrices\n    logger.debug(f'Oligomer 1 Setting Matrix: {self.setting_matrix1.tolist()}')\n    logger.debug(f'Oligomer 2 Setting Matrix: {self.setting_matrix2.tolist()}')\n    ext_tx_dof1, ext_tx_dof2, *_ = self.ref_frame_tx_dof\n    logger.debug(f'Oligomer 1 Reference Frame Tx DOF: {ext_tx_dof1}')\n    logger.debug(f'Oligomer 2 Reference Frame Tx DOF: {ext_tx_dof2}')\n    logger.info(f'Resulting SCM Symmetry: {self.resulting_symmetry}')\n    logger.info(f'SCM Dimension: {self.dimension}')\n    logger.info(f'SCM Unit Cell Specification: {self.uc_specification}\\n')\n    # rot_step_deg1, rot_step_deg2 = get_rotation_step(self, rot_step_deg1, rot_step_deg2, initial=True, log=logger)\n    logger.info('ROTATIONAL SAMPLING INFORMATION')\n    logger.info(f'Oligomer 1 ROT Sampling Range: '\n                f'{self.rotation_range1 if self.is_internal_rot1 else None}')\n    logger.info('Oligomer 2 ROT Sampling Range: '\n                f'{self.rotation_range2 if self.is_internal_rot2 else None}')\n    # logger.info('Oligomer 1 ROT Sampling Step: '\n    #             f'{rot_step_deg1 if self.is_internal_rot1 else None}')\n    # logger.info('Oligomer 2 ROT Sampling Step: '\n    #             f'{rot_step_deg2 if self.is_internal_rot2 else None}\\n')\n    # Get Degeneracy Matrices\n    # logger.info('Searching For Possible Degeneracies')\n    if self.degeneracy_matrices1 is None:\n        logger.info('No Degeneracies Found for Oligomer 1')\n    elif len(self.degeneracy_matrices1) == 1:\n        logger.info('1 Degeneracy Found for Oligomer 1')\n    else:\n        logger.info(f'{len(self.degeneracy_matrices1)} Degeneracies Found for Oligomer 1')\n    if self.degeneracy_matrices2 is None:\n        logger.info('No Degeneracies Found for Oligomer 2\\n')\n    elif len(self.degeneracy_matrices2) == 1:\n        logger.info('1 Degeneracy Found for Oligomer 2\\n')\n    else:\n        logger.info(f'{len(self.degeneracy_matrices2)} Degeneracies Found for Oligomer 2\\n')\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntryFactory","title":"SymEntryFactory","text":"<pre><code>SymEntryFactory(**kwargs)\n</code></pre> <p>Return a SymEntry instance by calling the Factory instance with the SymEntry entry number and symmetry map (sym_map)</p> <p>Handles creation and allotment to other processes by saving expensive memory load of multiple instances and allocating a shared pointer to the SymEntry</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def __init__(self, **kwargs):\n    self._entries = {}\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntryFactory.__call__","title":"__call__","text":"<pre><code>__call__(entry: int, sym_map: list[str] = None, **kwargs) -&gt; SymEntry\n</code></pre> <p>Return the specified SymEntry object singleton</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>int</code>)         \u2013          <p>The entry number</p> </li> <li> <code>sym_map</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>The particular mapping of the symmetric groups</p> </li> </ul> <p>Returns:     The instance of the specified SymEntry</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def __call__(self, entry: int, sym_map: list[str] = None, **kwargs) -&gt; SymEntry:\n    \"\"\"Return the specified SymEntry object singleton\n\n    Args:\n        entry: The entry number\n        sym_map: The particular mapping of the symmetric groups\n    Returns:\n        The instance of the specified SymEntry\n    \"\"\"\n    if sym_map is None:\n        sym_map_string = 'None'\n    else:\n        sym_map_string = '|'.join('None' if sym is None else sym for sym in sym_map)\n\n    if entry == 0:\n        # Don't add this to the self._entries\n        return CrystSymEntry(sym_map=sym_map, **kwargs)\n\n    entry_key = f'{entry}|{sym_map_string}'\n    symmetry = self._entries.get(entry_key)\n    if symmetry:\n        return symmetry\n    else:\n        self._entries[entry_key] = sym_entry = SymEntry(entry, sym_map=sym_map, **kwargs)\n        return sym_entry\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.SymEntryFactory.get","title":"get","text":"<pre><code>get(entry: int, sym_map: list[str] = None, **kwargs) -&gt; SymEntry\n</code></pre> <p>Return the specified SymEntry object singleton</p> <p>Parameters:</p> <ul> <li> <code>entry</code>             (<code>int</code>)         \u2013          <p>The entry number</p> </li> <li> <code>sym_map</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>The particular mapping of the symmetric groups</p> </li> </ul> <p>Returns:     The instance of the specified SymEntry</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def get(self, entry: int, sym_map: list[str] = None, **kwargs) -&gt; SymEntry:\n    \"\"\"Return the specified SymEntry object singleton\n\n    Args:\n        entry: The entry number\n        sym_map: The particular mapping of the symmetric groups\n    Returns:\n        The instance of the specified SymEntry\n    \"\"\"\n    return self.__call__(entry, sym_map, **kwargs)\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.construct_uc_matrix","title":"construct_uc_matrix","text":"<pre><code>construct_uc_matrix(string_vector: Iterable[str]) -&gt; ndarray\n</code></pre> <p>Calculate a matrix specifying the degrees of freedom in each dimension of the unit cell</p> <p>Parameters:</p> <ul> <li> <code>string_vector</code>             (<code>Iterable[str]</code>)         \u2013          <p>The string vector as parsed from the symmetry combination table</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Float array with shape (3, 3) the values to specify unit cell dimensions from basis vector constraints</p> </li> </ul> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def construct_uc_matrix(string_vector: Iterable[str]) -&gt; np.ndarray:\n    \"\"\"Calculate a matrix specifying the degrees of freedom in each dimension of the unit cell\n\n    Args:\n        string_vector: The string vector as parsed from the symmetry combination table\n\n    Returns:\n        Float array with shape (3, 3) the values to specify unit cell dimensions from basis vector constraints\n    \"\"\"\n    string_position = {'e': 0, 'f': 1, 'g': 2}\n    variable_matrix = np.zeros((3, 3))  # default is float\n    for col_idx, string in enumerate(string_vector):  # ex ['4*e', 'f', '0']\n        if string[-1] != '0':\n            row_idx = string_position[string[-1]]\n            variable_matrix[row_idx][col_idx] = float(string.split('*')[0]) if '*' in string else 1.\n\n            if '-' in string:\n                variable_matrix[row_idx][col_idx] *= -1\n\n    # for entry 6 - unit cell string_vector is ['4*e', '4*e', '4*e']\n    #  [[4, 4, 4], [0, 0, 0], [0, 0, 0]]\n    #  component1 string vector is ['0', 'e', '0']\n    #   [[0, 1, 0], [0, 0, 0], [0, 0, 0]]\n    #  component2 string vector is ['0', '0', '0']\n    #   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    # for entry 85 - string_vector is ['4*e', '4*f', '4*g']\n    #  [[4, 0, 0], [0, 4, 0], [0, 0, 4]]\n    #  component1 string vector is ['0', '0', '0']\n    #   [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    #  component2 string vector is ['e', 'f', 'g']\n    #   [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    return variable_matrix\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.get_rot_matrices","title":"get_rot_matrices","text":"<pre><code>get_rot_matrices(step_deg: int | float, axis: str = 'z', rot_range_deg: int | float = 360) -&gt; ndarray | None\n</code></pre> <p>Return a group of rotation matrices to rotate coordinates about a specified axis in set step increments</p> <p>Parameters:</p> <ul> <li> <code>step_deg</code>             (<code>int | float</code>)         \u2013          <p>The number of degrees for each rotation step</p> </li> <li> <code>axis</code>             (<code>str</code>, default:                 <code>'z'</code> )         \u2013          <p>The axis about which to rotate</p> </li> <li> <code>rot_range_deg</code>             (<code>int | float</code>, default:                 <code>360</code> )         \u2013          <p>The range with which rotation is possible</p> </li> </ul> <p>Returns:     The rotation matrices with shape (rotations, 3, 3)</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def get_rot_matrices(step_deg: int | float, axis: str = 'z', rot_range_deg: int | float = 360) -&gt; np.ndarray | None:\n    \"\"\"Return a group of rotation matrices to rotate coordinates about a specified axis in set step increments\n\n    Args:\n        step_deg: The number of degrees for each rotation step\n        axis: The axis about which to rotate\n        rot_range_deg: The range with which rotation is possible\n    Returns:\n        The rotation matrices with shape (rotations, 3, 3)\n    \"\"\"\n    if rot_range_deg == 0:\n        return None\n\n    # Todo use scipy.Rotation to create these!\n    rot_matrices = []\n    axis = axis.lower()\n    if axis == 'x':\n        for step in range(0, int(rot_range_deg//step_deg)):\n            rad = math.radians(step * step_deg)\n            rot_matrices.append([[1., 0., 0.], [0., math.cos(rad), -math.sin(rad)], [0., math.sin(rad), math.cos(rad)]])\n    elif axis == 'y':\n        for step in range(0, int(rot_range_deg//step_deg)):\n            rad = math.radians(step * step_deg)\n            rot_matrices.append([[math.cos(rad), 0., math.sin(rad)], [0., 1., 0.], [-math.sin(rad), 0., math.cos(rad)]])\n    elif axis == 'z':\n        for step in range(0, int(rot_range_deg//step_deg)):\n            rad = math.radians(step * step_deg)\n            rot_matrices.append([[math.cos(rad), -math.sin(rad), 0.], [math.sin(rad), math.cos(rad), 0.], [0., 0., 1.]])\n    else:\n        raise ValueError(f\"Axis '{axis}' isn't supported\")\n\n    return np.array(rot_matrices)\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.make_rotations_degenerate","title":"make_rotations_degenerate","text":"<pre><code>make_rotations_degenerate(rotations: ndarray | list[ndarray] | list[list[list[float]]] = None, degeneracies: ndarray | list[ndarray] | list[list[list[float]]] = None) -&gt; ndarray\n</code></pre> <p>From a set of degeneracy matrices and a set of rotation matrices, produce the complete combination of the specified transformations</p> <p>Parameters:</p> <ul> <li> <code>rotations</code>             (<code>ndarray | list[ndarray] | list[list[list[float]]]</code>, default:                 <code>None</code> )         \u2013          <p>A group of rotations with shape (rotations, 3, 3)</p> </li> <li> <code>degeneracies</code>             (<code>ndarray | list[ndarray] | list[list[list[float]]]</code>, default:                 <code>None</code> )         \u2013          <p>A group of degeneracies with shape (degeneracies, 3, 3)</p> </li> </ul> <p>Returns:     The matrices resulting from the multiplication of each rotation by each degeneracy.         Product has length = (rotations x degeneracies, 3, 3) where the first 3x3 array on axis 0 is the identity</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def make_rotations_degenerate(rotations: np.ndarray | list[np.ndarray] | list[list[list[float]]] = None,\n                              degeneracies: np.ndarray | list[np.ndarray] | list[list[list[float]]] = None) \\\n        -&gt; np.ndarray:\n    \"\"\"From a set of degeneracy matrices and a set of rotation matrices, produce the complete combination of the\n    specified transformations\n\n    Args:\n        rotations: A group of rotations with shape (rotations, 3, 3)\n        degeneracies: A group of degeneracies with shape (degeneracies, 3, 3)\n    Returns:\n        The matrices resulting from the multiplication of each rotation by each degeneracy.\n            Product has length = (rotations x degeneracies, 3, 3) where the first 3x3 array on axis 0 is the identity\n    \"\"\"\n    if rotations is None:\n        rotations = identity_matrix[None, :, :]  # Expand to shape (1, 3, 3)\n    elif np.all(identity_matrix == rotations[0]):\n        pass  # This is correct\n    else:\n        logger.warning(f'{make_rotations_degenerate.__name__}: The argument \"rotations\" is missing an identity '\n                       'matrix which is recommended to produce the correct matrices. Adding now')\n        rotations = [identity_matrix] + list(rotations)\n\n    if degeneracies is None:\n        degeneracies = identity_matrix[None, :, :]  # Expand to shape (1, 3, 3)\n    elif np.all(identity_matrix == degeneracies[0]):\n        pass  # This is correct\n    else:\n        logger.warning(f'{make_rotations_degenerate.__name__}: The argument \"degeneracies\" is missing an identity '\n                       'matrix which is recommended to produce the correct matrices. Adding now')\n        degeneracies = [identity_matrix] + list(degeneracies)\n\n    return np.concatenate([np.matmul(rotations, degen_mat) for degen_mat in degeneracies])\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.parse_symmetry_specification","title":"parse_symmetry_specification","text":"<pre><code>parse_symmetry_specification(specification: str) -&gt; list[str]\n</code></pre> <p>Parse the typical symmetry specification string with format RESULT:{SUBSYMMETRY1}{SUBSYMMETRY2}... to a list</p> <p>Parameters:</p> <ul> <li> <code>specification</code>             (<code>str</code>)         \u2013          <p>The specification string</p> </li> </ul> <p>Returns:     The parsed string with each member split into a list - ['RESULT', 'SUBSYMMETRY1', 'SUBSYMMETRY2', ...]</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def parse_symmetry_specification(specification: str) -&gt; list[str]:\n    \"\"\"Parse the typical symmetry specification string with format RESULT:{SUBSYMMETRY1}{SUBSYMMETRY2}... to a list\n\n    Args:\n        specification: The specification string\n    Returns:\n        The parsed string with each member split into a list - ['RESULT', 'SUBSYMMETRY1', 'SUBSYMMETRY2', ...]\n    \"\"\"\n    return [split.strip('}:') for split in specification.split('{')]\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.parse_symmetry_to_sym_entry","title":"parse_symmetry_to_sym_entry","text":"<pre><code>parse_symmetry_to_sym_entry(sym_entry_number: int = None, symmetry: str = None, sym_map: list[str] = None) -&gt; SymEntry | None\n</code></pre> <p>Take a symmetry specified in a number of ways and return the symmetry parameters in a SymEntry instance</p> <p>Parameters:</p> <ul> <li> <code>sym_entry_number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The integer corresponding to the desired SymEntry</p> </li> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The symmetry specified by a string</p> </li> <li> <code>sym_map</code>             (<code>list[str]</code>, default:                 <code>None</code> )         \u2013          <p>A symmetry map where each successive entry is the corresponding symmetry group number for the structure</p> </li> </ul> <p>Returns:     The SymEntry instance or None if parsing failed</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def parse_symmetry_to_sym_entry(sym_entry_number: int = None, symmetry: str = None, sym_map: list[str] = None) -&gt; \\\n        SymEntry | None:\n    \"\"\"Take a symmetry specified in a number of ways and return the symmetry parameters in a SymEntry instance\n\n    Args:\n        sym_entry_number: The integer corresponding to the desired SymEntry\n        symmetry: The symmetry specified by a string\n        sym_map: A symmetry map where each successive entry is the corresponding symmetry group number for the structure\n    Returns:\n        The SymEntry instance or None if parsing failed\n    \"\"\"\n    if sym_map is None:  # Find sym_map from symmetry\n        if symmetry is not None:\n            symmetry = symmetry.strip()\n            if symmetry in space_group_symmetry_operators:  # space_group_symmetry_operators in Hermann-Mauguin notation\n                # Only have the resulting symmetry, set it and then solve by lookup_sym_entry_by_symmetry_combination()\n                sym_map = [symmetry]\n            elif len(symmetry) &gt; 3:\n                if ':{' in symmetry:  # Symmetry specification of typical type result:{subsymmetry}{}...\n                    sym_map = parse_symmetry_specification(symmetry)\n                elif CRYST in symmetry.upper():  # This is crystal specification\n                    return None  # Have to set SymEntry up after parsing cryst records\n                else:  # This is some Rosetta based symmetry?\n                    sym_str1, sym_str2, sym_str3, *_ = symmetry\n                    sym_map = f'{sym_str1} C{sym_str2} C{sym_str3}'.split()\n                    logger.error(f\"Symmetry specification '{symmetry}' isn't understood, trying to solve anyway\\n\\n\")\n            elif symmetry in valid_symmetries:\n                # logger.debug(f'{parse_symmetry_to_sym_entry.__name__}: The functionality of passing symmetry as '\n                #              f\"{symmetry} hasn't been tested thoroughly yet\")\n                # Specify as [result, entity1, None as there are no other entities]\n                # If the symmetry is specified as C2 and the structure is A2B2, then this may fail\n                sym_map = [symmetry, symmetry, None]\n            elif len(symmetry) == 3 and symmetry[1].isdigit() and symmetry[2].isdigit():  # like I32, O43 format\n                sym_map = [*symmetry]\n            else:  # C35\n                raise ValueError(\n                    f\"{symmetry} isn't a supported symmetry... {highest_point_group_msg}\")\n        elif sym_entry_number is not None:\n            return symmetry_factory.get(sym_entry_number)\n        else:\n            raise utils.SymmetryInputError(\n                f\"{parse_symmetry_to_sym_entry.__name__}: Can't initialize without 'symmetry' or 'sym_map'\")\n\n    if sym_entry_number is None:\n        try:  # To lookup in the all_sym_entry_dict\n            sym_entry_number = utils.dictionary_lookup(all_sym_entry_dict, sym_map)\n            if not isinstance(sym_entry_number, int):\n                raise TypeError\n        except (KeyError, TypeError):\n            # The prescribed symmetry is a point, plane, or space group that isn't in Nanohedra symmetry combinations.\n            # Try to load a custom input\n            sym_entry_number = lookup_sym_entry_by_symmetry_combination(*sym_map)\n\n    return symmetry_factory.get(sym_entry_number, sym_map=sym_map)\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.sdf_lookup","title":"sdf_lookup","text":"<pre><code>sdf_lookup(symmetry: str = None) -&gt; AnyStr\n</code></pre> <p>From the set of possible point groups, locate the proper symmetry definition file depending on the specified symmetry. If none is specified, a C1 symmetry will be returned (this doesn't make sense but is completely viable)</p> <p>Parameters:</p> <ul> <li> <code>symmetry</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Can be a valid_point_group, or None</p> </li> </ul> <p>Returns:     The location of the symmetry definition file on disk</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def sdf_lookup(symmetry: str = None) -&gt; AnyStr:\n    \"\"\"From the set of possible point groups, locate the proper symmetry definition file depending on the specified\n    symmetry. If none is specified, a C1 symmetry will be returned (this doesn't make sense but is completely viable)\n\n    Args:\n        symmetry: Can be a valid_point_group, or None\n    Returns:\n        The location of the symmetry definition file on disk\n    \"\"\"\n    if not symmetry or symmetry.upper() == 'C1':\n        return os.path.join(putils.symmetry_def_files, 'C1.sym')\n    else:\n        symmetry = symmetry.upper()\n\n    for file, ext in map(os.path.splitext, os.listdir(putils.symmetry_def_files)):\n        if symmetry == file:\n            return os.path.join(putils.symmetry_def_files, file + ext)\n\n    raise FileNotFoundError(\n        f\"For symmetry: {symmetry}, couldn't locate correct symmetry definition file at '{putils.symmetry_def_files}'\")\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.symmetry_groups_are_allowed_in_entry","title":"symmetry_groups_are_allowed_in_entry","text":"<pre><code>symmetry_groups_are_allowed_in_entry(symmetry_operators: Iterable[str], *groups: Iterable[str], result: str = None, entry_number: int = None) -&gt; bool\n</code></pre> <p>Check if the provided symmetry operators are allowed in a SymEntry</p> <p>Parameters:</p> <ul> <li> <code>symmetry_operators</code>             (<code>Iterable[str]</code>)         \u2013          <p>The symmetry operators of interest</p> </li> <li> <code>groups</code>             (<code>Iterable[str]</code>, default:                 <code>()</code> )         \u2013          <p>The groups provided in the symmetry</p> </li> <li> <code>result</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>The resulting symmetry</p> </li> <li> <code>entry_number</code>             (<code>int</code>, default:                 <code>None</code> )         \u2013          <p>The SymEntry number of interest</p> </li> </ul> <p>Returns:     True if the symmetry operators are valid, False otherwise</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def symmetry_groups_are_allowed_in_entry(symmetry_operators: Iterable[str], *groups: Iterable[str], result: str = None,\n                                         entry_number: int = None) -&gt; bool:\n    \"\"\"Check if the provided symmetry operators are allowed in a SymEntry\n\n    Args:\n        symmetry_operators: The symmetry operators of interest\n        groups: The groups provided in the symmetry\n        result: The resulting symmetry\n        entry_number: The SymEntry number of interest\n    Returns:\n        True if the symmetry operators are valid, False otherwise\n    \"\"\"\n    if result is not None:\n        # if group1 is None and group2 is None:\n        if not groups:\n            raise ValueError(\n                f\"When using the argument 'result', must provide at least 1 group. Got {groups}\")\n    elif entry_number is not None:\n        entry = symmetry_combinations.get(entry_number)\n        if entry is None:\n            raise utils.SymmetryInputError(\n                f\"The entry number {entry_number} isn't an available {SymEntry.__name__}\")\n\n        group1, _, _, _, group2, _, _, _, _, result, *_ = entry\n        groups = (group1, group2)  # Todo modify for more than 2\n    else:\n        raise ValueError(\n            'Must provide entry_number, or the result and *groups arguments. None were provided')\n\n    # Find all sub_symmetries that are viable in the component group members\n    for group in groups:\n        group_members = sub_symmetries.get(group, [None])\n        for sym_operator in symmetry_operators:\n            if sym_operator in [result, *groups]:\n                continue\n            elif sym_operator in group_members:\n                continue\n            else:\n                return False\n\n    return True  # Assume correct unless proven incorrect\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.get_int_dof","title":"get_int_dof","text":"<pre><code>get_int_dof(*groups: Iterable[str]) -&gt; list[tuple[int, int], ...]\n</code></pre> <p>Usage int_dof1, int_dof2, *_ = get_int_dof(int_dof_group1, int_dof_group2)</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def get_int_dof(*groups: Iterable[str]) -&gt; list[tuple[int, int], ...]:\n    \"\"\"Usage\n    int_dof1, int_dof2, *_ = get_int_dof(int_dof_group1, int_dof_group2)\n    \"\"\"\n    group_int_dofs = []\n    for group_int_dof in groups:\n        int_rot = int_tx = 0\n        for int_dof in group_int_dof:\n            if int_dof.startswith('r'):\n                int_rot = 1\n            if int_dof.startswith('t'):\n                int_tx = 1\n        group_int_dofs.append((int_rot, int_tx))\n\n    return group_int_dofs\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.lookup_sym_entry_by_symmetry_combination","title":"lookup_sym_entry_by_symmetry_combination","text":"<pre><code>lookup_sym_entry_by_symmetry_combination(result: str, *symmetry_operators: str) -&gt; int\n</code></pre> <p>Given the resulting symmetry and the symmetry operators for each Entity, solve for the SymEntry</p> <p>Parameters:</p> <ul> <li> <code>result</code>             (<code>str</code>)         \u2013          <p>The global symmetry</p> </li> <li> <code>symmetry_operators</code>             (<code>str</code>, default:                 <code>()</code> )         \u2013          <p>Additional operators which specify sub-symmetric systems in the larger result</p> </li> </ul> <p>Returns:     The entry number of the SymEntry</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def lookup_sym_entry_by_symmetry_combination(result: str, *symmetry_operators: str) -&gt; int:\n    \"\"\"Given the resulting symmetry and the symmetry operators for each Entity, solve for the SymEntry\n\n    Args:\n        result: The global symmetry\n        symmetry_operators: Additional operators which specify sub-symmetric systems in the larger result\n    Returns:\n        The entry number of the SymEntry\n    \"\"\"\n\n    # def print_matching_entries(entries):\n    #     if not entries:\n    #         return\n    #     print_query_header()\n    #     for _entry in entries:\n    #         _group1, _int_dof_group1, _, _ref_frame_tx_dof_group1, _group2, _int_dof_group2, _, \\\n    #             _ref_frame_tx_dof_group2, _, _result, dimension, *_ = symmetry_combinations[_entry]\n    #         int_dof1, int_dof2, *_ = get_int_dof(_int_dof_group1, _int_dof_group2)\n    #         print(query_output_format_string.format(\n    #             _entry, result, dimension,\n    #             _group1, *int_dof1, str(_ref_frame_tx_dof_group1),\n    #             _group2, *int_dof2, str(_ref_frame_tx_dof_group2)))\n    def specification_string(result, symmetry_operators):\n        return f'{result}:{{{\"}{\".join(symmetry_operators)}}}'\n\n    def report_multiple_solutions(entries: list[int]):\n        # entries = sorted(entries)\n        # print(f'\\033[1mFound specified symmetries matching including {\", \".join(map(str, entries))}\\033[0m')\n        # print(f'\\033[1mFound specified symmetries matching\\033[0m')\n        print_matching_entries(specification_string(result, symmetry_operators), entries)\n        print(repeat_with_sym_entry)\n\n    result = str(result)\n    result_entries = []\n    matching_entries = []\n    for entry_number, entry in symmetry_combinations.items():\n        group1, _, _, _, group2, _, _, _, _, resulting_symmetry, *_ = entry\n        if resulting_symmetry == result:\n            result_entries.append(entry_number)\n\n            if symmetry_operators and \\\n                    symmetry_groups_are_allowed_in_entry(symmetry_operators, entry_number=entry_number):\n                matching_entries.append(entry_number)  # Todo include the groups?\n\n    if matching_entries:\n        if len(matching_entries) != 1:\n            # Try to solve\n            # good_matches: dict[int, list[str]] = defaultdict(list)\n            good_matches: dict[int, list[str]] = {entry_number: [None, None] for entry_number in matching_entries}\n            for entry_number in matching_entries:\n                group1, _, _, _, group2, _, _, _, _, resulting_symmetry, *_ = symmetry_combinations[entry_number]\n                match_tuple = good_matches[entry_number]\n                for sym_op in symmetry_operators:\n                    if sym_op == group1 and match_tuple[0] is None:\n                        match_tuple[0] = sym_op\n                    elif sym_op == group2 and match_tuple[1] is None:\n                        match_tuple[1] = sym_op\n            # logger.debug(f'good matches: {good_matches}')\n\n            # max_ops = 0\n            exact_matches = []\n            for entry_number, ops in good_matches.items():\n                number_ops = sum(False if op is None else True for op in ops)\n                if number_ops == len(symmetry_operators):\n                    exact_matches.append(entry_number)\n                # elif number_ops &gt; max_ops:\n                #     max_ops = number_ops\n\n            if exact_matches:\n                if len(exact_matches) == 1:\n                    matching_entries = exact_matches\n                else:  # Still equal, report bad\n                    report_multiple_solutions(exact_matches)\n                    sys.exit(1)\n                    # -------- TERMINATE --------\n            else:  # symmetry_operations are 3 or greater. Get the highest symmetry\n                all_matches = []\n                for entry_number, matching_ops in good_matches.items():\n                    if all(matching_ops):\n                        all_matches.append(entry_number)\n\n                if all_matches:\n                    if len(all_matches) == 1:\n                        matching_entries = all_matches\n                    else:  # Still equal, report bad\n                        report_multiple_solutions(exact_matches)\n                        sys.exit(1)\n                        # -------- TERMINATE --------\n                else:  # None match all, this must be 2 or more sub-symmetries\n                    # max_symmetry_number = 0\n                    # symmetry_number_to_entries = defaultdict(list)\n                    # for entry_number, matching_ops in good_matches.items():\n                    #     if len(matching_ops) == max_ops:\n                    #         total_symmetry_number = sum([valid_subunit_number[op] for op in matching_ops])\n                    #         symmetry_number_to_entries[total_symmetry_number].append(entry_number)\n                    #         if total_symmetry_number &gt; max_symmetry_number:\n                    #             max_symmetry_number = total_symmetry_number\n                    #\n                    # exact_matches = symmetry_number_to_entries[max_symmetry_number]\n                    # if len(exact_matches) == 1:\n                    #     matching_entries = exact_matches\n                    # else:  # Still equal, report bad\n                    # print('non-exact matches')\n                    print_matching_entries(specification_string(result, symmetry_operators), exact_matches)\n                    # report_multiple_solutions(exact_matches)\n                    sys.exit(1)\n                    # -------- TERMINATE --------\n    elif symmetry_operators:\n        if result in space_group_symmetry_operators:  # space_group_symmetry_operators in Hermann-Mauguin notation\n            matching_entries = [0]  # 0 = CrystSymEntry\n        else:\n            raise ValueError(\n                f\"The specified symmetries '{', '.join(symmetry_operators)}' couldn't be coerced to make the resulting \"\n                f\"symmetry='{result}'. Try to reformat your symmetry specification if this is the result of a typo to \"\n                'include only symmetries that are group members of the resulting symmetry such as '\n                f'{\", \".join(all_sym_entry_dict.get(result, {}).keys())}\\nUse the format {example_symmetry_specification} '\n                'during your specification')\n    else:  # No symmetry_operators\n        if result_entries:\n            report_multiple_solutions(result_entries)\n            sys.exit()\n        else:  # no matches\n            raise ValueError(\n                f\"The resulting symmetry {result} didn't match any possible symmetry_combinations. You are likely \"\n                'requesting a symmetry that is outside of the parameterized SymEntry entries. If this is a '\n                '\\033[1mchiral\\033[0m plane/space group, modify the function '\n                f'{lookup_sym_entry_by_symmetry_combination.__name__} to use '\n                f'non-Nanohedra compatible chiral space_group_symmetry_operators. {highest_point_group_msg}')\n\n    logger.debug(f'Found matching SymEntry.number {matching_entries[0]}')\n    return matching_entries[0]\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.print_matching_entries","title":"print_matching_entries","text":"<pre><code>print_matching_entries(match_string, matching_entries: Iterable[int])\n</code></pre> <p>Report the relevant information from passed SymEntry entry numbers</p> <p>Parameters:</p> <ul> <li> <code>match_string</code>         \u2013          <p>The string inserted into \"All entries found matching {match_string}:\"</p> </li> <li> <code>matching_entries</code>             (<code>Iterable[int]</code>)         \u2013          <p>The matching entry numbers</p> </li> </ul> <p>Returns:     None</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def print_matching_entries(match_string, matching_entries: Iterable[int]):\n    \"\"\"Report the relevant information from passed SymEntry entry numbers\n\n    Args:\n        match_string: The string inserted into \"All entries found matching {match_string}:\"\n        matching_entries: The matching entry numbers\n    Returns:\n        None\n    \"\"\"\n    if not matching_entries:\n        print(f'\\033[1mNo entries found matching {match_string}\\033[0m\\n')\n        return\n    else:\n        matching_entries = sorted(matching_entries)\n\n    print(f'\\033[1mAll entries found matching {match_string}:\\033[0m')\n    print_query_header()\n    for entry in matching_entries:\n        group1, int_dof_group1, _, ref_frame_tx_dof_group1, group2, int_dof_group2, _, \\\n            ref_frame_tx_dof_group2, result_point_group, result, dimension, cell_lengths, cell_angles, tot_dof, \\\n            ring_size = symmetry_combinations[entry]\n        int_dof1, int_dof2, *_ = get_int_dof(int_dof_group1, int_dof_group2)\n        # print(entry, result, dimension,\n        #       group1, *int_dof1, ref_frame_tx_dof_group1,\n        #       group2, *int_dof2, ref_frame_tx_dof_group2, tot_dof, ring_size)\n        if ref_frame_tx_dof_group1 is None:\n            ref_frame_tx_dof_group1 = '0,0,0'\n        if ref_frame_tx_dof_group2 is None:\n            ref_frame_tx_dof_group2 = '0,0,0'\n        print(query_output_format_string.format(\n                entry, result, dimension,\n                group1, *int_dof1, f'&lt;{ref_frame_tx_dof_group1}&gt;',\n                str(group2), *int_dof2, f'&lt;{ref_frame_tx_dof_group2}&gt;', tot_dof, ring_size,\n                True if entry in nanohedra_symmetry_combinations else False))\n</code></pre>"},{"location":"reference/utils/SymEntry/#utils.SymEntry.query","title":"query","text":"<pre><code>query(mode: query_modes_literal, *additional_mode_args, nanohedra: bool = True)\n</code></pre> <p>Perform a query of the symmetry combinations</p> <p>Parameters:</p> <ul> <li> <code>mode</code>             (<code>query_modes_literal</code>)         \u2013          <p>The type of query to perform. Viable options are: 'all-entries', 'combination', 'counterpart', 'dimension', and 'result'</p> </li> <li> <code>*additional_mode_args</code>         \u2013          <p>Additional query args required</p> </li> <li> <code>nanohedra</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>True if only Nanohedra docking symmetries should be queried</p> </li> </ul> <p>Returns:     None</p> Source code in <code>symdesign/utils/SymEntry.py</code> <pre><code>def query(mode: query_modes_literal, *additional_mode_args, nanohedra: bool = True):\n    \"\"\"Perform a query of the symmetry combinations\n\n    Args:\n        mode: The type of query to perform. Viable options are:\n            'all-entries', 'combination', 'counterpart', 'dimension', and 'result'\n        *additional_mode_args: Additional query args required\n        nanohedra: True if only Nanohedra docking symmetries should be queried\n    Returns:\n        None\n    \"\"\"\n    if nanohedra:\n        symmetry_combinations_of_interest = nanohedra_symmetry_combinations\n    else:\n        symmetry_combinations_of_interest = symmetry_combinations\n\n    matching_entries = []\n    if mode == 'all-entries':\n        match_string = mode\n        # all_entries()\n        # def all_entries():\n        matching_entries.extend(symmetry_combinations_of_interest.keys())\n    else:\n        if not additional_mode_args:\n            # raise ValueError(\n            #     f\"Can't query with mode '{mode}' without additional arguments\")\n            instructions = defaultdict(str, {\n                'combination': 'Provide multiple symmetry groups from the possible groups '\n                               f'{\", \".join(valid_symmetries)}\\n'})\n            mode_instructions = instructions[mode]\n            more_info_prompt = f\"For the query mode '{mode}', more information is needed\\n\" \\\n                               f\"{mode_instructions}What {mode} is requested?\"\n            additional_mode_args = utils.query.format_input(more_info_prompt)\n\n        if mode == 'combination':\n            combination = additional_mode_args\n            match_string = f\"{mode} {''.join(f'{{{group}}}' for group in combination)}\"\n            # query_combination(*additional_mode_args)\n            # def query_combination(*combination):\n            for entry_number, entry in symmetry_combinations_of_interest.items():\n                group1, _, _, _, group2, *_ = entry\n                if combination == (group1, group2) or combination == (group2, group1):\n                    matching_entries.append(entry_number)\n        elif mode == 'result':\n            result, *_ = additional_mode_args\n            match_string = f'{mode}={result}'\n            # query_result(result)\n            # def query_result(desired_result: str):\n            for entry_number, entry in symmetry_combinations_of_interest.items():\n                _, _, _, _, _, _, _, _, _, entry_result, *_ = entry\n                if result == entry_result:\n                    matching_entries.append(entry_number)\n        elif mode == 'group':\n            group, *_ = additional_mode_args\n            match_string = f'{mode}={group}'\n            # query_counterpart(counterpart)\n            # def query_counterpart(group: str):\n            for entry_number, entry in symmetry_combinations_of_interest.items():\n                group1, _, _, _, group2, *_ = entry\n                if group in (group1, group2):\n                    matching_entries.append(entry_number)\n\n        elif mode == 'dimension':\n            dim, *_ = additional_mode_args\n            match_string = f'{mode}={dim}'\n            # dimension(dim)\n            # def dimension(dim: int):\n            try:\n                dim = int(dim)\n            except ValueError:\n                pass\n\n            if dim in [0, 2, 3]:\n                for entry_number, entry in symmetry_combinations_of_interest.items():\n                    _, _, _, _, _, _, _, _, _, _, dimension, *_ = entry\n                    if dimension == dim:\n                        matching_entries.append(entry_number)\n            else:\n                print(f\"Dimension '{dim}' isn't supported. Valid dimensions are: 0, 2 or 3\")\n                sys.exit()\n        else:\n            raise ValueError(\n                f\"The mode '{mode}' isn't available\")\n\n    # Report those found\n    # print(matching_entries)\n    print_matching_entries(match_string, matching_entries)\n</code></pre>"},{"location":"reference/utils/cluster/","title":"cluster","text":""},{"location":"reference/utils/cluster/#utils.cluster.predict_best_pose_from_transformation_cluster","title":"predict_best_pose_from_transformation_cluster","text":"<pre><code>predict_best_pose_from_transformation_cluster(train_trajectories_file, training_clusters)\n</code></pre> <p>From full training Nanohedra, Rosetta Sequence Design analyzed trajectories, train a linear model to select the best trajectory from a group of clustered poses given only the Nanohedra Metrics</p> <p>Parameters:</p> <ul> <li> <code>train_trajectories_file</code>             (<code>str</code>)         \u2013          <p>Location of a Cluster Trajectory Analysis .csv with complete metrics for cluster</p> </li> <li> <code>training_clusters</code>             (<code>dict</code>)         \u2013          <p>Mapping of cluster representative to cluster members</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>(sklearn.linear_model)</p> </li> </ul> Source code in <code>symdesign/utils/cluster.py</code> <pre><code>def predict_best_pose_from_transformation_cluster(train_trajectories_file, training_clusters):  # UNUSED\n    \"\"\"From full training Nanohedra, Rosetta Sequence Design analyzed trajectories, train a linear model to select the\n    best trajectory from a group of clustered poses given only the Nanohedra Metrics\n\n    Args:\n        train_trajectories_file (str): Location of a Cluster Trajectory Analysis .csv with complete metrics for cluster\n        training_clusters (dict): Mapping of cluster representative to cluster members\n\n    Returns:\n        (sklearn.linear_model)\n    \"\"\"\n    possible_lin_reg = {'MultiTaskLassoCV': sklearn.linear_model.MultiTaskLassoCV,\n                        'LassoCV': sklearn.linear_model.LassoCV,\n                        'MultiTaskElasticNetCV': sklearn.linear_model.MultiTaskElasticNetCV,\n                        'ElasticNetCV': sklearn.linear_model.ElasticNetCV}\n    idx_slice = pd.IndexSlice\n    trajectory_df = pd.read_csv(train_trajectories_file, index_col=0, header=[0, 1, 2])\n    # 'dock' category is synonymous with nanohedra metrics\n    trajectory_df = trajectory_df.loc[:, idx_slice[['pose', 'no_constraint'],\n                                                   ['mean', 'dock', 'seq_design'], :]].droplevel(1, axis=1)\n    # scale the data to a standard gaussian distribution for each trajectory independently\n    # Todo ensure this mechanism of scaling is correct for each cluster individually\n    scaler = sklearn.preprocessing.StandardScaler()\n    train_traj_df = pd.concat([scaler.fit_transform(trajectory_df.loc[cluster_members, :])\n                               for cluster_members in training_clusters.values()], keys=list(training_clusters.keys()),\n                              axis=0)\n\n    # standard_scale_traj_df[train_traj_df.columns] = standard_scale.transform(train_traj_df)\n\n    # select the metrics which the linear model should be trained on\n    nano_traj = train_traj_df.loc[:, metrics.fragment_metrics]\n\n    # select the Rosetta metrics to train model on\n    # potential_training_metrics = set(train_traj_df.columns).difference(nanohedra_metrics)\n    # rosetta_select_metrics = query_user_for_metrics(potential_training_metrics, mode='design', level='pose')\n    rosetta_metrics = {'shape_complementarity': sklearn.preprocessing.StandardScaler(),  # I think a gaussian dist is preferable to MixMax\n                       # 'protocol_energy_distance_sum': 0.25,  This will select poses by evolution\n                       'int_composition_similarity': sklearn.preprocessing.StandardScaler(),  # gaussian preferable to MixMax\n                       'interface_energy': sklearn.preprocessing.StandardScaler(),  # gaussian preferable to MaxAbsScaler,\n                       # 'observed_evolution': 0.25}  # also selects by evolution\n                       }\n    # assign each metric a weight proportional to it's share of the total weight\n    rosetta_select_metrics = {item: 1 / len(rosetta_metrics) for item in rosetta_metrics}\n    # weighting scheme inherently standardizes the weights between [0, 1] by taking a linear combination of the metrics\n    targets = metrics.prioritize_design_indices(train_trajectories_file, weights=rosetta_select_metrics)  # weight=True)\n\n    # for proper MultiTask model training, must scale the selected metrics. This is performed on trajectory_df above\n    # targets2d = train_traj_df.loc[:, rosetta_select_metrics.keys()]\n    pose_traj_df = train_traj_df.loc[:, idx_slice['pose', 'int_composition_similarity']]\n    no_constraint_traj_df = \\\n        train_traj_df.loc[:, idx_slice['no_constraint',\n                                       set(rosetta_metrics.keys()).difference('int_composition_similarity')]]\n    targets2d = pd.concat([pose_traj_df, no_constraint_traj_df])\n\n    # split training and test dataset\n    trajectory_train, trajectory_test, target_train, target_test = \\\n        sklearn.model_selection.train_test_split(nano_traj, targets, random_state=42)\n    trajectory_train2d, trajectory_test2d, target_train2d, target_test2d = \\\n        sklearn.model_selection.train_test_split(nano_traj, targets2d, random_state=42)\n    # calculate model performance with cross-validation, alpha tuning\n    alphas = np.logspace(-10, 10, 21)  # Todo why log space here?\n    # then compare between models based on various model scoring parameters\n    reg_scores, mae_scores = [], []\n    for lin_reg, model in possible_lin_reg.items():\n        if lin_reg.startswith('MultiTask'):\n            trajectory_train, trajectory_test = trajectory_train2d, trajectory_test2d\n            target_train, target_test = target_train2d, target_test2d\n        # else:\n        #     target = target_train\n        test_reg = model(alphas=alphas).fit(trajectory_train, target_train)\n        reg_scores.append(test_reg.score(trajectory_train, target_train))\n        target_test_prediction = test_reg.predict(trajectory_test, target_test)\n        mae_scores.append(sklearn.metrics.median_absolute_error(target_test, target_test_prediction))\n</code></pre>"},{"location":"reference/utils/cluster/#utils.cluster.chose_top_pose_from_model","title":"chose_top_pose_from_model","text":"<pre><code>chose_top_pose_from_model(test_trajectories_file, clustered_poses, model)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>test_trajectories_file</code>             (<code>str</code>)         \u2013          <p>Location of a Nanohedra Trajectory Analysis .csv with Nanohedra metrics</p> </li> <li> <code>clustered_poses</code>             (<code>dict</code>)         \u2013          <p>A set of clustered poses that share similar transformational parameters</p> </li> </ul> <p>Returns:</p> Source code in <code>symdesign/utils/cluster.py</code> <pre><code>def chose_top_pose_from_model(test_trajectories_file, clustered_poses, model):  # UNUSED\n    \"\"\"\n    Args:\n        test_trajectories_file (str): Location of a Nanohedra Trajectory Analysis .csv with Nanohedra metrics\n        clustered_poses (dict): A set of clustered poses that share similar transformational parameters\n    Returns:\n\n    \"\"\"\n    test_docking_df = pd.read_csv(test_trajectories_file, index_col=0, header=[0, 1, 2])\n\n    for cluster_representative, designs in clustered_poses.items():\n        trajectory_df = test_docking_df.loc[designs, metrics.fragment_metrics]\n        trajectory_df['model_predict'] = model.predict(trajectory_df)\n        trajectory_df.sort_values('model_predict')\n</code></pre>"},{"location":"reference/utils/guide/","title":"guide","text":""},{"location":"reference/utils/guide/#utils.guide.print_guide","title":"print_guide","text":"<pre><code>print_guide()\n</code></pre> <p>Print the program readme file</p> Source code in <code>symdesign/utils/guide.py</code> <pre><code>def print_guide():\n    \"\"\"Print the program readme file\"\"\"\n    with open(putils.readme_file, 'r') as f:\n        print(f.read(), end='')\n</code></pre>"},{"location":"reference/utils/nanohedra_parsing/","title":"nanohedra_parsing","text":""},{"location":"reference/utils/nanohedra_parsing/#utils.nanohedra_parsing.retrieve_pose_transformation_from_nanohedra_docking","title":"retrieve_pose_transformation_from_nanohedra_docking","text":"<pre><code>retrieve_pose_transformation_from_nanohedra_docking(pose_file: AnyStr) -&gt; list[dict]\n</code></pre> <p>Gather pose transformation information for the Pose from Nanohedra output</p> <p>Parameters:</p> <ul> <li> <code>pose_file</code>             (<code>AnyStr</code>)         \u2013          <p>The file containing pose information from Nanohedra output</p> </li> </ul> <p>Returns:     The pose transformation arrays as found in the pose_file</p> Source code in <code>symdesign/utils/nanohedra_parsing.py</code> <pre><code>def retrieve_pose_transformation_from_nanohedra_docking(pose_file: AnyStr) -&gt; list[dict]:\n    \"\"\"Gather pose transformation information for the Pose from Nanohedra output\n\n    Args:\n        pose_file: The file containing pose information from Nanohedra output\n    Returns:\n        The pose transformation arrays as found in the pose_file\n    \"\"\"\n    with open(pose_file, 'r') as f:\n        pose_transformation = {}\n        for line in f.readlines():\n            # all parsing lacks PDB number suffix such as PDB1 or PDB2 for hard coding in dict key\n            if line[:20] == 'ROT/DEGEN MATRIX PDB':\n                # data = eval(line[22:].strip())\n                data = [[float(item) for item in group.split(', ')]\n                        for group in line[22:].strip().strip('[]').split('], [')]\n                pose_transformation[int(line[20:21])] = {'rotation': np.array(data)}\n            elif line[:15] == 'INTERNAL Tx PDB':\n                try:  # This may have values of None\n                    data = np.array([float(item) for item in line[17:].strip().strip('[]').split(', ')])\n                except ValueError:  # we received a string which is not a float\n                    data = utils.symmetry.origin\n                pose_transformation[int(line[15:16])]['translation'] = data\n            elif line[:18] == 'SETTING MATRIX PDB':\n                # data = eval(line[20:].strip())\n                data = [[float(item) for item in group.split(', ')]\n                        for group in line[20:].strip().strip('[]').split('], [')]\n                pose_transformation[int(line[18:19])]['rotation2'] = np.array(data)\n            elif line[:22] == 'REFERENCE FRAME Tx PDB':\n                try:  # This may have values of None\n                    data = np.array([float(item) for item in line[24:].strip().strip('[]').split(', ')])\n                except ValueError:  # we received a string which is not a float\n                    data = utils.symmetry.origin\n                pose_transformation[int(line[22:23])]['translation2'] = data\n\n    return [pose_transformation[idx] for idx, _ in enumerate(pose_transformation, 1)]\n</code></pre>"},{"location":"reference/utils/nanohedra_parsing/#utils.nanohedra_parsing.get_components_from_nanohedra_docking","title":"get_components_from_nanohedra_docking","text":"<pre><code>get_components_from_nanohedra_docking(pose_file: AnyStr) -&gt; list[str]\n</code></pre> <p>Gather information on the docking componenet identifiers for the docked Pose from a Nanohedra output</p> <p>Parameters:</p> <ul> <li> <code>pose_file</code>             (<code>AnyStr</code>)         \u2013          <p>The file containing pose information from Nanohedra output</p> </li> </ul> <p>Returns:     The names of the models used during Nanohedra</p> Source code in <code>symdesign/utils/nanohedra_parsing.py</code> <pre><code>def get_components_from_nanohedra_docking(pose_file: AnyStr) -&gt; list[str]:\n    \"\"\"Gather information on the docking componenet identifiers for the docked Pose from a Nanohedra output\n\n    Args:\n        pose_file: The file containing pose information from Nanohedra output\n    Returns:\n        The names of the models used during Nanohedra\n    \"\"\"\n    entity_names = []\n    with open(pose_file, 'r') as f:\n        for line in f.readlines():\n            if line[:15] == 'DOCKED POSE ID:':\n                pose_identifier = line[15:].strip().replace('_DEGEN_', '-DEGEN_').replace('_ROT_', '-ROT_').replace('_TX_', '-tx_')\n            elif line[:31] == 'Canonical Orientation PDB1 Path':\n                canonical_pdb1 = line[31:].strip()\n            elif line[:31] == 'Canonical Orientation PDB2 Path':\n                canonical_pdb2 = line[31:].strip()\n\n        if pose_identifier:\n            entity_names = pose_identifier.split('-DEGEN_')[0].split('-')\n\n        if len(entity_names) != number_of_nanohedra_components:  # probably old format without use of '-'\n            entity_names = list(map(os.path.basename, [os.path.splitext(canonical_pdb1)[0],\n                                                       os.path.splitext(canonical_pdb2)[0]]))\n    return entity_names\n</code></pre>"},{"location":"reference/utils/nanohedra_parsing/#utils.nanohedra_parsing.get_sym_entry_from_nanohedra_directory","title":"get_sym_entry_from_nanohedra_directory","text":"<pre><code>get_sym_entry_from_nanohedra_directory(nanohedra_dir: AnyStr) -&gt; SymEntry\n</code></pre> <p>Handles extraction of Symmetry info from Nanohedra outputs.</p> <p>Parameters:</p> <ul> <li> <code>nanohedra_dir</code>             (<code>AnyStr</code>)         \u2013          <p>The path to a Nanohedra master output directory</p> </li> </ul> <p>Raises:     FileNotFoundError: If no nanohedra master log file is found     SymmetryError: If no symmetry is found Returns:     The SymEntry specified by the Nanohedra docking run</p> Source code in <code>symdesign/utils/nanohedra_parsing.py</code> <pre><code>def get_sym_entry_from_nanohedra_directory(nanohedra_dir: AnyStr) -&gt; utils.SymEntry.SymEntry:\n    \"\"\"Handles extraction of Symmetry info from Nanohedra outputs.\n\n    Args:\n        nanohedra_dir: The path to a Nanohedra master output directory\n    Raises:\n        FileNotFoundError: If no nanohedra master log file is found\n        SymmetryError: If no symmetry is found\n    Returns:\n        The SymEntry specified by the Nanohedra docking run\n    \"\"\"\n    log_path = os.path.join(nanohedra_dir, putils.master_log)\n    try:\n        with open(log_path, 'r') as f:\n            for line in f.readlines():\n                if 'Nanohedra Entry Number: ' in line:  # \"Symmetry Entry Number: \" or\n                    return utils.SymEntry.symmetry_factory.get(int(line.split(':')[-1]))  # sym_map inclusion?\n    except FileNotFoundError:\n        raise FileNotFoundError(\n            f'Nanohedra master directory is malformed. Missing required docking file {log_path}')\n    raise utils.InputError(\n        f\"Nanohedra master docking file {log_path} is malformed. Missing required info 'Nanohedra Entry Number:'\")\n</code></pre>"},{"location":"reference/utils/path/","title":"path","text":""},{"location":"reference/utils/path/#utils.path.pdb_db","title":"pdb_db  <code>module-attribute</code>","text":"<pre><code>pdb_db = join(database, 'pdbDB')\n</code></pre> <p>Local copy of the PDB database</p>"},{"location":"reference/utils/path/#utils.path.pisa_db","title":"pisa_db  <code>module-attribute</code>","text":"<pre><code>pisa_db = join(database, 'pisaDB')\n</code></pre> <p>Local copy of the PISA database</p>"},{"location":"reference/utils/path/#utils.path.get_uniclust_db","title":"get_uniclust_db","text":"<pre><code>get_uniclust_db() -&gt; str\n</code></pre> <p>Get the newest UniClust file by sorting alphanumerically</p> Source code in <code>symdesign/utils/path.py</code> <pre><code>def get_uniclust_db() -&gt; str:\n    \"\"\"Get the newest UniClust file by sorting alphanumerically\"\"\"\n    try:  # To get database files and remove any extra characters from filename\n        md5sums = '.md5sums'\n        return sorted(glob(os.path.join(hhsuite_db_dir, f'*{md5sums}')), reverse=True)[0].replace(md5sums, '')\n        # return sorted(os.listdir(hhsuite_db_dir), reverse=True)[0].replace(uniclust_hhsuite_file_identifier, '')\n    except IndexError:\n        return ''\n</code></pre>"},{"location":"reference/utils/path/#utils.path.make_path","title":"make_path","text":"<pre><code>make_path(path: AnyStr, condition: bool = True)\n</code></pre> <p>Make all required directories in specified path if it doesn't exist, and optional condition is True</p> <p>Parameters:</p> <ul> <li> <code>path</code>             (<code>AnyStr</code>)         \u2013          <p>The path to create</p> </li> <li> <code>condition</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>A condition to check before the path production is executed</p> </li> </ul> Source code in <code>symdesign/utils/path.py</code> <pre><code>def make_path(path: AnyStr, condition: bool = True):\n    \"\"\"Make all required directories in specified path if it doesn't exist, and optional condition is True\n\n    Args:\n        path: The path to create\n        condition: A condition to check before the path production is executed\n    \"\"\"\n    if condition:\n        os.makedirs(path, exist_ok=True)\n</code></pre>"},{"location":"reference/utils/path/#utils.path.ex_path","title":"ex_path","text":"<pre><code>ex_path(*directories: Sequence[str]) -&gt; AnyStr\n</code></pre> <p>Create an example path prepended with /path/to/provided/directories</p> <p>Parameters:</p> <ul> <li> <code>directories</code>             (<code>Sequence[str]</code>, default:                 <code>()</code> )         \u2013          <p>Example: ('provided', 'directories')</p> </li> </ul> Source code in <code>symdesign/utils/path.py</code> <pre><code>def ex_path(*directories: Sequence[str]) -&gt; AnyStr:\n    \"\"\"Create an example path prepended with /path/to/provided/directories\n\n    Args:\n        directories: Example: ('provided', 'directories')\n    \"\"\"\n    return os.path.join('path', 'to', *directories)\n</code></pre>"},{"location":"reference/utils/query/","title":"query","text":""},{"location":"reference/utils/query/#utils.query.validate_input","title":"validate_input","text":"<pre><code>validate_input(prompt: str, response: Iterable[str]) -&gt; str\n</code></pre> <p>Following a provided prompt, validate that the user input is a valid response then return the response outcome</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>             (<code>str</code>)         \u2013          <p>The desired prompt</p> </li> <li> <code>response</code>             (<code>Iterable[str]</code>)         \u2013          <p>The response values to accept as keys and the resulting data to return as values</p> </li> </ul> <p>Returns:     The data matching the chosen response key</p> Source code in <code>symdesign/utils/query.py</code> <pre><code>def validate_input(prompt: str, response: Iterable[str]) -&gt; str:\n    \"\"\"Following a provided prompt, validate that the user input is a valid response then return the response outcome\n\n    Args:\n        prompt: The desired prompt\n        response: The response values to accept as keys and the resulting data to return as values\n    Returns:\n        The data matching the chosen response key\n    \"\"\"\n    _input = input(f'{prompt}\\nChoose from [{\", \".join(response)}]{input_string}')\n    while _input not in response:\n        _input = input(f\"'Invalid input... '{_input}' isn't a valid response. Try again{input_string}'\")\n\n    return _input\n</code></pre>"},{"location":"reference/utils/query/#utils.query.validate_type","title":"validate_type","text":"<pre><code>validate_type(value: Any, dtype: Callable = str) -&gt; bool\n</code></pre> <p>Provide a user prompt to ensure the user input is what is desired</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>A True value indicates the user wants to proceed. False indicates we should get input again.</p> </li> </ul> Source code in <code>symdesign/utils/query.py</code> <pre><code>def validate_type(value: Any, dtype: Callable = str) -&gt; bool:\n    \"\"\"Provide a user prompt to ensure the user input is what is desired\n\n    Returns:\n        A True value indicates the user wants to proceed. False indicates we should get input again.\n    \"\"\"\n    try:\n        dtype(value)\n        return True\n    except ValueError:\n        print(f\"The value '{value}' can't be converted to the type {type(dtype).__name__}. Try again...\")\n        return False\n</code></pre>"},{"location":"reference/utils/query/#utils.query.boolean_choice","title":"boolean_choice","text":"<pre><code>boolean_choice() -&gt; bool\n</code></pre> <p>Retrieve user input from a boolean confirmation prompt \"Please specify [y/n] Input: \" to control program flow</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>A True value indicates the user wants to proceed. False indicates they do not</p> </li> </ul> Source code in <code>symdesign/utils/query.py</code> <pre><code>def boolean_choice() -&gt; bool:\n    \"\"\"Retrieve user input from a boolean confirmation prompt \"Please specify [y/n] Input: \" to control program flow\n\n    Returns:\n        A True value indicates the user wants to proceed. False indicates they do not\n    \"\"\"\n    confirm = input(boolean_input_string).lower()\n    while confirm not in bool_d:\n        confirm = input(f\"'{confirm}' isn't a valid choice. Chose either [y/n]{input_string}\").lower()\n\n    return bool_d[confirm]\n</code></pre>"},{"location":"reference/utils/query/#utils.query.verify_choice","title":"verify_choice","text":"<pre><code>verify_choice() -&gt; bool\n</code></pre> <p>Provide a verification prompt (If this is correct, indicate y, if not n, and you can re-input) to ensure a prior input was desired</p> <p>Returns:</p> <ul> <li> <code>bool</code>         \u2013          <p>A True value indicates the user wants to proceed. False indicates we should get input again.</p> </li> </ul> Source code in <code>symdesign/utils/query.py</code> <pre><code>def verify_choice() -&gt; bool:\n    \"\"\"Provide a verification prompt (If this is correct, indicate y, if not n, and you can re-input) to ensure a prior\n    input was desired\n\n    Returns:\n        A True value indicates the user wants to proceed. False indicates we should get input again.\n    \"\"\"\n    confirm = input(confirmation_string).lower()\n    while confirm not in bool_d:\n        confirm = input(f\"'{confirm}' isn't a valid choice. Chose either [y/n]{input_string}\").lower()\n\n    return bool_d[confirm]\n</code></pre>"},{"location":"reference/utils/query/#utils.query.connection_exception_handler","title":"connection_exception_handler","text":"<pre><code>connection_exception_handler(url: str, max_attempts: int = 2) -&gt; Response | None\n</code></pre> <p>Wrap requests GET commands in an exception handler which attempts to aqcuire the data multiple times if the connection is refused due to a high volume of requests</p> <p>Parameters:</p> <ul> <li> <code>url</code>             (<code>str</code>)         \u2013          <p>The url to GET information from</p> </li> <li> <code>max_attempts</code>             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>The number of queries that should be attempts without successful return</p> </li> </ul> <p>Returns:     The json formatted response to the url GET or None</p> Source code in <code>symdesign/utils/query.py</code> <pre><code>def connection_exception_handler(url: str, max_attempts: int = 2) -&gt; requests.Response | None:\n    \"\"\"Wrap requests GET commands in an exception handler which attempts to aqcuire the data multiple times if the\n    connection is refused due to a high volume of requests\n\n    Args:\n        url: The url to GET information from\n        max_attempts: The number of queries that should be attempts without successful return\n    Returns:\n        The json formatted response to the url GET or None\n    \"\"\"\n    global MAX_RESOURCE_ATTEMPTS\n    iteration = 1\n    while True:\n        try:  # Todo change data retrieval to POST\n            query_response = requests.get(url)\n        except requests.exceptions.ConnectionError:\n            logger.debug('Requests ran into a connection error. Sleeping, then retrying')\n            time.sleep(1)\n            iteration += 1\n        else:\n            try:\n                if query_response.status_code == 200:\n                    return query_response\n                elif query_response.status_code == 204:\n                    logger.warning('No response was returned. Your query likely found no matches!')\n                elif query_response.status_code == 429:\n                    logger.debug('Too many requests, pausing momentarily')\n                    time.sleep(2)\n                else:\n                    logger.debug(f'Your query returned an unrecognized status code ({query_response.status_code})')\n                    time.sleep(1)\n                    iteration += 1\n            except ValueError as error:  # The json response was bad...\n                logger.error(f\"A json response was missing or corrupted from '{url}' Error: {error}\")\n                break\n\n        if MAX_RESOURCE_ATTEMPTS &gt; 2:\n            # Quit this loop. We probably have no internet connection\n            break\n        elif iteration == max_attempts:\n            time.sleep(5)  # Try one long sleep then go once more\n        elif iteration &gt; max_attempts:\n            MAX_RESOURCE_ATTEMPTS += 1\n            logger.error('The maximum number of resource fetch attempts was made with no resolution. '\n                         f\"Offending request '{url}'\")\n            break\n\n    return\n</code></pre>"},{"location":"reference/utils/query/#utils.query.format_input","title":"format_input","text":"<pre><code>format_input(prompt: str) -&gt; str\n</code></pre> <p>Format the builtin input() using program specific formatting</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>             (<code>str</code>)         \u2013          <p>The desired prompt</p> </li> </ul> <p>Returns:     The input</p> Source code in <code>symdesign/utils/query.py</code> <pre><code>def format_input(prompt: str) -&gt; str:\n    \"\"\"Format the builtin input() using program specific formatting\n\n    Args:\n        prompt: The desired prompt\n    Returns:\n        The input\n    \"\"\"\n    return input(f'{prompt}{input_string}')\n</code></pre>"},{"location":"reference/utils/query/#utils.query.confirm_input_action","title":"confirm_input_action","text":"<pre><code>confirm_input_action(input_message: str) -&gt; bool\n</code></pre> <p>Given a prompt, query the user to verify their input is desired</p> <p>Parameters:</p> <ul> <li> <code>input_message</code>             (<code>str</code>)         \u2013          <p>A message specifying the program will take a course of action upon user consent</p> </li> </ul> <p>Returns:     True if the user wants to proceed with the described input_message otherwise False</p> Source code in <code>symdesign/utils/query.py</code> <pre><code>def confirm_input_action(input_message: str) -&gt; bool:\n    \"\"\"Given a prompt, query the user to verify their input is desired\n\n    Args:\n        input_message: A message specifying the program will take a course of action upon user consent\n    Returns:\n        True if the user wants to proceed with the described input_message otherwise False\n    \"\"\"\n    confirm = input(f'{input_message}\\n{confirmation_string}').lower()\n    while confirm not in bool_d:\n        confirm = input(f\"{confirm} isn't a valid choice, please try again{input_string}\")\n\n    return bool_d[confirm]\n</code></pre>"},{"location":"reference/utils/query/#utils.query.validate_input_return_response_value","title":"validate_input_return_response_value","text":"<pre><code>validate_input_return_response_value(prompt: str, response: dict[str, Any]) -&gt; Any\n</code></pre> <p>Following a provided prompt, validate that the user input is a valid response then return the response outcome</p> <p>Parameters:</p> <ul> <li> <code>prompt</code>             (<code>str</code>)         \u2013          <p>The desired prompt</p> </li> <li> <code>response</code>             (<code>dict[str, Any]</code>)         \u2013          <p>The response values to accept as keys and the resulting data to return as values</p> </li> </ul> <p>Returns:     The data matching the chosen response key</p> Source code in <code>symdesign/utils/query.py</code> <pre><code>def validate_input_return_response_value(prompt: str, response: dict[str, Any]) -&gt; Any:\n    \"\"\"Following a provided prompt, validate that the user input is a valid response then return the response outcome\n\n    Args:\n        prompt: The desired prompt\n        response: The response values to accept as keys and the resulting data to return as values\n    Returns:\n        The data matching the chosen response key\n    \"\"\"\n    choice = input(f'{prompt}\\nChoose from [{\", \".join(response)}]{input_string}')\n    while choice not in response:\n        choice = input(f\"{choice} isn't a valid choice, please try again{input_string}\")\n\n    return response[choice]\n</code></pre>"},{"location":"reference/utils/rosetta/","title":"rosetta","text":""},{"location":"reference/utils/symmetry/","title":"symmetry","text":""},{"location":"reference/utils/symmetry/#utils.symmetry.point_group_symmetry_operatorsT","title":"point_group_symmetry_operatorsT  <code>module-attribute</code>","text":"<pre><code>point_group_symmetry_operatorsT: dict[str, ndarray] = unpickle(point_group_symmetry_operatorT_location)\n</code></pre> <p>A mapping of the point group name (in Hermann-Mauguin notation) to the corresponding symmetry operators Formatted as {'symmetry': rotations[N, 3, 3], ...} where the rotations are pre-transposed to match requirements of  np.matmul(coords, rotation)</p>"},{"location":"reference/utils/symmetry/#utils.symmetry.space_group_symmetry_operatorsT","title":"space_group_symmetry_operatorsT  <code>module-attribute</code>","text":"<pre><code>space_group_symmetry_operatorsT: dict[str, ndarray] = unpickle(space_group_symmetry_operatorT_location)\n</code></pre> <p>A mapping of the space group name (in Hermann-Mauguin notation) to the corresponding symmetry operators Formatted as {'symmetry': (rotations[N, 3, 3], translations[N, 1, 3]), ...} where the rotations are pre-transposed to  match requirements of np.matmul(coords, rotation)</p>"},{"location":"reference/utils/symmetry/#utils.symmetry.generate_cryst1_record","title":"generate_cryst1_record","text":"<pre><code>generate_cryst1_record(dimensions: tuple[float, float, float, float, float, float], space_group: str) -&gt; str\n</code></pre> <p>Format the CRYST1 record from specified unit cell dimensions and space group for a .pdb file</p> <p>Parameters:</p> <ul> <li> <code>dimensions</code>             (<code>tuple[float, float, float, float, float, float]</code>)         \u2013          <p>Containing a, b, c (Angstroms) alpha, beta, gamma (degrees)</p> </li> <li> <code>space_group</code>             (<code>str</code>)         \u2013          <p>The space group of interest in compact format</p> </li> </ul> <p>Returns:     The CRYST1 record</p> Source code in <code>symdesign/utils/symmetry.py</code> <pre><code>def generate_cryst1_record(dimensions: tuple[float, float, float, float, float, float], space_group: str) -&gt; str:\n    \"\"\"Format the CRYST1 record from specified unit cell dimensions and space group for a .pdb file\n\n    Args:\n        dimensions: Containing a, b, c (Angstroms) alpha, beta, gamma (degrees)\n        space_group: The space group of interest in compact format\n    Returns:\n        The CRYST1 record\n    \"\"\"\n    if space_group in space_group_cryst1_fmt_dict or space_group in space_group_hm_to_simple_dict:\n        formatted_space_group = space_group_cryst1_fmt_dict.get(space_group, space_group)\n    elif space_group in layer_group_cryst1_fmt_dict or space_group in layer_group_hm_to_simple_dict:\n        formatted_space_group = layer_group_cryst1_fmt_dict.get(space_group, space_group)\n        dimensions = list(dimensions)\n        dimensions[2] = 1.\n        dimensions[4] = dimensions[5] = 90.\n    else:\n        raise ValueError(\n            f\"The space group '{space_group}' isn't supported\")\n\n    try:\n        return 'CRYST1{dim[0]:9.3f}{dim[1]:9.3f}{dim[2]:9.3f}{dim[3]:7.2f}{dim[4]:7.2f}{dim[5]:7.2f} {sg:&lt;11s}{z:4d}\\n'\\\n            .format(dim=dimensions, sg=formatted_space_group, z=space_group_number_operations[space_group])\n    except TypeError:  # NoneType was passed\n        raise ValueError(\n            f\"Can't {generate_cryst1_record.__name__} without passing 'dimensions'\")\n</code></pre>"},{"location":"reference/utils/symmetry/#utils.symmetry.get_ptgrp_sym_op","title":"get_ptgrp_sym_op","text":"<pre><code>get_ptgrp_sym_op(sym_type: str, expand_matrix_dir: Union[str, bytes] = os.path.join(putils.sym_op_location, 'POINT_GROUP_SYMM_OPERATORS')) -&gt; List[List]\n</code></pre> <p>Get the symmetry operations for a specified point group oriented in the canonical orientation</p> <p>Parameters:</p> <ul> <li> <code>sym_type</code>             (<code>str</code>)         \u2013          <p>The name of the symmetry</p> </li> <li> <code>expand_matrix_dir</code>             (<code>Union[str, bytes]</code>, default:                 <code>join(sym_op_location, 'POINT_GROUP_SYMM_OPERATORS')</code> )         \u2013          <p>The disk location of a directory with symmetry name labeled expand matrices</p> </li> </ul> <p>Returns:     The rotation matrices to perform point group expansion</p> Source code in <code>symdesign/utils/symmetry.py</code> <pre><code>def get_ptgrp_sym_op(sym_type: str,\n                     expand_matrix_dir: Union[str, bytes] = os.path.join(putils.sym_op_location,\n                                                                         'POINT_GROUP_SYMM_OPERATORS')) -&gt; List[List]:\n    \"\"\"Get the symmetry operations for a specified point group oriented in the canonical orientation\n\n    Args:\n        sym_type: The name of the symmetry\n        expand_matrix_dir: The disk location of a directory with symmetry name labeled expand matrices\n    Returns:\n        The rotation matrices to perform point group expansion\n    \"\"\"\n    expand_matrix_filepath = os.path.join(expand_matrix_dir, f'{sym_type}.txt')\n    with open(expand_matrix_filepath, 'r') as f:\n        line_count = 0\n        rotation_matrices = []\n        mat = []\n        for line in f.readlines():\n            line = line.split()\n            if len(line) == 3:\n                mat.append([float(s) for s in line])\n                line_count += 1\n                if line_count % 3 == 0:\n                    rotation_matrices.append(mat)\n                    mat = []\n\n        return rotation_matrices\n</code></pre>"},{"location":"reference/utils/types/","title":"types","text":""},{"location":"reference/visualization/","title":"visualization","text":""},{"location":"reference/visualization/SymmetryMates/","title":"SymmetryMates","text":""},{"location":"reference/visualization/SymmetryMates/#visualization.SymmetryMates.save_group","title":"save_group","text":"<pre><code>save_group(group: str = 'all', one_file: bool = True, out_dir: str = os.getcwd()) -&gt; None\n</code></pre> <p>Save all files inside a group to either one file or a list of files. Assumes the groups were generated using 'expand'</p> <p>Parameters:</p> <ul> <li> <code>group</code>             (<code>str</code>, default:                 <code>'all'</code> )         \u2013          <p>name of the group to save</p> </li> <li> <code>one_file</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Saves each protein in the group to one file, if False, then saves all members individually</p> </li> <li> <code>out_dir</code>             (<code>str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The directory location to save the files to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>symdesign/visualization/SymmetryMates.py</code> <pre><code>def save_group(group: str = 'all', one_file: bool = True, out_dir: str = os.getcwd()) -&gt; None:\n    \"\"\"Save all files inside a group to either one file or a list of files.\n    Assumes the groups were generated using 'expand'\n\n    Args:\n        group: name of the group to save\n        one_file: Saves each protein in the group to one file, if False, then saves all members individually\n        out_dir: The directory location to save the files to\n\n    Returns:\n        None\n    \"\"\"\n    if group == 'all':\n        # remove appended symmetry mate number '_#' from the group instances and take the set of structures\n        expanded_group = set('_'.join(name.split('_')[:-1]) for name in cmd.get_names('group_objects', enabled_only=1))\n        groups = [f'{group}_expanded' for group in expanded_group]\n    elif cmd.get_type(group) == 'object:group':\n        groups = [group]\n    else:\n        print('Error: please provide a group name to save.')\n        return None\n\n    if one_file:\n        for group in groups:\n            cmd.save(os.path.join(out_dir, f'{group}.pdb'), group)\n    else:\n        for group in groups:\n            stored.models = set()\n            cmd.iterate(group, 'stored.models.add(model)')\n            for model in stored.models:\n                cmd.save(os.path.join(out_dir, f'{model}.pdb'), model)\n</code></pre>"},{"location":"reference/visualization/VisualizeUtils/","title":"VisualizeUtils","text":""},{"location":"reference/visualization/VisualizeUtils/#visualization.VisualizeUtils.unpickle","title":"unpickle","text":"<pre><code>unpickle(file_name: AnyStr)\n</code></pre> <p>Unpickle (deserialize) and return a python object located at filename</p> Source code in <code>symdesign/visualization/VisualizeUtils.py</code> <pre><code>def unpickle(file_name: AnyStr):  # , protocol=pickle.HIGHEST_PROTOCOL):\n    \"\"\"Unpickle (deserialize) and return a python object located at filename\"\"\"\n    if '.pkl' not in file_name and '.pickle' not in file_name:\n        file_name = f'{file_name}.pkl'\n    try:\n        with open(file_name, 'rb') as serial_f:\n            new_object = load(serial_f)\n    except EOFError as ex:\n        raise ValueError(f\"The object serialized at location {file_name} couldn't be accessed. No data present!\")\n\n    return new_object\n</code></pre>"},{"location":"reference/visualization/VisualizeUtils/#visualization.VisualizeUtils.get_all_file_paths","title":"get_all_file_paths","text":"<pre><code>get_all_file_paths(dir: str, suffix: str = '', extension: str = None, sort: bool = True) -&gt; list[str]\n</code></pre> <p>Return all files in a directory with specified extensions and suffixes</p> <p>Parameters:</p> <ul> <li> <code>dir</code>             (<code>str</code>)         \u2013          <p>The directory to retrieve filepaths from</p> </li> <li> <code>suffix</code>             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>If a suffix should be appended to the files in the dir</p> </li> <li> <code>extension</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>If a particular extension should be used to search for files</p> </li> <li> <code>sort</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to return the files in alphanumerically sorted order</p> </li> </ul> Source code in <code>symdesign/visualization/VisualizeUtils.py</code> <pre><code>def get_all_file_paths(dir: str, suffix: str = '', extension: str = None, sort: bool = True) -&gt; list[str]:\n    \"\"\"Return all files in a directory with specified extensions and suffixes\n\n    Args:\n        dir: The directory to retrieve filepaths from\n        suffix: If a suffix should be appended to the files in the dir\n        extension: If a particular extension should be used to search for files\n        sort: Whether to return the files in alphanumerically sorted order\n    \"\"\"\n    if not extension:\n        extension = '.pdb'\n    if sort:\n        return [os.path.join(os.path.abspath(dir), file)\n                for file in sorted(glob(os.path.join(os.path.abspath(dir), '*%s*%s' % (suffix, extension))))]\n    else:\n        return [os.path.join(os.path.abspath(dir), file)\n                for file in glob(os.path.join(os.path.abspath(dir), '*%s*%s' % (suffix, extension)))]\n</code></pre>"},{"location":"reference/visualization/VisualizeUtils/#visualization.VisualizeUtils.save_group","title":"save_group","text":"<pre><code>save_group(group: str = 'all', one_file: bool = True, out_dir: str = os.getcwd()) -&gt; None\n</code></pre> <p>Save all files inside a group to either one file or a list of files. Assumes the groups were generated using 'expand'</p> <p>Parameters:</p> <ul> <li> <code>group</code>             (<code>str</code>, default:                 <code>'all'</code> )         \u2013          <p>name of the group to save</p> </li> <li> <code>one_file</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Saves each protein in the group to one file, if False, then saves all members individually</p> </li> <li> <code>out_dir</code>             (<code>str</code>, default:                 <code>getcwd()</code> )         \u2013          <p>The directory location to save the files to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>symdesign/visualization/VisualizeUtils.py</code> <pre><code>def save_group(group: str = 'all', one_file: bool = True, out_dir: str = os.getcwd()) -&gt; None:\n    \"\"\"Save all files inside a group to either one file or a list of files.\n    Assumes the groups were generated using 'expand'\n\n    Args:\n        group: name of the group to save\n        one_file: Saves each protein in the group to one file, if False, then saves all members individually\n        out_dir: The directory location to save the files to\n\n    Returns:\n        None\n    \"\"\"\n    if group == 'all':\n        # remove appended symmetry mate number '_#' from the group instances and take the set of structures\n        expanded_group = set('_'.join(name.split('_')[:-1]) for name in cmd.get_names('group_objects', enabled_only=1))\n        groups = ['%s_expanded' % group for group in expanded_group]\n    elif cmd.get_type(group) == 'object:group':\n        groups = [group]\n    else:\n        print('Error: please provide a group name to save.')\n        return None\n\n    if one_file:\n        for group in groups:\n            cmd.save(os.path.join(out_dir, '%s.pdb' % group), group)\n    else:\n        for group in groups:\n            stored.models = set()\n            cmd.iterate(group, 'stored.models.add(model)')\n            for model in stored.models:\n                cmd.save(os.path.join(out_dir, '%s.pdb' % model), model)\n</code></pre>"},{"location":"reference/visualization/shapes/","title":"shapes","text":""}]}