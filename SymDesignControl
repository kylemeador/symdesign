"""
Module for distribution of SymDesign commands. Includes pose initialization, distribution of Rosetta commands to
SLURM/PBS computational clusters, analysis of designed poses, and renaming of completed structures.

"""
import os
import sys
import argparse
import shutil
from csv import reader
from json import loads, dumps
from glob import glob
from itertools import repeat
import pandas as pd
import SymDesignUtils as SDUtils
import PathUtils as PUtils
import CmdUtils as CUtils
from AnalyzeOutput import analyze_output_s, analyze_output_mp
from PoseProcessing import initialization_s, initialization_mp, pose_rmsd_s, pose_rmsd_mp, cluster_poses
import AnalyzeMutatedSequences as Ams
# from AnalyzeMutatedSequences import filter_pose, get_pdb_sequences, select_sequences_s, select_sequences_mp, write_fasta_file
from ProteinExpression import find_expression_tags, add_expression_tag


def rename(des_dir, increment=PUtils.nstruct):
    """Rename the decoy numbers in a DesignDirectory by a specified increment

    Args:
        des_dir (DesignDirectory): A DesignDirectory object
    Keyword Args:
        increment=PUtils.nstruct (int): The number to increment by
    """
    for pdb in os.listdir(des_dir.design_pdbs):
        if os.path.splitext(pdb)[0][-1].isdigit():
            SDUtils.change_filename(os.path.join(des_dir.design_pdbs, pdb), increment=increment)
    SDUtils.modify_decoys(os.path.join(des_dir.scores, PUtils.scores_file), increment=increment)


def pair_directories(dirs2, dirs1):
    """Pair directories with the same pose name, returns source (dirs2) first, destination (dirs1) second
    Args:
        dirs2 (list): List of DesignDirectory objects
        dirs1 (list): List of DesignDirectory objects
    Returns:
        (list), (list): [(source, destination), ...], [directories missing a pair, ...]
    """
    success, pairs = [], []
    for dir1 in dirs1:
        for dir2 in dirs2:
            if str(dir1) == str(dir2):
                pairs.append((dir2, dir1))
                success.append(dir1)
                dirs2.remove(dir2)
                break

    return pairs, list(set(dirs1) - set(success))


def merge_directory_pair(pair):
    """Combine Rosetta design files of one pose with the files of a second pose

    Args:
        pair (tuple): source directory, destination directory
    """
    def merge_scores():
        with open(os.path.join(pair[1].scores, PUtils.scores_file), 'a') as f1:
            f1.write('\n')  # first ensure a new line at the end of first file
            with open(os.path.join(pair[0].scores, PUtils.scores_file), 'r') as f2:
                lines = [loads(line) for line in f2.readlines()]
            f1.write('\n'.join(dumps(line) for line in lines))
            f1.write('\n')  # first a new line at the end of the combined file

    def merge_designs():
        for pdb in os.listdir(pair[0].design_pdbs):
            shutil.copy(os.path.join(pair[0].design_pdbs, pdb), os.path.join(pair[1].design_pdbs, pdb))
    merge_scores()
    merge_designs()


def designs(des_dir, file_type):
    return os.path.join(des_dir.design_pdbs, file_type)


def status(design_directories, _stage, number=None):
    if not number:
        number = PUtils.stage_f[_stage]['len']
        if not number:
            return False
    complete, incomplete = [], []
    for des_dir in design_directories:
        files = glob(designs(des_dir, PUtils.stage_f[_stage]['path']))
        if number >= len(files):
            complete.append(des_dir.path)
        else:
            incomplete.append(des_dir.path)
    complete_path = os.path.join(args.directory, 'complete_' + _stage + '_pose_status')
    incomplete_path = os.path.join(args.directory, 'incomplete_' + _stage + '_pose_status')
    with open(complete_path, 'w') as f_com:
        for c in complete:
            f_com.write('%s\n' % c)
    with open(incomplete_path, 'w') as f_in:
        for n in incomplete:
            f_in.write('%s\n' % n)

    return True


def fix_files_mp(des_dir):
    with open(os.path.join(des_dir.scores, PUtils.scores_file), 'r+') as f1:
        lines = f1.readlines()

        # Remove extra newlines from file
        # clean_lines = []
        # for line in lines:
        #     if line == '\n':
        #         continue
        #     clean_lines.append(line.strip())

        # Take multi-entry '{}{}' json record and make multi-line
        # new_line = {}
        # for z, line in enumerate(lines):
        #     if len(line.split('}{')) == 2:
        #         sep = line.find('}{') + 1
        #         new_line[z] = line[sep:]
        #         lines[z] = line[:sep]
        # for error_idx in new_line:
        #     lines.insert(error_idx, new_line[error_idx])

        # f1.seek(0)
        # f1.write('\n'.join(clean_lines))
        # f1.write('\n')
        # f1.truncate()

        if lines[-1].startswith('{"decoy":"clean_asu_for_consenus"'):
            j = True
        else:
            j = False

    return j, None


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='\nControl all input/output of %s including:\n1. Pose initialization\n'
                                                 '2. Command distribution to computational nodes\n'
                                                 '3. Analysis of designs' % PUtils.program_name)
    parser.add_argument('-d', '--directory', type=str, help='Directory where %s poses are located. Default=CWD'
                                                            % PUtils.program_name, default=os.getcwd())
    parser.add_argument('-f', '--file', type=str, help='File with location(s) of %s poses' % PUtils.program_name,
                        default=None)
    parser.add_argument('-m', '--multi_processing', action='store_true',
                        help='Should job be run with multiprocessing?\nDefault=False')
    parser.add_argument('-b', '--debug', action='store_true', help='Debug all steps to standard out?\nDefault=False')
    parser.add_argument('-s', '--design_string', type=str, help='If pose names are specified by design string instead '
                                                                'of directories, which directory path to '
                                                                'prefix with?\nDefault=None', default=None)

    subparsers = parser.add_subparsers(title='SubModules', dest='sub_module',
                                       description='These are the different modes that designs are processed',
                                       help='Chose one of the SubModules followed by SubModule specific flags')

    parser_pose = subparsers.add_parser('pose', help='Gather output from %s and format for input into Rosetta. '
                                                     'Sets up interface design constrained evolutionary profiles '
                                                     'of homologous sequences and by fragment profiles extracted '
                                                     'from the PDB' % PUtils.nano)
    parser_pose.add_argument('-i', '--fragment_database', type=str,
                             help='Database to match fragments for interface specific scoring matrices. One of %s'
                                  '\nDefault=%s' %
                                  (','.join(list(PUtils.frag_directory.keys())), list(PUtils.frag_directory.keys())[0]),
                             default=list(PUtils.frag_directory.keys())[0])
    parser_pose.add_argument('symmetry_group', type=int,
                             help='What type of symmetry group does your design belong too? One of 0-Point Group, '
                                  '2-Plane Group, or 3-Space Group')  # TODO remove from input, make automatic
    parser_pose.add_argument('-c', '--command_only', action='store_true',
                             help='Should commands be written but not executed?\nDefault=False')
    parser_pose.add_argument('-x', '--suspend', action='store_true',
                             help='Should Rosetta design trajectory be suspended?\nDefault=False')
    parser_pose.add_argument('-p', '--mpi', action='store_true',
                             help='Should job be set up for cluster submission?\nDefault=False')

    parser_dist = subparsers.add_parser('distribute',
                                        help='Distribute specific design step commands to computational resources. '
                                             'In distribution mode, the --file or --directory argument specifies which '
                                             'pose commands should be distributed.')
    parser_dist.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                             help='The stage of design to be prepared. One of %s' %
                                  ', '.join(list(v for v in PUtils.stage_f.keys())), required=True)
    parser_dist.add_argument('-y', '--success_file', help='The name/location of file containing successful commands\n'
                                                          'Default={--stage}_stage_pose_successes', default=None)
    parser_dist.add_argument('-n', '--failure_file', help='The name/location of file containing failed commands\n'
                                                          'Default={--stage}_stage_pose_failures', default=None)
    parser_dist.add_argument('-m', '--max_jobs', type=int, help='How many jobs to run at once?\nDefault=80',
                             default=80)

    parser_analysis = subparsers.add_parser('analysis', help='Run analysis on all poses specified')
    parser_analysis.add_argument('-o', '--output', type=str,
                                 help='Name to output comma delimitted files.\nDefault=%s' % PUtils.analysis_file,
                                 default=PUtils.analysis_file)
    parser_analysis.add_argument('-n', '--no_save', action='store_true',
                                 help='Don\'t save trajectory information.\nDefault=False')
    parser_analysis.add_argument('-f', '--figures', action='store_true',
                                 help='Create and save all pose figures?\nDefault=False')
    parser_analysis.add_argument('-j', '--join', action='store_true',
                                 help='Join Trajectory and Residue Dataframes?\nDefault=False')
    parser_analysis.add_argument('-g', '--delta_g', action='store_true',
                                 help='Compute deltaG versus Refine structure?\nDefault=False')

    parser_merge = subparsers.add_parser('merge', help='Merge all completed designs from location 2 (-f2/-d2) to '
                                                       'location 1(-f/-d). Includes renaming. Highly suggested you copy'
                                                       ' original data!!!')
    parser_merge.add_argument('-d2', '--directory2', type=str, help='Directory 2 where poses should be copied '
                                                                    'from and appended to location 1 poses',
                              default=None)
    parser_merge.add_argument('-f2', '--file2', type=str, help='File 2 where poses should be copied from and appended '
                                                               'to location 1 poses', default=None)
    parser_merge.add_argument('-i', '--increment', type=int, help='How many to increment each design by?\nDefault=%d'
                                                                  % PUtils.nstruct)

    parser_modify = subparsers.add_parser('modify', help='Modify something for testing')

    parser_status = subparsers.add_parser('status', help='Get design status for selected designs')
    parser_status.add_argument('-n', '--number_designs', type=int, help='Number of trajectories per design',
                               default=None)
    parser_status.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                               help='The stage of design to check status of. One of %s'
                                    % ', '.join(list(v for v in PUtils.stage_f.keys())), default=None)

    parser_sequence = subparsers.add_parser('sequence', help='Generate protein sequences for selected designs')
    parser_sequence.add_argument('-c', '--consensus', action='store_true', help='Whether to grab the consensus sequence'
                                                                                '\nDefault=False')
    parser_sequence.add_argument('-d', '--dataframe', type=str, help='Dataframe.csv from analysis containing pose info')
    # TODO ^ require this or pose_design_file
    parser_sequence.add_argument('-f', '--filters', type=dict, help='Metrics with which to filter on\nDefault=None',
                                 default=None)
    parser_sequence.add_argument('-n', '--number', type=int, help='Number of top sequences to return per design',
                                 default=1)
    parser_sequence.add_argument('-p', '--pose_design_file', type=str, help='Name of pose, design .csv file to serve as'
                                                                            ' sequence selector')
    parser_sequence.add_argument('-s', '--selection_string', type=str, help='Output identifier for sequence selection')
    parser_sequence.add_argument('-w', '--weights', type=str, help='Weights of various metrics to final poses\n'
                                                                   'Default=1/number of --filters')

    parser_rename_scores = subparsers.add_parser('rename_scores', help='Rename Protocol names according to dictionary')

    args = parser.parse_args()

    # Start logging output
    if args.debug:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=1)
        logger.debug('Debug mode. Verbose output')
    else:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=2)

    logger.info('Starting %s with options:\n\t%s' %
                (os.path.basename(__file__),
                 '\n\t'.join([str(arg) + ':' + str(getattr(args, arg)) for arg in vars(args)])))

    # Grab all poses (directories) to be processed from either directory name or file
    all_poses, location = SDUtils.collect_designs(args.directory, file=args.file)
    assert all_poses != list(), logger.critical('No %s directories found within \'%s\'! Please ensure correct location'
                                                % (PUtils.nano, location))

    all_design_directories = SDUtils.set_up_directory_objects(all_poses, symmetry=args.design_string)
    logger.info('%d Poses found in \'%s\'' % (len(all_poses), location))
    logger.info('All pose specific logs are located in corresponding directories, ex:\n%s' %
                os.path.join(all_design_directories[0].path, os.path.basename(all_design_directories[0].path) + '.log'))
    if not args.file:
        # Make single file with names of each directory
        args.file = os.path.join(args.directory, 'all_pose_directories')
        with open(args.file, 'w') as design_f:
            for pose in all_poses:
                design_f.write(pose + '\n')

    # Parse SubModule specific commands
    results, exceptions = [], []
    if args.sub_module == 'pose':  # -c --command_only, -i --fragment_library, -p --mpi, -x --suspend
        if args.mpi:
            args.command_only = True
            extras = ' mpi %d' % CUtils.mpi
            logger.info('Setting job up for submission to MPI capable computer. Pose trajectories will run in parallel,'
                        ' %s at a time. This will speed up pose processing %f-fold.' %
                        (CUtils.mpi - 1, PUtils.nstruct / (CUtils.mpi - 1)))
        else:
            extras = ''
        if args.command_only:
            args.suspend = True
            logger.info(
                'Writing modelling commands out to file only, no modelling will occur until commands are executed')

        # Start pose processing and preparation for Rosetta
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(mpi=args.mpi, maximum=True, no_model=args.suspend)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.fragment_database), repeat(args.symmetry_group),
                              repeat(args.command_only), repeat(args.mpi), repeat(args.suspend),
                              repeat(args.debug))
            # results, exceptions = SDUtils.mp_try_starmap(initialization_mp, zipped_args, mp_threads)
            results, exceptions = zip(*SDUtils.mp_starmap(initialization_mp, zipped_args, mp_threads))
            results = list(results)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = initialization_s(des_directory, args.fragment_database, args.symmetry_group,
                                                 script=args.command_only, mpi=args.mpi, suspend=args.suspend,
                                                 debug=args.debug)
                results.append(result)
                exceptions.append(error)

        if args.command_only:
            all_commands = [[] for s in PUtils.stage_f]
            command_files = [[] for s in PUtils.stage_f]
            for des_directory in all_design_directories:
                for i, stage in enumerate(PUtils.stage_f):
                    all_commands[i].append(os.path.join(des_directory.path, stage + '.sh'))
            for i, stage in enumerate(PUtils.stage_f):
                if i > 3:  # No consensus
                    break
                command_files[i] = SDUtils.write_commands(all_commands[i], name=stage, loc=args.directory)
                logger.info('All \'%s\' commands were written to \'%s\'' % (stage, command_files[i]))
            logger.info('\nTo process all commands in correct order, execute:\ncd %s\n%s' %
                        (args.directory, '\n'.join(['python %s -f %s distribute -s %s' % (__file__, args.file, stage)
                                                    for stage in list(PUtils.stage_f.keys())[:3]])))

    elif args.sub_module == 'distribute':  # -s --stage, -y --success_file, -n --failure_file, -m --max_jobs
        # Create success and failures files
        if not args.success_file:
            args.success_file = os.path.join(args.directory, args.stage + '_stage_pose_success')
        # with open(args.success_file, 'w') as f:
        #     dummy = True
        if not args.failure_file:
            args.failure_file = os.path.join(args.directory, args.stage + '_stage_pose_failures')
        # with open(args.failure_file, 'w') as f:
        #     dummy = True
        logger.info('\nSuccessful poses will be listed in \'%s\'\nFailed poses will be listed in \'%s\''
                    % (args.success_file, args.failure_file))

        # Grab sbatch template and stage cpu divisor to facilitate array set up and command distribution
        with open(PUtils.sbatch_templates[args.stage]) as template_f:
            template_sbatch = template_f.readlines()

        # Make sbatch file from template, array details, and command distribution script
        filename = os.path.join(args.directory, args.stage + PUtils.sbatch)
        output = os.path.join(args.directory, 'output')
        if not os.path.exists(output):
            os.mkdir(output)

        command_divisor = CUtils.process_scale[args.stage]
        with open(filename, 'w') as new_f:
            for template_line in template_sbatch:
                new_f.write(template_line)
            out = 'output=%s/%s' % (output, '%A_%a.out')
            new_f.write(PUtils.sb_flag + out + '\n')
            array = 'array=1-%d%%%d' % (int(len(all_poses) / command_divisor + 0.5), args.max_jobs)
            new_f.write(PUtils.sb_flag + array + '\n')
            new_f.write('\npython %s --stage %s --success_file %s --failure_file %s --command_file %s\n' %
                        (PUtils.cmd_dist, args.stage, args.success_file, args.failure_file, args.file))

        logger.info('To distribute commands enter the following:\ncd %s\nsbatch %s'
                    % (args.directory, os.path.basename(filename)))

    elif args.sub_module == 'analysis':  # -f --figures, -n --no_save, -j --join, -g --delta_g
        save = True
        if args.no_save:
            save = False
        # Start pose analysis of all designed files
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.delta_g), repeat(args.join), repeat(args.debug),
                              repeat(save), repeat(args.figures))
            # results, exceptions = SDUtils.mp_try_starmap(analyze_output_mp, zipped_args, mp_threads)
            results, exceptions = zip(*SDUtils.mp_starmap(analyze_output_mp, zipped_args, mp_threads))
            results = list(results)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = analyze_output_s(des_directory, delta_refine=args.delta_g, merge_residue_data=args.join,
                                                 debug=args.debug, save_trajectories=save, figures=args.figures)
                results.append(result)
                exceptions.append(error)

        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        if len(all_design_directories) >= 1:
            # Save Design DataFrame
            design_df = pd.DataFrame(results)
            out_path = os.path.join(args.directory, args.output)
            design_df.to_csv(out_path)
            logger.info('Analysis of all poses written to %s' % out_path)
            if save:
                logger.info('Analysis of all Trajectories and Residues written to %s'
                            % all_design_directories[0].all_scores)

    elif args.sub_module == 'merge':  # -d2 directory2, -f2 file2, -i increment
        if args.directory2 or args.file2:
            # Grab all poses (directories) to be processed from either directory name or file
            all_poses2, location2 = SDUtils.collect_designs(args.directory2, file=args.file2)
            assert all_poses2 != list(), logger.critical(
                'No %s directories found within \'%s\'! Please ensure correct location' % (PUtils.nano, location2))
            all_design_directories2 = SDUtils.set_up_directory_objects(all_poses2)
            logger.info('%d Poses found in \'%s\'' % (len(all_poses2), location2))

            directory_pairs, failures = pair_directories(all_design_directories2, all_design_directories)
            if failures:
                logger.warning('The following directories have no partner:\n%s' % '\n'.join(fail.path
                                                                                            for fail in failures))
            if args.multi_processing:
                # Calculate the number of threads to use depending on computer resources
                mp_threads = SDUtils.calculate_mp_threads(maximum=True)
                logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
                zipped_args = zip(all_design_directories, repeat(args.increment))
                results = SDUtils.mp_starmap(rename, zipped_args, mp_threads)
                results2 = SDUtils.mp_map(merge_directory_pair, directory_pairs, mp_threads)
            else:
                logger.info('Starting processing. If single process is taking awhile, use -m during submission')
                for des_directory in all_design_directories:
                    rename(des_directory, increment=args.increment)
                for directory_pair in directory_pairs:
                    merge_directory_pair(directory_pair)

    elif args.sub_module == 'rename_scores':
        rename = {'combo_profile_switch': 'limit_to_profile', 'favor_profile_switch': 'favor_frag_limit_to_profile'}
        for des_directory in all_design_directories:
            SDUtils.rename_decoy_protocols(des_directory, rename)

    elif args.sub_module == 'modify':
        if args.multi_processing:
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            # results, exceptions = zip(*SDUtils.mp_map(fix_files_mp, all_design_directories, threads=mp_threads))
            pose_map = pose_rmsd_mp(all_design_directories, threads=mp_threads)
        else:
            pose_map = pose_rmsd_s(all_design_directories)

        pose_cluster_map = cluster_poses(pose_map)
        pose_cluster_file = SDUtils.pickle_object(pose_cluster_map, PUtils.clustered_poses,
                                                  out_path=all_design_directories[0].protein_data)

        # for protein_pair in pose_map:
        #     if os.path.basename(protein_pair) == '4f47_4grd':
        #     logger.info('\n'.join(['%s\n%s' % (pose1, '\n'.join(['%s\t%f' %
        #                                                          (pose2, pose_map[protein_pair][pose1][pose2])
        #                                                          for pose2 in pose_map[protein_pair][pose1]]))
        #                            for pose1 in pose_map[protein_pair]]))
        errors = []
        for i, result in enumerate(results):
            if not result:
                errors.append(all_design_directories[i].path)

        logger.error('%d directories missing consensus metrics!' % len(errors))
        with open('missing_consensus_metrics', 'w') as f:
            f.write('\n'.join(errors))

    elif args.sub_module == 'status':  # -n --number, -s --stage
        if args.number_designs:
            logger.info('Checking for %d files. If no stage is specified, results will be incorrect for all but design '
                        'stage' % args.number_designs)
        if args.stage:
            status(all_design_directories, args.stage, number=args.number_designs)
        else:
            for stage in PUtils.stage_f:
                s = status(all_design_directories, stage, number=args.number_designs)
                if s:
                    logger.info('For \'%s\' stage, default settings should generate %d files'
                                % (stage, PUtils.stage_f[stage]['len']))

    elif args.sub_module == 'sequence':  # -c consensus, -d dataframe, -f filters, -n number, -p pose_design_file, -s selection_string, -w weights
        if args.pose_design_file:
            # Grab all poses (directories) to be processed from either directory name or file
            with open(args.pose_design_file) as csv_file:
                csv_lines = [line for line in reader(csv_file)]
            all_poses, pose_design_numbers = zip(*csv_lines)
            # pose_design_numbers = list(map(list, pose_design_numbers))
            # all_poses, pose_design_numbers = zip(*reader(args.pose_design_file))
            all_design_directories = SDUtils.set_up_directory_objects(all_poses, symmetry=args.design_string)
            results.append(zip(all_design_directories, pose_design_numbers))
            location = args.pose_design_file
        else:
            output = False
            if args.dataframe:  # Figure out poses from a dataframe, filters, and weights
                # TODO parameterize
                # if not args.filters:
                #     sys.exit('VY made this and I am going to put in here!')
                design_requirements = {'percent_int_area_polar': 0.4, 'buns_per_ang': 0.002}
                crystal_means = {'int_area_total': 570, 'shape_complementarity': 0.63, 'number_hbonds': 5}
                symmetry_requirements = crystal_means
                filters = {}
                filters.update(design_requirements)
                filters.update(symmetry_requirements)
                if args.consensus:
                    consensus_weights = {'interaction_energy_complex': 0.5, 'percent_fragment': 0.5}
                    weights = consensus_weights
                else:
                    weights = {'protocol_energy_distance_sum': 0.25, 'shape_complementarity': 0.25,
                               'observed_evolution': 0.25, 'int_composition_diff': 0.25}
                # if not args.weights:
                #     sys.exit('VY made this and I am going to put in here!')

                selected_poses = Ams.filter_pose(args.dataframe, filters, weights,  # num_designs=args.number,
                                             consensus=args.consensus, filter_file=PUtils.filter_and_sort)

                # Sort results according to clustered poses
                pose_cluster_map = SDUtils.unpickle(
                    os.path.join(all_design_directories[0].protein_data, '%s.pkl' % PUtils.clustered_poses))
                # {building_blocks: {design_string: cluster_representative}, ...}
                pose_clusters_found, final_poses = [], []
                # for des_dir in all_design_directories:
                for pose in selected_poses:
                    if pose_cluster_map[pose.split('-')[0]][pose] not in pose_clusters_found:
                        pose_clusters_found.append(pose_cluster_map[pose.split('-')[0]][pose])
                        final_poses.append(pose)

                if len(final_poses) > args.number:
                    final_poses = final_poses[:args.number]

                all_design_directories = SDUtils.set_up_directory_objects(final_poses, symmetry=args.design_string)
                if args.consensus:
                    results.append(zip(all_design_directories, repeat('consensus')))
                    output = True

            if output:
                pass
            elif args.multi_processing:
                # Calculate the number of threads to use depending on computer resources
                mp_threads = SDUtils.calculate_mp_threads(maximum=True)
                logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
                # zipped_args = zip(all_design_directories, repeat(args.number))
                # results, exceptions = zip(*SDUtils.mp_starmap(select_sequences_mp, zipped_args, mp_threads))
                results, exceptions = zip(*SDUtils.mp_map(Ams.select_sequences_mp, all_design_directories, mp_threads))
                # results - contains tuple of (DesignDirectory, design index) for each sequence
                # could simply return the design index then zip with the directory
            else:
                results, exceptions = zip(*list(Ams.select_sequences_s(des_directory, number=args.number)
                                                for des_directory in all_design_directories))
                # for des_directory in all_design_directories:
                #     result, error = select_sequences_s(des_directory, number=args.number)
                #     results.append(result)
                #     exceptions.append(error)

        results = list(results)
        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        if not args.selection_string:
            args.selection_string = '%s_' % os.path.basename(os.path.splitext(location)[0])
        else:
            args.selection_string += '_'
        outdir = os.path.join(os.path.dirname(location), '%sSelected_PDBs' % args.selection_string)
        if not os.path.exists(outdir):
            os.makedirs(outdir)

        # Create symbolic links to the output PDB's
        final_sequences = {}
        for pose in results:
            pose_des_dirs, design = zip(*pose)
            for i, pose_des_dir in enumerate(pose_des_dirs):
                file = glob('%s*%s*' % (pose_des_dir.design_pdbs, design[i]))
                if file == list():  # If no file found, skip and add to exceptions
                    exceptions.append((pose_des_dir.path, 'No file found for %s*%s*' %
                                       (pose_des_dir.design_pdbs, design[i])))
                    continue
                # os.symlink(file[0], os.path.join(outdir, '%s_design_%s.pdb' % (str(pose_des_dir), design[i])))
                # Format sequences for expression
                # coming in as (chain: seq}
                design_pose = SDUtils.read_pdb(file[0])
                design_sequences = Ams.get_pdb_sequences(design_pose)
                pose_pdbs = os.path.basename(pose_des_dir.building_blocks).split('_')

                # need the original pose chain identity
                source_pose = SDUtils.read_pdb(pose_des_dir.source)
                if not source_pose.sequence_dictionary:
                    oligomers = [SDUtils.read_pdb(SDUtils.fetch_pdb(pdb)) for pdb in pose_pdbs]
                    for oligomer in oligomers[1:]:
                        oligomers[0].read_atom_list(oligomer.all_atoms)
                    source_pose = oligomers[0]
                    source_pose.reorder_chains()
                    source_pose.pose_numbering()
                # Insert loops identified by comparison of SEQRES and ATOM

                # missing_termini_d = {chain: Ams.generate_mutations_from_seq(pdb_atom_seq[chain],
                #                                                         template_pdb.sequence_dictionary[chain],
                #                                                         offset=True,
                #                                                         termini=True)
                #                      for chain in template_pdb.chain_id_list}

                pose_offset_d = Ams.pdb_to_pose_num(source_pose.sequence_dictionary)
                all_missing_residues_d = {chain: Ams.generate_mutations_from_seq(design_sequences[chain],
                                                                                 source_pose.sequence_dictionary,
                                                                                 offset=True, only_gaps=True)
                                          for chain in design_sequences}

                print(all_missing_residues_d)

                # Modify residue indices to pose numbering
                all_missing_residues_d = {chain: {residue + pose_offset_d[chain]: all_missing_residues_d[chain][residue]
                                                  for residue in all_missing_residues_d[chain]}
                                          for chain in all_missing_residues_d}
                print(all_missing_residues_d)

                # Insert residues into design PDB object
                for chain in all_missing_residues_d:
                    for residue in all_missing_residues_d[chain]:
                        design_pose.insert_residue(chain, residue, all_missing_residues_d[chain][residue]['from'])

                # Get modified sequence
                design_sequences_disordered = Ams.get_pdb_sequences(design_pose)
                print('Design Sequence:\n%s' % '\n'.join([design_sequences[chain] for chain in design_sequences]))
                print('Disordered Insertion:\n%s' %
                      '\n'.join([design_sequences_disordered[chain] for chain in design_sequences_disordered]))

                # I need the source sequence as mutations to get the mutation index on the design sequence
                mutations = {chain: Ams.generate_mutations_from_seq(source_pose.sequence_dictionary,
                                                                    design_sequences[chain], offset=True)
                             for chain in design_sequences}
                # Next I need the modified (residue inserted) design sequence to pull out the correct Met
                coding_offset = {chain: Ams.find_orf_offset(design_sequences_disordered[chain], mutations)
                                 for chain in design_sequences_disordered}

                print('Coding Offset:\n%s'
                      % '\n'.join(['%s: %s' % (chain, coding_offset[chain]) for chain in coding_offset]))

                pretag_sequences = {chain: design_sequences_disordered[chain][coding_offset[chain]:]
                                    for chain in coding_offset}
                print('Pre-tag Sequences:\n%s' % '\n'.join([pretag_sequences[chain] for chain in pretag_sequences]))

                # for residue in all_missing_residues_d:
                #     if all_missing_residues_d[residue]['from'] == 'M':

                # for chain in gapped_residues_d:
                #     for residue in gapped_residues_d[chain]:

                # Look for compatible tags
                tag_sequences = {pdb: find_expression_tags(pdb, chain)
                                 for pdb, chain in zip(pose_pdbs, pose.chain_id_list)}

                chains = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
                final_sequences[design] = {pdb: add_expression_tag(tag_sequences[pdb]['seq'],
                                                                   pretag_sequences[chains[j]])
                                           for j, pdb in enumerate(tag_sequences)}
                # final_sequences[design] = {pdb: add_expression_tag(tag_sequences[pdb]['seq'],
                #                                                    design_sequences[chains[j]])
                #                            for j, pdb in enumerate(tag_sequences)}
                # format sequence for M start

        print(final_sequences)

        # Write output sequences to fasta file
        # seq_file = write_fasta_file(final_sequences, '%sSelected_Sequences' % args.selection_string,
        #                             path=os.path.dirname(location))

    # Report any program exceptions
    any_exceptions = [exception for exception in exceptions if exception]
    # any_exceptions = list(set(exceptions))
    # if len(any_exceptions) > 1 or any_exceptions and any_exceptions[0]:
    if len(any_exceptions) > 1:
        logger.warning('\nThe following exceptions were thrown. Design for %d directories is inaccurate!!!\n' %
                       len(any_exceptions))
        all_exception_poses = []
        for exception in exceptions:
            if exception:
                logger.warning('%s: %s' % exception)
                all_exception_poses.append(exception[0])
        print('\n')
        # except_file = os.path.join(args.location, 'EXCEPTIONS.log')
        except_file = SDUtils.write_list_to_file(all_exception_poses, name='EXCEPTIONS.log', location=args.directory)
        logger.info('All poses with exceptions written to file: %s' % except_file)

    print('\n')
