"""
Module for distribution of SymDesign commands. Includes pose initialization, distribution of Rosetta commands to
SLURM/PBS computational clusters, analysis of designed poses, and renaming of completed structures.

"""
import os
# import sys
from random import random
import PDB
import argparse
import shutil
from csv import reader
from json import loads, dumps
from glob import glob
from itertools import repeat, product, combinations
import pandas as pd
import SymDesignUtils as SDUtils
import PathUtils as PUtils
import CmdUtils as CUtils
from AnalyzeOutput import analyze_output_s, analyze_output_mp
from PoseProcessing import initialization_s, initialization_mp, pose_rmsd_s, pose_rmsd_mp, cluster_poses
from NanohedraWrap import nanohedra_command_mp, nanohedra_command_s, nanohedra_recap_mp, nanohedra_recap_s
import AnalyzeMutatedSequences as Ams
# from AnalyzeMutatedSequences import filter_pose, get_pdb_sequences, select_sequences_s, select_sequences_mp, write_fasta_file
from ProteinExpression import find_all_matching_pdb_expression_tags, add_expression_tag, find_expression_tags


def rename(des_dir, increment=PUtils.nstruct):
    """Rename the decoy numbers in a DesignDirectory by a specified increment

    Args:
        des_dir (DesignDirectory): A DesignDirectory object
    Keyword Args:
        increment=PUtils.nstruct (int): The number to increment by
    """
    for pdb in os.listdir(des_dir.design_pdbs):
        if os.path.splitext(pdb)[0][-1].isdigit():
            SDUtils.change_filename(os.path.join(des_dir.design_pdbs, pdb), increment=increment)
    SDUtils.modify_decoys(os.path.join(des_dir.scores, PUtils.scores_file), increment=increment)


def pair_directories(dirs2, dirs1):
    """Pair directories with the same pose name, returns source (dirs2) first, destination (dirs1) second
    Args:
        dirs2 (list): List of DesignDirectory objects
        dirs1 (list): List of DesignDirectory objects
    Returns:
        (list), (list): [(source, destination), ...], [directories missing a pair, ...]
    """
    success, pairs = [], []
    for dir1 in dirs1:
        for dir2 in dirs2:
            if str(dir1) == str(dir2):
                pairs.append((dir2, dir1))
                success.append(dir1)
                dirs2.remove(dir2)
                break

    return pairs, list(set(dirs1) - set(success))


def merge_directory_pair(pair):
    """Combine Rosetta design files of one pose with the files of a second pose

    Args:
        pair (tuple): source directory, destination directory
    """
    def merge_scores():
        with open(os.path.join(pair[1].scores, PUtils.scores_file), 'a') as f1:
            f1.write('\n')  # first ensure a new line at the end of first file
            with open(os.path.join(pair[0].scores, PUtils.scores_file), 'r') as f2:
                lines = [loads(line) for line in f2.readlines()]
            f1.write('\n'.join(dumps(line) for line in lines))
            f1.write('\n')  # first a new line at the end of the combined file

    def merge_designs():
        for pdb in os.listdir(pair[0].design_pdbs):
            shutil.copy(os.path.join(pair[0].design_pdbs, pdb), os.path.join(pair[1].design_pdbs, pdb))
    merge_scores()
    merge_designs()


def designs(des_dir, file_type):
    return os.path.join(des_dir.design_pdbs, file_type)


def status(design_directories, _stage, number=None):
    if not number:
        number = PUtils.stage_f[_stage]['len']
        if not number:
            return False
    complete, incomplete = [], []
    for des_dir in design_directories:
        files = glob(designs(des_dir, PUtils.stage_f[_stage]['path']))
        if number >= len(files):
            complete.append(des_dir.path)
        else:
            incomplete.append(des_dir.path)
    complete_path = os.path.join(args.directory, 'complete_' + _stage + '_pose_status')
    incomplete_path = os.path.join(args.directory, 'incomplete_' + _stage + '_pose_status')
    with open(complete_path, 'w') as f_com:
        for c in complete:
            f_com.write('%s\n' % c)
    with open(incomplete_path, 'w') as f_in:
        for n in incomplete:
            f_in.write('%s\n' % n)

    return True


def fix_files_mp(des_dir):
    with open(os.path.join(des_dir.scores, PUtils.scores_file), 'r+') as f1:
        lines = f1.readlines()

        # Remove extra newlines from file
        # clean_lines = []
        # for line in lines:
        #     if line == '\n':
        #         continue
        #     clean_lines.append(line.strip())

        # Take multi-entry '{}{}' json record and make multi-line
        # new_line = {}
        # for z, line in enumerate(lines):
        #     if len(line.split('}{')) == 2:
        #         sep = line.find('}{') + 1
        #         new_line[z] = line[sep:]
        #         lines[z] = line[:sep]
        # for error_idx in new_line:
        #     lines.insert(error_idx, new_line[error_idx])

        # f1.seek(0)
        # f1.write('\n'.join(clean_lines))
        # f1.write('\n')
        # f1.truncate()

        if lines[-1].startswith('{"decoy":"clean_asu_for_consenus"'):
            j = True
        else:
            j = False

    return j, None


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='\nControl all input/output of %s including:\n1. Pose initialization\n'
                                                 '2. Command distribution to computational nodes\n'
                                                 '3. Analysis of designs' % PUtils.program_name)
    parser.add_argument('-c', '--command_only', action='store_true',
                        help='Should commands be written but not executed?\nDefault=False')
    parser.add_argument('-d', '--directory', type=str, help='Directory where %s poses are located. Default=CWD'
                                                            % PUtils.program_name, default=os.getcwd())
    parser.add_argument('-f', '--file', type=str, help='File with location(s) of %s poses' % PUtils.program_name,
                        default=None)
    parser.add_argument('-m', '--multi_processing', action='store_true',
                        help='Should job be run with multiprocessing?\nDefault=False')
    parser.add_argument('-b', '--debug', action='store_true', help='Debug all steps to standard out?\nDefault=False')
    parser.add_argument('-s', '--design_string', type=str, help='If pose names are specified by design string instead '
                                                                'of directories, which directory path to '
                                                                'prefix with?\nDefault=None', default=None)

    subparsers = parser.add_subparsers(title='SubModules', dest='sub_module',
                                       description='These are the different modes that designs are processed',
                                       help='Chose one of the SubModules followed by SubModule specific flags')

    parser_dock = subparsers.add_parser('dock', help='Submit jobs to %s.py\nIf a docking directory structure is set up,'
                                                     ' provide the overall directory location with program argument '
                                                     '-d/-f, otherwise, use the -d1 -d2 \'pose\' module arguments to '
                                                     'specify lists of oligomers to dock' % PUtils.nano.title())
    # parser_dock.add_argument('-c', '--command_only', action='store_true',
    #                          help='Should commands be written but not executed?\nDefault=False')
    parser_dock.add_argument('-d1', '--directory1', type=str, help='Disk location where the first symmetry oligomers '
                                                                   'are located\nREQUIRED', default=None)  # TODO REQ.
    parser_dock.add_argument('-d2', '--directory2', type=str, help='Disk location where the second symmetry oligomers '
                                                                   'are located\nDefault=None', default=None)
    parser_dock.add_argument('-e', '--entry', type=int, help='The entry number of %s docking combinations to use' %
                                                             PUtils.nano)
    parser_dock.add_argument('-o', '--output_directory', type=str, help='Where should the output from commands be '
                                                                        'written?\n'
                                                                        'Default=NanohedraEntry(entry)DockedPoses',
                             default=None)

    parser_pose = subparsers.add_parser('pose', help='Gather output from %s and format for input into Rosetta. '
                                                     'Sets up interface design constrained evolutionary profiles '
                                                     'of homologous sequences and by fragment profiles extracted '
                                                     'from the PDB' % PUtils.nano.title())
    parser_pose.add_argument('-i', '--fragment_database', type=str,
                             help='Database to match fragments for interface specific scoring matrices. One of %s'
                                  '\nDefault=%s' %
                                  (','.join(list(PUtils.frag_directory.keys())), list(PUtils.frag_directory.keys())[0]),
                             default=list(PUtils.frag_directory.keys())[0])
    parser_pose.add_argument('symmetry_group', type=int,
                             help='What type of symmetry group does your design belong too? One of 0-Point Group, '
                                  '2-Plane Group, or 3-Space Group')  # TODO remove from input, make automatic
    # parser_pose.add_argument('-c', '--command_only', action='store_true',
    #                          help='Should commands be written but not executed?\nDefault=False')
    parser_pose.add_argument('-x', '--suspend', action='store_true',
                             help='Should Rosetta design trajectory be suspended?\nDefault=False')
    parser_pose.add_argument('-p', '--mpi', action='store_true',
                             help='Should job be set up for cluster submission?\nDefault=False')

    parser_dist = subparsers.add_parser('distribute',
                                        help='Distribute specific design step commands to computational resources. '
                                             'In distribution mode, the --file or --directory argument specifies which '
                                             'pose commands should be distributed.')
    parser_dist.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                             help='The stage of design to be prepared. One of %s' %
                                  ', '.join(list(v for v in PUtils.stage_f.keys())), required=True)
    parser_dist.add_argument('-y', '--success_file', help='The name/location of file containing successful commands\n'
                                                          'Default={--stage}_stage_pose_successes', default=None)
    parser_dist.add_argument('-n', '--failure_file', help='The name/location of file containing failed commands\n'
                                                          'Default={--stage}_stage_pose_failures', default=None)
    parser_dist.add_argument('-m', '--max_jobs', type=int, help='How many jobs to run at once?\nDefault=80',
                             default=80)

    parser_analysis = subparsers.add_parser('analysis', help='Run analysis on all poses specified')
    parser_analysis.add_argument('-o', '--output', type=str,
                                 help='Name to output comma delimitted files.\nDefault=%s' % PUtils.analysis_file,
                                 default=PUtils.analysis_file)
    parser_analysis.add_argument('-n', '--no_save', action='store_true',
                                 help='Don\'t save trajectory information.\nDefault=False')
    parser_analysis.add_argument('-f', '--figures', action='store_true',
                                 help='Create and save all pose figures?\nDefault=False')
    parser_analysis.add_argument('-j', '--join', action='store_true',
                                 help='Join Trajectory and Residue Dataframes?\nDefault=False')
    parser_analysis.add_argument('-g', '--delta_g', action='store_true',
                                 help='Compute deltaG versus Refine structure?\nDefault=False')

    parser_merge = subparsers.add_parser('merge', help='Merge all completed designs from location 2 (-f2/-d2) to '
                                                       'location 1(-f/-d). Includes renaming. Highly suggested you copy'
                                                       ' original data!!!')
    parser_merge.add_argument('-d2', '--directory2', type=str, help='Directory 2 where poses should be copied '
                                                                    'from and appended to location 1 poses',
                              default=None)
    parser_merge.add_argument('-f2', '--file2', type=str, help='File 2 where poses should be copied from and appended '
                                                               'to location 1 poses', default=None)
    parser_merge.add_argument('-i', '--increment', type=int, help='How many to increment each design by?\nDefault=%d'
                                                                  % PUtils.nstruct)

    parser_modify = subparsers.add_parser('modify', help='Modify something for testing')

    parser_status = subparsers.add_parser('status', help='Get design status for selected designs')
    parser_status.add_argument('-n', '--number_designs', type=int, help='Number of trajectories per design',
                               default=None)
    parser_status.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                               help='The stage of design to check status of. One of %s'
                                    % ', '.join(list(v for v in PUtils.stage_f.keys())), default=None)

    parser_sequence = subparsers.add_parser('sequence', help='Generate protein sequences for selected designs')
    parser_sequence.add_argument('-c', '--consensus', action='store_true', help='Whether to grab the consensus sequence'
                                                                                '\nDefault=False')
    parser_sequence.add_argument('-d', '--dataframe', type=str, help='Dataframe.csv from analysis containing pose info')
    # TODO ^ require this or pose_design_file
    parser_sequence.add_argument('-f', '--filters', type=dict, help='Metrics with which to filter on\nDefault=None',
                                 default=None)
    parser_sequence.add_argument('-n', '--number', type=int, help='Number of top sequences to return per design',
                                 default=1)
    parser_sequence.add_argument('-p', '--pose_design_file', type=str, help='Name of pose, design .csv file to serve as'
                                                                            ' sequence selector')
    parser_sequence.add_argument('-s', '--selection_string', type=str, help='Output identifier for sequence selection')
    parser_sequence.add_argument('-w', '--weights', type=str, help='Weights of various metrics to final poses\n'
                                                                   'Default=1/number of --filters')

    parser_rename_scores = subparsers.add_parser('rename_scores', help='Rename Protocol names according to dictionary')

    args = parser.parse_args()

    # Start logging output
    if args.debug:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=1)
        logger.debug('Debug mode. Verbose output')
    else:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=2)

    logger.info('Starting %s with options:\n\t%s' %
                (os.path.basename(__file__),
                 '\n\t'.join([str(arg) + ':' + str(getattr(args, arg)) for arg in vars(args)])))

    # Grab all poses (directories) to be processed from either directory name or file
    pdb_filepaths, all_design_directories, location = None, None, None
    if args.sub_module not in ['dock', 'distribute']:
        all_poses, location = SDUtils.collect_directories(args.directory, file=args.file)
        assert all_poses != list(), logger.critical('No %s directories found within \'%s\'! Please ensure correct '
                                                    'location' % (PUtils.nano.title(), location))
        all_design_directories = SDUtils.set_up_directory_objects(all_poses, symmetry=args.design_string)
        logger.info('All pose specific logs are located in corresponding directories, ex:\n%s' %
                    os.path.join(all_design_directories[0].path,
                                 '%s.log' % os.path.basename(all_design_directories[0].path)))
        if not args.file:
            # Make single file with names of each directory
            args.file = os.path.join(args.directory, 'all_pose_directories')
            with open(args.file, 'w') as design_f:
                design_f.write('\n'.join(pose for pose in all_poses))
    elif args.sub_module == 'distribute':
        all_design_directories, location = SDUtils.collect_directories(args.directory, file=args.file, type=args.sub_module)
    elif args.sub_module == 'dock':
        if args.directory or args.file:
            all_design_directories, location = SDUtils.collect_directories(args.directory, file=args.file,
                                                                           type=args.sub_module)
        elif args.directory1:
            location = args.directory1
            pdb1_filepaths = SDUtils.get_all_pdb_file_paths(args.directory1)
            if args.directory2:
                pdb2_filepaths = SDUtils.get_all_pdb_file_paths(args.directory2)
                pdb_filepaths = product(pdb1_filepaths, pdb2_filepaths)
            else:
                # if pdb1_path == pdb2_path:
                pdb_filepaths = list(combinations(pdb1_filepaths, 2))
                # pdb_list = glob(os.path.join(args.directory1, '*.pdb*')
                all_design_directories = pdb_filepaths  # for logging purposes below
        else:
            exit('No docking directories/files were specified\nPlease specify --directory or --file, or --directory1, '
                 'and/or --directory2. See %s' % PUtils.help(args.sub_module))

    logger.info('%d Poses found in \'%s\'' % (len(all_design_directories), location))

    if args.command_only:
        args.suspend = True
        logger.info('Writing modelling commands out to file only, no modelling will occur until commands are executed')

    # Parse SubModule specific commands
    results, exceptions = [], []

    if args.sub_module == 'dock':  # -d1 --directory1, -d2 --directory2, -e --entry, -o --output_directory
        # Initialize docking procedure
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(maximum=True, no_model=args.suspend)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            if not pdb_filepaths:  # single directory docking (already made directories)
                zipped_args = zip(all_design_directories, repeat(args.design_string))
                results, exceptions = zip(*SDUtils.mp_starmap(nanohedra_recap_mp, zipped_args, mp_threads))
            else:
                zipped_args = zip(repeat(args.entry), pdb_filepaths, repeat(args.output_directory),
                                  repeat(args.design_string))
                results, exceptions = zip(*SDUtils.mp_starmap(nanohedra_command_mp, zipped_args, mp_threads))
            results = list(results)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            if not pdb_filepaths:  # single directory docking (already made directories)
                for dock_directory in all_design_directories:
                    result, error = nanohedra_recap_s(dock_directory, args.design_string)
                    results.append(result)
                    exceptions.append(error)
            else:
                for path1, path2 in pdb_filepaths:
                    result, error = nanohedra_command_s(args.entry, path1, path2, args.output_directory,
                                                        args.design_string)
                    results.append(result)
                    exceptions.append(error)

        if args.command_only:
            # Make single file with names of each directory
            args.file = os.path.join(args.directory, 'complete_docked_directories_flipped')
            with open(args.file, 'w') as design_f:
                command_directories = map(os.path.dirname, results)  # Specific for docking commands due to no established directory
                design_f.write('\n'.join(docking_pair for docking_pair in command_directories if docking_pair))

            all_commands = [result for result in results if result]
            command_file = SDUtils.write_commands(all_commands, name=PUtils.nano, loc=args.directory)
            logger.info('All \'%s\' commands were written to \'%s\'' % (PUtils.nano, command_file))
            logger.info('\nTo process commands, execute:\n%s' %  # cd %s\n%s' % (args.directory,
                        'python %s -f %s distribute -s %s' % (__file__, args.file, PUtils.nano))

    elif args.sub_module == 'pose':  # -c --command_only, -i --fragment_library, -p --mpi, -x --suspend
        if args.mpi:
            args.command_only = True
            extras = ' mpi %d' % CUtils.mpi
            logger.info('Setting job up for submission to MPI capable computer. Pose trajectories will run in parallel,'
                        ' %s at a time. This will speed up pose processing %f-fold.' %
                        (CUtils.mpi - 1, PUtils.nstruct / (CUtils.mpi - 1)))
        else:
            extras = ''

        # Start pose processing and preparation for Rosetta
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(mpi=args.mpi, maximum=True, no_model=args.suspend)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.fragment_database), repeat(args.symmetry_group),
                              repeat(args.command_only), repeat(args.mpi), repeat(args.suspend),
                              repeat(args.debug))
            results, exceptions = zip(*SDUtils.mp_starmap(initialization_mp, zipped_args, mp_threads))
            results = list(results)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = initialization_s(des_directory, args.fragment_database, args.symmetry_group,
                                                 script=args.command_only, mpi=args.mpi, suspend=args.suspend,
                                                 debug=args.debug)
                results.append(result)
                exceptions.append(error)

        if args.command_only:
            all_commands = [[] for s in PUtils.stage_f]
            command_files = [[] for s in PUtils.stage_f]
            for des_directory in all_design_directories:
                for i, stage in enumerate(PUtils.stage_f):
                    all_commands[i].append(os.path.join(des_directory.path, stage + '.sh'))
            for i, stage in enumerate(PUtils.stage_f):
                if i > 3:  # No consensus
                    break
                command_files[i] = SDUtils.write_commands(all_commands[i], name=stage, loc=args.directory)
                logger.info('All \'%s\' commands were written to \'%s\'' % (stage, command_files[i]))
            logger.info('\nTo process all commands in correct order, execute:\ncd %s\n%s' %
                        (args.directory, '\n'.join(['python %s -f %s distribute -s %s' % (__file__, args.file, stage)
                                                    for stage in list(PUtils.stage_f.keys())[:3]])))
            # TODO make args.file equal to command_files[i] and change distribution to no stage prefix...

    elif args.sub_module == 'distribute':  # -s --stage, -y --success_file, -n --failure_file, -m --max_jobs
        # Create success and failures files
        ran_num = int(100 * random())
        if not args.success_file:
            args.success_file = os.path.join(args.directory, '%s_sbatch-%d_success.log' % (args.stage, ran_num))
        if not args.failure_file:
            args.failure_file = os.path.join(args.directory, '%s_sbatch-%d_failures.log' % (args.stage, ran_num))
        logger.info('\nSuccessful poses will be listed in \'%s\'\nFailed poses will be listed in \'%s\''
                    % (args.success_file, args.failure_file))

        # Grab sbatch template and stage cpu divisor to facilitate array set up and command distribution
        with open(PUtils.sbatch_templates[args.stage]) as template_f:
            template_sbatch = template_f.readlines()

        # Make sbatch file from template, array details, and command distribution script
        filename = os.path.join(args.directory, '%s_%s-%d.sh' % (args.stage, PUtils.sbatch, ran_num))
        output = os.path.join(args.directory, 'output')
        if not os.path.exists(output):
            os.mkdir(output)

        command_divisor = CUtils.process_scale[args.stage]
        with open(filename, 'w') as new_f:
            for template_line in template_sbatch:
                new_f.write(template_line)
            out = 'output=%s/%s' % (output, '%A_%a.out')
            new_f.write(PUtils.sb_flag + out + '\n')
            array = 'array=1-%d%%%d' % (int(len(all_poses) / command_divisor + 0.5), args.max_jobs)
            new_f.write(PUtils.sb_flag + array + '\n')
            new_f.write('\npython %s --stage %s --success_file %s --failure_file %s --command_file %s\n' %
                        (PUtils.cmd_dist, args.stage, args.success_file, args.failure_file, args.file))

        logger.info('To distribute commands enter the following:\ncd %s\nsbatch %s'
                    % (args.directory, os.path.basename(filename)))

    elif args.sub_module == 'analysis':  # -f --figures, -n --no_save, -j --join, -g --delta_g
        save = True
        if args.no_save:
            save = False
        # Start pose analysis of all designed files
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.delta_g), repeat(args.join), repeat(args.debug),
                              repeat(save), repeat(args.figures))
            # results, exceptions = SDUtils.mp_try_starmap(analyze_output_mp, zipped_args, mp_threads)
            results, exceptions = zip(*SDUtils.mp_starmap(analyze_output_mp, zipped_args, mp_threads))
            results = list(results)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = analyze_output_s(des_directory, delta_refine=args.delta_g, merge_residue_data=args.join,
                                                 debug=args.debug, save_trajectories=save, figures=args.figures)
                results.append(result)
                exceptions.append(error)

        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        if len(all_design_directories) >= 1:
            # Save Design DataFrame
            design_df = pd.DataFrame(results)
            out_path = os.path.join(args.directory, args.output)
            design_df.to_csv(out_path)
            logger.info('Analysis of all poses written to %s' % out_path)
            if save:
                logger.info('Analysis of all Trajectories and Residues written to %s'
                            % all_design_directories[0].all_scores)

    elif args.sub_module == 'merge':  # -d2 directory2, -f2 file2, -i increment
        if args.directory2 or args.file2:
            # Grab all poses (directories) to be processed from either directory name or file
            all_poses2, location2 = SDUtils.collect_directories(args.directory2, file=args.file2)
            assert all_poses2 != list(), logger.critical(
                'No %s directories found within \'%s\'! Please ensure correct location' % (PUtils.nano.title(),
                                                                                           location2))
            all_design_directories2 = SDUtils.set_up_directory_objects(all_poses2)
            logger.info('%d Poses found in \'%s\'' % (len(all_poses2), location2))

            directory_pairs, failures = pair_directories(all_design_directories2, all_design_directories)
            if failures:
                logger.warning('The following directories have no partner:\n%s' % '\n'.join(fail.path
                                                                                            for fail in failures))
            if args.multi_processing:
                # Calculate the number of threads to use depending on computer resources
                mp_threads = SDUtils.calculate_mp_threads(maximum=True)
                logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
                zipped_args = zip(all_design_directories, repeat(args.increment))
                results = SDUtils.mp_starmap(rename, zipped_args, mp_threads)
                results2 = SDUtils.mp_map(merge_directory_pair, directory_pairs, mp_threads)
            else:
                logger.info('Starting processing. If single process is taking awhile, use -m during submission')
                for des_directory in all_design_directories:
                    rename(des_directory, increment=args.increment)
                for directory_pair in directory_pairs:
                    merge_directory_pair(directory_pair)

    elif args.sub_module == 'rename_scores':
        rename = {'combo_profile_switch': 'limit_to_profile', 'favor_profile_switch': 'favor_frag_limit_to_profile'}
        for des_directory in all_design_directories:
            SDUtils.rename_decoy_protocols(des_directory, rename)

    elif args.sub_module == 'modify':
        if args.multi_processing:
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            # results, exceptions = zip(*SDUtils.mp_map(fix_files_mp, all_design_directories, threads=mp_threads))
            pose_map = pose_rmsd_mp(all_design_directories, threads=mp_threads)
        else:
            pose_map = pose_rmsd_s(all_design_directories)

        pose_cluster_map = cluster_poses(pose_map)
        pose_cluster_file = SDUtils.pickle_object(pose_cluster_map, PUtils.clustered_poses,
                                                  out_path=all_design_directories[0].protein_data)

        # for protein_pair in pose_map:
        #     if os.path.basename(protein_pair) == '4f47_4grd':
        #     logger.info('\n'.join(['%s\n%s' % (pose1, '\n'.join(['%s\t%f' %
        #                                                          (pose2, pose_map[protein_pair][pose1][pose2])
        #                                                          for pose2 in pose_map[protein_pair][pose1]]))
        #                            for pose1 in pose_map[protein_pair]]))
        errors = []
        for i, result in enumerate(results):
            if not result:
                errors.append(all_design_directories[i].path)

        logger.error('%d directories missing consensus metrics!' % len(errors))
        with open('missing_consensus_metrics', 'w') as f:
            f.write('\n'.join(errors))

    elif args.sub_module == 'status':  # -n --number, -s --stage
        if args.number_designs:
            logger.info('Checking for %d files. If no stage is specified, results will be incorrect for all but design '
                        'stage' % args.number_designs)
        if args.stage:
            status(all_design_directories, args.stage, number=args.number_designs)
        else:
            for stage in PUtils.stage_f:
                s = status(all_design_directories, stage, number=args.number_designs)
                if s:
                    logger.info('For \'%s\' stage, default settings should generate %d files'
                                % (stage, PUtils.stage_f[stage]['len']))

    elif args.sub_module == 'sequence':  # -c consensus, -d dataframe, -f filters, -n number, -p pose_design_file, -s selection_string, -w weights
        if args.pose_design_file:
            # Grab all poses (directories) to be processed from either directory name or file
            with open(args.pose_design_file) as csv_file:
                csv_lines = [line for line in reader(csv_file)]
            all_poses, pose_design_numbers = zip(*csv_lines)
            # pose_design_numbers = list(map(list, pose_design_numbers))
            # all_poses, pose_design_numbers = zip(*reader(args.pose_design_file))
            all_design_directories = SDUtils.set_up_directory_objects(all_poses, symmetry=args.design_string)
            results.append(zip(all_design_directories, pose_design_numbers))
            location = args.pose_design_file
        else:
            output = False
            if args.dataframe:  # Figure out poses from a dataframe, filters, and weights
                # TODO parameterize
                # if not args.filters:
                #     sys.exit('VY made this and I am going to put in here!')
                design_requirements = {'percent_int_area_polar': 0.2, 'buns_per_ang': 0.002}
                crystal_means1 = {'int_area_total': 570, 'shape_complementarity': 0.63, 'number_hbonds': 5}
                crystal_means2 = {'shape_complementarity': 0.63, 'number_hbonds': 5}  # 'int_area_total': 570,
                symmetry_requirements = crystal_means1
                filters = {}
                filters.update(design_requirements)
                filters.update(symmetry_requirements)
                if args.consensus:
                    consensus_weights1 = {'interaction_energy_complex': 0.5, 'percent_fragment': 0.5}
                    consensus_weights2 = {'interaction_energy_complex': 0.33, 'percent_fragment': 0.33,
                                          'shape_complementarity': 0.33}
                    filters = {'percent_int_area_polar': 0.2}
                    weights = consensus_weights2
                else:
                    weights1 = {'protocol_energy_distance_sum': 0.25, 'shape_complementarity': 0.25,
                                'observed_evolution': 0.25, 'int_composition_diff': 0.25}
                    # Used without the interface area filter
                    weights2 = {'protocol_energy_distance_sum': 0.20, 'shape_complementarity': 0.20,
                                'observed_evolution': 0.20, 'int_composition_diff': 0.20, 'int_area_total': 0.20}
                    weights = weights1
                # if not args.weights:
                #     sys.exit('VY made this and I am going to put in here!')

                selected_poses = Ams.filter_pose(args.dataframe, filters, weights,  # num_designs=args.number,
                                                 consensus=args.consensus, filter_file=PUtils.filter_and_sort)

                # Sort results according to clustered poses
                pose_cluster_map = SDUtils.unpickle(
                    os.path.join(all_design_directories[0].protein_data, '%s.pkl' % PUtils.clustered_poses))
                # {building_blocks: {design_string: cluster_representative}, ...}
                pose_clusters_found, final_poses = [], []
                # for des_dir in all_design_directories:
                for pose in selected_poses:
                    if pose_cluster_map[pose.split('-')[0]][pose] not in pose_clusters_found:
                        pose_clusters_found.append(pose_cluster_map[pose.split('-')[0]][pose])
                        final_poses.append(pose)

                if len(final_poses) > args.number:
                    final_poses = final_poses[:args.number]
                logger.info('Final poses after clustering:\n%s' % '\n'.join(pose for pose in final_poses))

                all_design_directories = SDUtils.set_up_directory_objects(final_poses, symmetry=args.design_string)
                if args.consensus:
                    results.append(zip(all_design_directories, repeat('consensus')))
                    output = True

            if output:
                pass
            elif args.multi_processing:
                # Calculate the number of threads to use depending on computer resources
                mp_threads = SDUtils.calculate_mp_threads(maximum=True)
                logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
                sequence_weights = {'buns_per_ang': 0.2, 'observed_evolution': 0.3, 'shape_complementarity': 0.25,
                                    'int_energy_res_summary_delta': 0.25}
                # sequence_weights = None  # Remove once calculated
                zipped_args = zip(all_design_directories, repeat(sequence_weights))
                results, exceptions = zip(*SDUtils.mp_starmap(Ams.select_sequences_mp, zipped_args, mp_threads))
                # results, exceptions = zip(*SDUtils.mp_map(Ams.select_sequences_mp, all_design_directories, mp_threads))
                # results - contains tuple of (DesignDirectory, design index) for each sequence
                # could simply return the design index then zip with the directory
            else:
                results, exceptions = zip(*list(Ams.select_sequences_s(des_directory, number=args.number)
                                                for des_directory in all_design_directories))
                # for des_directory in all_design_directories:
                #     result, error = select_sequences_s(des_directory, number=args.number)
                #     results.append(result)
                #     exceptions.append(error)

        results = list(results)
        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        if not args.selection_string:
            args.selection_string = '%s_' % os.path.basename(os.path.splitext(location)[0])
        else:
            args.selection_string += '_'
        outdir = os.path.join(os.path.dirname(location), '%sSelected_PDBs' % args.selection_string)
        outdir_traj = os.path.join(outdir, 'Trajectories')
        outdir_res = os.path.join(outdir, 'Residues')

        if not os.path.exists(outdir):
            os.makedirs(outdir)
            os.makedirs(outdir_traj)
            os.makedirs(outdir_res)

        # Create symbolic links to the output PDB's
        chains = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
        final_sequences, inserted_sequences = {}, {}
        for pose in results:
            pose_des_dirs, design = zip(*pose)
            for i, pose_des_dir in enumerate(pose_des_dirs):
                file = glob('%s*%s*' % (pose_des_dir.design_pdbs, design[i]))
                if file == list():  # If no file found, skip and add to exceptions
                    exceptions.append((pose_des_dir.path, 'No file found for %s*%s*' %
                                       (pose_des_dir.design_pdbs, design[i])))
                    continue
                try:
                    os.symlink(file[0], os.path.join(outdir, '%s_design_%s.pdb' % (str(pose_des_dir), design[i])))
                    os.symlink(pose_des_dir.trajectories, os.path.join(outdir_traj, os.path.basename(pose_des_dir.trajectories)))
                    os.symlink(pose_des_dir.residues, os.path.join(outdir_res, os.path.basename(pose_des_dir.residues)))
                except FileExistsError:
                    pass

                # Format sequences for expression
                # coming in as (chain: seq}
                design_pose = SDUtils.read_pdb(file[0])
                design_sequences = Ams.get_pdb_sequences(design_pose)
                pose_pdbs = os.path.basename(pose_des_dir.building_blocks).split('_')
                # need the original pose chain identity
                # source_pose = SDUtils.read_pdb(pose_des_dir.asu)  # Why can't I use design_sequences? localds quality!
                source_pose = SDUtils.read_pdb(pose_des_dir.source)  # Think this is the best, need to modify chains
                source_pose.reorder_chains()
                source_pose.atom_sequences = Ams.get_pdb_sequences(source_pose, source='atom')
                # if not source_pose.sequences:
                oligomers = [SDUtils.read_pdb(SDUtils.fetch_pdb(pdb), coordinates_only=False) for pdb in pose_pdbs]
                oligomer_chain_database_chain_map = {chain: oligomers[k].chain_id_list[0]
                                                     for k, chain in enumerate(source_pose.chain_id_list)}
                # print('SEQRES:\n%s' % '\n'.join(['%s - %s' % (chain, oligomer.sequences[chain])
                #                                  for oligomer in oligomers for chain in oligomer.chain_id_list]))
                source_seqres = {chain: oligomers[k].seqres_sequences[oligomers[k].chain_id_list[0]]
                                 for k, chain in enumerate(chains[:len(oligomers)])}

                # seqres_pose = PDB.PDB()
                # for oligomer, _chain in zip(oligomers, reversed(chains)):
                #     # print('Before', oligomer.chain_id_list)
                #     oligomer.rename_chain(oligomer.chain_id_list[0], _chain)
                #     # print('After', oligomer.chain_id_list)
                #     seqres_pose.read_atom_list(oligomer.chain(_chain))
                #     # print('In', seqres_pose.chain_id_list)
                # # print('Out', seqres_pose.chain_id_list)
                # seqres_pose.pose_numbering()  # Why is this necessary
                # seqres_pose.seqres_sequences = source_seqres  # Ams.get_pdb_sequences(seqres_pose,source='seqres')
                # print('Reorder', seqres_pose.chain_id_list)
                # Insert loops identified by comparison of SEQRES and ATOM

                # missing_termini_d = {chain: Ams.generate_mutations_from_seq(pdb_atom_seq[chain],
                #                                                         template_pdb.sequences[chain],
                #                                                         offset=True,
                #                                                         termini=True)
                #                      for chain in template_pdb.chain_id_list}
                # print('Source ATOM Sequences:\n%s' % '\n'.join(['%s - %s' % (chain, source_pose.atom_sequences[chain])
                #                                                 for chain in design_sequences]))
                # print('Source SEQRES Sequences:\n%s' % '\n'.join(['%s - %s' % (chain, source_seqres[chain])
                #                                                   for chain in source_seqres]))
                pose_offset_d = Ams.pdb_to_pose_num(source_seqres)
                # all_missing_residues_d = {chain: Ams.generate_mutations_from_seq(design_sequences[chain],
                #                                                                  seqres_pose.seqres_sequences[chain],
                #                                                                  offset=True, only_gaps=True)
                #                           for chain in design_sequences}

                # Find all gaps between the SEQRES and ATOM record
                all_missing_residues_d = {chain: Ams.generate_mutations_from_seq(source_pose.atom_sequences[chain],
                                                                                 source_seqres[chain],
                                                                                 offset=True, only_gaps=True)
                                          for chain in source_seqres}
                # pose_insert_offset_d = Ams.pdb_to_pose_num(all_missing_residues_d)

                # print('Pre-pose numbering:\n%s' %
                #       '\n'.join(['%s - %s' % (chain, ', '.join([str(res) for res in all_missing_residues_d[chain]]))
                #                  for chain in all_missing_residues_d]))

                # Modify residue indices to pose numbering
                all_missing_residues_d = {chain: {residue + pose_offset_d[chain]: all_missing_residues_d[chain][residue]
                                                  for residue in all_missing_residues_d[chain]}
                                          for chain in all_missing_residues_d}
                # Modify residue indices to include prior pose inserts pose numbering
                # all_missing_residues_d = {chain: {residue + pose_insert_offset_d[chain]: all_missing_residues_d[chain][residue]
                #                                   for residue in all_missing_residues_d[chain]}
                #                           for chain in all_missing_residues_d}
                # print('Post-pose numbering:\n%s' %
                #       '\n'.join(['%s - %s' % (chain, ', '.join([str(res) for res in all_missing_residues_d[chain]]))
                #                  for chain in all_missing_residues_d]))

                # print('Design Sequences:\n%s' % '\n'.join(['%s - %s' % (chain, design_sequences[chain])
                #                                            for chain in design_sequences]))
                # Insert residues into design PDB object
                for chain in all_missing_residues_d:
                    # design_pose.pose_numbering()  TODO for correct pdb_number considering insert_residues function
                    for residue in all_missing_residues_d[chain]:
                        design_pose.insert_residue(chain, residue, all_missing_residues_d[chain][residue]['from'])
                        # if chain == 'B':
                        #     print('%s\tLocation %d' % (design_pose.getStructureSequence(chain), residue))

                # Get modified sequence
                design_sequences_disordered = Ams.get_pdb_sequences(design_pose)
                # print('Disordered Insertions:\n%s' %
                #       '\n'.join(['%s - %s' % (chain, design_sequences_disordered[chain])
                #                  for chain in design_sequences_disordered]))

                # I need the source sequence as mutations to get the mutation index on the design sequence
                mutations = {chain: Ams.generate_mutations_from_seq(source_seqres[chain],
                                                                    design_sequences[chain], offset=True)
                             for chain in design_sequences}
                # print('Mutations:\n%s' %
                #       '\n'.join(['%s - %s' % (chain, mutations[chain]) for chain in mutations]))
                # Next I need the modified (residue inserted) design sequence to pull out the correct Met
                coding_offset = {chain: Ams.find_orf_offset(design_sequences_disordered[chain], mutations[chain])
                                 for chain in design_sequences_disordered}

                # print('Coding Offset:\n%s'
                #       % '\n'.join(['%s: %s' % (chain, coding_offset[chain]) for chain in coding_offset]))
                # Apply the coding sequence constraint to the design sequence
                pretag_sequences = {chain: design_sequences_disordered[chain][coding_offset[chain]:]
                                    for chain in coding_offset}
                # print('Pre-tag Sequences:\n%s' % '\n'.join([pretag_sequences[chain] for chain in pretag_sequences]))

                # for residue in all_missing_residues_d:
                #     if all_missing_residues_d[residue]['from'] == 'M':

                # for chain in gapped_residues_d:
                #     for residue in gapped_residues_d[chain]:

                tag_sequences = {}
                # Check if sequence already has tag and re-use
                for pdb, chain in zip(pose_pdbs, pretag_sequences):
                    if find_expression_tags(pretag_sequences[chain]) == dict():
                        # Look for compatible tags
                        tag_sequences[pdb] = \
                            find_all_matching_pdb_expression_tags(pdb, oligomer_chain_database_chain_map[chain])
                    else:
                        tag_sequences[pdb] = None

                # tag_sequences = {pdb: find_all_matching_pdb_expression_tags(pdb,
                #                                                             oligomer_chain_database_chain_map[chain])
                #                  for pdb, chain in zip(pose_pdbs, source_pose.chain_id_list)}

                for j, pdb in enumerate(tag_sequences):
                    if tag_sequences[pdb]:
                        seq = add_expression_tag(tag_sequences[pdb]['seq'], pretag_sequences[chains[j]])
                    else:
                        seq = pretag_sequences[chains[j]]
                    # If no MET start site, include one
                    if seq[0] != 'M':
                        seq = 'M%s' % seq

                    design_string = '%s_design_%s_%s' % (pose_des_dir, design[i], pdb)
                    final_sequences[design_string] = seq
                    # Find sequence additions compared to the design.
                    full_insertions = {residue: {'to': aa}
                                       for residue, aa in enumerate(final_sequences[design_string], 1)}
                    full_insertions.update(Ams.generate_mutations_from_seq(design_sequences[chains[j]],
                                                                           final_sequences[design_string],
                                                                           offset=True, blanks=True))

                    # Reduce to sequence only
                    inserted_sequences[design_string] = '%s\n%s' % (''.join([full_insertions[idx]['to']
                                                                             for idx in full_insertions]),
                                                                    final_sequences[design_string])

                # full_insertions = {pdb: Ams.generate_mutations_from_seq(design_sequences[chains[j]],
                #                                                         final_sequences['%s_design_%s_%s' %
                #                                             (pose_des_dir, design[i], pdb)], offset=True, blanks=True)
                #                    for j, pdb in enumerate(tag_sequences)}
                # for pdb in full_insertions:
                #     inserted_sequences['%s_design_%s_%s' % (pose_des_dir, design[i], pdb)] = '%s\n%s' % \
                #         (''.join([full_insertions[pdb][idx]['to'] for idx in full_insertions[pdb]]),
                #          final_sequences['%s_design_%s_%s' % (pose_des_dir, design[i], pdb)])

                # final_sequences[design] = {pdb: add_expression_tag(tag_sequences[pdb]['seq'],
                #                                                    design_sequences[chains[j]])
                #                            for j, pdb in enumerate(tag_sequences)}

        # Write output sequences to fasta file
        seq_comparison_file = Ams.write_fasta_file(inserted_sequences, '%sSelected_Sequences_Expression_Additions' %
                                                   args.selection_string, outpath=outdir)
        seq_file = Ams.write_fasta_file(final_sequences, '%sSelected_Sequences' % args.selection_string, outpath=outdir)

    # Report any program exceptions
    any_exceptions = [exception for exception in exceptions if exception]
    # any_exceptions = list(filter(bool, exceptions))
    # any_exceptions = list(set(exceptions))
    # if len(any_exceptions) > 1 or any_exceptions and any_exceptions[0]:
    if len(any_exceptions) > 1:
        logger.warning('\nThe following exceptions were thrown. Design for %d directories is inaccurate!!!\n' %
                       len(any_exceptions))
        all_exception_poses = []
        for exception in any_exceptions:
            # if exception:
            des_dir, exception_msg = exception
            try:
                logger.warning('%s: %s' % (des_dir.path, exception_msg))
            except AttributeError:
                logger.warning('%s: %s' % (des_dir, exception_msg))
# logger.warning('%s: %s' % exception)
            all_exception_poses.append(des_dir)
        print('\n')
        # except_file = os.path.join(args.location, 'EXCEPTIONS.log')
        except_file = SDUtils.write_list_to_file(all_exception_poses, name='EXCEPTIONS.log', location=args.directory)
        logger.info('All poses with exceptions written to file: %s' % except_file)

    print('\n')
