"""
Module for distribution of SymDesign commands. Includes pose initialization, distribution of Rosetta commands to
SLURM/PBS computational clusters, analysis of designed poses, and renaming of completed structures.

"""
import os
import argparse
import shutil
from json import loads, dumps
from glob import glob
from itertools import repeat
import pandas as pd
import SymDesignUtils as SDUtils
import PathUtils as PUtils
import CmdUtils as CUtils
from AnalyzeOutput import analyze_output_s, analyze_output_mp
from PoseProcessing import initialization_s, initialization_mp
from AnalyzeMutatedSequences import select_sequences_s, select_sequences_mp


def rename(des_dir, increment=PUtils.nstruct):
    """Rename the decoy numbers in a DesignDirectory by a specified increment

    Args:
        des_dir (DesignDirectory): A DesignDirectory object
    Keyword Args:
        increment=PUtils.nstruct (int): The number to increment by
    """
    for pdb in os.listdir(des_dir.design_pdbs):
        if os.path.splitext(pdb)[0][-1].isdigit():
            SDUtils.change_filename(os.path.join(des_dir.design_pdbs, pdb), increment=increment)
    SDUtils.modify_decoys(os.path.join(des_dir.scores, PUtils.scores_file), increment=increment)


def pair_directories(dirs2, dirs1):
    """Pair directories with the same pose name, returns source (dirs2) first, destination (dirs1) second
    Args:
        dirs2 (list): List of DesignDirectory objects
        dirs1 (list): List of DesignDirectory objects
    Returns:
        (list), (list): [(source, destination), ...], [directories missing a pair, ...]
    """
    success, pairs = [], []
    for dir1 in dirs1:
        for dir2 in dirs2:
            if str(dir1) == str(dir2):
                pairs.append((dir2, dir1))
                success.append(dir1)
                dirs2.remove(dir2)
                break

    return pairs, list(set(dirs1) - set(success))


def merge_directory_pair(pair):
    """Combine Rosetta design files of one pose with the files of a second pose

    Args:
        pair (tuple): source directory, destination directory
    """
    def merge_scores():
        with open(os.path.join(pair[1].scores, PUtils.scores_file), 'a') as f1:
            f1.write('\n')  # first ensure a new line at the end of first file
            with open(os.path.join(pair[0].scores, PUtils.scores_file), 'r') as f2:
                lines = [loads(line) for line in f2.readlines()]
            f1.write('\n'.join(dumps(line) for line in lines))

    def merge_designs():
        for pdb in os.listdir(pair[0].design_pdbs):
            shutil.copy(os.path.join(pair[0].design_pdbs, pdb), os.path.join(pair[1].design_pdbs, pdb))
    merge_scores()
    merge_designs()


def designs(des_dir, file_type):
    return os.path.join(des_dir.design_pdbs, file_type)


def status(design_directories, _stage, number=None):
    if not number:
        number = PUtils.stage_f[_stage]['len']
        if not number:
            return False
    complete, incomplete = [], []
    for des_dir in design_directories:
        files = glob(designs(des_dir, PUtils.stage_f[_stage]['path']))
        if number >= len(files):
            complete.append(des_dir.path)
        else:
            incomplete.append(des_dir.path)
    complete_path = os.path.join(args.directory, 'complete_' + _stage + '_pose_status')
    incomplete_path = os.path.join(args.directory, 'incomplete_' + _stage + '_pose_status')
    with open(complete_path, 'w') as f_com:
        for c in complete:
            f_com.write('%s\n' % c)
    with open(incomplete_path, 'w') as f_in:
        for n in incomplete:
            f_in.write('%s\n' % n)

    return True


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='\nControl all input/output of %s including:\n1. Pose initialization\n'
                                                 '2. Command distribution to computational nodes\n'
                                                 '3. Analysis of designs' % PUtils.program_name)
    parser.add_argument('-d', '--directory', type=str, help='Directory where %s poses are located. Default=CWD'
                                                            % PUtils.program_name, default=os.getcwd())
    parser.add_argument('-f', '--file', type=str, help='File with location(s) of %s poses' % PUtils.program_name,
                        default=None)
    parser.add_argument('-m', '--multi_processing', action='store_true',
                        help='Should job be run with multiprocessing?\nDefault=False')
    parser.add_argument('-b', '--debug', action='store_true', help='Debug all steps to standard out?\nDefault=False')
    parser.add_argument('-s', '--design_string', type=str, help='If pose names are specified by strings instead of '
                                                                'directories, which design directory string to prefix'
                                                                '\nDefault=None', default=None)

    subparsers = parser.add_subparsers(title='SubModules', dest='sub_module',
                                       description='These are the different modes that designs are processed',
                                       help='Chose one of the SubModules followed by SubModule specific flags')

    parser_pose = subparsers.add_parser('pose', help='Gather output from %s and format for input into Rosetta. '
                                                     'Sets up interface design constrained evolutionary profiles '
                                                     'of homologous sequences and by fragment profiles extracted '
                                                     'from the PDB' % PUtils.nano)
    parser_pose.add_argument('-i', '--fragment_database', type=str,
                             help='Database to match fragments for interface specific scoring matrices. One of %s'
                                  '\nDefault=%s' %
                                  (','.join(list(PUtils.frag_directory.keys())), list(PUtils.frag_directory.keys())[0]),
                             default=list(PUtils.frag_directory.keys())[0])
    # changed to string from pathname so that user can easily define
    parser_pose.add_argument('symmetry_group', type=int,
                             help='What type of symmetry group does your design belong too? One of 0-Point Group, '
                                  '2-Plane Group, or 3-Space Group')  # TODO remove from input, make automatic
    parser_pose.add_argument('-c', '--command_only', action='store_true',
                             help='Should commands be written but not executed?\nDefault=False')
    parser_pose.add_argument('-x', '--suspend', action='store_true',
                             help='Should Rosetta design trajectory be suspended?\nDefault=False')
    parser_pose.add_argument('-p', '--mpi', action='store_true',
                             help='Should job be set up for cluster submission?\nDefault=False')

    parser_dist = subparsers.add_parser('distribute',
                                        help='Distribute specific design step commands to computational resources. '
                                             'In distribution mode, the --file or --directory argument specifies which '
                                             'pose commands should be distributed.')
    parser_dist.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                             help='The stage of design to be prepared. One of %s' %
                                  ', '.join(list(v for v in PUtils.stage_f.keys())), required=True)
    parser_dist.add_argument('-y', '--success_file', help='The name/location of file containing successful commands\n'
                                                          'Default={--stage}_stage_pose_successes', default=None)
    parser_dist.add_argument('-n', '--failure_file', help='The name/location of file containing failed commands\n'
                                                          'Default={--stage}_stage_pose_failures', default=None)
    parser_dist.add_argument('-m', '--max_jobs', type=int, help='How many jobs to run at once?\nDefault=80',
                             default=80)

    parser_analysis = subparsers.add_parser('analysis', help='Run analysis on all poses specified')
    parser_analysis.add_argument('-o', '--output', type=str,
                                 help='Name to output comma delimitted files.\nDefault=%s' % PUtils.analysis_file,
                                 default=PUtils.analysis_file)
    parser_analysis.add_argument('-n', '--no_save', action='store_true',
                                 help='Don\'t save trajectory information.\nDefault=False')
    parser_analysis.add_argument('-f', '--figures', action='store_true',
                                 help='Create and save all pose figures?\nDefault=False')
    parser_analysis.add_argument('-j', '--join', action='store_true',
                                 help='Join Trajectory and Residue Dataframes?\nDefault=False')
    parser_analysis.add_argument('-g', '--delta_g', action='store_true',
                                 help='Compute deltaG versus Refine structure?\nDefault=False')

    parser_merge = subparsers.add_parser('merge', help='Merge all completed designs from location 2 (-f2/-d2) to '
                                                       'location 1(-f/-d). Includes renaming. Highly suggested you copy'
                                                       ' original data!!!')
    parser_merge.add_argument('-d2', '--directory2', type=str, help='Directory 2 where poses should be copied '
                                                                    'from and appended to location 1 poses',
                              default=None)
    parser_merge.add_argument('-f2', '--file2', type=str, help='File 2 where poses should be copied from and appended '
                                                               'to location 1 poses', default=None)
    parser_merge.add_argument('-i', '--increment', type=int, help='How many to increment each design by?\nDefault=%d'
                                                                  % PUtils.nstruct)

    parser_status = subparsers.add_parser('status', help='Get design status for selected designs')
    parser_status.add_argument('-n', '--number_designs', type=int, help='Number of trajectories per design',
                               default=None)
    parser_status.add_argument('-s', '--stage', choices=tuple(v for v in PUtils.stage_f.keys()),
                               help='The stage of design to check status of. One of %s'
                                    % ', '.join(list(v for v in PUtils.stage_f.keys())), default=None)

    parser_sequence = subparsers.add_parser('sequence', help='Generate protein sequences for selected designs')
    parser_sequence.add_argument('-s', '--selection_string', type=str, help='Identifier for sequence selection')
    parser_sequence.add_argument('-n', '--number', type=int, help='Number of top sequences to return per design',
                                 default=1)
    parser_rename_scores = subparsers.add_parser('rename_scores', help='Rename Protocol names according to dictionary')

    args = parser.parse_args()

    # Start logging output
    if args.debug:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=1)
        logger.debug('Debug mode. Verbose output')
    else:
        logger = SDUtils.start_log(name=os.path.basename(__file__), level=2)

    logger.info('Starting %s with options:\n\t%s' %
                (os.path.basename(__file__),
                 '\n\t'.join([str(arg) + ':' + str(getattr(args, arg)) for arg in vars(args)])))

    # Grab all poses (directories) to be processed from either directory name or file
    all_poses, location = SDUtils.collect_designs(args.directory, file=args.file)
    assert all_poses != list(), logger.critical('No %s directories found within \'%s\'! Please ensure correct location'
                                                % (PUtils.nano, location))

    all_design_directories = SDUtils.set_up_directory_objects(all_poses, symmetry=args.design_string)
    logger.info('%d Poses found in \'%s\'' % (len(all_poses), location))
    logger.info('All pose specific logs are located in corresponding directories, ex:\n%s' %
                os.path.join(all_design_directories[0].path, os.path.basename(all_design_directories[0].path) + '.log'))
    if not args.file:
        # Make single file with names of each directory
        args.file = os.path.join(args.directory, 'all_pose_directories')
        with open(args.file, 'w') as design_f:
            for pose in all_poses:
                design_f.write(pose + '\n')

    # Parse SubModule specific commands
    results, exceptions = [], []
    if args.sub_module == 'pose':
        if args.mpi:
            args.command_only = True
            extras = ' mpi %d' % CUtils.mpi
            logger.info('Setting job up for submission to MPI capable computer. Pose trajectories will run in parallel,'
                        ' %s at a time. This will speed up pose processing %f-fold.' %
                        (CUtils.mpi - 1, PUtils.nstruct / (CUtils.mpi - 1)))
        else:
            extras = ''
        if args.command_only:
            args.suspend = True
            logger.info(
                'Writing modelling commands out to file only, no modelling will occur until commands are executed')

        # Start pose processing and preparation for Rosetta
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(mpi=args.mpi, maximum=True, no_model=args.suspend)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.fragment_database), repeat(args.symmetry_group),
                              repeat(args.command_only), repeat(args.mpi), repeat(args.suspend),
                              repeat(args.debug))
            # results, exceptions = SDUtils.mp_try_starmap(initialization_mp, zipped_args, mp_threads)
            results, exceptions = zip(*SDUtils.mp_starmap(initialization_mp, zipped_args, mp_threads))
            results = list(results)
            exceptions = list(exceptions)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = initialization_s(des_directory, args.fragment_database, args.symmetry_group,
                                                 script=args.command_only, mpi=args.mpi, suspend=args.suspend,
                                                 debug=args.debug)
                results.append(result)
                exceptions.append(error)

        if args.command_only:
            all_commands = [[] for s in PUtils.stage_f]
            command_files = [[] for s in PUtils.stage_f]
            for des_directory in all_design_directories:
                for i, stage in enumerate(PUtils.stage_f):
                    all_commands[i].append(os.path.join(des_directory.path, stage + '.sh'))
            for i, stage in enumerate(PUtils.stage_f):
                if i > 3:  # No consensus
                    break
                command_files[i] = SDUtils.write_commands(all_commands[i], name=stage, loc=args.directory)
                logger.info('All \'%s\' commands were written to \'%s\'' % (stage, command_files[i]))
            logger.info('\nTo process all commands in correct order, execute:\ncd %s\n%s\n%s\n%s' %
                        (args.directory, 'python ' + __file__ + ' distribute -s refine',
                         'python ' + __file__ + ' distribute -s design',
                         'python ' + __file__ + ' distribute -s metrics'))

    elif args.sub_module == 'distribute':  # -s --stage, -y --success_file, -n --failure_file, -m --max_jobs
        # Create success and failures files
        if not args.success_file:
            args.success_file = os.path.join(args.directory, args.stage + '_stage_pose_success')
        # with open(args.success_file, 'w') as f:
        #     dummy = True
        if not args.failure_file:
            args.failure_file = os.path.join(args.directory, args.stage + '_stage_pose_failures')
        # with open(args.failure_file, 'w') as f:
        #     dummy = True
        logger.info('Successful poses will be listed in \'%s\'\nFailed poses will be listed in \'%s\''
                    % (args.success_file, args.failure_file))

        # Grab sbatch template and stage cpu divisor to facilitate array set up and command distribution
        with open(PUtils.sbatch_templates[args.stage]) as template_f:
            template_sbatch = template_f.readlines()

        # Make sbatch file from template, array details, and command distribution script
        filename = os.path.join(args.directory, args.stage + PUtils.sbatch)
        output = os.path.join(args.directory, 'output')
        if not os.path.exists(output):
            os.mkdir(output)

        command_divisor = CUtils.process_scale[args.stage]
        with open(filename, 'w') as new_f:
            for template_line in template_sbatch:
                new_f.write(template_line)
            out = 'output=%s/%s' % (output, '%A_%a.out')
            new_f.write(PUtils.sb_flag + out + '\n')
            array = 'array=1-%d%%%d' % (int(len(all_poses) / command_divisor + 0.5), args.max_jobs)
            new_f.write(PUtils.sb_flag + array + '\n')
            new_f.write('\npython %s --stage %s --success_file %s --failure_file %s --command_file %s\n' %
                        (PUtils.cmd_dist, args.stage, args.success_file, args.failure_file, args.file))

        logger.info('To distribute commands enter the following:\ncd %s\nsbatch %s'
                    % (args.directory, os.path.basename(filename)))

    elif args.sub_module == 'analysis':  # -f --figures, -n --no_save, -j --join, -g --delta_g
        save = True
        if args.no_save:
            save = False
        # Start pose analysis of all designed files
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.delta_g), repeat(args.join), repeat(args.debug),
                              repeat(save), repeat(args.figures))
            # results, exceptions = SDUtils.mp_try_starmap(analyze_output_mp, zipped_args, mp_threads)
            results, exceptions = zip(*SDUtils.mp_starmap(analyze_output_mp, zipped_args, mp_threads))
            results = list(results)
            exceptions = list(exceptions)
        else:
            logger.info('Starting processing. If single process is taking awhile, use -m during submission')
            for des_directory in all_design_directories:
                result, error = analyze_output_s(des_directory, delta_refine=args.delta_g, merge_residue_data=args.join,
                                                 debug=args.debug, save_trajectories=save, figures=args.figures)
                results.append(result)
                exceptions.append(error)

        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        if len(all_design_directories) >= 1:
            # Save Design DataFrame
            design_df = pd.DataFrame(results)
            out_path = os.path.join(args.directory, args.output)
            design_df.to_csv(out_path)
            logger.info('Analysis of all poses written to %s' % out_path)
            if save:
                logger.info('Analysis of all Trajectories and Residues written to %s'
                            % all_design_directories[0].all_scores)

    elif args.sub_module == 'merge':  # -d2 directory2, -f2 file2, -i increment
        if args.directory2 or args.file2:
            # Grab all poses (directories) to be processed from either directory name or file
            all_poses2, location2 = SDUtils.collect_designs(args.directory2, file=args.file2)
            assert all_poses2 != list(), logger.critical(
                'No %s directories found within \'%s\'! Please ensure correct location' % (PUtils.nano, location2))
            all_design_directories2 = SDUtils.set_up_directory_objects(all_poses2)
            logger.info('%d Poses found in \'%s\'' % (len(all_poses2), location2))

            directory_pairs, failures = pair_directories(all_design_directories2, all_design_directories)
            if failures:
                logger.warning('The following directories have no partner:\n%s' % '\n'.join(fail.path
                                                                                            for fail in failures))
            if args.multi_processing:
                # Calculate the number of threads to use depending on computer resources
                mp_threads = SDUtils.calculate_mp_threads(maximum=True)
                logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
                zipped_args = zip(all_design_directories, repeat(args.increment))
                results = SDUtils.mp_starmap(rename, zipped_args, mp_threads)
                results2 = SDUtils.mp_map(merge_directory_pair, directory_pairs, mp_threads)
            else:
                logger.info('Starting processing. If single process is taking awhile, use -m during submission')
                for des_directory in all_design_directories:
                    rename(des_directory, increment=args.increment)
                for directory_pair in directory_pairs:
                    merge_directory_pair(directory_pair)

    elif args.sub_module == 'rename_scores':
        rename = {'combo_profile_switch': 'limit_to_profile', 'favor_profile_switch': 'favor_frag_limit_to_profile'}
        for des_directory in all_design_directories:
            SDUtils.rename_decoy_protocols(des_directory, rename)

    elif args.sub_module == 'status':  # -n --number, -s --stage
        if args.number_designs:
            logger.info('Checking for %d files. If no stage is specified, results will be incorrect for all but design '
                        'stage' % args.number_designs)
        if args.stage:
            status(all_design_directories, args.stage, number=args.number_designs)
        else:
            for stage in PUtils.stage_f:
                s = status(all_design_directories, stage, number=args.number_designs)
                if s:
                    logger.info('For \'%s\' stage, default settings should generate %d files'
                                % (stage, PUtils.stage_f[stage]['len']))

    elif args.sub_module == 'sequence':
        if args.multi_processing:
            # Calculate the number of threads to use depending on computer resources
            mp_threads = SDUtils.calculate_mp_threads(maximum=True)
            logger.info('Starting multiprocessing using %s threads' % str(mp_threads))
            zipped_args = zip(all_design_directories, repeat(args.number))
            results, exceptions = zip(*SDUtils.mp_starmap(select_sequences_mp, zipped_args, mp_threads))  # , exceptions
        else:
            for des_directory in all_design_directories:
                result, error = select_sequences_s(des_directory, number=args.number)
                results.append(result)
                exceptions.append(error)
        outfile = os.path.join(os.path.dirname(location), 'Selected_Sequences')  # % args.selection_string)  # %s_
        # outfile = os.path.join(os.path.dirname(location), 'Selected_Sequences' % args.selection_string)  # %s_
        outdir = os.path.join(os.path.dirname(location), 'Selected_PDBs')  # % args.selection_string)  # %s_
        if not os.path.exists(outdir):
            os.makedirs(outdir)

        results = list(results)
        exception = list(exceptions)
        failures = [index for index, exception in enumerate(exceptions) if exception]
        for index in reversed(failures):
            del results[index]

        with open(outfile, 'w') as out_f:
            for des_dir_idx, result in enumerate(results):
                out_f.write('%s\n' % '\n'.join(os.path.join(all_design_directories[des_dir_idx].design_pdbs, seq)
                                               for seq in result))
                for seq in result:
                    file = glob('%s*%s*' % (all_design_directories[des_dir_idx].design_pdbs, seq))
                    print(file)  # g
                    os.symlink(file[0], os.path.join(outdir, '%s_design_%s' %
                                                     (str(all_design_directories[des_dir_idx]), seq)))

    any_exceptions = list(set(exceptions))
    if len(any_exceptions) > 1 or any_exceptions and any_exceptions[0]:
        logger.warning('\nThe following exceptions were thrown. Design for these directories is inaccurate!!!\n')
        for exception in exceptions:
            if exception:
                logger.warning('%s: %s' % exception)

    print('\n')
